{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"An advanced stream processing engine that understands streaming SQL queries to capture, analyze, and process streaming data, and allows us to integrate and act on event streams in real-time. WSO2 Streaming Integrator allows you to connect any data source to any destination with its 60+ prebuilt, production-grade connectors. It comes with a web-based IDE for designing, developing, testing, and deploying stream processing applications with a graphical drag-and-drop experience or by writing streaming SQL queries.                  Quick Start Guide <p>                     Let's get started with WSO2 Streaming Integrator by running a simple integration use case in your local environment.                 </p> What can WSO2 Streaming Integrator do? Routing and Transformation <p>                     Supports content-based routing, header-based routing, and policy-based routing. Transforms the message to different formats.                 </p> Service Orchestration <p>                     Has the ability to present multiple fine-grained services using a single coarse-grained service.                 </p> Asynchronous Messaging <p>                     Messages can be queued and do not require an immediate response to continue processing.                 </p> SaaS Integration <p>                     Connectors are available across various categories such as payments, CRM, ERP, social networks, and legacy systems.                 </p> Microservices Integration <p>                     Lightweight runtime for container-based deployments. Native integration with container-management platforms.                 </p> Data Integration <p>                     Decouples the data from the datasource layer and exposes them as data services.                 </p> File Integration <p>                     Supports processing of files with large amounts of data.                 </p> Enterprise Integration Patterns <p>                     Support for all enterprise integration patterns (EIPs) and common enterprise messaging scenarios.                 </p> Periodic Execution of Integration Processes <p>                     Execute an integration process at a specified time.                 </p>"},{"location":"concepts/","title":"Concepts in Streaming Integrator","text":""},{"location":"page-not-found/","title":"","text":"<p>Try one of the navigation links above or use the search engine in the top right corner.</p>"},{"location":"admin/adding-third-party-non-osgi-libraries/","title":"Adding Third Party Non OSGi Libraries","text":"<p>The Streaming Integrator is OSGi-based. Therefore, when you integrate third party products such as Oracle with the Streaming Integrator, you need to check whether the libraries you need to add to the Streaming Integrator are OSGi-based. If they are not, you need to convert them to OSGi bundles before adding them to the <code>&lt;SI_HOME&gt;/lib</code> directory.</p> <p>To convert jar files to OSGi bundles, follow the procedure given below:</p> <ol> <li> <p>Download the non-OSGi jar for the required third party product, and save it in a preferred directory in your machine.</p> </li> <li> <p>In your CLI, navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory. Then issue the following command.     <code>./jartobundle.sh &lt;PATH_TO_NON-OSGi_JAR&gt; ../lib</code></p> <p>This\u00a0generates the converted file in the <code>&lt;SI_HOME&gt;/lib</code> directory.</p> </li> <li> <p>Restart the WSO2 SI server.</p> </li> </ol>"},{"location":"admin/configuring-Cluster-Coordination/","title":"Configuring Cluster Coordination","text":"<p>Multiple WSO2 SI nodes can be configured to work together by configuring a\u00a0cluster coordination strategy that is used\u00a0in various deployments such as the Minimum High Available(HA) Deployment and Scalable High Available(HA) Deployment . At present, cluster coordination is supported via an RDBMS instance using and RDBMS coordination strategy. Support for\u00a0cluster coordination via a Zookeeper instance will be supported in the near future.</p> <p>At any given time, there is a leader in an SI cluster that is arbitrarily selected among the members of the cluster. The RDBMS coordination strategy that is used for cluster coordination works on the concept of heartbeats where the members of the cluster periodically send heartbeat signals via the datasource to the leader of the cluster. If the leader node does not detect a pre configured consecutive number of heartbeats from a specific node, the relevant node is removed from the cluster. Similarly, if the leader node fails to update its heartbeat, the cluster re-elects a new leader.</p>"},{"location":"admin/configuring-Cluster-Coordination/#prerequisites","title":"Prerequisites","text":"<p>In order to configure a cluster, the following prerequisites must be completed:</p> <ul> <li>A minimum of two binary packs of WSO2 SI must be available.</li> <li>A working RDBMS instance must be available to be shared among the     nodes of the cluster.</li> </ul>"},{"location":"admin/configuring-Cluster-Coordination/#configuring-the-cluster-with-the-rdbms-coordination-strategy","title":"Configuring the Cluster with the RDBMS coordination strategy","text":"<p>To configure a cluster for several nodes, the cluster.config section of the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> should be configured for all the nodes as follows:</p> Parameter Purpose Sample Values <code>enabled</code> Set this value to <code>true</code> to enable cluster coordination for the node. <code>true/false</code> <code>groupId</code> The group ID is used to identify the cluster to which the node belongs. Nodes that belong to the same cluster must be configured with the same group ID. <code>group-1</code> <code>coordinationStrategyClass</code> The clustering class to be used. <code>org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy</code> <code>strategyConfig &gt; datasource</code> The shared datasource to be used in the cluster. The datasource specified must be properly configured in the <code>deployment.yaml</code> file. For detailed instructions to configure a datasource, see Configuring Datasources . <code>WSO2_CARBON_DB</code> <code>strategyConfig &gt; heartbeatInterval</code> This value defines the time interval in milliseconds between heartbeat pulses sent by nodes to indicate that they are still alive within the cluster. <code>1000</code> <code>strategyConfig &gt; heartbeatMaxRetry</code> The number of times the heartbeat pulse can be unavailable until a node is identified as unresponsive. If a node fails to send its heartbeat pulse to the leader of the cluster after a number of retries equal to the number specified here, that node is removed from the cluster. <code>2</code> <code>strategyConfig &gt; eventPollingInterval</code> The time interval in millseconds at which a node listens to identify the changes happening within the cluster. The changes may include a new node joining the cluster, a node being removed from the cluster and the coordinator changed event. <code>1000</code> <p>Following is a sample segment of the configurations needed for RDBMS coordination in the deployment.yaml</p> <p>Sample deployment.yaml segment</p> <pre><code>    cluster.config:\nenabled: true\ngroupId:  &lt;GROUP ID&gt;\ncoordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy\nstrategyConfig:\ndatasource: &lt;DATASOURCE NAME&gt;\nheartbeatInterval: 5000\nheartbeatMaxRetry: 5\neventPollingInterval: 5000\n</code></pre>"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/","title":"Configuring Database and File System State Persistence","text":"<p>This section explains how to prevent the loss of data that can result from a system failure by persisting the state of WSO2 SI periodically either into a database system or into the file system.</p>"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#prerequisites","title":"Prerequisites","text":"<p>Before configuring RDBMS database persistence, the following prerequisites must be completed.</p> <ul> <li>One or more Siddhi Applications must be running in the WSO2 SI     server.</li> <li>A working RDBMS instance that can be used for data persistence must     exist.</li> <li> <p>The requirements of the datasource must be already defined.</p> </li> <li> <p>Database persistence involves updating the databases connected to     WSO2 Streaming Integrator with the latest information relating to the     events that are being processed by WSO2 SI at a given time.</p> </li> </ul>"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-database-system-persistence","title":"Configuring database system persistence","text":"<p>The supported databases are H2, MySQL, Postgres, MSSQL and Oracle. The relevant jdbc driver jar should be downloaded and added to the <code>&lt;SI_HOME&gt;/lib</code> directory to prior to using database system persistence.</p> <p>To configure periodic data persistence, update the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file under <code>state.persistence</code> as follows:</p> Parameter Purpose Required Value <code>enabled</code> This enables data persistence. <code>             true            </code> <code>intervalInMin</code> The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 <code>revisionsToKeep</code> The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 <code>persistenceStore</code> The persistence store . <code>org.wso2.carbon.streaming.integrator.core.persistence.DBPersistenceStore</code> <code>             config &gt; datasource            </code> The datasource to be used in persisting the state. The provided datasource should be properly defined in the deployment.yaml. For detailed instructions of how to configure a datasource, see Configuring Datasources . <pre><code>WSO2_PERSISTENCE_DB (Datasource with this name should be defined in wso2.datasources)</code></pre> <code>             config &gt; table            </code> The table that should be created and used for the persisting of the state. <pre><code>PERSISTENCE_TABLE</code></pre> <p>The following is a sample segment of the required configurations in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file\u00a0to configure file system persistence.</p> <p>Sample deployment.yaml segment</p> <pre><code>    state.persistence:\n      enabled: true\n      intervalInMin: 1\n      revisionsToKeep: 3\n      persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.DBPersistenceStore\n      config:\n        datasource: &lt;DATASOURCE NAME&gt;   # A datasource with this name should be defined in wso2.datasources namespace\n        table: &lt;TABLE NAME&gt;\n</code></pre>"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-file-system-persistence","title":"Configuring file system persistence","text":"<p>This section explains how to persist the states of Siddhi applications during a required time interval in the file system in order to maintain back-ups. To configure state persistence,\u00a0update the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file under <code>state.persistence</code> as follows:</p> Parameter Purpose Required Value <code>             enabled            </code> This enables data persistence. <code>             true            </code> <code>             intervalInMin            </code> The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted <code>             1            </code> <code>             revisionsToKeep            </code> The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. <code>             3            </code> <code>             persistenceStore                         </code> The persistence store. <pre><code>org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore</code></pre> <code>config &gt; location</code> A fully qualified folder location to where the revision files should be persisted. <pre><code>siddhi-app-persistence</code></pre> <p>The following is a sample segment of the required configurations in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file\u00a0to configure file system persistence.</p> <p>Sample deployment.yaml segment</p> <pre><code>    state.persistence:\nenabled: true\nintervalInMin: 1\nrevisionsToKeep: 2\npersistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore\nconfig:\nlocation: siddhi-app-persistence\n</code></pre>"},{"location":"admin/creating-business-rules-templates/","title":"Working with Business Rules","text":"<p>In streaming integration, there are common use cases for analyzing statistics that involve operations such as calculating the average, minimum, maximum etc., for different endpoints. The Business Rules Manager allows you to define templates and generate business rules from them for different scenarios with common requirements.</p>"},{"location":"admin/creating-business-rules-templates/#creating-business-rules","title":"Creating Business Rules","text":"<p>This section explains how to create a business rule. A business rule can be created from a template or from scratch.</p>"},{"location":"admin/creating-business-rules-templates/#creating-business-rules-from-a-template","title":"Creating Business Rules from a Template","text":"<p>Creating business rules from an existing template allows you to use sources, sinks and filters that have been already defined, and assign variable values to process events.</p> <p>Before you begin:</p> <ul> <li>The business rule template must be already configured in the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file. For detailed instructions, see Business Rules Templates.</li> <li>If you want to deploy the business rule after creating it, you need to start the SI server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory and issuing one of the following commands:<ul> <li>On Windows: <code>server.bat --run</code></li> <li>On Linux/Mac OS: \u00a0<code>./server.sh</code></li> </ul> </li> </ul> <p>To create a business rule from a template, follow the procedure below:</p> <ol> <li> <p>Navigate to the <code>&lt;SI_TOOLING_HOME&gt;</code> directory from the terminal and start the Streaming Integrator Tooling by issuing one of the following commands:</p> <ul> <li>On Windows: <code>tooling.bat --run</code></li> <li>On Linux/Mac OS: \u00a0<code>./tooling.sh</code></li> </ul> </li> <li> <p>Access the Business Rule Manager via the URL that appears in the terminal for Business Rules in the <code>https://&lt;SI_TOOLING_HOST&gt;:&lt;HTTPS_PORT&gt;/business-rules</code> format.</p> <p>Tip</p> <p>The default URL is <code>https://0.0.0.0:9743/business-rules</code>. If required, you can change the host name (i.e., <code>0.0.0.0</code>) or the web UI application name (i.e., <code>business-rules</code>). For instructions, see Changing the Host Name and Context Path of SI Tooling.</p> <p>This opens the following:</p> <p></p> </li> <li> <p>Click Create to open the following page.</p> <p></p> </li> <li> <p>Then click From Template to open the Select a Template Group page, where the available templates are displayed.</p> </li> <li> <p>Click on the template group that contains the required template to create a business rule from it. In this example, the business rule is created based on a template in the <code>Sweet Factory</code> template group that is packed with the Streaming Integrator by default. Therefore, click Sweet Factory to open this template group.</p> <p></p> </li> <li> <p>In the template group, expand the Rule Template list as shown below, and click on the required template. For this example, click Identify Continuous Production Decrease.</p> <p></p> </li> <li> <p>If you want to change the rule template from which you want to create the business rule, select the required value for the Rule Template field.</p> </li> <li> <p>Enter a name for the business rule in the Business Rule Name field.</p> </li> <li> <p>Enter values for the rest of the fields following the instructions in the UI.</p> <p>Info</p> <p>The fields displayed for the business rule differ based on the selected template.</p> </li> <li> <p>If you want to save the business rule and deploy it later, click Save. If you want to deploy the business rule immediately, click Save and Deploy.</p> </li> </ol>"},{"location":"admin/creating-business-rules-templates/#creating-a-business-rule-from-scratch","title":"Creating a business rule from scratch","text":"<p>Creating a business rule from scratch allows you to define the filter logic for the rule at the time of creating, instead of using the filter logic that has been already defined in a template. However, you can select the required source and sink configurations from an existing template.</p> <p>Before you begin:</p> <p>If you want to deploy the business rule after creating it, you need to start the SI server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory and issuing one of the following commands:     - On Windows: <code>server.bat --run</code>     - On Linux/Mac OS: \u00a0<code>./server.sh</code></p> <p>To create a business rule from scratch, follow the procedure below:</p> <ol> <li> <p>Navigate to the <code>&lt;SI_TOOLING_HOME&gt;</code> directory from the terminal and start the Streaming Integrator Tooling by issuing one of the following commands:</p> <ul> <li>On Windows: <code>tooling.bat --run</code></li> <li>On Linux/Mac OS: \u00a0<code>./tooling.sh</code></li> </ul> </li> <li> <p>Access the Business Rule Manager via one of the following URLs.</p> Protocol URL Format Example HTTP <code>http://&lt;SI_TOOLING_HOST&gt;:&lt;HTTP_PORT&gt;/business-rules</code> <code>http://0.0.0.0:9090/business-rules</code> HTTPS <code>https://&lt;SI_TOOLING_HOST&gt;:&lt;HTTPS_PORT&gt;/business-rules</code> <code>https://0.0.0.0:9443/business-rules</code> <p>Tip<p>The URLs given above are the defaul URLs. If required, you can change the host name (i.e., <code>0.0.0.0</code>) or the web UI application name (i.e., <code>business-rules</code>). For instructions, see Changing the Host Name and Context Path of SI Tooling.</p> </p> <p>This opens the following:</p> <p></p> </li> <li> <p>Click Create to open the following page, and then click From Scratch.</p> <p></p> <p>This opens the Select a Template Group page where the available template groups are displayed as shown in the example below.</p> <p></p> </li> <li> <p>Click on the template group from which you want to select the required sources and sinks for your business rule. For this example,\u00a0click Stock Exchange to open that template group as shown below.</p> <p></p> </li> <li> <p>Click Input to expand the Input section. Then select the rule template from which the source and input configurations for the business rule must be selected.</p> <p></p> <p>This displays the list of available sources and the exposed attributes of the selected template as shown below.</p> <p></p> </li> <li> <p>Click Filters to expand the Filters section, and click + to add a new filter. A table is displayed as shown below.</p> <p></p> </li> <li> <p>To define a filter, follow the steps below:</p> <ol> <li>In the Attribute field, select the attribute based on which you want to define the filter condition.</li> <li>In the Operator field, select an operator.</li> <li>In the Value/Attribute field, enter the value or another attribute based on which, the Operator is applied to the Attribute.</li> </ol> <p>e.g., If you want to filter events where the <code>price</code> is less than 100 (which is a value), select values for the fields as follows:</p> Field Value Attribute <code>price</code> Operator <code>&lt;</code> Value/Attribute (Value) <code>100</code> <p>If you want to filter events where the <code>price</code> is equal to the <code>volume</code> (which is another attribute), select values for the fields as follows:</p> Field Value Attribute <code>price</code> Operator <code>==</code> Value/Attribute (Attribute) <code>volume</code> <p>Once you have defined two or more filters, enter the rule logic in the Rule Logic field using <code>OR</code>, <code>AND</code>, and <code>NOT</code> conditions. The examples of how you can use these keywords are explained in the table below.</p> Keyword Example <code>OR</code> <code>1 OR 2</code> returns events that match either filter 1 or 2. <code>AND</code> <code>1 AND 2</code> returns events that match both filters 1 and 2. <code>NOT</code> <code>NOT 1</code> returns events that do not match filter 1. </li> <li> <p>Click Output to expand the Output section. Then select the rule template from which the sink and output configurations for the business rule must be selected.</p> <p></p> <p>This displays the section for mapping configurations as shown in the example below.</p> <p></p> </li> <li> <p>Select the relevant attribute names for the Input column. When publishing the events to which the rule is applied via the selected predefined sink, each input event you select is published with the corresponding name in the Output column.</p> <p>Info</p> <p>The output mappings displayed differ based on the output rule template you select.</p> </li> <li> <p>If you want to save the rule and deploy it later, click Save. If you want to deploy the rule immediately, click Save and Deploy.</p> </li> </ol>"},{"location":"admin/creating-business-rules-templates/#managing-business-rules","title":"Managing Business Rules","text":"<p>Once you have created one or more business rules, you can manage them by viewing, editing, deploying, undeploying, and deleting them as required.</p>"},{"location":"admin/creating-business-rules-templates/#viewing-business-rules","title":"Viewing business rules","text":"<p>Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below.</p> <p></p> <p>To view a business rule, click the icon for viewing (marked in the above image) for the relevant row. This opens the rule as shown in the example below.</p> <p></p>"},{"location":"admin/creating-business-rules-templates/#editing-business-rules","title":"Editing business rules","text":"<p>Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below.</p> <p></p> <p>To edit a business rule, click the icon for editing (marked in the above image) for the relevant row. This opens the rule as shown in the example below.</p> <p></p> <p>Modify values for the parameters displayed as required and click Save.</p>"},{"location":"admin/creating-business-rules-templates/#deploying-business-rules","title":"Deploying business rules","text":"<p>Before you begin:</p> <p>Start the Streaming Integrator server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory from the CLI, and issuing one of the following commands: - On Windows: <code>server.bat --run</code> - On Linux/Mac OS: \u00a0<code>./server.sh</code></p> <p>To deploy a business rule that you have previously saved, click the icon for deploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully deployed. </p>"},{"location":"admin/creating-business-rules-templates/#undeploying-business-rules","title":"Undeploying business rules","text":"<p>To undeploy a business rule, click the icon for undeploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully undeployed.</p> <p></p>"},{"location":"admin/creating-business-rules-templates/#viewing-deployment-information","title":"Viewing deployment information","text":"<p>If you want to view information relating to the deployment of a business rule,\u00a0click the icon for viewing deployment information (marked in the image below) for the relevant row.</p> <p></p> <p>As a result, the deployment information including the host and port of the nodes in which the rule is deployed and the deployment status are displayed as shown in the image below. </p> <p>Possible deployment statuses are as follows:</p> <ul> <li> <p>Saved: The business rule is created, but not yet deployed in any Streaming Integrator node.</p> </li> <li> <p>Deployed: The business rule is created and deployed in all the required nodes in the Streaming Integrator cluster.</p> </li> <li> <p>Partially Deployed: The business rule is created and deployed only in some of the required nodes in the Streaming Integrator cluster.</p> </li> <li> <p>Partially Undeployed: The business rule has been previously deployed, and then un-deployed only in some of the nodes in the Streaming Integrator cluster.</p> </li> </ul> <p>Info</p> <p>Required nodes are configured with respect to a rule template. For detailed instructions, see Deploying business rules in SI server.</p>"},{"location":"admin/creating-business-rules-templates/#deleting-business-rules","title":"Deleting business rules","text":"<p>To delete a business rule, click the icon for deleting (marked in the image below) for the relevant row. A message appears to confirm whether you want to proceed with the deletion. Click Delete in the message. As a result, another message appears to inform you that the rule is successfully deleted.</p> <p></p>"},{"location":"admin/creating-business-rules-templates/#creating-a-business-rules-template","title":"Creating a Business Rules Template","text":"<p>To create a business template using the Business Rules Template editor, follow the procedure below:</p> <ol> <li> <p>If you have not already started the Streaming Integrator tooling, navigate to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory from the terminal and start the Streaming Integrator Tooling as mentioned below.</p> <ul> <li>On Windows: <code>tooling.bat --run</code></li> <li>On Linux/Mac OS: <code>./ tooling.sh</code></li> </ul> </li> <li> <p>Access the Business Rules Template Editor via the URL that appears for it in the start up logs as shown in the example below.</p> <p></p> <p>Info</p> <p>The default URL is <code>http://localhost:9390/template-editor</code>. If required, you can change the host name (i.e., <code>localhost</code>) or the web UI application name (i.e., <code>template-editor</code>). For instructions, see Changing the Host Name and Context Path of SI Tooling.</p> </li> <li> <p>The Template Editor opens as shown below. There are two views from which you can interact and create a template group. Design view allows you to visualize a template group and interact with it. Code view allows you to interact with a template group by typing content. (For more information about template group structure, see     Business Rules Templates.)</p> <p>Warning</p> <p>Do not template sensitive information such as passwords in a Siddhi application or expose them directly in a Siddhi application. For detailed instructions to protect sensitive data by obfuscating them, see Protecting Sensitive Data via the Secure Vault.</p> </li> </ol> <p></p> <p>You can create a template group using the design view or the code view as explained in the following sections.</p>"},{"location":"admin/creating-business-rules-templates/#create-from-design-view","title":"Create from Design View","text":"<p>To create a business rules template group from the design view, follow the procedure below:</p> <ol> <li> <p>Enter a UUID (Universally Unique Identifier), name and a description for the template group as follows.</p> Field Name UUID sweet-factory Name Sweet Factory Description Analyzes Sweet Factory scenarios <p></p> </li> <li> <p>Expand the first rule template that exists by default, and enter the following details.</p> Field Name Value UUID identifying-continuous-production-decrease Name Identify Continuous Production Decrease Description Alert factory managers if the rate of production continuously decreases for a specified time period Type Template Instance Count One <p></p> </li> <li> <p>To include a Siddhi application template, expand the first template that is displayed by default, and enter the following Siddhi application template.</p> <p></p> <pre><code>    @App:name('SweetFactory-TrendAnalysis')\n\n@source(type='http', @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double, factoryId int);\n\n@sink(type='log', @map(type='text', @payload(\"\"\"\n    Hi ${username},\n    Production at Factory {{ factoryId }} has gone\n    from {{ initalamout }} to {{ finalAmount }} in ${timeInterval} seconds!\"\"\")))\ndefine stream ContinousProdReductionStream (factoryId int, initaltime long, finalTime long, initalamout double, finalAmount double);\n\nfrom SweetProductionStream#window.timeBatch(${timeInterval} sec)\nselect factoryId, sum(amount) as amount, currentTimeMillis() as ts\ninsert into ProdRateStream;\n\npartition with ( factoryId of ProdRateStream )\nbegin\nfrom every e1=ProdRateStream,\ne2=ProdRateStream[ts - e1.ts &lt;= ${timeRange} and e1.amount &gt; amount ]*,\ne3=ProdRateStream[ts - e1.ts &gt;= ${timeRange} and e1.amount &gt; amount ]\nselect e1.factoryId, e1.ts as initaltime, e3.ts as finalTime, e1.amount as initalamout, e3.amount as finalAmount\ninsert into ContinousProdReductionStream;\nend;\n</code></pre> </li> <li> <p>To add variable attributes to the script, click Add Variables.</p> <p>Info</p> <p>A script is a javascript that can be applied when the inputs provided by the business user who uses the template need to be processed before replacing the values for the template variables. e.g., If the average value is not provided, a function within the script can derive it by calculating it from the minimum value and the maximum value provided by the business user.</p> <p></p> </li> <li> <p>To specify the attributes that need to be considered as variables, select the relevant check boxes under Select templated elements. In this example, you can select the username and timeRange check boxes to to select the attributes with those names as the variables.</p> <p></p> <p>Then click Add To Script to update the script with the selected variables with auto-generated function bodies as shown below. </p> </li> <li> <p>Edit the script to add the required functions. In this example, let's rename <code>myFunction1(input)</code> to <code>getUsername(email)</code>, and <code>myFunction2(input)</code> to <code>validateTimeRange(number)</code>.</p> <p></p> <pre><code>    var username = getUsername('${userInputForusername}');\nvar timeRange = validateTimeRange('${userInputFortimeRange}');\n/**\n    * Extracts the username from given email\n    * @returns Extracted username\n    * @param email Provided email\n    */\nfunction getUsername(email) {\nif (email.match(/\\S+@\\S+/g)) {\nif (email.match(/\\S+@\\S+/g)[0] === email) {\nreturn email.split('@')[0];\n}\nthrow 'Invalid email address provided';\n}\nthrow 'Invalid email address provided';\n}\n\n\n/**\n    * Validates the given value for time range\n    * @returns Processed input\n    * @param input User given value\n    */\nfunction validateTimeRange(number) {\nif (!isNaN(number) &amp;&amp; (number &gt; 0)) {\nreturn number;\n} else {\nthrow 'A positive number expected for time range';\n}\n}\n</code></pre> </li> <li> <p>To generate properties, click Generate against Properties.</p> <p></p> <p>This expands the Properties section as follows.</p> <p></p> </li> <li> <p>Enter values for the available properties as follows. For this example, let's enter values as shown in the following table.</p> <p>Info</p> <p>A property is defined for each templated attribute (defined in the <code>${templatedElement}</code> format), so that it is self descriptive for the business user who uses the template. The values configured for each property are as follows:</p> <ul> <li> <p>Field Name: The name with which the templated attribute is displayed to the business user.</p> </li> <li> <p>Field Description: A description of the property for the business user to understand its purpose.</p> </li> <li> <p>Default Value: The value assigned to the property by default. The business user can change this value if required.</p> </li> <li> <p>Options: this is an optional configuration that allows you to define a set of values for a property so that the business user can select the required value from a list. This is useful when the possible value for the property is limited to a set of options.</p> </li> </ul> Property Field Name Field Description Default Value <code>timeInterval</code> <code>Time interval (in seconds)</code> <code>Production amounts are considered per time interval</code> <code>6</code> <code>userInputForusername</code> <code>Manager Email ID</code> <code>Email address to show in greeting</code> <code>example@email.com</code> <code>userInputFortimeRange</code> <code>Time Range (in milliseconds)</code> <code>Time period in which, product amounts are analyzed for decrease</code> <code>5</code> <p></p> </li> <li> <p>To save the template, click the save icon at the top of the page.</p> <p></p> </li> </ol>"},{"location":"admin/creating-business-rules-templates/#create-from-code-view","title":"Create from code view","text":"<p>When you use the code view, the same parameters for which you enter values in the design view are represented as JSON keys. For each parameter, you can specify a value against the relevant JSON key as shown in the extract below.</p> <p></p> <p>When you update the code view with a valid\u00a0template group definition, the design view is updated simultaneously as shown below.</p> <p></p> <p>However, if the\u00a0content you enter in the code view is an invalid template group, the design view is not updated, and an error is displayed as follows.</p> <p></p> <p>When an error is detected in the entered template group structure, the Recover button is displayed with the error message.</p> <p></p> <p>When you click Recover, the code view is reset to the latest detected valid template group definition. At any given time, the design view displays information based on the\u00a0latest detected valid template group definition.</p> <p>Info</p> <p>It is not recommended to add Siddhi application templates and scripts using the code view because they need to be provided as a single line, and the possible escape characters should be handled carefully.</p>"},{"location":"admin/creating-business-rules-templates/#editing-a-business-rules-template","title":"Editing a Business Rules Template","text":"<p>WSO2 SI allows you to make edits to a business rules template that you have already created and saved. To edit a template via the Template Editor tool, follow the steps below.</p> <ol> <li> <p>Start the WSO2 SI Tooling profile by issuing one of the following commands.</p> <ul> <li>For Windows: <code>tooling.bat</code></li> <li>For Linux:\u00a0<code>./tooling.sh</code></li> </ul> </li> <li> <p>Access the Template Editor via the URL that appears for it in the start up logs as shown in the example below.</p> <p></p> <p>Info</p> <p>The default URL is <code>http://localhost:9390/template-editor</code>. If required, you can change the host name (i.e., <code>localhost</code>) or the web UI application name (i.e., <code>template-editor</code>). For instructions, see Changing the Host Name and Context Path of SI Tooling.</p> </li> <li> <p>The Template Editor opens as follows.</p> <p></p> <p>To open an existing template, click the Open icon in the top panel (marked in the image above). In the Open Template File dialog box, click Choose File and browse for the required template. Once you have selected the template, click Load to open it in the Template Editor.</p> <p></p> </li> <li> <p>Edit the template as required. You can update it in the Design View or the Source View as you prefer. For more information, see Creating a Business Rule Template.</p> </li> <li> <p>Save your edits by clicking the Save icon in the top panel.</p> <p></p> </li> </ol>"},{"location":"admin/creating-business-rules-templates/#business-rules-templates","title":"Business Rules Templates","text":"<p>Rule Templates are used as specifications to gain inputs from users, through dynamically generated fields for the purpose of creating business rules. A template group is a business domain level grouping. The definition of a template looks as follows.</p> <pre><code>    {\n\"templateGroup\" : {\n\"name\" : \"&lt;Name of the template group&gt;\",\n\"uuid\":\"&lt;UUID for the template group&gt;\",\n\"description\" : \"&lt;(Optional) description for the template group&gt;\",\n\"ruleTemplates\" : [\n{\n\"name\" : \"&lt;Name of the rule template&gt;\" ,\n\"uuid\" : \"&lt;UUID for the rule template&gt;\",\n\"type\" : \"template\",\n\"instanceCount\" : \"one &lt;or&gt; many\",\n\"description\" : \"&lt;(Optional) description for the rule template&gt;\",\n\"script\" : \"&lt;(Optional) Javascript with reference to the properties&gt;\",\n\"templates\" : [\n{ \"type\" : \"siddhiApp\",\n\"content\" : \"&lt;SiddhiApp_1 with ${templatedProperty_x}&gt;\"\n},\n{ \"type\" : \"siddhiApp\",\n\"content\" : \"&lt;SiddhiApp_n with ${templatedProperty_y}&gt;\"\n}\n],\n\"properties\" : {\n\"templatedProperty_x\" : {\"fieldName\" : \"&lt;Field name for the property&gt;\", \"description\" : \"&lt;Description for the property&gt;\", \"defaultValue\" : \"&lt;Default value for the property&gt;\"},\n\"templatedProperty_y\" : {\"fieldName\" : \"&lt;Field name for the property&gt;\", \"description\" : \"&lt;Description for the property&gt;\", \"defaultValue\" : \"&lt;Default value for the property&gt;\", \"options\" : [\"&lt;option_1&gt;\", \"&lt;option_n&gt;\"]}\n}\n},\n{\n\"name\" : \"&lt;Name of the rule template&gt;\",\n\"uuid\" : \"&lt;UUID for the rule template&gt;\",\n\"type\" : \"input\",\n\"instanceCount\" : \"one &lt;or&gt; many\",\n\"description\" : \"&lt;(Optional) description for the rule template&gt;\",\n\"script\" : \"&lt;(Optional) Javascript with reference to the properties&gt;\",\n\"templates\" : [\n{ \"type\" : \"siddhiApp\",\n\"content\" : \"&lt;SiddhiApp with ${templatedProperty_x}&gt;\",\n\"exposedStreamDefinition\" :\"&lt;Exposed stream definition&gt;\"\n}\n],\n\"properties\" : {\n\"templatedProperty_x\" : {\"fieldName\" : \"&lt;Field name for the property&gt;\", \"description\" : \"&lt;Description for the property&gt;\", \"defaultValue\" : \"&lt;Default value for the property&gt;\", \"options\" : [\"&lt;option_1&gt;\", \"&lt;option_n&gt;\"]}\n}\n},\n{\n\"name\" : \"&lt;Name of the rule template&gt;\",\n\"uuid\" : \"&lt;UUID for the rule template&gt;\",\n\"type\" : \"output\",\n\"instanceCount\" : \"one &lt;or&gt; many\",\n\"description\" : \"&lt;(Optional) description for the rule template&gt;\",\n\"script\" : \"&lt;(Optional) Javascript with reference to the properties&gt;\",\n\"templates\" : [\n{ \"type\" : \"siddhiApp\",\n\"content\" : \"&lt;SiddhiApp with ${templatedProperty_x}&gt;\",\n\"exposedStreamDefinition\" :\"&lt;Exposed stream definition&gt;\"\n}\n],\n\"properties\" : {\n\"templatedProperty_x\" : {\"fieldName\" : \"&lt;Field name for the property&gt;\", \"description\" : \"&lt;Description for the property&gt;\", \"defaultValue\" : \"&lt;Default value for the property&gt;\", \"options\" : [\"&lt;option_1&gt;\", \"&lt;option_n&gt;\"]}\n}\n}\n]\n}\n}\n</code></pre> <p>The following parameters are configured:</p>"},{"location":"admin/creating-business-rules-templates/#template-group-basic-data","title":"Template Group basic data","text":"<p>The following parameters are configured under <code>templateGroup</code>.</p> Parameter Description Required/Optional <code>             name            </code> A name for the template group Required <code>             uuid            </code> A uniquely identifiable id for the template group Required <code>             description            </code> A description for the template. Optional"},{"location":"admin/creating-business-rules-templates/#rule-template-details","title":"Rule Template details","text":"<p>Multiple rule templates can be defined under a <code>templateGroup</code>. For each <code>ruleTemplate</code>, the following set of parameters need to be configured:</p> Parameter Description Required/Optional <code>             name            </code> A name for the rule template Required <code>             uuid            </code> A uniquely identifiable id for the rule template Required <code>             type            </code> The type of the rule template. Possible values are as follows: <ul> <li><code>                template:               </code> Used only to create an entire business rule from template</li> <li><code>                input :               </code> Used only in creating a business rule from scratch</li> <li><code>                output :               </code> Used only in creating a business rule from scratch <code> </code></li> </ul> Required <code>             instanceCount            </code> <p>This specifies whether the business rules derived from the template can be deployed only on one node, or whether they can be deployed on many nodes.</p> <p>Possible values are as follows:</p> <ul> <li>one</li> <li>many</li> </ul> Required <code>             script            </code>  The Java script to be executed on the templated fields. Developers can use this script for: <ul> <li>validating purposes.</li> <li>deriving values for a templated parameter by combining some other entered parameters</li> </ul> <p>You need to mention each templated element that needs to be derived from entered parameters as a variable in the global scope of the javascript.</p> <p>You also need to template the entered parameters in the script itself. These values are later replaced with their respective entered values.</p> <p>Consider the following script</p> <pre><code>/* \n* Validates a number and returns after adding 10 to it\n* @throws Error when a non number is entered\n*/\nfunction deriveValue(value){\nif( !isNan(value) ) {\nreturn value + 10;\n}\nthrow \"A number is required\";\n}\nvar derivedValue = deriveValue(${enteredValue});</code></pre> <p><code> </code></p> <p><code>               enteredValue              </code> should be defined as a property under <code>               properties              </code> in order to be filled by the user and replaced later.</p> <p>The derived value stored in <code>               derivedValue              </code> is then used to replace <code>               ${derivedValue}</code> in the SiddhiApp template. </p> Optional <code>             description            </code> A brief description of the rule template. Optional <code>             templates            </code> <p>These are the artifacts (i.e SiddhiApps) with templated parameters that are instantiated with replaced values when a business rule is created.</p> Required <code>             properties            </code> <p>You can add a field name, description, default value and possible values (optional) for the templated parameters.</p> Required"},{"location":"admin/creating-business-rules-templates/#deploying-business-rules-in-si-server","title":"Deploying business rules in SI server","text":"<p>To deploy a business rule in the Streaming Integrator server, follow the procedure below.</p> <p>Before you begin:</p> <p>Both the Streaming Integrator server(s) and Streaming Integrator tooling must be up and running.</p> <ol> <li> <p>Save the template group you created as a <code>.json</code> file in the <code>&lt;SI_TOOLING_HOME&gt;/wso2/server/resources/businessRules/templates</code> directory.</p> </li> <li> <p>In the <code>BusinessRules</code> section of the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file, add a configuration for the template you created as shown below.</p> <pre><code>        wso2.business.rules.manager:\n\ndatasource: &lt;datasourceName&gt;\n- nodeURL1:\n- ruleTemplateUUID1\n- ruleTemplateUUID2\nnodeURL2:\n- ruleTemplateUUID1\n- ruleTemplateUUID2\n</code></pre> <p>Info</p> <p>Specify the value for <code>nodeURL1</code> in the <code>IP:Port</code> format. If you add this configuration, a business rule which is derived from this rule template (when you run the Streaming Integrator Tooling and the Streaming Integrator servers of your SI setup) is deployed only in the nodes under which - this rule template has been specified. If you do not specifically add business rule template IDs in the configuration, business rules are deployed in all the available Streaming Integrator servers.</p> </li> </ol>"},{"location":"admin/creating-business-rules-templates/#configuring-business-rules-manager-permissions","title":"Configuring Business Rules Manager Permissions","text":"<p>There are two permission levels for a business rules application:</p> <ul> <li>Manager: User roles with this permission level have administrative privileges over business rules. They are allowed to create, view, edit, deploy or delete business rules.</li> <li>Viewer: User roles with this permission level are only allowed to view business rules.</li> </ul> <p>This section covers how to configure Business Rules Manager permissions.</p> <p>Before you begin:</p> <p>Before configuring Business Rules Manager permissions, the user roles to be assigned permissions must be already defined in the user store with the required user IDs. For detailed instructions, see User Management.</p> <p>You need to define the roles related to the Business Rules Manager under the <code>wso2.business.rules.manager</code> component namespace in the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <p>The following is a sample configuration of user roles for the Business Rules Manager.</p> <pre><code>    wso2.business.rules.manager:\n      roles:\n        manager:\n          - name: role1\n            id: 1\n        viewer:\n          - name: role2\n            id: 2\n</code></pre>"},{"location":"admin/general-data-protection-regulations/","title":"General Data Protection Regulations (GDPR) for Streaming Integrator","text":"<p>The General Data Protection Regulation (GDPR) is a new legal framework formalized by the European Union (EU) in 2016. This regulation is effective since 28, May 2018, and can affect any organization that processes Personally Identifiable Information (PII) of individuals who live in Europe. Organizations that fail to demonstrate GDPR compliance are subjected to financial penalties.</p> <p>Info</p> <p>Do you want to learn more about GDPR?</p> <p>If you are new to GDPR, we recommend that you take a look at our article series on Creating a Winning GDPR Strategy.</p> <ul> <li> <p>Part 1 - Introduction to GDPR</p> </li> <li> <p>Part 2 - 7 Steps for GDPR Compliance</p> </li> <li> <p>Part 3 - Identity and Access Management to the Rescue</p> </li> <li> <p>Part 4 - GDPR Compliant Consent Design</p> </li> </ul> <p>For more resources on GDPR, see the white papers, case studies, solution briefs, webinars, and talks published on our WSO2 GDPR homepage. You can also find the original GDPR legal text here.</p>"},{"location":"admin/general-data-protection-regulations/#removing-personally-identifiable-information-via-the-forget-me-tool","title":"Removing personally identifiable information via the Forget-me tool","text":"<p>In the Streaming Integrator,\u00a0streams\u00a0specify the schema for events to be selected into the streaming integration event flow to be processed. This schema can include user IDs and other PII (Personally Identifiable Information) that you want to delete from log files and such. This can be done via the Forget-me Tool.</p> <p>Step 1: Configure the config.json file</p> <p>The <code>&lt;SI_HOME&gt;/wso2/tools/identity-anonymization-tool-x.x.x/conf/config.json</code> file specifies the locations from which persisted data\u00a0needs to be removed.</p> <p>The <code>log-file</code> processor is specified in the configuration file of the Forget-Me tool as shown on the sample below in order to remove data with PII from the logs. If you have configured logs with PII to be saved in another location, you can add it to this list of processors.</p> <pre><code>{\n\"processors\" : [\n\"log-file\"\n],\n\"directories\": [\n{\n\"dir\": \"log-config\",\n\"type\": \"log-file\",\n\"processor\" : \"log-file\",\n\"log-file-path\" : \"logs\",\n\"log-file-name-regex\" : \"(.)*\"\n}\n]\n}\n</code></pre> <p>This extract shows the default configuration of the Streaming Integrator. The Streaming Integrator only saves PII in log files by default. Therefore, this configuration allows the Forget-me tool to delete these logs that are saved in the <code>&lt;SI_HOME&gt;/wso2/server/logs</code> directory.</p> <p>Step 2: Execute the Forget-me tool</p> <p>To execute the Forget-me tool, issue the following command pointing to the <code>&lt;SP_HOME&gt;</code> directory.</p> <p><code>forget-me -U &lt;USERNAME&gt; -d &lt;CONF_DIR&gt; -carbon &lt;SP_HOME&gt;</code></p>"},{"location":"admin/general-data-protection-regulations/#removing-references-to-deleted-user-identities","title":"Removing references to deleted user identities","text":"<p>This section covers how to remove references to deleted user identities in the Streaming Integrator by running the Forget-me tool.</p> <p>Before you begin:</p> <ul> <li>Note that this tool is designed to run in offline mode (i.e., the server should be shut down or run on another machine) in order to prevent unnecessary load to the server. If this tool runs in online mode (i.e., when the server is running), DB lock situations on the H2 databases may occur.</li> <li>If you have configured any JDBC database other than the H2 database provided by default, copy the relevant JDBC driver to the <code>&lt;SP_HOME&gt;/wso2/tools/identity-anonymization-tool/lib</code> directory.</li> </ul> <ol> <li> <p>Open a new terminal window and navigate to the <code>&lt;SP_HOME&gt;/bin</code> directory.</p> </li> <li> <p>Execute one of the following commands depending on your operating system:</p> <ul> <li>On Linux/Mac OS: <code>./forgetme.sh -U &lt;username&gt;</code></li> <li>On Windows: <code>forgetme.bat -U\u00a0&lt;username&gt;</code></li> </ul> </li> </ol> <p>Note</p> <p>The commands\u00a0specified\u00a0above use only the <code>-U &lt;username&gt;</code> option, which is the only required option to run the tool. There are several other optional command line options that you can specify based on your requirement. The supported options are described in detail below.</p> <p> Command Line Option Description Required Sample Value U The name of the user whose identity references you want to remove. Yes <code>               -U john.doe              </code> d The configuration directory to use when the tool is run.             If you do not specify a value for this option, the <code>               &lt;SP_HOME&gt;/wso2/tools/identity-anonymization-tool-x.x.x/conf              </code> directory (which is the default configuration directory of the tool) is used.           No <code>               -d &lt;TOOL_HOME&gt;/conf              </code> T <p>The tenant domain of the user whose identity references you want to remove.</p> <p>If you specify a tenant domain via this option, use the <code>TID</code> option to specify the ID of which the references must be removed.</p> No <p><code>                -T acme-company               </code></p> <p>The default value is <code>                carbon.super               </code></p> TID <p>The tenant ID of the user whose identity references you want to remove.</p> <p>It is required to specify a tenant ID if you have specified a tenant domain via the <code>TID</code> option.</p> No <code>               -TID 2346              </code> D The user store domain name of the user whose identity references you want to remove. No <p><code>                -D Finance-Domain               </code></p> <p>The default value is <code>                PRIMARY               </code> .</p> pu The pseudonym with which the user name of the user whose identity references you want to remove should be replaced. If you do not specify a pseudonym when you run the tool, a random UUID value is generated as the pseudonym by default. No <p><code>                -pu \u201c123-343-435-545-dfd-4\u201d               </code></p> carbon <p>The CARBON HOME. This should be replaced with the variable <code>                $CARBON_HOME               </code> in directories configured in the main configuration file.</p> No <code>               -carbon \u201c/usr/bin/wso2sp/wso2sp4.1.0              </code> </p>"},{"location":"admin/general-data-protection-regulations/#creating-gdpr-compliant-siddhi-applications","title":"Creating GDPR compliant Siddhi applications","text":"<p>The obfuscation/removal of such PII (Personally Identifiable Information) can be handled in the Streaming Integrator via Siddhi Applications that can either modify or remove records that contain the PII. These Siddhi Applications can be written in \u00a0a way to match the original queries that captured data for persistence so that the same data can be modified or removed as required. For more information about writing Siddhi Queries, see Siddhi Query Guide.</p> <p>The following sections explain how obfuscation/deletion of sensitive data can be managed via Siddhi queries in a custom Siddhi application developed based on a specific user case.</p> <ul> <li>Obfuscating PII</li> <li>Deleting PII</li> </ul>"},{"location":"admin/general-data-protection-regulations/#obfuscating-pii","title":"Obfuscating PII","text":"<p>Let's consider a Siddhi application that includes the following store definition to persist streaming data.</p> <p><code>define table customerTable (customerId string, customerName string, entryVector int);</code></p> <p>In this example, the customer ID is considered PII, and a customer with the <code>XXX</code> ID wants that ID to be hidden in the system so that he/she cannot be personally identified with it. Therefore, you need to obfuscate the value for the <code>customerId</code> attribute. This can be done by creating an algorithm to create a hashed value or a pseudonym to replace a specific value for the <code>customerId</code> attribute.</p> <p>Let's consider that such an algorithm exists (e.g., as a function named <code>anonymize</code>). To invoke this function, you need to add a new query to the Siddhi application as shown in the sample below.</p> <p><code>define table customerTable (customerId string, customerName string, entryVector int);</code> <code>define stream UpdateStream (customerId string);</code></p> <p><code>from UpdateStream</code> <code>select *</code> <code>update customerTable</code> <code>set customerTable.customerName = anonymize(customerTable.customerName)</code> <code>on customerTable.customerId == XXX;</code></p> <p>In the above Siddhi application, the query in bold is triggered when a new event is received in the <code>UpdateStream</code> stream where the value for the <code>customerId</code> attribute is <code>XXX</code>. Once it is triggered, the <code>XXX</code> customer ID is replaced with a pseudonym.</p> <p>For more information about writing custom functions, see Siddhi Query Guide - Writing Custom Extensions.</p>"},{"location":"admin/general-data-protection-regulations/#deleting-pii","title":"Deleting PII","text":"<p>Let's assume that the customer ID in the scenario described above needs to be deleted. To do this, you can write a Siddhi query to delete the value for the <code>customerId</code> attribute when is equal to <code>XXX</code> as shown below.</p> <p><code>define table customerTable (customerId string, customerName string, entryVector int);</code> <code>define stream DeleteStream (customerId string);</code></p> <p><code>from DeleteStream</code> <code>delete customerTable</code> <code>on customerTable.customerId == customerId;</code></p> <p>In the above Siddhi application, the query in bold is triggered when a new event is received in the <code>DeleteStream</code> stream where the value for the <code>customerId</code> attribute is XXX.\u00a0Once it is triggered, the <code>XXX</code> customer ID is deleted.</p> <p>For more information about the Delete operator used here, see Siddhi Query Guide - Delete.</p>"},{"location":"admin/general-data-protection-regulations/#forget-me-tool-overview","title":"Forget-me tool overview","text":"<p>The Forget-me tool is shipped with the Streaming Integrator by default in the <code>&lt;SI_HOME&gt;/wso2/tools/identity-anonymization-tool-x.x.x</code> directory. If required, you can change the default location of the configurations of this tool or make changes to the default configurations. You can also run the Forget-me tool in the standalone mode.</p>"},{"location":"admin/general-data-protection-regulations/#changing-the-default-configurations-location","title":"Changing the default configurations location","text":"<p>You can change the default location of the tool configurations if required. You may want to do this if you are working with a multi-product environment where you want to manage configurations in a single location for ease of use. Note that this is optional .</p> <p>To change the default configurations location for the embedded tool, do the following:</p> <ol> <li> <p>Open the <code>forgetme.sh</code> file found inside the <code>&lt;SI_HOME&gt;/bin</code> directory.</p> </li> <li> <p>The location path is the value given after <code>-d</code> within the following line. Modify the value after <code>-d</code> to change the location.</p> <p>Info</p> <p>The default location path is <code>$CARBON_HOME/repository/components/tools/forget-me/conf</code>.</p> <pre><code>    sh $CARBON_HOME/repository/components/tools/identity-anonymization-tool/bin/forget-me -d $CARBON_HOME/repository/components/tools/identity-anonymization-tool/conf -carbon $CARBON_HOME $@\n</code></pre> </li> </ol>"},{"location":"admin/general-data-protection-regulations/#changing-the-default-configurations-of-the-tool","title":"Changing the default configurations of the tool","text":"<p>All configurations related to this tool can be found inside the <code>&lt;SI_HOME&gt;/wso2/tools/identity-anonymization-tool/conf</code> directory. The default configurations are set up as follows:</p> <ul> <li> <p>Read Logs: <code>&lt;SI_HOME&gt;/wso2/server/logs</code>, <code>&lt;SI_TOOLING_HOME&gt;/wso2/server/logs</code></p> </li> <li> <p>Read Datasource: <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file, <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file</p> </li> <li> <p>Default datasources: <code>WSO2_CARBON_DB,\u00a0WSO2_METRICS_DB</code>, <code>WSO2_PERMISSIONS_DB</code>, <code>WSO2_DASHBOARD_DB</code>, <code>BUSINESS_RULES_DB</code>, <code>SAMPLE_DB</code>, <code>WSO2_STATUS_DASHBOARD_DB</code></p> </li> <li> <p>Log file name regex: The regex patterns defined in all the files in the <code>&lt;SI_HOME&gt;/wso2/tools/identity-anonymization-tool/conf/log-config</code> directory are considered.</p> </li> </ul> <p>For information on changing these configurations, see Configuring the config.json file in the Product Administration Guide.</p>"},{"location":"admin/general-data-protection-regulations/#running-the-forget-me-tool-in-the-standalone-mode","title":"Running the Forget-me tool in the standalone mode","text":"<p>This tool can run standalone and therefore cater to multiple products. This means that if you are using multiple WSO2 products and need to delete the user's identity from all products at once, you can do so by running the tool in standalone mode.</p> <p>For information on how to build and run the Forget-Me tool, see Removing References to Deleted User Identities in WSO2 Products in the WSO2 Administration Guide.</p>"},{"location":"admin/introduction-to-User-Management/","title":"Introduction to User Management","text":"<p>User management is a mechanism which involves defining and managing users, roles and their access levels in a system.\u00a0A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions.\u00a0A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc.</p> <p>Any user management system has users, roles, user stores and user permissions as its basic components .</p>"},{"location":"admin/introduction-to-User-Management/#users","title":"Users","text":"<p>Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed.</p>"},{"location":"admin/introduction-to-User-Management/#permission","title":"Permission","text":"<p>A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically.</p>"},{"location":"admin/introduction-to-User-Management/#user-roles","title":"User Roles","text":"<p>A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually.</p>"},{"location":"admin/introduction-to-User-Management/#user-store","title":"User Store","text":"<p>A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within SP or externally to it. User stores used in SP differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the \\&lt;SP_HOME&gt;/conf/\\&lt;PROFILE&gt;/deployment.yaml file interfaced through 'Local' IdP Client is enabled.</p>"},{"location":"admin/managing-grafana-dashboards/","title":"Managing Dashboards","text":"<p>WSO2 Streaming Integrator uses Grafana to host and view its pre-configured dashboards that are designed to view statistics relating to its performance as well as streaming activities. The pre-configured dashboards are imported to Grafana as JSON files. Once they are imported, you can organize them in folders, view them, and/or delete them.</p>"},{"location":"admin/managing-grafana-dashboards/#importing-dashboards","title":"Importing dashboards","text":"<p>To import dashboards, follow the procedure below:</p> <ol> <li> <p>Download the required JSON file (i.e., based on the statistics you need to view) from here.</p> </li> <li> <p>Start Grafana and access it via <code>http://localhost:3000/</code>.</p> </li> <li> <p>To load a new dashboard, click the plus icon (+) in the side panel. Then click Import.</p> </li> <li> <p>In the Import page, click Upload .json file. Then browse and select the .json file of the preconfigured dashboard that you downloaded (i.e., in step 5, substep 1).</p> </li> <li> <p>If required, change the unique identifier displayed in the Unique Identifier (uid).</p> </li> <li> <p>Click Import.</p> </li> </ol>"},{"location":"admin/managing-grafana-dashboards/#organizing-dashboards-in-folders","title":"Organizing dashboards in folders","text":"<p>!!! Before you begin:     Download the related JSON file(s) of one or more dashboards from here, and import them to Grafana. For instructions, see Importing dashboards.</p> <p>The dashboards you import are saved in the General folder by default. If required, you can create a design a folder structure that matches your requirement and save the dashboards in the different folders of the structure based on your categorization of the dashboards.</p> <p>Creating new folders</p> <p>To create a new folder, follow the procedure below:</p> <ol> <li> <p>Start and access Grafana via <code>http://localhost:3000/</code>.</p> </li> <li> <p>In the left pane, click the Dashboards icon, and then click Manage.</p> <p></p> <p>This opens the Dashboards/Manage tab. </p> <p></p> </li> <li> <p>Click New Folder. In the Folder Name field that appears, enter a name (e.g., <code>cdc-statistics</code>) for the new folder that you are creating. Then click Create.</p> </li> <li> <p>Navigate back to the Dashboards/Manage tab. You can do this by clicking Manage Dashboards in the message that appears in the page of your new folder.</p> <p></p> <p>The new folder you created appears as shown in the following image.</p> <p></p> </li> </ol> <p>Moving dashboards between folders</p> <p>To move selected dashboards to a specific folder, follow the procedure below:</p> <ol> <li> <p>In the Dashboards/Manage tab, select the dashboards you want to move. Then click Move.</p> <p></p> </li> <li> <p>In the Choose Dashboard Folder dialog box that appears, select the folder to which you want to move the selected dashboards.</p> <p></p> </li> <li> <p>Click Move to move the selected dashboards. A message appears to inform that the selected dashboards are successfully moved, and the Dashboards/Manage tab displays the selected dashboards under the folder you selected to move them.</p> </li> </ol>"},{"location":"admin/managing-grafana-dashboards/#deleting-dashboards","title":"Deleting dashboards","text":"<p>!!! Before you begin:     Download the related JSON file(s) of one or more dashboards from here, and import them to Grafana. For instructions, see Importing dashboards.</p> <p>In the Dashboards/Manage tab, select the dashboard(s) you want to delete. Then click Delete.</p> <p></p> <p>In the message that appears to confirm whether you want to delete the dashboard, click Delete.</p>"},{"location":"admin/managing-grafana-dashboards/#viewing-dashboards","title":"Viewing dashboards","text":"<p>To view a dashboard, follow the procedure below:</p> <ol> <li> <p>Navigate to the Dashboards/Manage tab. </p> </li> <li> <p>To expand the folder that contains the dashboard you want to view, click on it.</p> <p></p> </li> <li> <p>Click on the specific dashboard that you want to view.</p> <p></p> </li> <li> <p>Once the dashboard opens, click on the bar shown below to expand it and specify the time range for which you want to view statistics.</p> <p></p> <p>Once you expand the time range bar, you can select the required time interval or specify a custom time interval.</p> <p></p> </li> </ol>"},{"location":"admin/monitoring-received-events-count-via-logs/","title":"Monitoring the Received Events Count via Logs","text":"<p>You can monitor the total number of event received by the WSO2 Streaming Integrator via its sources per given time interval. This is done by configuring WSO2 Streaming Integrator to log the received event count for a specified time interval. The purpose of this log is to estimate the events received by the server during the time it is active. The event counter is reset each time the Streaming Integrator server is restarted.</p> <p>To configure WSO2 Streaming Integrator to log the total received events count, follow the steps below:</p> <ol> <li> <p>Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> </li> <li> <p>Add a parameter named <code>enableLoggingEventCount</code> and set it to <code>true</code> as shown below:</p> <p><code>enableLoggingEventCount: true</code></p> <p>!!!  info     This is set to <code>false</code> by default.</p> </li> <li> <p>Add another parameter named <code>loggingDuration</code> and give the time interval (in minutes) for which you want the total received event count to be logged. e.g., If you want the total received event count to be logged every minute, you can set the parameter as follows:</p> <p><code>loggingDuration: 1</code></p> <p>If you do not specify the logging duration, the received event count is logged every one minute by default.</p> </li> <li> <p>Save the change. The configuration is as follows:</p> <pre><code> enableLoggingEventCount: true\n loggingDuration: 1\n</code></pre> </li> <li> <p>Restart WSO2 Streaming Integrator for the change to be effective. </p> <p>The total received event count is logged as shown in the example below:</p> <pre><code>[2020-06-10 16:11:21,875]  INFO {io.siddhi.core.util.statistics.RecievedEventCounter} - Event received for Stream SweetProductionStream in siddhi App ReceiveHTTPinXMLFormatWithCustomMapping for last 1 minute(s): 60 .Total Events: 3636.\n</code></pre> </li> </ol>"},{"location":"admin/monitoring-si-performance-via-grafana/","title":"Monitoring WSO2 Streaming Integrator","text":"<p>Before you begin:</p> <p>To enable WSO2 Streaming Integrator to publish statistics in Grafana, follow the instructions in Setting up Grafana to Display WSO2 SI Statistics. To enable a Siddhi application to publish statistics to publish its statistics to Grafana, you need to specify Prometheus as the the statistics reporter via the <code>@App:statistics</code> annotation as shown below. <code>@App:statistics(reporter = 'prometheus')</code> For the dashboards to display statistics, at least one Siddhi application with the above configuration needs to be deployed in the Streaming Integrator Server.</p>"},{"location":"admin/monitoring-si-performance-via-grafana/#setting-up-grafana-to-monitor-wso2-streaming-integrator","title":"Setting Up Grafana to Monitor WSO2 Streaming Integrator","text":"<p>WSO2 Streaming Integrator performs streaming operations via Siddhi applications, which are applications written in the Siddhi Query Language. </p> <p>Each Siddhi application contains a combination ofSiddhi components.</p> <p>For the purpose of monitoring the performance of the Siddhi components used in your WSO2 Streaming Integrator instance, it provides ten pre-configured dashboards. To view them in Grafana, follow the steps below:</p> <ol> <li> <p>Download the following dashboards from the Streaming Integrator Git Repository. To download a specific dashboard, click on the dashboard name given below:</p> <ul> <li> <p>Siddhi Overall Statistics</p> </li> <li> <p>Siddhi Server Statistics</p> </li> <li> <p>Siddhi Stream Statistics</p> </li> <li> <p>Siddhi Source Statistics</p> </li> <li> <p>Siddhi Sink Statistics</p> </li> <li> <p>Siddhi Query Statistics</p> </li> <li> <p>Siddhi Window Statistics</p> </li> <li> <p>Siddhi Trigger Statistics</p> </li> <li> <p>Siddhi Table Statistics</p> </li> <li> <p>Siddhi Aggregation Statistics</p> </li> <li> <p>Siddhi On Demand Query Statistics</p> </li> </ul> </li> <li> <p>Start and access Grafana. Then import the eleven dashboards you downloaded in the previous step. For more information, see Managing Grafana Dashboards - Importing Dashboards.</p> </li> </ol>"},{"location":"admin/monitoring-si-performance-via-grafana/#accessing-grafana-dashboards-for-monitoring","title":"Accessing Grafana Dashboards for Monitoring","text":"<p>To navigate through the Grafana dashboards you set up for monitoring WSO2 Streaming Integrator and to analyze statistics, follow the procedure below:</p> <ol> <li> <p>Start Grafana and access it via <code>http://localhost:3000/</code>.</p> </li> <li> <p>In the left pane, click the Dashboards icon, and then click Manage to open the Dashboards page. Here, you can click on the following dashboards:</p> <ul> <li> <p>Siddhi Overall Statistics</p> <p>This displays the overall statistics related to your Streaming Integrator instance and the Siddhi components of the Siddhi applications that are currently deployed in it.</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Overall Statistics.</p> </li> <li> <p>Siddhi Server Statistics</p> <p>This displays statistics related to the Siddhi server. </p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Server Statistics.</p> </li> <li> <p>Siddhi Stream Statistics</p> <p>This displays statistics related to streams in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Stream Statistics.</p> </li> <li> <p>Siddhi Source Statistics</p> <p>This displays statistics related to sources in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Source Statistics.</p> </li> <li> <p>Siddhi Sink Statistics</p> <p>This displays statistics related to sinks in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Sink Statistics.</p> </li> <li> <p>Siddhi Query Statistics</p> <p>This displays statistics related to queries in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Query Statistics.</p> </li> <li> <p>Siddhi Window Statistics</p> <p>This displays statistics related to windows in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Window Statistics.</p> </li> <li> <p>Siddhi Trigger Statistics</p> <p>This displays statistics related to triggers in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Trigger Statistics.</p> </li> <li> <p>Siddhi Table Statistics</p> <p>This displays statistics related to tables in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Table Statistics.</p> </li> <li> <p>Siddhi Aggregation Statistics</p> <p>This displays statistics related to aggregations in the Siddhi applications currently deployed in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Aggregation Statistics.</p> </li> <li> <p>Siddhi On Demand Query Statistics</p> <p>This displays statistics related to on-demand-queries in your WSO2 Streaming Integrator server</p> <p>For a detailed description of the information displayed in this dashboard, see Viewing Server Statistics.</p> </li> </ul> </li> </ol>"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/","title":"Protecting Sensitive Data via the Secure Vault","text":"<p>The Streaming Integrator uses several artifacts for its functionality including deployment configurations for tuning its operation as well as deployable artifacts for extending its functionality. In each of these scenarios, there can be situations where the data specified is of a sensitive nature e.g., access tokens, passwords, etc.</p>"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#securing-sensitive-data-in-deployment-configurations","title":"Securing sensitive data in deployment configurations","text":"<p>The Streaming Integrator offers the Cipher tool to encrypt sensitive data in deployment configurations. This tool works in conjunction with WSO2 Secure Vault to replace sensitive data that is in plain text with an alias. The actual value is then encrypted and securely stored in the SecureVault. At runtime, the actual value is retrieved from the alias and used. For more information, see WSO2 Secure Vault.</p> <ol> <li> <p>Open the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/secrets.properties</code> file and enter the following information:</p> <ol> <li> <p>Enter the required sensitive data element with the value in plain text as shown in the example below.     <code>wso2.sample.password1=plainText ABC@123</code></p> </li> <li> <p>Enter the alias to be used in the required configuration file instead of the actual value of sensitive data you specified in the previous step as shown in the example below.     <code>password: ${sec:wso2.sample.password1}</code></p> </li> </ol> </li> <li> <p>To encrypt the sensitive data element and store it in the secure vault, run the Cipher tool by issuing the following command.     <code>sh &lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/bin/ciphertool.sh -runtime server</code></p> </li> </ol>"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#protecting-sensitive-data-in-siddhi-applications","title":"Protecting sensitive data in Siddhi applications","text":"<p>A parameter named ref is used to secure sensitive information in Siddhi applications that are deployed in the Streaming Integrator. For Siddhi applications that use storage technologies supported by Carbon Data sources, it is also possible to use Carbon data sources instead of specifying the connection parameters directly on the Siddhi file.</p>"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#using-the-ref-parameter","title":"Using the ref parameter","text":"<p>Siddhi 4.0 supports the ref parameter that enables the user to specify parts of their definition outside the Siddhi App. Extensions that support this functionality include:</p> <ul> <li>Stores</li> <li>Sources</li> <li>Sinks</li> </ul> <p>This method of securing sensitive data involves defining the store parameters required via a connection instance in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file, and referring to those from Siddhi applications via the <code>ref</code> parameter.</p> <p>Example</p> <p>In the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file, some connection parameters are defined for a store named <code>store1</code> as follows:</p> <pre><code>siddhi:\n   refs:\n      -\n         ref:\n            name: 'store1'\n            type: 'rdbms'\n            properties:\n                jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE'\n                username: 'root'\n                password: ${sec:store1.password}\n                field.length='currentTime:100'\n                jdbc.driver.name: 'org.h2.Driver'\n</code></pre> <p>The Siddhi application includes the following configuration:</p> <pre><code>@Store(ref='store1')\n@PrimaryKey('id')\n@Index('houseId')\ndefine table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string);\n</code></pre> <p>Here <code>@Store(ref='store1')</code> refers to <code>store1</code> defined in the <code>deployment.yaml</code> file. As a result, the properties defined for this store are applicable to the Siddhi application when it is connected to the store.</p>"},{"location":"admin/protecting-sensitive-data-via-the-secure-vault/#using-carbon-datasources","title":"Using carbon datasources","text":"<p>Currently, Carbon Data sources only support relational data source definitions. You can also define RDBMS Store artifacts using Carbon Data sources or JNDI instead of directly specifying the connection parameters. Then the datasource definitions defined in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file can be secured via the process described under Securing sensitive data in deployment configurations.</p>"},{"location":"admin/setting-up-grafana-dashboards/","title":"Setting up Dashboards to Display WSO2 SI Statistics","text":"<p>This section explains how WSO2 Streaming Integrator servers can be configured to publish data to Prometheus and display statistics in Grafana dashboards.</p> <p>Before you begin:</p> <ul> <li>Download and install Prometheus. For instructions, see the Prometheus Getting Started Guide. </li> <li>Download Grafana from Grafana Labs - Download Grafana.</li> </ul>"},{"location":"admin/setting-up-grafana-dashboards/#configuring-servers","title":"Configuring Servers","text":"<p>In order to configure a Grafana dashboard, follow the steps below:</p> <ol> <li> <p>Configure Prometheus reporter in Streaming Integrator</p> <p>Enable statistics for the Prometheus reporter as follows.</p> <ul> <li>To enable statistics for the Prometheus reporter, open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file and set the <code>enabled</code> parameter in the <code>wso2.metrics</code> section to <code>true</code>, and update the other parameters in the section as shown below. You also need to add the <code>metrics.prometheus:</code> as shown.</li> </ul> <p><pre><code> wso2.metrics:\n# Enable Metrics\nenabled: true\nreporting:\nconsole:\n- # The name for the Console Reporter\nname: Console\n\n# Enable Console Reporter\nenabled: false\n\n# Polling Period in seconds.\n# This is the period for polling metrics from the metric registry and printing in the console\npollingPeriod: 2\n\nmetrics.prometheus:\nreporting:\nprometheus:\n- name: prometheus\nenabled: true\nserverURL: \"http://localhost:9005\"\n</code></pre> - To enable statistics for the Prometheus reporter at Siddhi application level, use the <code>@App:statistics</code> annotation in the required Siddhi applications to set the <code>reporter</code> parameter as shown in the example below.</p> <pre><code>  @App:name('TestMetrics')\n  @App:statistics(reporter = 'prometheus')\n  define stream TestStream (message string);\n</code></pre> </li> <li> <p>Start Streaming Integrator server</p> <p>To start the Streaming runtime, navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory and issue the appropriate command based on your operating system.</p> <ul> <li>For Windows: <code>server.bat --run</code></li> <li>For Linux/MacOS: <code>./server.sh</code></li> </ul> </li> <li> <p>Start the Prometheus server</p> <p>Start and run the Prometheus server by following the procedure below.</p> <ol> <li> <p>Open the <code>&lt;PROMETHEUS_HOME&gt;/prometheus.yml</code> file and add the following configuration in the <code>scrape_configs:</code> section.</p> <p><pre><code> scrape_configs:\n- job_name: 'prometheus'\nstatic_configs:\n- targets: ['localhost:9005']\n</code></pre>     2. Start the Prometheus server by issuing the following command in the terminal.</p> <p><code>./prometheus</code></p> </li> </ol> </li> <li> <p>Start and configure the Grafana server</p> <p>To start and configure the Grafana server, follow the procedure below:</p> <ol> <li> <p>Start Grafana</p> <p>Info</p> <p>The procedure to start Grafana depends on your operating system and the installation process. e.g., If your operating system is Mac OS and you have installed Grafana via Homebrew, you start Grafana by issuing the <code>brew services start grafana</code> command.</p> </li> <li> <p>Access Grafana via <code>http://localhost:3000/</code>.</p> </li> <li> <p>In the Data Sources section, click Add your first data source. In the Add data source page that appears, click Select for Prometheus.</p> </li> <li> <p>In the Add data source page -&gt; Settings tab, update the configurations for Prometheus as follows.</p> <p></p> <pre><code>1. Click **Default** to make Prometheus the default data source.\n\n2. Under **HTTP**, enter `http://localhost:9090` as the URL.\n\n3. Click **Save &amp; Test**. If the data source is successfully configured, it is indicated via a message.\n    ![Save and Test](https://wso2.github.io/docs-si/images/cdc-monitoring/save-and-test.png)</code></pre> </li> </ol> </li> <li> <p>Load dashboards into Grafana</p> <p>WSO2 Streaming Integrator offers pre-built dashboards for monitoring streaming data flows and server statistics. To load them, follow the procedure below:</p> <ol> <li> <p>Download the required JSON file (i.e., based on the statistics you need to view) from here.</p> </li> <li> <p>Start Grafana and access it via <code>http://localhost:3000/</code>.</p> </li> <li> <p>To load a new dashboard, click the plus icon (+) in the side panel. Then click Import.</p> </li> <li> <p>In the Import page, click Upload .json file. Then browse and select the .json file of the preconfigured dashboard that you downloaded (i.e., in step 5, substep 1).</p> </li> <li> <p>If required, change the unique identifier displayed in the Unique Identifier (uid).</p> </li> <li> <p>Click Import.</p> </li> </ol> </li> </ol>"},{"location":"admin/supporting-different-transports/","title":"Supporting Different Transports","text":"<p>Follow the relevant section for the steps that need to be carried out before using the required transport to receive and publish events via the Streaming Integrator.</p>"},{"location":"admin/supporting-different-transports/#kafka-transport","title":"Kafka transport","text":"<p>To enable the Streaming Integrator to receive and publish events via the Kafka transport, follow the steps below:</p> <ol> <li> <p>Download the Kafka broker from the Kafka site.</p> </li> <li> <p>Convert and copy the Kafka client jars from the <code>&lt;KAFKA_HOME&gt;/libs</code> directory to the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory as follows:</p> <ol> <li> <p>Create a directory in a preferred location in your machine and copy the following JARs to it from the <code>&lt;KAFKA_HOME&gt;/libs</code> directory.</p> <p>Info</p> <p>This directory is referred to as the <code>&lt;SOURCE_DIRECTORY&gt;</code> in the next steps.</p> <ul> <li><code>kafka_2.11-0.10.2.1.jar</code></li> <li><code>kafka-clients-0.10.2.1.jar</code></li> <li><code>metrics-core-2.2.0.jar</code></li> <li><code>scala-library-2.11.8.jar</code></li> <li><code>scala-parser-combinators_2.11-1.0.4.jar</code></li> <li><code>zkclient-0.10.jar</code></li> <li><code>zookeeper-3.4.9.jar</code></li> </ul> </li> <li> <p>Create another directory in a preferred location in your machine.</p> <p>Info</p> <p>This directory is referred to as the <code>&lt;DESTINATION_DIRECTORY&gt;</code> in the next steps.</p> </li> <li> <p>To convert all the Kafka jars you copied into the <code>&lt;SOURCE_DIRECTORY&gt;</code> , issue one of the following commands.</p> <ul> <li>For Windows:: <code>&lt;SI_TOOLING_HOME&gt;/bin/jartobundle.bat &lt;SOURCE_DIRECTORY_PATH&gt;\u00a0&lt;DESTINATION_DIRECTORY_PATH&gt;</code></li> <li>For Linux: <code>&lt;SI_TOOLING_HOME&gt;/bin/jartobundle.sh\u00a0&lt;SOURCE_DIRECTORY_PATH&gt;\u00a0&lt;DESTINATION_DIRECTORY_PATH&gt;</code></li> </ul> </li> <li> <p>Copy the converted files from the <code>&lt;DESTINATION_DIRECTORY&gt;</code> to the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</p> </li> <li> <p>Copy the jars that are not converted from the <code>&lt;SOURCE_DIRECTORY&gt;</code> to the <code>&lt;SI_TOOLING_HOME&gt;/samples/sample-clients/lib</code> directory.</p> </li> </ol> </li> <li> <p>The Kafka server should be started before sending events from the Streaming Integrator to a Kafka consumer.</p> </li> </ol>"},{"location":"admin/supporting-different-transports/#jms-transport","title":"JMS transport","text":"<p>To configure the Apache ActiveMQ message broker, follow the steps below:</p> <ol> <li> <p>Install Apache ActiveMQ JMS.</p> <p>Info</p> <p>This guide uses ActiveMQ versions 5.7.0 - 5.9.0. If you want to use a later version, for instructions on the necessary changes to the configuration steps, go to Apache ActiveMQ Documentation.</p> </li> <li> <p>Download the <code>activemq-client-5.x.x.jar</code> from here.</p> </li> <li> <p>Register the <code>InitialContextFactory</code> implementation according to the OSGi JNDI spec and copy the client jar to the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory as follows.</p> <ol> <li> <p>Navigate to the <code>SI_TOOLING_HOME&gt;/bin</code> directory and issue the following command.</p> <ul> <li>For Linux: <code>./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;/activemq-client-5.x.x.jar &lt;Output Jar Path&gt;</code></li> <li>For Windows: <code>./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;\\activemq-client-5.x.x.jar &lt;Output Jar Path&gt;</code></li> </ul> <p>Info</p> <p>If required, you can provide privileges via <code>chmod +x icf-provider.(sh|bat)</code>.</p> <p>Once the client jar is successfully converted, the <code>activemq-client-5.x.x</code> directory is created. This directory contains the following:</p> <ul> <li><code>activemq-client-5.x.x.jar</code> (original jar)</li> <li><code>activemq-client-5.x.x_1.0.0.jar</code> (OSGi-converted jar)</li> </ul> <p>In addition, the following messages are logged in the terminal.</p> <pre><code>INFO: Executing 'jar uf &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar -C &lt;absolute_path&gt;/activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar\n</code></pre> </li> <li> <p>Copy <code>activemq-client-5.x.x/activemq-client-5.x.x.jar</code> and place it in the <code>&lt;SI_TOOLING_HOME&gt;/samples/sample-clients/lib</code> directory.</p> </li> <li> <p>Copy <code>activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar</code> and place it in the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</p> </li> </ol> </li> <li> <p>Create\u00a0a directory in a preferred location in your machine and copy the following JARs to it from the <code>&lt;ActiveMQ_HOME&gt;/libs</code> directory.</p> <p>Info</p> <p>This directory is referred to as the <code>&lt;SOURCE_DIRECTORY&gt;</code> in the next steps.</p> <ul> <li><code>hawtbuf-1.9.jar</code></li> <li><code>geronimo-jms_1.1_spec-1.1.1.jar</code></li> <li><code>geronimo-j2ee-management_1.1_spec-1.0.1.jar</code></li> </ul> </li> <li> <p>Create\u00a0another directory in a preferred location in your machine.</p> <p>Info</p> <p>This directory will be referred to as the <code>&lt;DESTINATION_DIRECTORY&gt;</code> in the next steps.</p> </li> <li> <p>To convert all the Kafka jars you copied into the <code>&lt;SOURCE_DIRECTORY&gt;</code>, issue the following command.</p> <ul> <li>For Windows: <code>&lt;SI_TOOLING_HOME&gt;/bin/jartobundle.bat &lt;SOURCE_DIRECTORY_PATH&gt;\u00a0&lt;DESTINATION_DIRECTORY_PATH&gt;</code></li> <li>For Linux: <code>&lt;SI_TOOLING_HOME&gt;/bin/jartobundle.sh\u00a0&lt;SOURCE_DIRECTORY_PATH&gt;\u00a0&lt;DESTINATION_DIRECTORY_PATH&gt;</code></li> </ul> </li> <li> <p>Copy the converted files from the <code>&lt;DESTINATION_DIRECTORY&gt;</code> to the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</p> </li> <li> <p>Copy the jars that are not converted from the <code>&lt;SOURCE_DIRECTORY&gt;</code> to the <code>&lt;SI_TOOLING_HOME&gt;/samples/sample-clients/lib</code> directory.</p> </li> </ol>"},{"location":"admin/supporting-different-transports/#mqtt-transport","title":"MQTT transport","text":"<p>To configure the MQTT message broker, follow the steps below:</p> <ol> <li> <p>Download the <code>org.eclipse.paho.client.mqttv3-1.1.1.jar</code> file from here.</p> </li> <li> <p>Place the file you downloaded in the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</p> </li> </ol>"},{"location":"admin/supporting-different-transports/#rabbitmq-transport","title":"RabbitMQ transport","text":"<p>To configure the RabbitMQ message broker, follow the steps below:</p> <ol> <li> <p>Download RabbitMQ from here.</p> </li> <li> <p>Create a directory in a preferred location in your machine. This directory is referred to as at the <code>&lt;SOURCE_DIRECTORY&gt;</code> in the rest of the procedure.</p> </li> <li> <p>Copy the following files from the <code>&lt;RabbitMQ_HOME&gt;/plugins</code> directory to the <code>&lt;SOURCE_DIRECTORY&gt;</code> you created.</p> </li> <li> <p>Create another directory in a preferred location in your machine.\u00a0This directory is referred to as the <code>&lt;DESTINATION_DIRECTORY&gt;</code> in this procedure.</p> </li> </ol>"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/","title":"User Management via the IdP Client Interface","text":"<p>In WSO2 Streaming Integrator, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication.</p> <p>IdP clients can be switched by specifying te required IdP client in the <code>auth.configs:</code> section in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>    auth.configs:\n# Type of the IdP Client used for the user authentication\ntype: local\n</code></pre> <p>The active IdP client is <code>local</code> by default.</p> <p>Following are the IdP Clients available for WS)2 Streaming Integrator:</p> <ul> <li> <p>Local IdP Client</p> </li> <li> <p>External IdP Client</p> </li> </ul>"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#local-idp-client","title":"Local IdP Client","text":"<p>The local IdP Client interacts with the file-based user store that is defined in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file under the <code>auth.configs</code> namespace as follows:</p> <pre><code>    auth.configs:  \n      type: 'local'\n      userManager:\n        adminRole: admin\n        userStore:\n          users:\n           - \n             user:\n               username: admin\n               password: YWRtaW4=\n               roles: 1\n          roles:\n           -     \n             role:\n               id: 1\n               displayName: admin\n</code></pre> <p>The above user and role is added by default.</p>"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#parameters","title":"Parameters","text":"<p>The parameters used in the above configurations are as follows:</p> <p>Note</p> <p>If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role.</p> Parameter Default Value Description <p><code>              userManager &gt; adminRole             </code></p> <p><code>              admin             </code></p> <p>The name of the role that has administration privileges.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              users &gt; user &gt; username             </code></p> <p><code>              admin             </code></p> <p>The username of the user.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              users &gt; user &gt; password             </code></p> <p><code>              YWRtaW4=             </code></p> <p>The Base64(UTF-8) encrypted password of the user.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              users &gt; user &gt; roles             </code></p> <p><code>              1             </code></p> <p>A comma separated list of the IDs of the roles assigned to the user.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              roles &gt; role &gt; id             </code></p> <p>1</p> <p>The unique ID for the role.</p> <p>userManager &gt; userStore &gt;</p> <p>roles &gt; role &gt; admin</p> <p>admin</p> <p>The name of the role.</p> <p>Furthermore, Local IdP Client functionality can be controlled via the properties defined\u00a0in the <code>&lt;SP_HOME&gt;/conf/&lt;PROFILE&gt;/deployment.yam</code> l file under the <code>auth.configs</code> namespace as shown below.</p> <pre><code>    auth.configs:\ntype: local\nproperties:\nsessionTimeout: 3600\nrefreshSessionTimeout: 86400     </code></pre> <p>The following are the properties that can be configured for the local IdP provider:</p> Property Default Value Description <code>             properties &gt; sessiontimeout            </code> <code>             3600            </code> <p>The number of seconds for which the session is valid once the user logs in.</p> !!! info <p>The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached.</p> <pre><code>properties &gt; refreshSessionTimeout</code></pre> <code>             86400            </code> The number of seconds for which the refresh token used to extend the session is valid. <p>The complete default configuration of the <code>local</code> IdP Client is as follows:</p> <pre><code>    auth.configs:  type: 'local'\nproperties:\nsessionTimeout: 3600\nrefreshSessionTimeout: 86400\nuserManager:\nadminRole: admin\nuserStore:\nusers:\n- user:\nusername: admin\npassword: YWRtaW4=\nroles: 1\nroles:\n-     role:\nid: 1\ndisplayName: admin\n</code></pre>"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#external-idp-client","title":"External IdP Client","text":"<p>External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 SP authenticates by requesting an access token from the identity provider using the password grant type.</p> <p>Note</p> <p>The identity provider with which WSO2 SP interacts with to authenticate users must be started before the SP server.</p> <p>The auth manager must be configured under the <code>auth.configs</code> namespace as shown below:</p> <pre><code>    auth.configs:\ntype: external\nauthManager:\nadminRole: admin\n</code></pre> <p>The parameters used in the above configurations areas follows:</p> Parameter Default Value Description <code>userManager &gt; adminRole</code> <code>admin</code> The name of the role that has administration privilages. <p>Furthermore, external IdP client functionality can be controlled via the properties defined\u00a0in the <code>&lt;SP_HOME&gt;/conf/&lt;PROFILE&gt;/deployment.yaml</code> file under the <code>auth.configs</code> namespace as shown below.</p> <pre><code>    auth.configs: type: external\nproperties:\nkmDcrUrl: https://localhost:9443/identity/connect/register\nkmTokenUrl: https://localhost:9443/oauth2\nkmUsername: admin\nkmPassword: admin\nidpBaseUrl: https://localhost:9443/scim2\nidpUsername: admin\nidpPassword: admin\nportalAppContext: portal\nstatusDashboardAppContext: monitoring\nbusinessRulesAppContext : business-rules\ndatabaseName: WSO2_OAUTH_APP_DB\ncacheTimeout: 900\nbaseUrl: https://localhost:9643\ngrantType: password\n</code></pre> <p>The following are the properties that can be configured for the external IdP provider:</p>  Property   Default Value   Description  kmDcrUrl <pre><code>https://localhost:9443/identity/connect/register</code></pre> The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. <code>             dcrAppOwner            </code> <code>             kmUsername            </code> <code>             kmTokenUrl            </code> <p><code>                             https://localhost:9443/oauth2                           </code></p> The token endpoint of the key manager in the IdP. <p><code>              kmUsername             </code></p> <code>             admin            </code> The username for the key manager in the IdP. <p><code>              kmPassword             </code></p> <code>             admin            </code> The password for the key manager in the IdP. <p><code>              idpBaseUrl             </code></p> <p><code>                             https://localhost:9443/scim2                           </code></p> The SCIM2 endpoint of the IdP. <p><code>              idpUsername             </code></p> <code>             admin            </code> The username for the IdP. <p><code>              idpPassword             </code></p> <code>             admin            </code> The password for the IdP. <p><code>              portalAppContext             </code></p> <code>             portal            </code> The application context of the Dashboard Portal application in WSO2 SP. <code>             statusDashboardAppContext            </code> <code>             monitoring            </code> The application context of the Status Dashboard application in WSO2 SP. <p><code>              businessRulesAppContext             </code></p> <code>             business-rules            </code> The application context of the Business Rules application in WSO2 SP. <code>             databaseName            </code> <p><code>              WSO2_OAUTH_APP_DB             </code></p> The name of the wso2.datasource used to store the OAuth application credentials <code>             cacheTimeout            </code> <code>             900            </code> The cache timeout for the validity period of the token in seconds. <p><code>              baseUrl             </code></p> <p><code>                             https://localhost:9643                           </code></p> <p>The base URL to which the token should be redirected after the code returned</p> <p>from the Authorization Code grant type is used to get the token.</p> <p><code>              grantType             </code></p> <code>             password            </code> The grant type used in the OAuth application token request. <p><code>              spClientId/ portalClientId/             </code></p> <p><code>              statusDashboardClientId/ businessRulesClientId             </code></p> N/A <p>The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store.</p> <p><code>              spClientId/ portalClientId/             </code></p> <p><code>              statusDashboardClientId/ businessRulesClientId             </code></p> N/A <p>The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store.</p> <p> </p>"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#writing-custom-idp-client","title":"Writing custom IdP Client","text":"<p>When writing a custom IdP client, the following two interfaces must be implemented:</p> <ul> <li>IdPClientFactory     : This is a factory\u00a0OSGi service that initialtes the custom IdP     client using the properties from     <code>IdPClientConfiguration.</code></li> <li>IdPClient     :\u00a0\u00a0An interface with functions to provide user authentication and     retrieval by the other services.</li> </ul>"},{"location":"admin/user-management/","title":"User Management","text":"<p>Info</p> <p>User management in WSO2 Streaming Integrator has the following features,</p> <ul> <li> <p>The concept of single user store, which is either local or external.</p> </li> <li> <p>File based user store as the default embedded store.</p> </li> <li> <p>Ability to connect to an external Identity Provider using SCIM2 and     OAuth2 protocols.</p> </li> <li> <p>Ability to extend user authentication as per the scenario</p> </li> </ul>"},{"location":"admin/user-management/#introduction-to-user-management","title":"Introduction to user management","text":"<p>User management is a mechanism which involves defining and managing users, roles and their access levels in a system.\u00a0A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions.\u00a0A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc.</p> <p>Any user management system has users, roles, user stores and user permissions as its basic components .</p>"},{"location":"admin/user-management/#users","title":"Users","text":"<p>Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed.</p>"},{"location":"admin/user-management/#permission","title":"Permission","text":"<p>A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically.</p>"},{"location":"admin/user-management/#user-roles","title":"User Roles","text":"<p>A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually.</p>"},{"location":"admin/user-management/#user-store","title":"User Store","text":"<p>A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within WSO2 Streaming Integrator or externally to it. User stores used in WSO2 Streaming Integrator differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the <code>&lt;SI_HOME&gt;&gt;/conf/server/deployment.yaml</code> file interfaced through 'Local' IdP Client is enabled.</p>"},{"location":"admin/user-management/#user-management-via-the-idp-client-interface","title":"User Management via the IdP Client Interface","text":"<p>In WSO2 Streaming Integrator, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication.</p> <p>IdP clients can be switched by specifying te required IdP client in the <code>auth.configs:</code> section in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>    auth.configs:\n# Type of the IdP Client used for the user authentication\ntype: local\n</code></pre> <p>The active IdP client is <code>local</code> by default.</p> <p>Following are the IdP Clients available for WSO2 Streaming Integrator:</p> <ul> <li>Local IdP Client</li> <li>External IdP Client</li> </ul>"},{"location":"admin/user-management/#local-idp-client","title":"Local IdP Client","text":"<p>The local IdP Client interacts with the file-based user store that is defined in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file under the <code>auth.configs</code> namespace as follows:</p> <pre><code>    auth.configs:\n      type: 'local'\n      userManager:\n        adminRole: admin\n        userStore:\n          users:\n           -\n             user:\n               username: admin\n               password: YWRtaW4=\n               roles: 1\n          roles:\n           -\n             role:\n               id: 1\n               displayName: admin\n</code></pre> <p>The above user and role is added by default.</p>"},{"location":"admin/user-management/#parameters","title":"Parameters","text":"<p>The parameters used in the above configurations are as follows:</p> <p>Note</p> <p>If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role.</p> Parameter Default Value Description <p><code>              userManager &gt; adminRole             </code></p> <p><code>              admin             </code></p> <p>The name of the role that has administration privileges.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              users &gt; user &gt; username             </code></p> <p><code>              admin             </code></p> <p>The username of the user.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              users &gt; user &gt; password             </code></p> <p><code>              YWRtaW4=             </code></p> <p>The Base64(UTF-8) encrypted password of the user.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              users &gt; user &gt; roles             </code></p> <p><code>              1             </code></p> <p>A comma separated list of the IDs of the roles assigned to the user.</p> <p><code>              userManager &gt; userStore &gt;             </code></p> <p><code>              roles &gt; role &gt; id             </code></p> <p>1</p> <p>The unique ID for the role.</p> <p>userManager &gt; userStore &gt;</p> <p>roles &gt; role &gt; admin</p> <p>admin</p> <p>The name of the role.</p> <p>Furthermore, Local IdP Client functionality can be controlled via the properties defined\u00a0in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file under the <code>auth.configs</code> namespace as shown below.</p> <pre><code>    auth.configs:\ntype: local\nproperties:\nsessionTimeout: 3600\nrefreshSessionTimeout: 86400\n</code></pre> <p>The following are the properties that can be configured for the local IdP provider:</p> Property Default Value Description <code>             properties &gt; sessiontimeout            </code> <code>             3600            </code> <p>The number of seconds for which the session is valid once the user logs in.</p> !!! info <p>The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached.</p> <pre><code>properties &gt; refreshSessionTimeout</code></pre> <code>             86400            </code> The number of seconds for which the refresh token used to extend the session is valid. <p>The complete default configuration of the <code>local</code> IdP Client is as follows:</p> <pre><code>    auth.configs:\ntype: 'local'\nproperties:\nsessionTimeout: 3600\nrefreshSessionTimeout: 86400\nuserManager:\nadminRole: admin\nuserStore:\nusers:\n-\nuser:\nusername: admin\npassword: YWRtaW4=\nroles: 1\nroles:\n-\nrole:\nid: 1\ndisplayName: admin\n</code></pre>"},{"location":"admin/user-management/#external-idp-client","title":"External IdP Client","text":"<p>External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 Streaming Integrator authenticates by requesting an access token from the identity provider using the password grant type.</p> <p>Note</p> <p>The identity provider with which WSO2 Streaming Integrator interacts with to authenticate users must be started before the Streaming Integrator server.</p> <p>The auth manager must be configured under the <code>auth.configs</code> namespace as shown below:</p> <pre><code>    auth.configs:\ntype: external\nauthManager:\nadminRole: admin\n</code></pre> <p>The parameters used in the above configurations areas follows:</p> Parameter Default Value Description <code>userManager &gt; adminRole</code> <code>admin</code> The name of the role that has administration privilages. <p>Furthermore, external IdP client functionality can be controlled via the properties defined\u00a0in the <code>&lt;SI_HOME&gt;/conf/&lt;PROFILE&gt;/deployment.yaml</code> file under the <code>auth.configs</code> namespace as shown below.</p> <pre><code>    auth.configs:\ntype: external\nproperties:\nkmDcrUrl: https://localhost:9443/identity/connect/register\nkmTokenUrl: https://localhost:9443/oauth2\nkmUsername: admin\nkmPassword: admin\nidpBaseUrl: https://localhost:9443/scim2\nidpUsername: admin\nidpPassword: admin\nportalAppContext: portal\nstatusDashboardAppContext: monitoring\nbusinessRulesAppContext : business-rules\ndatabaseName: WSO2_OAUTH_APP_DB\ncacheTimeout: 900\nbaseUrl: https://localhost:9643\ngrantType: password\n</code></pre> <p>The following are the properties that can be configured for the external IdP provider:</p>  Property   Default Value   Description  kmDcrUrl <pre><code>https://localhost:9443/identity/connect/register</code></pre> The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. <code>             dcrAppOwner            </code> <code>             kmUsername            </code> <code>             kmTokenUrl            </code> <p><code>                             https://localhost:9443/oauth2                           </code></p> The token endpoint of the key manager in the IdP. <p><code>              kmUsername             </code></p> <code>             admin            </code> The username for the key manager in the IdP. <p><code>              kmPassword             </code></p> <code>             admin            </code> The password for the key manager in the IdP. <p><code>              idpBaseUrl             </code></p> <p><code>                             https://localhost:9443/scim2                           </code></p> The SCIM2 endpoint of the IdP. <p><code>              idpUsername             </code></p> <code>             admin            </code> The username for the IdP. <p><code>              idpPassword             </code></p> <code>             admin            </code> The password for the IdP. <p><code>              portalAppContext             </code></p> <code>             portal            </code> The application context of the Dashboard Portal application in WSO2 Streaming Integrator. <code>             statusDashboardAppContext            </code> <code>             monitoring            </code> The application context of the Status Dashboard application in WSO2 Streaming Integrator. <p><code>              businessRulesAppContext             </code></p> <code>             business-rules            </code> The application context of the Business Rules application in WSO2 Streaming Integrator. <code>             databaseName            </code> <p><code>              WSO2_OAUTH_APP_DB             </code></p> The name of the wso2.datasource used to store the OAuth application credentials <code>             cacheTimeout            </code> <code>             900            </code> The cache timeout for the validity period of the token in seconds. <p><code>              baseUrl             </code></p> <p><code>                             https://localhost:9643                           </code></p> <p>The base URL to which the token should be redirected after the code returned</p> <p>from the Authorization Code grant type is used to get the token.</p> <p><code>              grantType             </code></p> <code>             password            </code> The grant type used in the OAuth application token request. <p><code>              spClientId/ portalClientId/             </code></p> <p><code>              statusDashboardClientId/ businessRulesClientId             </code></p> N/A <p>The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store.</p> <p><code>              spClientId/ portalClientId/             </code></p> <p><code>              statusDashboardClientId/ businessRulesClientId             </code></p> N/A <p>The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store.</p> <p> </p>"},{"location":"admin/user-management/#writing-custom-idp-client","title":"Writing custom IdP Client","text":"<p>When writing a custom IdP client, the following two interfaces must be implemented:</p> <ul> <li>IdPClientFactory     : This is a factory\u00a0OSGi service that initialtes the custom IdP     client using the properties from     <code>IdPClientConfiguration.</code></li> <li>IdPClient     :\u00a0\u00a0An interface with functions to provide user authentication and     retrieval by the other services.</li> </ul>"},{"location":"admin/viewing-aggregation-statistics/","title":"Viewing Aggregation Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-aggregation-statistics/#viewing-aggregation-statistics","title":"Viewing Aggregation Statistics","text":"<p>The information displayed in this dashboard is as follows.</p> <p></p>"},{"location":"admin/viewing-cdc-statistics/","title":"Viewing cdc statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-cdc-statistics/#viewing-cdc-statistics","title":"Viewing CDC Statistics","text":"<p>This section covers the dashboards that are available to view statistics relating to change data capture. </p>"},{"location":"admin/viewing-cdc-statistics/#viewing-cdc-statistics_1","title":"Viewing CDC statistics","text":""},{"location":"admin/viewing-cdc-statistics/#streaming","title":"Streaming","text":""},{"location":"admin/viewing-cdc-statistics/#scheduled","title":"Scheduled","text":""},{"location":"admin/viewing-cdc-statistics/#viewing-cdc-scheduled-statistics","title":"Viewing CDC scheduled statistics","text":""},{"location":"admin/viewing-cdc-statistics/#total-changes","title":"Total Changes","text":""},{"location":"admin/viewing-cdc-statistics/#last-change-occurred-at","title":"Last Change Occurred At","text":""},{"location":"admin/viewing-cdc-statistics/#table-status","title":"Table Status","text":""},{"location":"admin/viewing-cdc-statistics/#idle-time","title":"Idle Time","text":""},{"location":"admin/viewing-cdc-statistics/#mapping-errors","title":"Mapping Errors","text":""},{"location":"admin/viewing-cdc-statistics/#transport-level-errors","title":"Transport Level Errors","text":""},{"location":"admin/viewing-cdc-statistics/#polling-details","title":"Polling Details","text":""},{"location":"admin/viewing-cdc-statistics/#received-changes","title":"Received Changes","text":""},{"location":"admin/viewing-cdc-statistics/#viewing-cdc-streaming-statistics","title":"Viewing CDC streaming statistics","text":""},{"location":"admin/viewing-cdc-statistics/#total-changes_1","title":"Total Changes","text":""},{"location":"admin/viewing-cdc-statistics/#total-inserts","title":"Total Inserts","text":""},{"location":"admin/viewing-cdc-statistics/#last-insert","title":"Last Insert","text":""},{"location":"admin/viewing-cdc-statistics/#total-updates","title":"Total Updates","text":""},{"location":"admin/viewing-cdc-statistics/#last-update","title":"Last Update","text":""},{"location":"admin/viewing-cdc-statistics/#total-deletes","title":"Total Deletes","text":""},{"location":"admin/viewing-cdc-statistics/#last-delete","title":"Last Delete","text":""},{"location":"admin/viewing-cdc-statistics/#last-change-occuured-at","title":"Last Change Occuured At","text":""},{"location":"admin/viewing-cdc-statistics/#table-status_1","title":"Table Status","text":""},{"location":"admin/viewing-cdc-statistics/#idle-time_1","title":"Idle Time","text":""},{"location":"admin/viewing-cdc-statistics/#mapping-errors_1","title":"Mapping Errors","text":""},{"location":"admin/viewing-cdc-statistics/#transport-level-errors_1","title":"Transport Level Errors","text":""},{"location":"admin/viewing-cdc-statistics/#received-changes_1","title":"Received Changes","text":""},{"location":"admin/viewing-dashboards/","title":"Monitoring ETL Flows with Grafana","text":"<p>Before you begin:</p> <p>To enable WSO2 Streaming Integrator to publish statistics in Grafana, follow the instructions in Setting up Grafana to Display WSO2 SI Statistics.</p>"},{"location":"admin/viewing-dashboards/#setting-up-grafana-to-monitor-wso2-streaming-integrator","title":"Setting Up Grafana to Monitor WSO2 Streaming Integrator","text":"<p>For the purpose of monitoring ETL (Extract, Transform, Load) statistics WSO2 provides nine pre-configured dashboards. To view them in Grafana, follow the steps below:</p> <ol> <li> <p>Download the following dashboards (i.e., the JSON file with the dashboard configuration) from here.</p> Directory Dashboard <code>overview-statistics</code> - WSO2 Streaming Integrator - Overall Statistics.json  - WSO2 Streaming Integrator - App Statistics.json <code>file-statistics</code> - WSO2 Streaming Integrator - File Statistics.json  - WSO2 Streaming Integrator - File Source Statistics.json  - WSO2 Streaming Integrator - File Sink Statistics.json <code>rdbms-statistics</code> - WSO2 Streaming Integration - RDBMS Statistics.json  - WSO2 Streaming Integration - RDBMS Table Statistics.json <code>cdc-statistics</code> - WSO2 Streaming Integrator - CDC Statistics.json  - WSO2 Streaming Integrator - CDC Streaming Statistics.json  - WSO2 Streaming Integrator - CDC Scheduled Statistics.json <code>kafka-statistics</code> - WSO2 Streaming Integrator - Kafka Statistics.json  - WSO2 Streaming Integrator - Kafka Source Statistics.json  - WSO2 Streaming Integrator - Kafka Sink Statistics.json </li> <li> <p>Start and access Grafana. Then import the nine dashboards you downloaded in the previous step. For more information, see Managing Grafana Dashboards - Importing Dashboards.</p> </li> <li> <p>In the Dashboards/Manage tab, create four new folders named <code>overview-statistics</code>, <code>file-statistics</code>, <code>rdbms-statistics</code>, <code>cdc-statistics</code>, and <code>kafka-statistics</code> . Then organize the dashboards you imported in the new folders you created as shown in the image below. For instructions, see Managing Grafana Dashboards - Organizing Dashboards in Folders.</p> <p>Info</p> <p>The following is a suggested method to organize your dashboards, and the following instructions are provided based on this structure.</p> <p></p> </li> </ol>"},{"location":"admin/viewing-dashboards/#accessing-grafana-dashboards-for-monitoring","title":"Accessing Grafana Dashboards for Monitoring","text":"<p>To navigate through the Grafana dashboards you set up for monitoring WSO2 Streaming Integrator and to analyze statistics, follow the procedure below:</p> <ol> <li> <p>To start and access the dashboards in Grafana, follow the steps below:</p> <ol> <li> <p>Start the Prometheus server by issung the following command from the <code>&lt;PROMETHEUS_HOME&gt;</code> directory.</p> <p><code>./prometheus</code></p> </li> <li> <p>Start Grafana.</p> <p>Info</p> <p>The procedure to start Grafana depends on your operating system and the installation process. e.g., If your operating system is Mac OS and you have installed Grafana via Homebrew, you start Grafana by issuing the <code>brew services start grafana</code> command.</p> </li> <li> <p>Start the WSO2 Streaming Integrator server by navigating to the <code>&lt;SI_HOME&gt;/bin</code>directory, and issuing the appropriate command out of the following based on your operating system.</p> <ul> <li>For Linux: <code>./server.sh</code></li> <li>For Windows: <code>server.bat</code></li> </ul> </li> </ol> <p>Access Grafana via <code>http://localhost:3000/</code>.</p> </li> <li> <p>In the left pane, click the Dashboards icon, and then click Manage to open the Dashboards page.</p> </li> <li> <p>In the Dashboards page, click WSO2 Streaming Integrator Overall Statistics.</p> <p></p> <p>The overall statistics are displayed as shown in the following example.</p> <p></p> <p>This provides an overview on the overall performance of the WSO2 Streaming Integrator instance by displaying the total number of input events consumed and the total number of output events. It also provides the breakdown of the total inputs and outputs by Siddhi application. The (Consume/Publish)/Sec graph indicates the rate at which the consumed input events are published per second.</p> </li> <li> <p>To view the statistics specific to a Siddhi application, click on the relevant Siddhi application.</p> <p></p> <p>The WSO2 Streaming Integrator App Statistics dashboard opens. This allows you to access any statistics relating to change data capture, file processing and data store integration activities carried out via the selected Siddhi application.</p> <p></p> </li> </ol>"},{"location":"admin/viewing-dashboards/#viewing-cdc-statistics","title":"Viewing CDC statistics**","text":"<p>CDC related statistics are displayed in the Sources section of the WSO2 Streaming Integrator App Statistics dashboard. To view the detailed statistics for a specific table, click on that table.</p> <p>As a result, the CDC Statistics dashboard opens with change data statistice specific to that table.</p> <p></p> <p>If you click on a table for which change data is captured in a streaming manner (i.e., a table in the CDC Streaming Statistics section), the CDC Streaming Statistics dashboard opens with change data related statistics specific to the selected table as shown in the example below.</p> <p></p> <p>If you click on a table for which change data is captured in a scheduled manner by polling the table (i.e., a table in the CDC Scheduled Statistics section), the CDC Scheduled Statistics dashboard opens with change data related statistics specific to the selected table as shown in the example below.</p> <p></p>"},{"location":"admin/viewing-dashboards/#viewing-file-statistics","title":"Viewing File statistics**","text":"<ul> <li>When the content of a file is used as input data, the file is displayed in the Sources section as shown in the example below.</li> </ul> <p>To view statistics for a specific file, click on it. As a result, the WSO2 Streaming Integrator - File Statistics dashboard opens as shown in the example below.</p> <p></p> <p>When you click on the same file again, the WSO2 Streaming Integrator - File Source Statistics dashboard opens with more detailed information about this source file as shown in the example below. </p> <p> </p> <ul> <li>When the content of a file is generated as an output of the Siddhi application, the file is displayed in the Destinations section as shown in the example below.</li> </ul> <p> </p> <p>To view statistics for a specific file, click on it. As a result, the WSO2 Streaming Integrator - File Statistics dashboard opens as shown in the example below.</p> <p></p> <p>When you click on the same file again, the WSO2 Streaming Integrator - File Sink Statistics dashboard opens with more detailed information about this sink file as shown in the example below. </p> <p> </p>"},{"location":"admin/viewing-dashboards/#viewing-rdbms-statistics","title":"Viewing RDBMS statistics**","text":"<p>When the records of an RDBMS database is used as input data by the selected Siddhi application, the database is displayed in the Sources section. When any output generated by the selected Siddhi application is saved in a database, that database is displayed under Destinations.</p> <p>In each section, all the databases of the relevant category is listed in a table as shown in the example below.</p> <p></p> <p>To view detailed statistics of a specific database, click on that database.</p> <p></p> <p>The RDBMS Statistics dashboard opens with RDBMS statistics specific to the selected database.</p> <p></p> <p>To view statistics relating to a specific table, click on the relevant table name in the RDBMS Table table.</p> <p></p> <p>The RDBMS Table Statistics dashboard opens with statistics specific to the selected table.</p> <p></p>"},{"location":"admin/viewing-dashboards/#viewing-kafka-statistics","title":"Viewing Kafka statistics","text":"<p>Kafka related statistics are displayed in the Sources and Destinations sections of the WSO2 Streaming Integrator App Statistics dashboard.</p> <p>To view the detailed overall Kafka Statistics for a specific app, click on any stream link on the table. As a result, you will be taken to WSO2 Streaming Integrator Kafka Statistics dashboard.</p> <p>This page has two sections: Source and Sink. </p> <ul> <li> <p>The Source section shows the overall Kafka Source metrics such as total reads, input event rate etc. It also contains a table that lists all the streams that are associated to a Kafka source. If you click on a stream, it will take you to WSO2 Streaming Integrator Kafka Source Statistics dashboard.</p> <p>This dashboard shows the Kafka Source statistics related to the selected stream. A stream is associated to one or more topics. Furthermore, each topic is associated to a partition. You can use this dashboard to view statistics for each separate Topic-Partition combination to which the selected stream is subscribed.</p> </li> <li> <p>The Sink section shows the overall Kafka Sink metrics such as total writes, output event rate etc. It also contains a table that lists all the streams that are associated to a Kafka source. If you click on a stream, it will take you to WSO2 Streaming Integrator Kafka Sink Statistics dashboard.</p> <p>This dashboard shows the Kafka Sink statistics related to the selected stream. Here, a stream publishes to a specific Kafka Topic and to one or more partitions. You can use this dashboard to view statistics for each separate Topic-Partition combination to which the selected stream publishes messages.</p> </li> </ul>"},{"location":"admin/viewing-file-statistics/","title":"Viewing file statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-file-statistics/#viewing-file-statistics","title":"Viewing File Statistics","text":"<p>This section covers the dashboards that are available to view statistics relating to files. </p>"},{"location":"admin/viewing-file-statistics/#viewing-file-statistics_1","title":"Viewing file statistics","text":""},{"location":"admin/viewing-file-statistics/#source","title":"Source","text":""},{"location":"admin/viewing-file-statistics/#sink","title":"Sink","text":""},{"location":"admin/viewing-file-statistics/#file-operations","title":"File Operations","text":""},{"location":"admin/viewing-file-statistics/#viewing-file-source-statistics","title":"Viewing file source statistics","text":""},{"location":"admin/viewing-file-statistics/#viewing-file-sink-statistics","title":"Viewing file sink statistics","text":""},{"location":"admin/viewing-on-demand-query-statistics/","title":"Viewing On Demand Query Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-on-demand-query-statistics/#viewing-on-demand-query-statistics","title":"Viewing On Demand Query Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p>"},{"location":"admin/viewing-overall-statistics/","title":"Viewing Overall Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-overall-statistics/#viewing-overall-statistics","title":"Viewing Overall Statistics","text":"<p>This dashboard displays the overall Siddhi statistics of the Siddhi applications currently deployed in your WSO2 Streaming Integrator instance.</p> <p>There are nine other dashboards linked to the Siddhi overall statistics dashboard which represent the Siddhi components and their statistics. The Siddhi Grafana Dashboard represents the memory, latency and throughput statistics for the Siddhi components as follows.</p> No. Element Memory Latency Throughput                01                              Stream                              -                              -                              \u2714\ufe0f                              02                              Query                              \u2714\ufe0f                              \u2714\ufe0f                              -                              03                              Source                              -                              -                              \u2714\ufe0f                              04                              Sink                              -                              -                              \u2714\ufe0f                              05                              Source Mapper                              -                              \u2714\ufe0f                              -                              06                              Sink Mapper                              -                              \u2714\ufe0f                              -                              07                              Table                              -                              \u2714\ufe0f                              \u2714\ufe0f                              08                              Window                              \u2714\ufe0f                              \u2714\ufe0f                              \u2714\ufe0f                              09                              Aggregation                              \u2714\ufe0f                              \u2714\ufe0f                              \u2714\ufe0f                              10                              Trigger                              -                              -                              \u2714\ufe0f                              11                              On-Demand Query                              -                              \u2714\ufe0f                              -               <p>The following information is displayed in this dashboard.</p>"},{"location":"admin/viewing-overall-statistics/#servers-updown","title":"Servers Up/Down","text":"<p>This indicates the number of active servers against time. When a new server is started, it is indicated by a vertical line. You can move the cursor over this vertical line to check the host and port at which the new active server is running.</p> <p>This also provides a link to the Siddhi Server Statistics dashboard to view server statistics.</p> <p>Purpose</p> <p>This allows you to identify the following:</p> <ul> <li> <p>The times at which a specific server started and stopped.</p> </li> <li> <p>The duration of time for which a specific server was active.</p> </li> <li> <p>The number of servers that were active at a specific time.</p> </li> </ul> <p>Recommended action</p> <p>Check the throughput, system load average, memory usage, CPU usage etc., for the time periods during which a specific server/combination of servers was active in order to evaluate server performance.</p>"},{"location":"admin/viewing-overall-statistics/#siddhi-app-count","title":"Siddhi App Count","text":"<p>This indicates the total number of Siddhi applications deployed in the currently active servers.</p> <p>Purpose</p> <p>This allows you to get an overall understanding of the level of activity carried out by the currently active servers.</p> <p>Recommended action</p> <p>If the Siddhi application count is relatively high, you can check the throughput for each Siddhi application to identify the Siddhi applications that you can remove from your Streaming Integrator deployment to reduce the system overhead. To do this, you can filter statistics for each individual Siddhi application in the following dashboards: - Siddhi Stream Statistics - Siddhi Source Statistics - Siddhi Sink Statistics - Siddhi Query Statistics - Siddhi Window Statistics - Siddhi Trigger Statistics - Siddhi Table Statistics - Siddhi Aggregation Statistics - Siddhi On Demand Query Statistics</p>"},{"location":"admin/viewing-overall-statistics/#server-statistics-summary-table","title":"Server Statistics Summary Table","text":"<p>This lists the currently active Streaming Integrator servers and displays the following for each server:   - The total events received by the server   - The system load average   - The total memory used by each server.</p> <p>Purpose</p> <p>To evaluate the performance of each server as follows:</p> <ul> <li>By analysing the efficiency of the server by comparing its events received with the overhead it incurs in terms of the system load average and the memory used.</li> <li>By comparing the events received, system load average and the memory usage of each server with that of other servers.</li> </ul> <p>Recommended action</p> <p>Once you analyze the performance of each server, you can make changes to your Streaming Integrator deployment to use server resources in a more optimum manner (e.g., by removing low performing servers, adding better performing servers, etc.)</p>"},{"location":"admin/viewing-overall-statistics/#overall-throughput","title":"Overall Throughput","text":"<p>This shows the overall throughput of your current Streaming Integrator deployment. </p> <p>Purpose</p> <p>To monitor the overall throughput and evaluate it against other statistics such as the system load average, memory used, the number of Siddhi applications deployed in the system etc.</p> <p>Recommended action</p> <p>Determine whether the overall throughput of your Streaming Integrator server is sufficient based on the system resources allocated, and identify ways in which you can improve it (e.g., reducing Siddhi elements that are deployed, but do not generate a sufficient throughput, making adjustments to the system resources allocated. etc.)</p>"},{"location":"admin/viewing-overall-statistics/#system-load-average","title":"System Load Average","text":"<p>This shows the average system load of your current Streaming Integrator deployment.</p> <p>Purpose</p> <p>To monitor the system load average and compare it with other statistics such as the overall throughput, memory used, the number of Siddhi applications deployed in the system etc. and take appropriate measures to reduce it if it is too high, and to optimize the system better if it is relatively low.</p> <p>Recommended action</p> <p>If the system load average is too high, you can take appropriate measures to reduce it (by adding more system resources or by identifying Siddhi applications/elements generating a low throughput that you can remove). If the system load average is too low, you can find ways to optimise it (e.g., reducing the system resources allocated, etc.)</p>"},{"location":"admin/viewing-overall-statistics/#cpu-usage","title":"CPU Usage","text":"<p>This shows the CPU usage of your current Streaming Integrator deployment.</p> <p>Purpose</p> <p>To monitor the CPU usage of your current Streaming Integrator deployment and to identify scenarios where you may need to make changes to the CPU resource allocation.</p> <p>Recommended action</p> <p>If there is a spike in the CPU usage, you can identify whether you need to allocate more CPU resources to your deployment or redeploy some of the Siddhi elements with high throughputs to a different Streaming Integrator deployment.  Similarly, if there is a fall in the CPU usage, you can determine whether some of the CPU resources can be removed from the WSO2 Streaming Integrator deployment.</p>"},{"location":"admin/viewing-overall-statistics/#memory-usage","title":"Memory Usage","text":"<p>This shows the memory usage of your current Streaming Integrator deployment and to identify scenarios where you may need to make changes to the memory allocation.</p> <p>Purpose</p> <p>To monitor the memory usage of your Streaming Integrator deployment and allocate more memory when needed.</p> <p>Recommended action</p> <p>If there is a spike in the memory usage, you can identify whether you need to allocate more memory resources to your deployment or redeploy some of the Siddhi elements with high throughputs to a different Streaming Integrator deployment.  Similarly, if there is a fall in the memory usage, you can determine whether some of the memory resources can be removed from the WSO2 Streaming Integrator deployment.</p>"},{"location":"admin/viewing-overall-statistics/#thread-count","title":"Thread Count","text":"<p>This shows the number of JVM (Java Virtual Machine) threads that are currently active.</p> <p>Purpose</p> <p>You can use these statistics to profile your JVM.</p>"},{"location":"admin/viewing-overall-statistics/#threads-blocked","title":"Threads Blocked","text":"<p>This shows the number of JVM threads that are currently blocked.</p> <p>Purpose</p> <p>You can use these statistics to profile your JVM.</p>"},{"location":"admin/viewing-overall-statistics/#memory-heap-used","title":"Memory Heap Used","text":"<p>This shows the JVM memory heap that is currently consumed by your Streaming Integrator deployment.</p> <p>Purpose</p> <p>You can evaluate the performance of your Streaming Integrator servers based on the memory heap usage.</p>"},{"location":"admin/viewing-overall-statistics/#file-descriptors-open","title":"File Descriptors Open","text":"<p>The number of JVM file descriptors that are currently open.</p> <p></p>"},{"location":"admin/viewing-overall-statistics/#stream-statistics","title":"Stream Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of streams in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The throughput of each stream listed.</p> </li> <li> <p>The total stream count.</p> </li> </ul> <p>The Stream Statistics widget also provides a link to open the Siddhi Stream Statistics dashboard where you can view stream statistics.</p> <p>Purpose</p> <p>This allows you to monitor each stream in your Streaming Integrator deployment and identify the streams that generate a lot of activity.</p> <p>Recommended action</p> <p>Filter stream statistics for each Siddhi application to identify active Siddhi applications. You can also identify streams that are duplicated in different Siddhi applications and merge such Siddhi applications if possible to reduce system overhead.</p>"},{"location":"admin/viewing-overall-statistics/#query-statistics","title":"Query Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of queries in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The latency of each query listed.</p> </li> <li> <p>The total query count.</p> </li> </ul> <p>The Query Statistics widget also provides a link to the Siddhi Query Statistics dashboard where you can view query statistics.</p> <p>Purpose</p> <p>This allows you to monitor each query in your WSO2 Streaming Integrator deployment and identify the main queries to be reviewed in order to reduce the overall latency.</p> <p>Recommended action</p> <p>Identify queries with high latency and review them to use the Siddhi logic in a more efficient manner. You can also identify queries that are duplicated in different Siddhi applications and merge such Siddhi applications if possible to reduce system overhead</p>"},{"location":"admin/viewing-overall-statistics/#source-and-source-mapper-statistics","title":"Source and Source Mapper Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of sources in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The throughput of each source listed.</p> </li> <li> <p>The total source count in all the Siddhi applications you have selected to monitor.</p> </li> </ul> <p>The Source and Source Mapper Statistics widget also provides a link to the Siddhi Source Statistics dashboard where you can view source statistics.</p> <p>Purpose</p> <p>To monitor how each source contributes to the overall throughput of your Siddhi application deployment.</p> <p>Recommended action</p>"},{"location":"admin/viewing-overall-statistics/#sink-statistics","title":"Sink Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of sinks in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The throughput of each sink listed.</p> </li> <li> <p>The total sink count in all the Siddhi applications you have selected to monitor.</p> </li> </ul> <p>The Sink Statistics widget also provides a link to the Siddhi Sink Statistics dashboard where you can view sink statistics.</p> <p>Purpose</p> <p>To monitor the amount of data published by your WSO2 Streaming Integrator deployment to different destinations, brokers, files, databases and cloud storages.</p> <p>Recommended action</p>"},{"location":"admin/viewing-overall-statistics/#table-statistics","title":"Table Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of tables in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The throughput of each table listed.</p> </li> <li> <p>The total table count in all the Siddhi applications you have selected to monitor.</p> </li> </ul> <p>The Table Statistics widget also provides a link to the Siddhi Table Statistics dashboard where you can view table statistics.</p> <p>Purpose</p> <p>To monitor the tables defined in your WSO2 Streaming Integrator deployment and assessing the extent to which each table is used.</p> <p>Recommended action</p>"},{"location":"admin/viewing-overall-statistics/#window-statistics","title":"Window Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of windows in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The throughput of each window listed.</p> </li> <li> <p>The total window count in all the Siddhi applications you have selected to monitor.</p> </li> </ul> <p>The Window Statistics widget also provides a link to the Siddhi Window Statistics dashboard where you can view window statistics.</p> <p>Purpose</p> <p>To monitor the windows defined in your WSO2 Streaming Integrator deployment and assessing the extent to which each window is used.</p> <p>Recommended action</p>"},{"location":"admin/viewing-overall-statistics/#aggregation-statistics","title":"Aggregation Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of aggregations in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The throughput of each aggregation listed.</p> </li> <li> <p>The total aggregation count in all the Siddhi applications you have selected to monitor.</p> </li> </ul> <p>The Aggregation Statistics widget also provides a link to the Siddhi Aggregation Statistics dashboard where you can view aggregation statistics.</p> <p>Purpose</p> <p>To monitor the aggregations defined in your WSO2 Streaming Integrator deployment and assessing the extent to which each aggregation is used.</p> <p>Recommended action</p>"},{"location":"admin/viewing-overall-statistics/#trigger-statistics","title":"Trigger Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of triggers in all the Siddhi applications that you are monitoring in your Streaming Integrator deployment.</p> </li> <li> <p>The throughput of each trigger listed.</p> </li> <li> <p>The total trigger count in all the Siddhi applications you have selected to monitor.</p> </li> </ul> <p>The Trigger Statistics widget also provides a link to the Siddhi Trigger Statistics dashboard where you can view trigger statistics.</p> <p>Purpose</p> <p>To monitor the triggers defined in your WSO2 Streaming Integrator deployment and assessing the extent to which each trigger is used.</p> <p>Recommended action</p>"},{"location":"admin/viewing-overall-statistics/#on-demand-query-statistics","title":"On Demand Query Statistics","text":"<p>This indicates the following:</p> <ul> <li> <p>The complete list of on demand queries you have carried out via REST API for the Siddhi applications you have selected to monitor.</p> </li> <li> <p>The throughput of each on-demand query listed.</p> </li> <li> <p>The total on-demand query count.</p> </li> </ul> <p>The On-Demand Query Statistics widget also provides a link to the Siddhi On-Demand Query Statistics dashboard where you can view on-demand query statistics.</p> <p>Purpose</p> <p>Recommended action</p>"},{"location":"admin/viewing-overview-statistics/","title":"Viewing overview statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-overview-statistics/#viewing-overview-statistics","title":"Viewing Overview Statistics","text":""},{"location":"admin/viewing-overview-statistics/#viewing-overall-statistics","title":"Viewing overall statistics","text":"<p>This dashboard displays statistics related to streaming events that are received and/or published by all the file, RDBMS, and CDC related Siddhi extensions.</p>"},{"location":"admin/viewing-overview-statistics/#total-inputs","title":"Total Inputs","text":"<p>This indicates all the streaming events received by WSO2 Streaming Integrator via CDC and file related Siddhi extensions.</p>"},{"location":"admin/viewing-overview-statistics/#total-outputs","title":"Total Outputs","text":"<p>This indicates all the streaming events published by WSO2 Enterprise Integrator via CDC and file related Siddhi extensions.</p>"},{"location":"admin/viewing-overview-statistics/#overview-statistics","title":"Overview Statistics","text":""},{"location":"admin/viewing-overview-statistics/#consumepublishsec","title":"(Consume/Publish)/Sec","text":""},{"location":"admin/viewing-overview-statistics/#viewing-application-statistics","title":"Viewing application statistics","text":""},{"location":"admin/viewing-overview-statistics/#total-inputs_1","title":"Total Inputs","text":""},{"location":"admin/viewing-overview-statistics/#inputssec","title":"Inputs/Sec","text":""},{"location":"admin/viewing-overview-statistics/#total-outputs_1","title":"Total Outputs","text":""},{"location":"admin/viewing-overview-statistics/#outputssec","title":"Outputs/Sec","text":""},{"location":"admin/viewing-overview-statistics/#transport-level-errors","title":"Transport Level Errors","text":""},{"location":"admin/viewing-overview-statistics/#mapping-errors","title":"Mapping Errors","text":""},{"location":"admin/viewing-overview-statistics/#inputsoutputssec","title":"(Inputs/Outputs)Sec","text":""},{"location":"admin/viewing-overview-statistics/#sources","title":"Sources","text":""},{"location":"admin/viewing-overview-statistics/#destinations","title":"Destinations","text":""},{"location":"admin/viewing-query-statistics/","title":"Viewing query statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-query-statistics/#viewing-sink-statistics","title":"Viewing Sink Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p> <ul> <li>The throughput of each query</li> <li>The total query count</li> <li>The memory consumed by the queries over time.</li> <li>The latency of the queries over time</li> </ul>"},{"location":"admin/viewing-rdbms-statistics/","title":"Viewing rdbms statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-rdbms-statistics/#viewing-rdbms-statistics","title":"Viewing RDBMS Statistics","text":""},{"location":"admin/viewing-rdbms-statistics/#viewing-rdbms-statistics_1","title":"Viewing RDBMS statistics","text":""},{"location":"admin/viewing-rdbms-statistics/#total-tables","title":"Total tables","text":""},{"location":"admin/viewing-rdbms-statistics/#total-inserts","title":"Total Inserts","text":""},{"location":"admin/viewing-rdbms-statistics/#total-updates","title":"Total Updates","text":""},{"location":"admin/viewing-rdbms-statistics/#total-deletes","title":"Total Deletes","text":""},{"location":"admin/viewing-rdbms-statistics/#changed-rows","title":"Changed Rows","text":""},{"location":"admin/viewing-rdbms-statistics/#rdbms-statistics","title":"RDBMS Statistics","text":""},{"location":"admin/viewing-rdbms-statistics/#viewing-rdbms-table-statistics","title":"Viewing RDBMS table statistics","text":""},{"location":"admin/viewing-server-statistics/","title":"Viewing Server Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-server-statistics/#viewing-server-statistics","title":"Viewing Server Statistics","text":"<p>Siddhi Server Statistics Dashboard represents a detailed view of the active server instances. It also includes the JVM metrics related to the active servers.</p> <p>The information displayed is as follows.</p> <p></p> <p>This indicates the number of active servers against time. When a new server is started, it is indicated by a vertical line. You can move the cursor over this vertical line to check the host and port at which the new active server is running.</p> <p>Purpose</p> <p>This allows you to identify the following:</p> <ul> <li> <p>The times at which a specific server started and stopped.</p> </li> <li> <p>The duration of time for which a specific server was active.</p> </li> <li> <p>The number of servers that were active at a specific time.</p> </li> </ul> <p>Recommended action</p> <p>Check the throughput, system load average, memory usage, CPU usage etc., for the time periods during which a specific server/combination of servers was active in order to evaluate server performance.</p>"},{"location":"admin/viewing-server-statistics/#siddhi-app-count","title":"Siddhi App Count","text":"<p>This indicates the number of active servers against time. When a new server is started, it is indicated by a vertical line. You can move the cursor over this vertical line to check the host and port at which the new active server is running.</p> <p>This indicates the total number of Siddhi applications deployed in the currently active servers.</p> <p>Purpose</p> <p>This allows you to get an overall understanding of the level of activity carried out by the currently active servers.</p> <p>Recommended action</p> <p>If the Siddhi application count is relatively high, you can check the throughput for each Siddhi application to identify the Siddhi applications that you can remove from your Streaming Integrator deployment to reduce the system overhead. To do this, you can filter statistics for each individual Siddhi application in the following dashboards: - Siddhi Stream Statistics - Siddhi Source Statistics - Siddhi Sink Statistics - Siddhi Query Statistics - Siddhi Window Statistics - Siddhi Trigger Statistics - Siddhi Table Statistics - Siddhi Aggregation Statistics - Siddhi On Demand Query Statistics</p>"},{"location":"admin/viewing-server-statistics/#server-statistics-summary-table","title":"Server Statistics Summary Table","text":"<p>This lists the currently active Streaming Integrator servers and displays the following for each server:   - The total events received by the server   - The system load average   - The total memory used by each server.</p> <p>Purpose</p> <p>To evaluate the performance of each server as follows:</p> <ul> <li>By analysing the efficiency of the server by comparing its events received with the overhead it incurs in terms of the system load average and the memory used.</li> <li>By comparing the events received, system load average and the memory usage of each server with that of other servers.</li> </ul> <p>Recommended action</p> <p>Once you analyze the performance of each server, you can make changes to your Streaming Integrator deployment to use server resources in a more optimum manner (e.g., by removing low performing servers, adding better performing servers, etc.)</p>"},{"location":"admin/viewing-server-statistics/#overall-throughput","title":"Overall Throughput","text":"<p>This shows the overall throughput of all the servers in your current Streaming Integrator deployment. </p> <p>Purpose</p> <p>To monitor the overall throughput and evaluate it against other statistics such as the system load average, memory used, the number of Siddhi applications deployed in the system etc.</p> <p>Recommended action</p> <p>Determine whether the overall throughput of your Streaming Integrator server is sufficient based on the system resources allocated, and identify ways in which you can improve it (e.g., reducing Siddhi elements that are deployed, but do not generate a sufficient throughput, making adjustments to the system resources allocated. etc.)</p>"},{"location":"admin/viewing-server-statistics/#system-load-average","title":"System Load Average","text":"<p>This shows the average system load of your current Streaming Integrator deployment.</p> <p>Purpose</p> <p>To monitor the system load average and compare it with other statistics such as the overall throughput, memory used, the number of Siddhi applications deployed in the system etc. and take appropriate measures to reduce it if it is too high, and to optimize the system better if it is relatively low.</p> <p>Recommended action</p> <p>If the system load average is too high, you can take appropriate measures to reduce it (by adding more system resources or by identifying Siddhi applications/elements generating a low throughput that you can remove). If the system load average is too low, you can find ways to optimise it (e.g., reducing the system resources allocated, etc.)</p>"},{"location":"admin/viewing-server-statistics/#cpu-usage","title":"CPU Usage","text":"<p>This shows the CPU usage of your current Streaming Integrator deployment.</p> <p>Purpose</p> <p>To monitor the CPU usage of your current Streaming Integrator deployment and to identify scenarios where you may need to make changes to the CPU resource allocation.</p> <p>Recommended action</p> <p>If there is a spike in the CPU usage, you can identify whether you need to allocate more CPU resources to your deployment or redeploy some of the Siddhi elements with high throughputs to a different Streaming Integrator deployment.  Similarly, if there is a fall in the CPU usage, you can determine whether some of the CPU resources can be removed from the WSO2 Streaming Integrator deployment.</p>"},{"location":"admin/viewing-server-statistics/#memory-usage","title":"Memory Usage","text":"<p>This shows the memory usage of your current Streaming Integrator deployment.</p> <p>Purpose</p> <p>To monitor the memory usage of your Streaming Integrator deployment and allocate more memory when needed.</p> <p>Recommended action</p> <p>If there is a spike in the memory usage, you can identify whether you need to allocate more memory resources to your deployment or redeploy some of the Siddhi elements with high throughputs to a different Streaming Integrator deployment.  Similarly, if there is a fall in the memory usage, you can determine whether some of the memory resources can be removed from the WSO2 Streaming Integrator deployment.</p>"},{"location":"admin/viewing-server-statistics/#jvm-physical-memory","title":"JVM Physical Memory","text":"<p>The amount of JVM physical memory consumed by your WSO2 Streaming Integrator deployment over time.</p> <p>Purpose</p> <p>You can use these statistics to profile your JVM.</p>"},{"location":"admin/viewing-server-statistics/#jvm-threads","title":"JVM Threads","text":"<p>This shows the number of JVM threads that are currently active.</p> <p>Purpose</p> <p>You can use these statistics to profile your JVM.</p>"},{"location":"admin/viewing-server-statistics/#jvm-class-load","title":"JVM Class Load","text":"<p>Purpose</p> <p>You can use these statistics to profile your JVM.</p>"},{"location":"admin/viewing-sink-statistics/","title":"Viewing Query Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-sink-statistics/#viewing-query-statistics","title":"Viewing Query Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p>"},{"location":"admin/viewing-source-statistics/","title":"Viewing Source Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-source-statistics/#viewing-source-statistics","title":"Viewing Source Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p>"},{"location":"admin/viewing-stream-statistics/","title":"Viewing Stream Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-stream-statistics/#viewing-stream-statistics","title":"Viewing Stream Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p> <ul> <li>The throughput of each stream</li> <li>The total stream count</li> <li>A graphical representation of the throughput of each stream over time.</li> </ul>"},{"location":"admin/viewing-stream-statistics/#filtering-stream-statistics","title":"Filtering stream statistics","text":"<ul> <li> <p>If you want to view stream statistics only for a specific time interval, you can select a pre-defined time interval or define a custom time interval in the following field. This field is is located in the top right of the dashboard.</p> <p></p> </li> <li> <p>If you want to view statistics relating to streams in a one or more selected Streaming Integrator servers in your Streaming Integrator deployment, expand the Instance field and select the required instance(s).</p> <p>The stream statistics of all the servers </p> </li> <li> <p>If you want to view statistics relating to streams in one or more selected Siddhi appli</p> </li> </ul>"},{"location":"admin/viewing-stream-statistics/#purpose","title":"Purpose","text":"<p>To identify the active streams </p>"},{"location":"admin/viewing-table-statistics/","title":"Viewing Table Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-table-statistics/#viewing-table-statistics","title":"Viewing Table Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p>"},{"location":"admin/viewing-trigger-statistics/","title":"Viewing Trigger Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-trigger-statistics/#viewing-trigger-statistics","title":"Viewing Trigger Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p>"},{"location":"admin/viewing-window-statistics/","title":"Viewing Window Statistics","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"admin/viewing-window-statistics/#viewing-window-statistics","title":"Viewing Window Statistics","text":"<p>This dashboard displays the following information for your current WSO2 Streaming Integrator deployment:</p>"},{"location":"admin/working-with-Keystores/","title":"Working with Keystores","text":"<p>A keystore is a repository that stores the cryptographic keys and certificates that are used for various security purposes, such as encrypting sensitive information and for establishing trust between your server and outside parties that connect to your server. The usage of keys and certificates contained in a keystore are explained below.</p> <p>Key pairs :\u00a0According to public-key cryptography, a key pair (private key and the corresponding public key) is used for encrypting sensitive information and for authenticating the identity of parties that communicate with your server. For example, information that is encrypted in your server using the public key can only be decrypted using the corresponding private key. Therefore, if any party wants to decrypt this encrypted data, they should have the corresponding private key, which is usually kept as a secret (not publicly shared).</p> <p>Digital certificate :\u00a0When there is a key pair, it is also necessary to have a digital certificate to verify the identity of the keys. Typically, the public key of a key pair is embedded in this digital certificate, which also contains additional information such as the owner, validity, etc. of the keys.\u00a0For example, if an external party wants to verify the integrity of data or validate the identity of the signer (by validating the digital signature), it is necessary for them to have this digital certificate.</p> <p>Trusted certificates :\u00a0To establish trust, the digital certificate containing the public key should be signed by a trusted certifying authority (CA). You can generate self-signed certificates for the public key (thereby creating your own certifying authority), or you can get the certificates signed by an external CA. Both types of trusted certificates can be effectively used depending on the sensitivity of the information that is protected by the keys. When the certificate is signed by a reputed CA, all the parties who trust this CA also trust the certificates signed by them.</p> <p>Identity and Trust</p> <p>The key pair and the CA-signed certificates in a keystore establishes two security functions in your server: The key pair with the digital certificate is an indication of identity and the CA-signed certificate provides trust to the identity. Since the public key is used to encrypt information, the keystore containing the corresponding private key should always be protected, as it can decrypt the sensitive information. Furthermore, the privacy of the private key is important as it represents its own identity and protects the integrity of data. However, the CA-signed digital certificates should be accessible to outside parties that require to decrypt and use the information.</p> <p>To facilitate this requirement, the certificates must be copied to a separate keystore (called a Truststore), which can then be shared with outside parties. Therefore, in a typical setup, you have one keystore for identity (containing the private key) that is protected, and a separate keystore for trust (containing CA certificates) that is shared with outside parties.</p>"},{"location":"admin/working-with-Keystores/#setting-up-keystores-in-the-streaming-integrator","title":"Setting up keystores in the Streaming Integrator","text":"<p>The Streaming Integrator uses keystores mainly for the following purposes:</p> <ul> <li> <p>Authenticating the communication over Secure Sockets Layer (SSL)/Transport Layer Security (TLS) protocols.</p> </li> <li> <p>Communication over Secure Sockets Layer (SSL) when invoking Rest APIs.</p> </li> <li> <p>Protecting sensitive information via Cipher Tool.</p> </li> </ul>"},{"location":"admin/working-with-Keystores/#default-keystore-settings-in-wso2-products","title":"Default keystore settings in WSO2 products","text":"<p>The Streaming Integrator is shipped with the following default keystore files stored in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory.</p> <ul> <li> <p><code>wso2carbon.jks</code>:\u00a0This keystore contains a key pair and is used by default in your Streaming Integrator and Streaming Integrator Tooling servers for all of the purposes explained above, except protecting sensitive information via Cipher tool.</p> </li> <li> <p><code>securevault.jks</code>: This is the default keystore used by the secure vault to protect sensitive information via Cipher tool.</p> </li> <li> <p><code>client-truststore.jks</code>:\u00a0This is the default trust store that\u00a0contains the\u00a0trusted certificates of\u00a0the\u00a0keystore used in SSL communication.</p> </li> </ul> <p>By default, the following files provide paths to these keystores:</p> <ul> <li> <p><code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/wso2/server/bin/carbon.sh</code> file</p> <p>This script is run when you start an Streaming Integrator server. It contains the following parameters, and makes references to the two files mentioned above by default.</p> Parameter Default Value Description <code>keyStore</code> <code>\"$CARBON_HOME/resources/security/wso2carbon.jks\" \\</code> This specifies the path to the keystore to be used when running the Streaming Integrator server on a secure network. <code>keyStorePassword</code> <code>\"wso2carbon\" \\</code> The password to access the keystore. <code>trustStore</code> <code>\"$CARBON_HOME/resources/security/client-truststore.jks\" \\</code> This specifies the path to the trust store to be used when running the server on a secure network. <code>trustStorePassword</code> <code>\"wso2carbon\" \\</code> The password to access the trust store. </li> <li> <p><code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file refers to the above keystore and trust store by default for the following configurations:</p> </li> <li> <p>Listener configurations</p> <p>This specifies the key store to be used when the Streaming Integrator is receiving events via a secure network, and the password to access the key store.</p> </li> <li> <p>Databridge configurations</p> <p>This specifies the key store to be used when the Streaming Integrator is publishing events via databrige using a secure network, and the\u00a0password to access the key store.</p> </li> <li> <p>Secure vault configurations</p> <p>This specifies the key store to be used when you are configuring a secure vault to protect sensitive information.</p> </li> </ul> <p>Note</p> <p>It is recommended to replace this default keystore with a new keystore that has self-signed or CA signed certificates when the products are deployed in production environments. This is because <code>wso2carbon.jks</code> is available with open source WSO2 products, which means anyone can have access to the private key of the default keystore.</p>"},{"location":"admin/working-with-Keystores/#managing-keystores","title":"Managing keystores","text":"<p>You can view the default keystores and truststores in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory. Once you create your own keystore, you can delete the default keystores, but you need to ensure that they are no longer referenced  in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/wso2/server/bin/carbon.sh</code> file or the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p>"},{"location":"admin/working-with-Keystores/#creating-new-keystores","title":"Creating new keystores","text":"<p>The Streaming Integrator is shipped with two default keystores named <code>wso2carbon.jks</code> and <code>securevault.jks</code>. These keystores are stored in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory. They come with a private/public key pair that is used to encrypt sensitive information for communication over SSL and for encryption/signature purposes in WS-Security. However, note that because these keystores are available with open source WSO2 products, anyone can have access to the private keys of the default keystores. It is therefore recommended to replace these with keystores that have self-signed or CA signed certificates when the Streaming Integrator is deployed in production environments.</p>"},{"location":"admin/working-with-Keystores/#creating-a-keystore-using-an-existing-certificate","title":"Creating a keystore using an existing certificate","text":"<p>Secure Sockets Layer (SSL) is a protocol that is used to secure communication between systems. This protocol uses a public key, a private key and a random symmetric key to encrypt data. As SSL is widely used in many systems, certificates may already exist that can be reused. In such situations, you can use the CA-signed certificates to generate a Java keystore using OpenSSL and the Java keytool.</p> <ol> <li> <p>First, export certificates to the PKCS12/PFX format. Give strong passwords whenever required.</p> <p>Info</p> <p>It is required to have same password for both the keystore and key.</p> <p>Execute the following command to export the certificates:</p> <p><code>openssl pkcs12 -export -in &lt;certificate file&gt;.crt -inkey &lt;private&gt;.key -name \"&lt;alias&gt;\" -certfile &lt;additional certificate file&gt; -out &lt;pfx keystore name&gt;.pfx</code></p> </li> <li> <p>Convert the PKCS12 to a Java keystore using the following command:</p> <p><code>keytool -importkeystore -srckeystore &lt;pkcs12 file name&gt;.pfx -srcstoretype pkcs12 -destkeystore &lt;JKS name&gt;.jks -deststoretype JKS</code></p> <p>Now you have a keystore with CA-signed certificates.</p> </li> </ol>"},{"location":"admin/working-with-Keystores/#creating-a-keystore-using-a-new-certificate","title":"Creating a keystore using a new certificate","text":"<p>If there are no certificates signed by a Certification Authority, you can follow the steps in this section to create a keystore with keys and a new certificate. In the following steps, you are using the keytool that is available with your JDK installation.</p> <p>Step 1: Creating keystore with private key and public certificate</p> <ol> <li> <p>Open a command prompt and go to the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory. All the keystores should be stored here.</p> </li> <li> <p>Create the keystore that includes the private key by executing the following command:</p> <p><code>keytool -genkey -alias certalias -keyalg RSA -keysize 2048 -keystore newkeystore.jks -dname \"CN=&lt;testdomain.org&gt;,OU=Home,O=Home,L=SL,S=WS,C=LK\" -storepass mypassword -keypass mypassword</code></p> <p>This command creates a keystore with the following details:</p> <ul> <li> <p>Keystore name: <code>newkeystore.jks</code></p> </li> <li> <p>Alias of public certificate: <code>certalias</code></p> </li> <li> <p>Keystore password: <code>mypassword</code></p> </li> <li> <p>Private key password: <code>mypassword</code> (this is required to be the same as keystore password)</p> </li> </ul> <p>Tip</p> <p>If you did not specify values for the <code>'-keypass'</code> and the <code>'-storepass'</code> in the above command, you are requested to give a value for the <code>'-storepass'</code> (password of the keystore). As a best practice, use a password generator to generate a strong password. You are then asked to enter a value for <code>-keypass</code>. Click Enter, because we need the same password for both the keystore and the key. Also, if you did not specify values for <code>-dname</code>, you will be asked to provide those details individually.</p> </li> <li> <p>Open the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory and check if the new keystore file is created. Make a backup of it and move it to a secure location. This is important because if not your private key is available only on one location.</p> </li> </ol> <p>Step 2: Creating CA-signed certificates for public key</p> <p>Now we have a <code>.jks</code> file. This keystore (<code>.jks</code> file) can be used to generate a certificate signing request (CSR). This CSR file must be certified by a certificate authority or certification authority (CA), which is an entity that issues digital certificates. These certificates can certify the ownership of a public key.</p> <ol> <li> <p>Execute the following command to generate the CSR:</p> <p><code>keytool -certreq -alias certalias -file newcertreq.csr -keystore newkeystore.jks</code></p> <p>Tip</p> <p>As mentioned before, use the same alias that you used during the keystore creation process.</p> <p>You are asked to give the keystore password. Once the password is given, the command outputs the <code>newcertreq.csr</code> file to the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory. This is the CSR that you must submit to a CA.</p> </li> <li> <p>You must provide this CSR file to the CA. For testing purposes, try the 90 days trial SSL certificate from Comodo.</p> <p>Info</p> <p>It is preferable to have a wildcard certificate or multiple domain certificates if you wish to have multiple sub-domains like gateway.sampledomain.org, publisher.sampledomain.org, identity.sampledomain.org, etc., for the deployment. For such requirements you must modify the CSR request by adding subject alternative names. Most of the SSL providers give instructions to generate the CSR in such cases.</p> </li> <li> <p>After accepting the request, a signed certificate is provided along with several intermediate certificates (depending on the CA) as a bundle (.zip file).</p> <p>!!!info \"The following is a sample certificate by the CA (Comodo)</p> <pre><code>```text\nThe Root certificate of the CA: AddTrustExternalCARoot.crt\nIntermediate certificates:  COMODORSAAddTrustCA.crt , COMODORSADomainValidationSecureServerCA.crt\nSSL Certificate signed by CA: test_sampleapp_org.crt\n```</code></pre> </li> </ol> <p>Step 3: Importing CA-signed certificates to keystore</p> <ol> <li> <p>Before importing the CA-signed certificate to the keystore, add the root CA certificate and the two intermediate certificates by executing the commands given below. Note that the sample certificates given above are used as examples.</p> <pre><code>keytool -import -v -trustcacerts -alias ExternalCARoot -file AddTrustExternalCARoot.crt -keystore newkeystore.jks -storepass mypassword\nkeytool -import -v -trustcacerts -alias TrustCA -file COMODORSAAddTrustCA.crt -keystore newkeystore.jks -storepass mypassword\nkeytool -import -v -trustcacerts -alias SecureServerCA -file COMODORSADomainValidationSecureServerCA.crt -keystore newkeystore.jks -storepass mypassword\n</code></pre> <p>Info</p> <p>Optionally, you can append the <code>-storepass &lt;keystore password&gt;</code> option to avoid having to enter the password when prompted later in the interactive mode.</p> </li> <li> <p>After you add the root certificate and all other intermediate certificates, add the CA-signed SSL certificate to the keystore by executing the following command:</p> <p><code>keytool -import -v -alias &lt;certalias&gt; -file &lt;test_sampleapp_org.crt&gt; -keystore newkeystore.jks -keypass myppassword -storepass mykpassword</code></p> <p>Tip</p> <p>In this command, use the same alias that you used when you created the keystore.</p> </li> </ol> <p>Now you have a Java keystore including a CA-signed certificate that can be used in a production environment. Next, you must add its public key to the <code>client-truststore.jks</code> file to enable backend communication and inter-system communication via SSL.</p> <p>Adding the public key to client-truststore.jks</p> <p>In SSL handshake, the client needs to verify the certificate presented by the server. For this purpose, the client usually stores the certificates it trusts in a trust store. The Streaming Integrator is shipped with the trust store named <code>client-truststore.jks</code> that resides in the same directory as the keystore (i.e., <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/</code>). Therefore, you need to import the new public certificate into this trust store for front-end and backend communication of the Streaming Integrator to take place in the required manner over SSL.</p> <p>Tip</p> <p>In this example, you are using the default <code>client-truststore.jks</code> file in the Streaming Integrator as the trust store.</p> <p>To add the public key of the signed certificate to the client trust store:</p> <ol> <li> <p>Get a copy of the <code>client-truststore.jks</code> file from the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory.</p> </li> <li> <p>Export the public key from your <code>.jks</code> file using the by issuing the following command.</p> <p><code>keytool -export -alias certalias -keystore newkeystore.jks -file &lt;public key name&gt;.pem</code></p> </li> <li> <p>Import the public key you extracted in the previous step to the <code>client-truststore.jks</code> file by issuing the following command.</p> <p><code>keytool -import -alias certalias -file &lt;public key name&gt;.pem -keystore client-truststore.jks -storepass wso2carbon</code></p> <p>Tip</p> <p><code>wso2carbon</code> is the keystore password of the default <code>client-truststore.jks</code> file.</p> </li> </ol> <p>Now, you have an SSL certificate stored in a Java keystore and a public key added to the <code>client-truststore.jks</code> file. Note that both these files should be placed in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security</code> directory. You can now replace the default <code>wso2carbon.jks</code> keystore in your product with the newly created keystore by updating the relevant configuration files in your product. For more information, see Configuring Keystores.</p>"},{"location":"admin/working-with-Keystores/#configuring-keystores","title":"Configuring keystores","text":"<p>Once you have created a new key store and updated the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/resources/security/client-truststore.jks</code> file, you must update the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/&lt;PROFILE/deployment.yaml</code> file to make that keystore work for the required functions. Keystores are used for multiple functions in the Streaming Integrator including securing the servlet transport, databridge communication, encrypting confidential information in configuration files etc.</p> <p>Tip</p> <ul> <li>The <code>wso2carbon.jks</code> keystore file that is shipped with the Streaming Integrator is used as the default keystore for all functions. However, in a production environment, it is recommended to create new keystores with keys and certificates because the Streaming Integrator is an open source integrator, and anyone who downloads it has access to the default keystore.</li> <li>To find all the functions that require a keystore, you can search for <code>.jks</code> in the deployment.yaml file.</li> </ul> <p>e.g., If you want to secure the listener configured for the Streaming Integrator using a keystore, you can enter details relating to the keystore as shown below. In this example, the details of the default key is used.</p> <pre><code>listenerConfigurations:\n-\nid: \"msf4j-https\"\nhost: \"0.0.0.0\"\nport: 9743\nscheme: https\nkeyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\"\nkeyStorePassword: wso2carbon\ncertPass: wso2carbon\n</code></pre> Parameter Description <code>keyStoreFile</code> The path to the keystore file. <code>keyStorePassword</code> The password with which the keystore can be accessed. <code>certPass</code> The alias of the public certificate issued by the certification authority."},{"location":"concepts/concepts/","title":"Key Concepts","text":"Concept Description Streaming Data A continuous flow of data generated by one or more sources. A data stream is often made of discrete data bundles that are transmitted one after the other. These data bundles are often referred to as events or messages. Stream Processing A paradigm for processing streaming data. Stream processing lets users process continuous streams of data on the fly and derive results faster in near-real-time.  This contrasts with conventional processing methods that require data to be stored prior to processing. Streaming Data Integration The process that makes streaming data available for downstream destinations. Each consumer may expect data in different formats, protocols, or mediums (DB, file, Network, etc.). These requirements are addressed via Streaming Data Integration that involves converting data into different formats and publishing data in different protocols through different mediums. The data data is often processed using Stream Processing and other techniques to add value before integrating data with its consumers. Streaming Integration In contrast to Streaming Data Integration that only focuses on making streaming data available for downstream, Streaming Integration involves integrating streaming data as well as trigger action based on data streams. The action can be a single request to a service or a complex enterprise integration flow. Data Transformation This refers to converting data from one format to another (e.g, JSON to XML) or altering the original structure of data. Data Cleansing This refers to filtering out corrupted, inaccurate or irrelevant data from a data stream based on one or more conditions. Data cleansing also involves modifying/replacing content to hide/remove unwanted data parts from a message (e.g., obscuring). Data Correlation This refers to combining different data items based on relationships between data. In the context of streaming data, it is often required to correlate data across multiple streams received from different sources. The relationship can be expressed in terms of a boolean condition, sequence or a pattern of occurrences of data items. Data Enrichment The process of adding value to raw data by enhancing and improving data by adding more information that is more consumable for users. Data Summarization This refers to aggregating data to produce a summarized representation of raw data. In the context of Stream Processing, such aggregations can be done based on temporal criteria in real-time with moving time windows. Real-time ETL This refers to performing data extraction, transformation, and loading in real-time. In contrast to traditional ETL that uses batch processing, Real-time ETL extracts data as and when it is available, transforms it on the fly, and integrates it. Change Data Capture A technique or a process that makes it possible to capture changes in the data in real-time. This enables real-time ETL with databases because it allows users to receive real-time notifications when data in the database tables are changing."},{"location":"connectors/configuring-System-Parameters-for-Siddhi-Extensions/","title":"Configuring System Parameters for Siddhi Extensions","text":"<p>The pre-written Siddhi extensions supported by the Streaming Integrator are configured with default values for system  parameters. If you need to override those values, you can refer to those extensions from the  <code>&lt;SI_HOME&gt;/conf/&lt;RUNTIME&gt;/deployment.yaml</code> file and add the system parameters with the required values as key-value  pairs. To do this, follow the procedure below:</p> <ol> <li> <p>Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.    </p> </li> <li> <p>The extensions belong to the Siddhi component. Therefore, to edit the Siddhi component, add a main section to the     file named <code>siddhi</code>. Then add a subsection named <code>extensions</code> to indicate that the configurations related to Siddhi     extensions as shown below.</p> </li> </ol> <pre><code>siddhi:\n  extensions:\n</code></pre> <ol> <li> <p>For each separate extension you want to configure, add a sub-section named <code>extension</code> under the <code>extensions</code> subsection.     <pre><code>siddhi:\n  extensions:\n    - extension:\n</code></pre></p> </li> <li> <p>Under each <code>extension</code> subsection, add two key-value pairs as follows.</p> Key Value <code>name</code> The name of the extension. e.g., <code>tcp</code> <code>namespace</code> The archetype of the extension. e.g., <code>source</code> <p>Info</p> <p>The archetypes of extensions supported are <code>source</code>, <code>sink</code>, <code>execution</code>, <code>io</code>, <code>map</code> , <code>script</code>, and <code>store</code>.</p> </li> <li> <p>Add a subsection named properties to overide the system properties. Then add the system properties with the required     values as key value pairs, as shown below.</p> <pre><code>siddhi: extensions: - extension: name: [extension-name]\n        namespace: [extension-namespace]\n        properties: [key]: [value]\n</code></pre> </li> </ol> <p>Following are examples for overriding default values for system properties.</p> <p>Example 1: Defining host and port for TCP</p> <pre><code>siddhi: extensions: - extension: name: tcp\n        namespace: source\n        properties: host: 0.0.0.0\n          port: 5511\n</code></pre> <p>Example 2: Overwriting the default RDBMS configuration</p> <pre><code>siddhi:\n  extensions:\n    - extension:\n        name: rdbms\n        namespace: store\n        properties:\n          mysql.batchEnable: true\n          mysql.batchSize: 1000\n          mysql.indexCreateQuery: \"CREATE INDEX {{ TABLE_NAME }}_INDEX ON {{ TABLE_NAME }} ({{ INDEX_COLUMNS }})\"\n          mysql.recordDeleteQuery: \"DELETE FROM {{ TABLE_NAME }} {{ CONDITION }}\"\n          mysql.recordExistsQuery: \"SELECT 1 FROM {{ TABLE_NAME }} {{ CONDITION }} LIMIT 1\"\n</code></pre>"},{"location":"connectors/connectors-overview/","title":"Streaming Integrator Connectors Overview","text":"<p>WSO2 Streaming Integrator is powered by Siddhi. Siddhi supports an extension architecture to enhance its functionality by incorporating other libraries in a seamless manner.</p>"},{"location":"connectors/connectors-overview/#purpose","title":"Purpose","text":"<p>Streaming Integration use cases may require a wide range of functionalities. Extensions are supported because it is not possible to have all this functionality within the Siddhi core. If the functionality covered by the supported extensions have gaps when addressing the requirements of your use cases, you can write a custom extension.</p> <p>All extensions have a namespace. This is used to identify the relevant extensions together, and to let you specifically call the extension.</p>"},{"location":"connectors/connectors-overview/#syntax","title":"Syntax","text":"<p>The syntax of an extension is as follows:</p> <pre><code>    &lt;namespace&gt;:&lt;function name&gt;(&lt;parameter&gt;, &lt;parameter&gt;, ... )\n</code></pre> <p>The following parameters are configured when referring a script function.</p> Parameter Description <code>namespace</code> Allows Siddhi to identify the extension without conflict. <code>function name</code> The name of the function referred. <code>parameter</code> The function input parameter for function execution."},{"location":"connectors/connectors-overview/#extension-types","title":"Extension Types","text":"<p>Siddhi supports the following extension types:</p> <ul> <li> <p>Function</p> <p>For each event, this consumes zero or more parameters as input parameters and returns a single attribute. This can be used to manipulate existing event attributes to generate new attributes like any Function operation. This is implemented by extending <code>io.siddhi.core.executor.function.FunctionExecutor</code>.</p> <p>e.g., <code>math:sin(x)</code></p> <p>Here, the <code>sin</code> function of math extension returns the sin value for the <code>x</code> parameter.</p> </li> <li> <p>Aggregate Function</p> <p>For each event, this consumes zero or more parameters as input parameters and returns a single attribute with aggregated results. This can be used in conjunction with a window in order to find the aggregated results based on the given window like any Aggregate Function operation.</p> <p>This is implemented by extending <code>io.siddhi.core.query.selector.attribute.aggregator.AttributeAggregatorExecutor</code>.</p> <p>e.g., <code>custom:std(x)</code></p> <p>Here, the std aggregate function of the <code>custom</code> extension returns the standard deviation of the <code>x</code> value based on its assigned window query.</p> </li> <li> <p>Window</p> <p>This allows events to be collected, generated, dropped and expired anytime without altering the event format based on the given input parameters, similar to any other Window operator.</p> <p>This is implemented by extending <code>io.siddhi.core.query.processor.stream.window.WindowProcessor</code>.</p> <p>e.g., <code>custom:unique(key)</code></p> <p>Here, the <code>unique</code> window of the <code>custom</code> extension retains one event for each unique <code>key</code> parameter.</p> </li> <li> <p>Stream Function</p> <p>This allows events to be generated or dropped only during event arrival. It also allows events to be altered by adding one or more attributes to it.</p> <p>This is implemented by extending <code>io.siddhi.core.query.processor.stream.function.StreamFunctionProcessor</code>.</p> <p>e.g., <code>custom:pol2cart(theta,rho)</code></p> <p>Here, the <code>pol2cart</code> function of the <code>custom</code> extension returns all the events by calculating the cartesian coordinates <code>x</code> &amp; <code>y</code> and adding them as new attributes to the events.</p> </li> <li> <p>Stream Processor</p> <p>This allows events to be collected, generated, dropped and expired anytime by altering the event format. Altering the event format is done by adding one or more attributes to it based on the given input parameters.</p> <p>This is implemented by extending <code>io.siddhi.core.query.processor.stream.StreamProcessor</code>.</p> <p>e.g., <code>custom:perMinResults(&lt;parameter&gt;, &lt;parameter&gt;, ...)</code></p> <p>Here, the <code>perMinResults</code> function of the <code>custom</code> extension returns all events by adding one or more attributes to the events based on the conversion logic. Altered events are output every minute regardless of event arrivals.</p> </li> <li> <p>Sink</p> <p>Sinks provide a way to publish Siddhi events to external systems in the preferred data format. Sinks publish events from the streams to external endpoints via multiple transports and in various data formats.</p> <p>This is implemented by extending <code>io.siddhi.core.stream.output.sink.Sink</code>.</p> <p>e.g., @sink(type='sink_type', static_option_key1='static_option_value1')</p> <p>To configure a stream to publish events via a sink, connect the sink configuration to a stream definition by adding the <code>@sink</code> annotation with the required parameter values. The sink syntax is as given above.</p> </li> <li> <p>Source</p> <p>Source allows Siddhi to consume events from external systems and map the events to adhere to the associated stream. Sources receive events via multiple transports and in various data formats, and direct them into streams for processing.</p> <p>This is implemented by extending <code>io.siddhi.core.stream.input.source.Source</code>.</p> <p>e.g., <code>@source(type='source_type', static.option.key1='static_option_value1')</code></p> <p>To configure a stream that consumes events via a source, connect a source configuration to a stream definition by adding the <code>@source</code> annotation with the required parameter values. The source syntax is as given above.</p> </li> <li> <p>Store</p> <p>You can use the Store extension type to work with data/events stored in various data stores through the table abstraction.</p> <p>This is implemented by extending <code>io.siddhi.core.table.record.AbstractRecordTable</code>.</p> </li> <li> <p>Script</p> <p>Scripts allow you to define a function operation that is not provided in Siddhi core or its extension. It is not required to write an extension to define the function logic. Scripts allow you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function.</p> <p>This is implemented by extending <code>io.siddhi.core.function.Script</code>.</p> </li> <li> <p>Source Mapper</p> <p>Each <code>@source</code> configuration has a mapping denoted by the <code>@map</code> annotation that converts the format of incoming messages to Siddhi events. The <code>type</code> parameter of <code>@map</code> defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional.</p> <p>This is implemented by extending <code>io.siddhi.core.stream.output.sink.SourceMapper</code>.</p> <p>e.g., <code>@map(type='map_type', static_option_key1='static_option_value1')</code></p> </li> <li> <p>Sink Mapper</p> <p>Each <code>@sink</code> configuration has a mapping denoted by the <code>@map</code> annotation that converts the outgoing Siddhi events to the format of the configured message. The <code>type</code> parameter of <code>@map</code> defines the map type to be used to map the data. The other parameters to be configured depends on the mapper selected. Some of these parameters are optional.</p> <p>This is implemented by extending <code>io.siddhi.core.stream.output.sink.SinkMapper</code>.</p> <p>e.g., <code>@map(type='map_type', static_option_key1='static_option_value1')</code></p> </li> </ul>"},{"location":"connectors/connectors-overview/#example","title":"Example","text":"<p>A window extension created with <code>foo</code>as the namespace and <code>unique</code> as the function name can be referred in a Siddhi query as shown below.</p> <pre><code>    from StockExchangeStream[price &gt;= 20]#window.foo:unique(symbol)\nselect symbol, price\ninsert into StockQuote\n</code></pre>"},{"location":"connectors/connectors-overview/#available-extensions","title":"Available Extensions","text":"<p>For the complete list of pre-written Siddhi extensions that are currently available, see Siddhi Query Guide -  Extensions.</p>"},{"location":"connectors/connectors-overview/#further-references","title":"Further References","text":"<ul> <li>For instructions to download and install a Siddhi extension, see Downloading and Installing Siddhi Extensions.</li> <li>If you want to install/uninstall one or more Siddhi extensions in Streaming Integrator Tooling, see Installing Siddhi Extensions.</li> </ul>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/","title":"Downloading and Installing Siddhi Extensions","text":"<p>The Siddhi extensions supported for the Streaming Integrator are shipped with the product by default.\u00a0If you need to download and install a different version of an extension, you can download it via the command line or manually as covered in the following sections.</p>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#downloading-and-installing-siddhi-extensions-via-the-command-line","title":"Downloading and installing Siddhi extensions via the command line","text":"<p>To manage Siddhi extensions via the command line, see the following topics.</p>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#identifying-the-siddhi-extensions-to-installuninstall","title":"Identifying the Siddhi extensions to install/uninstall","text":"<p>The following are some actions that you are required to perform in order to identify the Siddhi extensions you need to install. Navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory in the CLI to issue these commands.</p> <ul> <li> <p>Viewing the list of extensions that are currently installed</p> <p>You can view the complete list of Siddhi extensions that are currently installed in your Streaming Integrator setup. All the extensions listed are completely installed with the dependencies.</p> <p>To perform this action, issue the appropriate command out of the following based on your operating system:</p> <ul> <li>For Windows     : <code>extension-installer.bat list</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh list</code></li> </ul> <p>The following is a sample response log for this command.</p> <p></p> </li> <li> <p>Viewing the installation status of all the supported Siddhi extensions</p> <p>You can view the complete list of Siddhi extensions supported for WSO2 Streaming Integrator together with the current installation status for each extension.</p> <p>The installation status can be one of the following:</p> Installation Status Description INSTALLED This indicates that the extension is completely installed. The installation includes the JAR of the extension itself as well as all its dependencies (if any). NOT_INSTALLED This indicates that the extension has not been installed. The JAR of the extension itself has not been installed. Dependencies (if any) may be already installed due to shared dependencies. PARTIALLY_INSTALLED This indicates that the JAR of the extension itself has been installed, but one or more dependencies of the extension still need to be installed. When this status is displayed with an asterisk (i.e., <code>PARTIALLY_INSTALLED (*)</code>), it means that there are one or more dependencies that need to be manually installed for the extension. If an extension has this status, you can view more information about the dependencies to be installed by checking the installation status of that specific extension individually. <p>To perform this action, issue the appropriate command out of the following based on your operating system:</p> <ul> <li>For Windows     : <code>extension-installer.bat list --all</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh list --all</code></li> </ul> <p>The following is a sample response for this command.</p> <p></p> </li> <li> <p>Checking the installation status of a specific Siddhi extension</p> <p>You can view the installation status of a specific extension individually together with details of dependencies that need to be manually downloaded (if any exist).</p> <p>To perform this action, issue the appropriate command out of the following based on your operating system:</p> <ul> <li>For Windows     : <code>extension-installer.bat list &lt;EXTENSION_NAME&gt;</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh list &lt;EXTENSION_NAME&gt;</code> </li> </ul> <p>Info</p> <p>Here, the <code>&lt;EXTENSION_NAME&gt;</code> refers to the name of the extension. When you use the command line to view the list of extensions that are currently installed or to view the installation status of all the supported Siddhi extensions, the extension names are displayed in the <code>name</code> column.e.g., The extension name of the gRPC extension is <code>grpc</code>.</p> <p>e.g., To view the installation status of the <code>cdc-oracle</code> extension (which is partially installed by default), issue the following command:</p> <ul> <li>For Windows     : <code>extension-installer.bat list cdc-oracle</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh list cdc-oracle</code></li> </ul> <p>The sample response is as follows.</p> <p></p> </li> </ul>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#installing-siddhi-extensions","title":"Installing Siddhi Extensions","text":""},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#installing-all-extensions-required-for-currently-deployed-siddhi-applications","title":"Installing all extensions required for currently deployed Siddhi applications","text":"<p>If the Siddhi applications deployed in your WSO2 Streaming Integrator setup use Siddhi extensions that are not currently installed, you can automatically install all those extensions at once. To do this, issue the appropriate command out of the following based on your operating system.</p> <ul> <li>For Windows     : <code>extension-installer.bat install</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh install</code> </li> </ul> <p>e.g., If a Siddhi application that is currently deployed in your WSO2 Streaming Integrator setup uses the Amazon S3 extension, and if this extension is not already installed, you can issue the command given above. As a result, the following message appears in the terminal informing you of extensions that are used in Siddhi applications, but not installed. It also prompts you to specify whether you want to install them.</p> <p></p> <p>If you enter <code>y</code> to specify that you want to proceed with the installation, the following message appears to inform you of the status of the installation and to prompt you to restart the WSO2 Streaming Integrator server once the installation is complete.</p> <p></p>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#installing-a-specific-siddhi-extension","title":"Installing a specific Siddhi extension","text":"<p>If you want to install a specific Siddhi extension, issue the appropriate command out of the following based on your operating system.</p> <ul> <li>For Windows     : <code>extension-installer.bat install &lt;EXTENSION_NAME&gt;</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh install &lt;EXTENSION_NAME&gt;</code> </li> </ul> <p>Info</p> <p>Here, the <code>&lt;EXTENSION_NAME&gt;</code> refers to the name of the extension. When you use the command line to view the list of extensions that are currently installed or to view the installation status of all the supported Siddhi extensions, the extension names are displayed in the <code>name</code> column.e.g., The extension name of the gRPC extension is <code>grpc</code>.</p> <p>e.g., To install the <code>grpc</code> Siddhi extension, issue the following command.</p> <ul> <li>For Windows     : <code>extension-installer.bat install grpc</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh install grpc</code> </li> </ul> <p>The following message appears to inform you of the status of the installation and to prompt you to restart the WSO2 Streaming Integrator server once the installation is complete.</p> <p></p>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#uninstalling-siddhi-extensions","title":"Uninstalling Siddhi Extensions","text":"<p>To uninstall a specific Siddhi application, issue the appropriate command out of the following based on your operating system.</p> <ul> <li>For Windows     : <code>extension-installer.bat uninstall &lt;EXTENSION_NAME&gt;</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh uninstall &lt;EXTENSION_NAME&gt;</code> </li> </ul> <p>Info</p> <p>Here, the <code>&lt;EXTENSION_NAME&gt;</code> refers to the name of the extension. When you use the command line to view the list of extensions that are currently installed or to view the installation status of all the supported Siddhi extensions, the extension names are displayed in the <code>name</code> column.e.g., The extension name of the gRPC extension is <code>grpc</code>.</p> <p>e.g., To un-install the <code>grpc</code> Siddhi extension, issue the following command.</p> <ul> <li>For Windows     : <code>extension-installer.bat uninstall grpc</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh uninstall grpc</code> </li> </ul> <p>A message appears to inform you of any other extension that shares dependencies with the extension being uninstalled. The message also prompts you to confirm whether you want to proceed with the installation or not.</p> <p></p> <p>If you enter <code>y</code> and proceed with the un-installation, the following log appears to inform you of the progress of the un-installation and then prompt you to restart the Streaming Integrator server once the un-installation is complete.</p> <p></p>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#downloading-and-installing-siddhi-extensions-manually","title":"Downloading and installing Siddhi extensions manually","text":""},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#downloading-siddhi-extensions","title":"Downloading Siddhi extensions","text":"<p>To download Siddhi extensions manually from the store and install them, follow the steps below.</p> <p>To download the Siddhi extensions, follow the steps below</p> <ol> <li> <p>Open the Siddhi Extensions page.    The available Siddhi extensions are displayed as follows. </p> </li> <li> <p>Click on the required extension. In this example, let's click on the IBM MQ extension. </p> </li> </ol> <p>In the dialog box that appears, enter your e-mail address and click Submit. The extension JAR is downloaded to     the default location in your machine (based on your settings).</p> <ol> <li> <p>If you are not using the latest version of the Streaming Integrator, and you     want to select the version of the extension that matches your current product version,\u00a0expand Version Support      in the left navigator for the selected extension.</p> <p>Tip</p> <p>Each extension has a separate Version Support navigator item for the Streaming Integrator, SP, CEP and DAS.</p> <p></p> </li> <li> <p>If you need to download an older version of an extension, follow the substeps below.</p> </li> <li> <p>Once you have clicked on the required extension,\u00a0click on the Older Versions tab. Then click on the link     displayed within the tab. </p> <p>You are directed to the maven central page where all the available versions of the extension are listed. </p> </li> <li> <p>Click on the relavent version. It directs you to the download page. To download the bundle, click on it. </p> </li> </ol>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#installing-siddhi-extensions_1","title":"Installing Siddhi extensions","text":"<p>To install the Siddhi extension in your Streaming Integrator pack, place the extension JAR you downloaded in the  <code>&lt;SI_HOME&gt;/lib</code> directory.</p>"},{"location":"connectors/downloading-and-Installing-Siddhi-Extensions/#uninstalling-siddhi-extensions_1","title":"Uninstalling Siddhi extensions","text":"<p>To uninstall a Siddhi extension, delete the relevant extension JAR in the <code>&lt;SI_HOME&gt;/lib</code> directory.</p>"},{"location":"connectors/writing-Custom-Siddhi-Extensions/","title":"Writing Custom Siddhi Extensions","text":"<p>Custom extensions can be written in order to apply use case specific logic that is not available in Siddhi out of the  box or as an existing extension.</p> <p>There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension  archetypes are explained below with their related maven archetypes. You can use these archetypes to generate maven projects for each extension type.</p>"},{"location":"connectors/writing-Custom-Siddhi-Extensions/#siddhi-execution","title":"siddhi-execution","text":"<p>Siddhi-execution provides following extension types:</p> <ul> <li>Function</li> <li>Aggregate Function</li> <li>Stream Function</li> <li>Stream Processor</li> <li>Window</li> </ul> <p>You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Siddhi Query Guide - Extensions.</p> <p>To install and implement the siddhi-io extension archetype, follow the procedure below:</p> <ol> <li> <p>Issue the following command from your CLI.    <pre><code>mvn archetype:generate\n-DarchetypeGroupId=org.wso2.siddhi.extension.archetype\n-DarchetypeArtifactId=siddhi-archetype-execution\n-DarchetypeVersion=1.0.1\n-DgroupId=org.wso2.extension.siddhi.execution\n-Dversion=1.0.0-SNAPSHOT\n</code></pre></p> </li> <li> <p>Enter the required execution name in the message that pops up as shown in the example below. <code>Define value for property 'executionType': ML</code></p> </li> <li>To confirm that all property values are correct, type <code>Y</code> in the console. If not, press <code>N</code>.</li> <li>Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant     extension logic. Then build the source code and place the build extension jar in\u00a0the <code>&lt;SI_HOME&gt;/lib</code> directory.</li> </ol>"},{"location":"connectors/writing-Custom-Siddhi-Extensions/#siddhi-io","title":"siddhi-io","text":"<p>Siddhi-io provides following extension types:</p> <ul> <li>sink</li> <li>source</li> </ul> <p>You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is  generally used to work with IO operations as follows:</p> <ul> <li>The Source extension type gets inputs to your Siddhi application.</li> <li>The Sink extension publishes outputs from your Siddhi application.</li> </ul> <p>For more information about these extension types, see Siddhi Query Guide - Extensions.</p> <p>To implement the siddhi-io extension archetype, follow the procedure below:</p> <ol> <li>Issue the following command from your CLI.</li> </ol> <pre><code> mvn archetype:generate\n-DarchetypeGroupId=org.wso2.siddhi.extension.archetype\n-DarchetypeArtifactId=siddhi-archetype-io\n-DarchetypeVersion=1.0.1\n-DgroupId=org.wso2.extension.siddhi.io\n-Dversion=1.0.0-SNAPSHOT\n</code></pre> <ol> <li> <p>Enter the required execution name in the message that pops up as shown in the example below. <code>Define value for property 'typeOf_IO': http</code></p> </li> <li> <p>To confirm that all property values are correct, type <code>Y</code> in the console. If not, press <code>N</code>.</p> </li> <li> <p>Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant     extension logic. Then build the source code and place the build extension jar in\u00a0the <code>&lt;SI_HOME&gt;/lib</code> directory.</p> </li> </ol>"},{"location":"connectors/writing-Custom-Siddhi-Extensions/#siddhi-map","title":"siddhi-map","text":"<p>Siddhi-map provides following extension types:</p> <ul> <li>Sink Mapper</li> <li>Source Mapper</li> </ul> <p>You can use one or more from above mentioned extension types and implement according to your requirement as follows.</p> <ul> <li>The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to    external endpoints(such as E-mail, TCP, Kafka, HTTP, etc).</li> <li>The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a    Siddhi application.</li> </ul> <p>For more information about these extension types, see Siddhi Query Guide - Extensions.</p> <p>To implement the siddhi-map extension archetype, follow the procedure below:</p> <ol> <li> <p>Issue the following command from your CLI.    <pre><code>mvn archetype:generate\n-DarchetypeGroupId=org.wso2.siddhi.extension.archetype\n-DarchetypeArtifactId=siddhi-archetype-map\n-DarchetypeVersion=1.0.1\n-DgroupId=org.wso2.extension.siddhi.map\n-Dversion=1.0.0-SNAPSHOT\n</code></pre></p> </li> <li> <p>Enter the required execution name in the message that pops up as shown in the example below. <code>Define value for property 'typeOf_IO': http</code></p> </li> <li> <p>To confirm that all property values are correct, type <code>Y</code> in the console. If not, press <code>N</code>.</p> </li> <li> <p>Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant     extension logic. Then build the source code and place the build extension jar in\u00a0the <code>&lt;SI_HOME&gt;/lib</code> directory.</p> </li> </ol>"},{"location":"connectors/writing-Custom-Siddhi-Extensions/#siddhi-script","title":"siddhi-script","text":"<p>Siddhi-script provides the <code>Script</code> extension type.</p> <p>The script extension type allows you to\u00a0write functions in other programming languages and execute them within Siddhi  queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function.</p> <p>For more information about these extension types, see Siddhi Query Guide - Extensions.</p> <p>To implement the siddhi-script extension archetype, follow the procedure below:</p> <ol> <li>Issue the following command from your CLI.</li> </ol> <pre><code>mvn archetype:generate\n-DarchetypeGroupId=org.wso2.siddhi.extension.archetype\n-DarchetypeArtifactId=siddhi-archetype-script\n-DarchetypeVersion=1.0.1\n-DgroupId=org.wso2.extension.siddhi.script\n-Dversion=1.0.0-SNAPSHOT\n</code></pre> <ol> <li> <p>Enter the required execution name in the message that pops up as shown in the example below. <code>Define value for property 'typeOfScript':</code></p> </li> <li> <p>To confirm that all property values are correct, type <code>Y</code> in the console. If not, press <code>N</code>.</p> </li> <li> <p>Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant     extension logic. Then build the source code and place the build extension jar in\u00a0the <code>&lt;SI_HOME&gt;/lib</code> directory.</p> </li> </ol>"},{"location":"connectors/writing-Custom-Siddhi-Extensions/#siddhi-store","title":"siddhi-store","text":"<p>Siddhi-store provides the <code>Store</code> extension type.</p> <p>The Store extension type allows you to work with\u00a0data/events stored in various data stores through the table abstraction.</p> <p>For more information about these extension types, see Siddhi Query Guide - Extensions.</p> <p>To implement the siddhi-store extension archetype, follow the procedure below:</p> <ol> <li> <p>Issue the following command from your CLI.    <pre><code>mvn archetype:generate\n-DarchetypeGroupId=org.wso2.siddhi.extension.archetype\n-DarchetypeArtifactId=siddhi-archetype-store\n-DarchetypeVersion=1.0.1\n-DgroupId=org.wso2.extension.siddhi.store\n-Dversion=1.0.0-SNAPSHOT\n</code></pre></p> </li> <li> <p>Enter the required execution name in the message that pops up as shown in the example below. <code>Define value for property 'storeType': RDBMS</code></p> </li> <li> <p>To confirm that all property values are correct, type <code>Y</code> in the console. If not, press <code>N</code>.</p> </li> <li> <p>Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant     extension logic. Then build the source code and place the build extension jar in\u00a0the <code>&lt;SI_HOME&gt;/lib</code> directory.</p> </li> </ol>"},{"location":"develop/creating-a-Siddhi-Application/","title":"Creating Siddhi Applications","text":"<p>Siddhi applications are files that define the Siddhi logic to process the events sent to the Streaming Integrator. They are written in the Siddhi Query Language using the Streaming Integrator Tooling.</p> <p>A Siddhi file contains the following configurations:</p> Configuration Description Stream A logical series of events ordered in time with a uniquely identifiable name, and set of defined attributes with specific data types defining its schema. Source This consumes data from external sources (such as <code>             TCP            </code> , <code>             Kafka            </code> , <code>             HTTP            </code> , etc) in the form of events, then converts each event (that can be in <code>             XML            </code> , <code>             JSON            </code> , <code>             binary            </code> , etc. format) to a Siddhi event, and passes that to a stream for processing. Sink This takes events arriving at a stream, maps them to a predefined data format (such as <code>             XML            </code> , <code>             JSON,            </code> <code>             binary            </code> , etc), and publishes them to external endpoints (such as <code>             E-mail            </code> , <code>             TCP            </code> , <code>             Kafka            </code> , <code>             HTTP            </code> , etc). Executional Element <p>An executional element can be one of the following:</p> <ul> <li>Stateless query: Queries that only consider currently incoming events when generating an output. e.g., filters</li> <li>Stateful query: Queries that consider both currently incoming events as well as past events when generating an output. e.g., windows, sequences, patterns, etc.</li> <li>Partitions: Collections of stream definitions and Siddhi queries separated from each other within a Siddhi application for the purpose of processing events in parallel and in isolation</li> </ul> <p>A Siddhi application can be created from the source view or the design view of the Streaming Integrator Tooling.</p>"},{"location":"develop/creating-a-Siddhi-Application/#creating-a-siddhi-application-in-the-source-view","title":"Creating a Siddhi application in the source view","text":"<p>To create a Siddhi application via the source view of the Streaming Integrator Tooling, follow the steps below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling by navigating to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issue one of the following commands:</p> <ul> <li> <p>For Windows: <code>streaming-integrator-tooling.bat</code></p> </li> <li> <p>For Linux: <code>./streaming-integrator-tooling.sh</code></p> </li> </ul> </li> </ol> <p>The Streaming Integrator Tooling opens as shown below.</p> <p></p> <ol> <li> <p>Click New to start defining a new Siddhi application. A new file opens as shown below.</p> <p></p> </li> <li> <p>Add the following sample Siddhi application to the file.</p> <pre><code>    @App:name(\"SweetProductionAnalysis\")\n\n@Source(type = 'tcp', context='SweetProductionData', @map(type='binary'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log', @map(type='json'))\ndefine stream ProductionAlertStream (name string, amount double);\n\nfrom SweetProductionStream\nselect *\ninsert into ProductionAlertStream;\n</code></pre> <p>Info</p> <p>Note the following in this Siddhi application  Configuration Description Stream <p>This stream contains two stream configurations:</p> <ul> <li> <p><code>                   SweetProductionStream                  </code></p> <pre><code>define stream SweetProductionStream (name string, amount double);</code></pre> <p>This is the input stream that defines the schema based on which events are selected to be processed by the <code>                   SweetProductionAnalysis                  </code> Siddhi application. Events received via the source in this application are directed to this stream.</p> </li> <li> <p><code>                   ProductionAlertStream                                     </code></p> <pre><code>define stream ProductionAlertStream (name string, amount double);</code></pre> <p>This is the output stream from which the sink configured in this application takes events to be published as the output.</p> </li> </ul> Source <pre><code>@Source(type = 'tcp', context='SweetProductionData', @map(type='binary'))</code></pre> <p>                   This source configuration has the following sections:                </p> <ul> <li> <pre><code>@Source(type = \u2018tcp\u2019, context='SweetProductionData'</code></pre> <p>This configuration defines <code>                   tcp                  </code> as the transport via which events are received to be processed by the <code>                   SweetProductionAnalysis                  </code> Siddhi application.</p> </li> <li> <pre><code>@map(type='binary'))\n                              </code></pre> <p>This configuration defines the input mapping. In this scenario, Binary Mapper is used which converts input events into binary events and feeds them into siddhi.</p> </li> </ul>                     The source types and map types are available as Siddhi extensions, and you can find via the operator finder as follows:                <ol> <li> <p>Click the Operator Finder icon to open the Operator Finder.</p> <p></p> </li> <li> <p>Move the cursor to the location in the Siddhi application where you want to add the source. </p> </li> <li> <p>Search for the required transport type. Once it appears in the search results, click the Add to Source icon on it. </p> </li> <li> <p>Similarly, search for the mapping type you want to include in the source configuration, and add it.</p> <p></p> </li> <li> <p>The source annotation is now displayed as follows. You can add the other properties as required, and save your changes. </p> </li> </ol> Sink <pre><code>@sink(type='log', @map(type='json'))</code></pre> <p>This sink configuration has the following sections:</p> <ul> <li> <pre><code>@sink(type='log')</code></pre> <p>This configuration defines <code>                   log                  </code> as the transport via which the processed events are published from the <code>                   ProductionAlertStream                  </code> output stream. Log sink simply publishes events into the console.</p> </li> <li> <pre><code>@map(type='json'))</code></pre> <p>This configuration defines the output mapping. Events are published with the <code>                  json                 </code> mapping type. Json mapper converts the events in the <code>                  ProductionAlertStream                 </code> to the Json format.</p> </li> </ul>                You can select the sink type and the map type from the Operator Finder.              Executional Elements <pre><code>from SweetProductionStream select * insert into ProductionAlertStream;</code></pre> <p>This is where the logic of the siddhi app is defined. In this scenario, all the events received in the <code>                 SweetProductionStream                </code> input stream are inserted into the <code>                 ProductionAlertStream                </code> output stream.</p> </p> </li> <li> <p>To save this Siddhi application, click File, and then click Save. By default siddhi applications are saved in the  <code>&lt;SI_HOME&gt;/wso2/editor/deployment/workspace</code> directory.</p> </li> <li> <p>To export the Siddhi application to your preferred location, click     File, and then click Export File.</p> </li> <li> <p>To see a graphical view of the event flow you defined in your Siddhi     application, click Design View.</p> <p></p> <p>The event flow is displayed as follows.  </p> <p></p> </li> </ol>"},{"location":"develop/creating-a-Siddhi-Application/#creating-a-siddhi-application-in-the-design-view","title":"Creating a Siddhi application in the design view","text":"<p>To create a Siddhi application via the design view of the Streaming Integrator Tooling, follow the steps below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling by navigating to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issue one of the following commands:</p> <ul> <li>For Windows: <code>streaming-integrator-tooling.bat</code></li> <li>For Linux: <code>./streaming-integrator-tooling.sh</code></li> </ul> <p>Streaming Integrator Tooling opens as shown below. </p> </li> <li> <p>Click New to start defining a new Siddhi application. A new file     opens as shown below.</p> <p></p> </li> <li> <p>To open the design view, click Design View.</p> </li> <li> <p>To define the input stream into which the events to be processed via     the Siddhi application should be received, drag and drop the stream     icon (shown below) into the grid. </p> <p>Once the stream component is added to the grid, move the cursor over it, and then click on the settings icon as shown below.</p> <p></p> <p>As as result, the Stream Configuration form opens as follows. </p> <p>Fill this form as follows to define a stream named <code>SweetProductionStream</code> with two attributes named <code>name</code> and <code>amount</code>:  </p> <ol> <li>In the Name field, enter <code>SweetProductionStream</code>.</li> <li> <p>In the Attributes table, enter two attributes as follows.     You can click +Attribute to add a new row in the table to     define a new attribute.</p> Attribute Name Attribute Type <code>name</code> <code>string</code> <code>amount</code> <code>double</code> </li> <li> <p>Click Submit to save the new stream definition. As a result,     the stream is displayed on the grid with the <code>SweetProductionStream</code> label as shown below. </p> </li> </ol> </li> <li> <p>To define the output stream to which the processed events need to be directed, drag and drop the     stream icon again. Place it after the <code>SweetProductionStream</code> stream. This stream     should be named <code>ProductionAlertStream</code> and have the following attributes.</p> Attribute Name Attribute Type <code>name</code> <code>string</code> <code>totalProduction</code> <code>long</code> </li> <li> <p>To add the source from which\u00a0events are received, drag and drop the     source icon (shown below) into the grid. The source is an input to     the SweetProductionStream input stream component. Therefore,     place this source component to the left of the input stream     component in the grid.       Once you add the source component, draw a line from it to the     SweetProductionStream input stream component by dragging the     cursor as demonstrated below.      Click the settings icon on the source component you added to open     the Source Configuration form. Then enter information as     follows. </p> <ol> <li> <p>In the Source Type field, select tcp .</p> </li> <li> <p>For this example, assume that events are received in the <code>binary</code> format. To indicate that     events are expected to be converted from this format, select     binary in the Map Type field.</p> </li> <li> <p>To indicate the context, select the context check box and     enter <code>SweetProductionData</code> in the field that appears below.</p> </li> <li>Click Submit.</li> </ol> </li> <li> <p>To add a query that defines the execution logic, drag and drop the     projection query icon (shown below) to the grid.      The query uses the events in the <code>SweetProductionStream</code> input stream as inputs and directs the      processed events (which are its output) to the <code>ProductionAlertStream</code> output stream. Therefore,     create two connections as demonstrated below. </p> </li> <li> <p>To define the execution logic, move the cursor over the query in the     grid, and click on the settings icon that appears. This opens the     Query Configuration form. Enter information in it as follows: </p> <ol> <li>Enter a name for the query in the Name field. In this example, let's enter <code>query</code> as the name.</li> <li>In order to specify how each user defined attribute in the input     stream is converted to generate the output events, select User     Defined Attributes in the Select field. As a result, the     User Defined Attributes table appears. The As column of     this table displays the attributes of the output stream. To     derive the value for each attribute, enter required     expressions/values in the\u00a0<code>Expression</code> column as explained below.<ol> <li>The value for <code>name</code> can be derived from the input stream without any further processing. Therefore, enter     <code>name</code> as the expression for the <code>name</code> attribute.</li> <li>To derive the value for the <code>totalProduction</code> attribute, the sum of the values for the <code>amount</code> attribute     of input events need to be calculated. Therefore, enter the expression as follows to apply the <code>sum()</code>     Siddhi function to the <code>amount</code> attribute. <code>sum(amount)</code></li> <li>Leave the default values of the Output section unchanged.</li> </ol> </li> <li>Click Submit to save the information.  </li> </ol> </li> <li> <p>To add a sink to publish the output events that are directed to the <code>ProductionAlertStream</code> output stream, drag and     drop the sink icon (shown below) into the grid.      Draw an arrow from the <code>ProductionAlertStream</code> output stream to the sink component to connect them.  </p> <p>Click the settings icon on the sink component you added to open the Sink Configuration form. Then enter information as follows.  1.  In this example, let's assume that output needs to be generated     as logs in the console. To indicate this, select <code>log</code> in the Sink Type field.</p> <ol> <li>In the Map Type field, select the format in which the output     must be generated. For this example, let's select <code>json</code>.</li> <li>Click Submit to save the information.</li> </ol> </li> <li> <p>To align the Siddhi components that you have added to the grid,     click Edit and then click Auto-Align. As a result, all the     components are horizontally aligned as shown below. </p> </li> <li>Click Source View. The siddhi application is displayed as follows. </li> <li> <p>Click File and then click Save as. The Save to Workspace dialog box appears. In the File Name      field, enter <code>SweetProductionAnalysis</code> and click Save.</p> <p></p> </li> </ol>"},{"location":"develop/deploying-Streaming-Applications/","title":"Deploying Siddhi Applications","text":"<p>After creating and testing a Siddhi application, you need to deploy it in the Streaming Integrator server. You can also deploy it in Docker and Kubernetes.</p> <p>To deploy your Siddhi application in the Streaming Integrator server, follow the procedure below:</p> <p>Info</p> <p>To deploy the Siddhi application, you need to run both the Streaming Integrator server and Streaming Integrator Tooling. The home directories of the Streaming Integrator server is referred to as <code>&lt;SI_HOME&gt;</code> and the home directory of Streaming Integrator Tooling is referred to as <code>&lt;SI_TOOLING_HOME&gt;</code>.</p> <ol> <li> <p>Start the Streaming Integrator server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory from the CLI, and issuing the appropriate command out of the following, based on your operating system.</p> </li> <li> <p>On Windows: <code>server.bat --run</code></p> </li> <li> <p>On Linux/Mac OS: \u00a0<code>./server.sh</code></p> </li> <li> <p>In the Streaming Integrator Tooling, click Deploy and then click Deploy to Server.</p> <p></p> <p>The Deploy Siddhi Apps to Server dialog box opens as follows.</p> <p></p> </li> <li> <p>In the Add New Server section, enter information as follows:</p> Field Value Host Your host Port <code>9443</code> User Name <code>admin</code> Password <code>admin</code> <p></p> <p>Then click Add.</p> </li> <li> <p>Select the check boxes for the Siddhi applications that you want to deploy as shown below. Then select the check boxes for the servers in which you want to deploy them.</p> <p></p> </li> <li> <p>Click Deploy.</p> <p>As a result, the Siddhi application(s) yoiu selected is saved in the <code>&lt;SI_HOME&gt;/deployment/siddhi-files</code> directory, and the following is message displayed in the dialog box.</p> <p></p> </li> </ol>"},{"location":"develop/developing-streaming-integration-solutions/","title":"Developing Streaming Integrator Solutions","text":"<p>This section provides an overview of the development flow in the Streaming Integrator.</p> <p>Developing a Streaming Integrator solution involves the following four steps.</p> <p></p> Step Description Step 1: Installing SI Tooling This involves downloading and installing the Streaming Integration Tooling in which Siddhi applications are designed. For more information, see the following topics: - Installing the Streaming Integrator in a Virtual Machine - Installing the Streaming Integrator in Docker - Installing the Streaming Integrator in Kubernetes Step 2: Creating Siddhi Applications Siddhi applications can be designed in the Streaming Integrator Tooling via the source view or the design view. For detailed instructions, see Creating Siddhi Applications. Step 3: Testing Siddhi Applications Once a Siddhi application is created, you can test it before using it in a production environmenty by simulating events to it. For more information, see Testing Siddhi Applications. Step 4: Deploying Siddhi Applications Once your Siddhi application is created and verified via the testing functionality in the Streaming Integrator Tooling, you can deploy it in the Streaming Integrator server, or deploy it in a Docker/Kubernetes environment. For more information about, see the following topics: - Deploying Siddhi Applications - Exporting Siddhi Applications Step 5: Running Siddhi Applications This involves running the Siddhi application in the server where you deployed them. To try this out, you can follow the Streaming Integrator Tutorials."},{"location":"develop/exporting-Siddhi-Applications/","title":"Exporting Siddhi Applications","text":"<p>The Streaming Integrator Tooling allows you to export one or more Siddhi files as a Docker or Kubernetes artifact in order to run those Siddhi applications within a Docker or Kubernetes environment.</p> <p>To export Siddhi files as Docker or Kubernetes artifacts, follow the steps given below.</p>"},{"location":"develop/exporting-Siddhi-Applications/#exporting-siddhi-applications-as-a-docker-image","title":"Exporting Siddhi applications as a Docker Image","text":"<p>To export one or more Siddhi applications as a Docker image, follow the procedure below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling by issuing one of the following commands from the <code>&lt;SI_HOME&gt;/bin</code> directory.</p> <ul> <li> <p>For Windows: <code>tooling.bat</code></p> </li> <li> <p>For Linux: <code>./tooling.sh</code></p> </li> </ul> <p>The Streaming Integrator Tooling opens as shown below.</p> <p></p> </li> <li> <p>Create the required Siddhi files. The rest of the documentation assumes that you have one or more Siddhi applications to be exported as a Docker image. For more information, see Creating a Siddhi Application.</p> </li> <li> <p>Click Export menu option and then click For Docker.</p> <p></p> <p>As a result, the following wizard opens.</p> <p></p> </li> <li> <p>In Step 1: Select Siddhi Apps, select one or more Siddhi applications to be included in the Docker image.</p> </li> <li> <p>Click Next.</p> </li> <li> <p>In Step 2: Template Siddhi Apps, template the Siddhi application if required. You can define a template within the Siddhi application via the <code>${...}</code> pattern. In the following example,  <code>${http.port}</code>) is added as a template within the source configuration.</p> <pre><code>@source(type='http', receiver.url='http://localhost:${http.port}/${http.context}',\n    @map(type = 'json'))\ndefine stream StockQuoteStream(symbol string, price double, volume int);\n</code></pre> </li> <li> <p>Once the templates are defined, click Next.</p> </li> <li> <p>In Step 3: Update Siddhi Runner configurations, update the <code>deployment.yaml</code> file of the Streaming Integrator to enter configurations specific for the Docker image. Similar to the previous step, you can template the configurations as well. Use the similar notation (i.e., <code>${...}</code>) to specify a template within the configuration file.</p> </li> <li> <p>Once the configuration is complete, click Next.</p> </li> <li> <p>In Step 4: Populate arguments template, define values for the template fields defined in Step 2 and Step 3. This step lists down all the template fields. Therefore you can set the relevant value for each field.</p> </li> <li> <p>Once the template values are set, click Next.</p> </li> <li> <p>In Step 5: Configure Custom Docker Image, select any additional OSGi bundles or JARs )(i.e., the ones that are not shipped with WSO2 Streaming Integrator Tooling by default) to be shipped with your docker image by selecting the relevant checkboxes. Only the additional OSGi bundles and JARs that you have already added to the libraries of your Streaming Integrator Tooling pack are displayed in this wizard.</p> </li> <li> <p>Once the required OSGi bundles and JARs are selected, click Next.</p> </li> <li> <p>In Step 6: Export Custom Docker Image, select the export mode. If you do not have an account in Docker Hub, you can select Download Artifacts and enter a name for the file you are exporting. If you have an account, select Push to docker registry and enter your credentials.</p> </li> <li> <p>Once the everything is complete, click Export to export a ZIP file with the following directory structure.</p> <pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 configurations.yaml\n\u2514\u2500\u2500 siddhi-files\n    \u251c\u2500\u2500 &lt;SIDDHI_FILE&gt;.siddhi\n    \u251c\u2500\u2500 ...\n    \u2514\u2500\u2500 &lt;SIDDHI_FILE&gt;.siddhi\n</code></pre> <p>For more information on Siddhi Docker artifacts, see Siddhi 5.1 as a Docker Microservice.</p> <p>Info</p> <p>This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input. </p> </li> </ol>"},{"location":"develop/exporting-Siddhi-Applications/#exporting-siddhi-applications-for-kubernetes","title":"Exporting Siddhi Applications for Kubernetes","text":"<p>To export one or more Siddhi applications for Kubernetes, follow the procedure below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling by issuing one of the following commands from the <code>&lt;SI_HOME&gt;/bin</code> directory.</p> <ul> <li> <p>For Windows: <code>tooling.bat</code></p> </li> <li> <p>For Linux: <code>./tooling.sh</code></p> </li> </ul> <p>The Streaming Integrator Tooling opens as shown below.</p> <p></p> </li> <li> <p>Create the required Siddhi files. The rest of the documentation assumes that you have one or more Siddhi applications to be exported as a Kubernetes artifact. For more information, see Creating a Siddhi Application.</p> </li> <li> <p>Click Export menu option and then click For Kubernetes.</p> <p></p> <p>As a result, the following wizard opens.</p> <p></p> </li> <li> <p>In Step 1: Select Siddhi Apps, select one or more Siddhi applications to be included in the Kubernetes artifact.</p> </li> <li> <p>Click Next.</p> </li> <li> <p>In Step 2: Template Siddhi Apps, template the Siddhi application if required. You can define a template within the Siddhi application via the <code>${...}</code> pattern. In the following example,  <code>${http.port}</code>) is added as a template within the source configuration.</p> <pre><code>@source(type='http', receiver.url='http://localhost:${http.port}/${http.context}',\n    @map(type = 'json'))\ndefine stream StockQuoteStream(symbol string, price double, volume int);\n</code></pre> </li> <li> <p>Once the templates are defined, click Next.</p> </li> <li> <p>In Step 3: Update Siddhi Runner configurations, update the <code>deployment.yaml</code> file of the Streaming Integrator to enter configurations specific for the Kubernetes artifact. Similar to the previous step, you can template the configurations as well. Use the similar notation (i.e., <code>${...}</code>) to specify a template within the configuration file.</p> </li> <li> <p>Once the configuration is complete, click Next.</p> </li> <li> <p>In Step 4: Populate arguments template, define values for the template fields defined in Step 2 and Step 3. This step lists down all the template fields. Therefore you can set the relevant value for each field.</p> </li> <li> <p>Once the template values are set, click Next.</p> </li> <li> <p>In Step 5: Select Docker Image select the relevant option to specify whether you want to create a new Docker image with extensions and artifacts, or whether to use an existing Docker image. If you selected Use an existing Docker image, enter the path to the Docker image in the data field that appears.</p> </li> <li> <p>Once the method to create the Docker image is selected, click Next.</p> </li> <li> <p>If you selected Use an existing Docker image, in Step 5: Select Docker Image, you are  directed to Step 8: Add Deployment Configurations. To proceed to that step, see step 18 of this procedure.</p> <p>If you selected Build custom docker image with extensions and artifacts in Step 5: Select Docker Image, you are directed to Step 6: Configure Docker Image. Here, select any additional OSGi bundles or JARs )(i.e., the ones that are not shipped with WSO2 Streaming Integrator Tooling by default) to be shipped with your docker image by selecting the relevant checkboxes. Only the additional OSGi bundles and JARs that you have already added to the libraries of your Streaming Integrator Tooling pack are displayed in this wizard.</p> </li> <li> <p>Once the required OSGi bundles and JARs are selected, click Next.</p> </li> <li> <p>In Step 7: Export Custom Docker Image, enter the credentials to access your Docker Hub account so that the Docker image can be pushed to the Docker registry.</p> </li> <li> <p>Click Next.</p> </li> <li> <p>In Step 8: Add Deployment Configurations, select an option to indicate the preferred deployment mode. Then select a value for the Persistence field to specify how persistence should be carried out.</p> </li> <li> <p>Once deployment configurations are entered, click Export. This downloads the ZIP file with the following directory structure.</p> <pre><code>\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 configurations.yaml\n\u251c\u2500\u2500 siddhi-files\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 &lt;SIDDHI_FILE&gt;.siddhi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 &lt;SIDDHI_FILE&gt;.siddhi\n\u2514\u2500\u2500 siddhi-process.yaml\n</code></pre> </li> <li> <p>Once the ZIP is downloaded, you can extract and open the <code>&lt;ZIP_HOME&gt;/siddhi-process.yaml</code> via a text editor to modify the SiddhiProcess configuration.</p> <p>For more information on SiddhiProcess Kubernetes configuration, see Siddhi 5.1 as a Kubernetes Microservice documentation.</p> <p>Info</p> <p>This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input. </p> </li> </ol>"},{"location":"develop/http-status-codes/","title":"Http status codes","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"develop/http-status-codes/#http-status-codes","title":"HTTP Status Codes","text":"<p>When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method (<code>POST, GET, PUT, DELETE</code>) executed. The following are the definitions of the various HTTP status codes that are returned.</p>"},{"location":"develop/http-status-codes/#http-status-codes-indicating-successful-delivery","title":"HTTP status codes indicating successful delivery","text":"Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful <code>GET</code> and <code>PUT</code> REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful <code>POST</code> REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful <code>DELETE</code> REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed.\u00a0This generally occurs when your successful in trying to undeploy an application."},{"location":"develop/http-status-codes/#error-http-status-codes","title":"Error HTTP status codes","text":"Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful <code>GET</code> and <code>PUT</code> REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred."},{"location":"develop/installing-siddhi-extensions/","title":"Installing Siddhi Extensions","text":"<p>Streaming Integrator Tooling uses Siddhi extensions to connect with various data sources. Siddhi extensions can be installed or un-installed using the Extension Installer.</p> <p>Tip</p> <p>The Extension Installer can install/un-install extensions within Streaming Integrator Tooling. When deploying Siddhi applications in Streaming Integrator Server, these have to be manually done. For more information, see Downloading and Installing Siddhi Extensions.</p>"},{"location":"develop/installing-siddhi-extensions/#managing-siddhi-extensions","title":"Managing Siddhi extensions","text":"<p>The following topics cover how to manage Siddhi extensions in Streaming Integrator Tooling</p>"},{"location":"develop/installing-siddhi-extensions/#finding-the-siddhi-extensions-to-installuninstall","title":"Finding the Siddhi extensions to install/uninstall","text":"<p>To access the Extension installer and find the extensions you need to install/uninstall, follow the steps below:</p> <ol> <li> <p>To start Streaming Integrator Tooling, navigate to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issue the appropriate command out of the following based on your operating system:</p> <ul> <li> <p>For Windows: <code>tooling.bat</code></p> </li> <li> <p>For Linux: <code>./tooling.sh</code></p> </li> </ul> </li> <li> <p>Click Tools menu option, and then click Extension Installer.</p> <p></p> <p>The Extension Installer dialog box opens as shown below.</p> <p></p> </li> <li> <p>Locate the extension that you want to install/un-install. You can enter the name, or a part of the name of the relevant extension in the Search field. It filters one or more extensions that match the entered key word.</p> <p>Info</p> <p>The status of the extension can be one of the following. - InstalledThis indicates that the extension is completely installed. The installation includes the JAR of the extension itself as well as all its dependencies (if any). - Not-InstalledThis indicates that the extension has not been installed. The JAR of the extension itself has not been installed. Dependencies (if any) may be already installed due to shared dependencies. For more information about shared dependencies, see step 2 of Un-installing an extension. - Partially-InstalledThis indicates that the JAR of the extension itself has been installed, but one or more dependencies of the extension still need to be installed. If these extensions need to be manually installed, it is indicated by an information icon next to the status. For more information, see Manually installable dependencies. - Restart-RequiredThis indicates that you need to restart Streaming Integrator Tooling in order to complete the installation/un-installation of the extension.</p> </li> </ol>"},{"location":"develop/installing-siddhi-extensions/#installing-an-extension","title":"Installing an extension","text":"<ol> <li> <p>To install an extension, click Install for it.</p> <p></p> <p>Then click Install in the confirmation dialog box that appears to confirm whether you want to proceed with the installation.</p> </li> <li> <p>Once the installation is complete, restart Streaming Integrator Tooling.</p> <p>After restarting Streaming Integrator Tooling, you can open the Extension installer and view the extension you installed with the updated status.</p> <p></p> </li> </ol>"},{"location":"develop/installing-siddhi-extensions/#un-installing-an-extension","title":"Un-installing an extension","text":"<ol> <li> <p>To un-install an extension, click UnInstall for it.</p> <p></p> <p>Then click UnInstall in the confirmation dialog box that appears to confirm whether you want to proceed to un-install the extension.</p> </li> <li> <p>If the extension you are un-installing has shared dependencies with one or more other extensions, a message appears with information as shown in the example below.</p> <p></p> <p>The names of the other extensions are in bold. The dependencies each extension shares with the extension you are deleting are listed under the extension name. In this example, the extension being un-installed shares the <code>mysql-connector-java</code> dependency with the <code>rdbms-mysql</code> extension, and the <code>siddhi-io-cdc</code> dependency with the <code>cdc-oracle</code>, <code>cdc-postgresql</code>, <code>cdc-mssql</code>, and <code>cdc-mongodb</code> extensions.</p> <p>If you want to proceed, click Confirm.</p> <p>Note</p> <p>If you click Confirm the other extensions that use the shared dependencies lose some of their dependencies. Therefore, if you need to continue to use those extensions, you need to reinstall them.</p> <p>If there are no shared dependencies, click UnInstall in the confirmation dialog that appears to confirm whether you want to proceed to un-install the extension.</p> </li> <li> <p>Once the un-installation is completed, restart Streaming Integrator Tooling for the un-installation to be effective.</p> </li> </ol>"},{"location":"develop/installing-siddhi-extensions/#manually-installable-dependencies","title":"Manually installable dependencies","text":"<p>Certain dependencies of some extensions cannot be automatically downloaded via the Extension Installer. These dependencies should be manually downloaded and installed in order to complete the installation of the extensions that use them.</p> <p>When there is at least one such dependency for an extension, an icon is displayed next to the status of the extension as shown below.</p> <p></p> <p>Click this information to open a dialog box as shown below with information about the dependency.</p> <p></p> <p>The dialog box displays all the dependencies that need to be manually installed. For each dependency, the dialog box provides the following information.</p> <ul> <li> <p>Instructions to download (and depending on the dependency, to convert) the JAR of the dependency.</p> </li> <li> <p>Installation Location where the downloaded JAR (and depending on the dependency, the converted OSGi bundle) needs to be placed in order to install the dependency. The following table especifies the directory in which you need to place the JAR/OSGi bundle depending on the installation location.</p> Installation Location Directory bundle in runtime Place the OSGi bundle you downloaded/converted in either the <code>&lt;SI_HOME&gt;/lib</code> or the <code>&lt;SI_HOME&gt;/bundles</code> directory based on the instructions. jar in runtime Place the non-OSGi bundle you downloaded in the <code>&lt;SI_HOME&gt;/jars</code> directory. jar in samples Place the non-OSGi bundle you dowloaded in the <code>&lt;SI_HOME&gt;/samples/sample-clients/lib</code> directory. </li> </ul>"},{"location":"develop/installing-siddhi-extensions/#configuring-extension-dependencies","title":"Configuring Extension Dependencies","text":"<p>Configurations of extensions are loaded from the <code>&lt;SI_HOME&gt;/wso2/server/resources/extensionsInstaller/extensionDependencies.json</code> configuration file.</p> <p>When you are working with custom extensions, and if you want a custom extension to be installable from the Extension Installer, you need to add the configuration of the extension to this configuration file.</p> <p>The configuration of an extension is a JSON object that looks as follows:</p> <pre><code>    \"&lt;extension_name&gt;\": {\n\"extension\": {...},\n\"dependencies\": [\n{...},\n{...}\n]\n}\n</code></pre> <p><code>&lt;extension_name&gt;</code> which is the key of this JSON object, is the uniquely identifiable name of the extension. The extension is described under <code>extension</code>.</p>"},{"location":"develop/installing-siddhi-extensions/#extension","title":"<code>extension</code>","text":"<p>This object contains information about the extension, denoted by the following properties.</p> Property Description <code>name</code>        The uniquely identifiable name of the extension.      <code>displayName</code>        The displayable name of the extension.      <code>version</code>        The version of the extension.      <p>The following is an example of the <code>extension</code> object, taken from the configuration of the <code>jms</code> extension.</p> <pre><code>  \"jms\": {\n\n\"extension\": {\n\"name\": \"jms\",\n\"displayName\": \"JMS\",\n\"version\": \"2.0.2\"\n},\n\n\"dependencies\": [...]\n}\n</code></pre>"},{"location":"develop/installing-siddhi-extensions/#dependencies","title":"<code>dependencies</code>","text":"<p>This is an array. Each member of this array is an object that denotes information of a dependency of the extension via the following properties.</p> <p>Info<p>The jar of the Siddhi extension itself should be added as a dependency too. e.g., In the configuration of the <code>jms</code> extension, you can see that <code>siddhi-io-jms</code> has been listed as a dependency under <code>dependencies</code>.</p> </p>        Property             Description      <code>name</code>        The uniquely identifiable name of the dependency. If this dependency denotes the jar of the Siddhi extension itself, it starts with <code>siddhi-</code>.      <code>version</code>        The version of the dependency.      <code>download</code> <p>This denotes download information of the dependency via the following properties.</p> <ul> <li><code>autoDownloadable</code>: This specifies whether the dependency is auto downloadable via the <code>true</code> and <code>false</code> values. If the value is <code>false</code>, the property is manually installable.</li> <li><code>url</code>: If the dependency is auto downloadable, this specifies the URL via which the JAR of the dependency is downloaded.</li> <li><code>instructions</code>: If the dependency is only manually installable, this property provides instructions to download (and if applicable, convert) the JAR of the dependency.</li> </ul> <code>usages</code> <p>This is an array. Each member of this array is an object that denotes a directory where the jar of the dependency needs to be placed. Each such directory (location) is denoted by the following properties:</p> <ul> <li><code>type</code>: The type of the JAR. Possible values are as follows:<li><code>BUNDLE</code>: This means that the dependency JAR is an OSGi bundle.</li><li><code>JAR</code>: This means that the dependency JAR is not converted to an OSGi bundle.</li></li> <li><code>usedBy</code>: This indicates whether the JAR is used in runtime or in samples. For more information, see the explanation of installation locations under Manually installable dependencies</li> </ul> <code>lookupRegex</code>        The regex pattern for the file name of the JAR. This is useful for to looking up and detecting whether the JAR is available in the locations mentioned under <code>usages</code>.      <p>The following examples, taken from the configuration of the <code>jms</code> extension, show the members of the <code>dependencies</code> array.</p> <p>Example 1: Auto downloadable dependency</p> <p>This denotes the <code>hawtbuf</code> dependency of the <code>jms</code> extension, which is auto downloadable from the URL specified in <code>download.url</code>.</p> <pre><code>  \"jms\": {\n\"extension\": {...},\n\n\"dependencies\": [\n\n{\n\"name\": \"hawtbuf\",\n\"version\": \"1.9\",\n\"download\": {\n\"autoDownloadable\": true,\n\"url\": \"https://repo1.maven.org/maven2/org/fusesource/hawtbuf/hawtbuf/1.9/hawtbuf-1.9.jar\"\n},\n\"usages\": [\n{\n\"type\": \"BUNDLE\",\n\"usedBy\": \"RUNTIME\"\n},\n{\n\"type\": \"JAR\",\n\"usedBy\": \"SAMPLES\"\n}\n],\n\"lookupRegex\": \"hawtbuf([_-])1.9.jar\"\n},\n\n...\n]\n}\n</code></pre> <p>Example 2: Manually installable dependency</p> <p>This denotes the <code>activemq-client</code> dependency of the <code>jms</code> extension. This dependency needs to be manually downloaded, and the conversions should be done based on the given <code>download.instructions</code>.</p> <pre><code>  \"jms\": {\n\"extension\": {...},\n\n\"dependencies\": [\n...,\n\n{\n\"name\": \"activemq-client\",\n\"version\": \"5.9.0\",\n\"download\": {\n\"autoDownloadable\": false,\n\"instructions\": \"Download the jar from 'https://repo1.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar' and convert ...\"\n},\n\"usages\": [\n{\n\"type\": \"BUNDLE\",\n\"usedBy\": \"RUNTIME\"\n}\n],\n\"lookupRegex\": \"activemq([_-])client([_-])5.9.0(.*).jar\"\n}\n]\n}\n</code></pre>"},{"location":"develop/siddhi-Application-Overview/","title":"Siddhi Application Overview","text":"<p>A Siddhi application (.siddhi) file is the deployment artifact\u00a0containing the Stream Processing logic for WSO2 Streaming Integrator.</p> <p>The format of a Siddhi application is as follows:</p> <pre><code>    @App:name(\"ReceiveAndCount\")\n@App:description('Receive events via HTTP transport and view the output on the console')\n\n/* \n        Sample Siddhi App block comment\n    */\n\n-- Sample Siddhi App line comment\n\n@Source(type = 'http',\nreceiver.url='http://localhost:8006/productionStream',\nbasic.auth.enabled='false',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream TotalCountStream (totalCount long);\n\n-- Count the incoming events\n@info(name='query1')\nfrom SweetProductionStream\nselect count() as totalCount\ninsert into TotalCountStream;\n</code></pre>"},{"location":"develop/siddhi-Application-Overview/#basic-information-about-siddhi-applications","title":"Basic information about Siddhi applications","text":"<p>Following\u00a0are some important\u00a0things to note about Siddhi applications:</p> <ul> <li> <p>The file name of each Siddhi application must be equal to the name specified via the <code>@App:name()</code> annotation.     e.g., In the sample Siddhi application given above, the application name is <code>ReceiveAndCount</code>. Therefore, the Siddhi file name must be <code>ReceiveAndCount.Siddhi</code>.</p> </li> <li> <p>It is optional to provide a description via the <code>@App:description()</code> annotation.</p> </li> <li> <p>The definitions of the required streams, windows, tables, triggers and aggregations need to be included before the Siddhi queries.     e.g., In the above sample Siddhi file, the streams \u00a0(lines 14 and 17) are defined before the queries (lines 21-23).</p> </li> <li> <p>Siddhi can infer the definition of the streams. It is not required to define\u00a0all the streams. However, if annotations need to be added to a stream, that stream must be defined.</p> </li> <li> <p>In the above sample, lines 4-6 nd 8 demonstrate how to include comments within Siddhi applications.</p> </li> </ul> <p>For more information about Siddhi applications, see Siddhi Application at\u00a0Siddhi Streaming SQL Guide.</p>"},{"location":"develop/siddhi-Application-Overview/#common-elements-of-a-siddhi-application","title":"Common elements of a Siddhi application","text":"<p>This section explains the common types of definitions and queries that are included in \u00a0Siddhi application:</p>"},{"location":"develop/siddhi-Application-Overview/#queries","title":"Queries","text":"<p>Queries define the logical processing and selections that must be executed for streaming events. They consume from the pre-defined streams/ windows/ tables/ aggregations, process them in a streaming manner, and insert the output to another stream, window or table. For more information about Siddhi queries, see Queries\u00a0at\u00a0Siddhi Streaming SQL Guide.</p>"},{"location":"develop/siddhi-Application-Overview/#streams","title":"Streams","text":"<p>Streams are one of the core elements of a stream processing application. A stream is a\u00a0logical series of events ordered in time with a uniquely identifiable name and set of defined attributes with specific data types defining its schema. In Siddhi, streams are defined by giving it a name and the set of attributes it contains. Lines 14 and 17 of the above sample are examples of defined streams. For more information on Siddhi streams,\u00a0see Streams at\u00a0Siddhi Streaming SQL Guide.</p>"},{"location":"develop/siddhi-Application-Overview/#tables","title":"Tables","text":"<p>A table is a collection of events that can be used to store streaming data. The capability to store events in a table allows you to query for stored events later or process them again with a different stream. Thegeneric table concept holds here as well, however, Siddhi tables also support numerous table specific data manipulations such as defining primary keys, indexing, etc. For more information on Siddhi tables, see Storage Integration and Tables\u00a0at\u00a0Siddhi Streaming SQL Guide.</p>"},{"location":"develop/siddhi-Application-Overview/#windows","title":"Windows","text":"<p>Windows allow you to retain a collection of streaming events based on a time duration (time window), or a given number of events (length window). It allows you to process events that fall into the defined window or expire from it. For more information on Siddhi windows, see Windows at\u00a0Siddhi Streaming SQL Guide.</p>"},{"location":"develop/siddhi-Application-Overview/#aggregations","title":"Aggregations","text":"<p>Aggregation allows you to aggregate streaming events for different time granularities. The time granularities supported are seconds, minutes, hours, days, months and years. Aggregations such as sum, min, avg can be calculated for the desired duration(s) via Siddhi aggregation. For more information on Siddhi aggregations, see Aggregations at\u00a0Siddhi Streaming SQL Guide.</p>"},{"location":"develop/siddhi-Application-Overview/#persisted-aggregations","title":"Persisted Aggregations.","text":"<p>Note : \"This capability is released as a product update on 11/06/2021. If you don't already have this update, you can get the latest updates now.</p> <p>With Persisted aggregation, the aggregation for higher granularities will be executing on top of the database at the end of each time granularity(Day - at the end of the day, Month - at the end of the month, Year - at the end of the year).   This is the recommended approach if the aggregation group by elements have lots of unique combinations.</p> <ul> <li> <p>Enabling Persisted Aggregation</p> <p>The persisted aggregation can be enabled by adding the @persistedAggregation(enable=\"true\") annotation on top of the aggregation definition. Furthermore, in order to execute the aggregation query, the cud function which is there in siddhi-store-rdbms is used. So in order to enable the \"cud\" operations, please add the following configuration on the deployment.yaml file.</p> <pre><code>   siddhi:\n     extensions:\n       -\n         extension:\n           name: cud\n           namespace: rdbms\n           properties:\n             perform.CUD.operations: true\n</code></pre> <p>In order to use persisted aggregation, A datasource needs to configured through deployment.yaml file and it should be pointed out in @store annotation of the aggregation definition.</p> <p>Furthermore when using persisted aggregation with MySQL, please provide the aggregation processing timezone in JDBC URL since by default MySQL database will use server timezone for some time-related conversions which are there in an aggregation query.</p> <pre><code>    jdbc:mysql://localhost:3306/TEST_DB?useSSL=false&amp;tz=useLegacyDatetimeCode=false&amp;serverTimezone=UTC\n</code></pre> <p>Also when using persisted aggregation with Oracle, add below configuration in the datasource configuration,</p> <p><pre><code>connectionInitSql: alter session set NLS_DATE_FORMAT='YYYY-MM-DD HH24:MI:SS'\n\neg:\n\n - name: APIM_ANALYTICS_DB\n   description: \"The datasource used for APIM statistics aggregated data.\"\n   jndiConfig:\n     name: jdbc/APIM_ANALYTICS_DB\n     definition:\n       type: RDBMS\n       configuration:\n         jdbcUrl: 'jdbc:oracle:thin:@localhost:1521:XE'\n         username: 'root'\n         password: '123'\n         driverClassName: oracle.jdbc.OracleDriver\n         maxPoolSize: 50\n         idleTimeout: 60000\n         connectionTestQuery: SELECT 1 FROM DUAL\n         connectionInitSql: alter session set NLS_DATE_FORMAT='YYYY-MM-DD HH24:MI:SS'\n         validationTimeout: 30000\n         isAutoCommit: false\n</code></pre> For an example please refer to the following query which will be executed on the database to update the table for below sample Aggregation ,</p> <pre><code>@persistedAggregation(enable=\"true\")\ndefine aggregation ResponseStreamAggregation\nfrom processedResponseStream\nselect api, version, apiPublisher, applicationName, protocol, consumerKey, userId, context, resourcePath, responseCode, \n    avg(serviceTime) as avg_service_time, avg(backendTime) as avg_backend_time, sum(responseTime) as total_response_count, time1, epoch, eventTime\ngroup by api, version, apiPublisher, applicationName, protocol, consumerKey, userId, context, resourcePath, responseCode, time1, epoch\naggregate by eventTime every min;\n</code></pre> </li> </ul> <p>The elements mentioned above work together in a Siddhi application to form an event flow. To understand how the elements os a Siddhi application are interconnected, you can view the design view of a Siddhi application. For more information, see Stream Processor Studio Overview.</p>"},{"location":"develop/simulating-Events/","title":"Simulating Events","text":"<p>Simulating events involves simulating predefined event streams.\u00a0These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This\u00a0is useful for debugging and monitoring the event receivers and publishers, execution plans and event formatters.</p> Function REST API Saving a simulation configuration <ul> <li>Single Event Simulation : <code>               POST               http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single              </code></li> <li>Multiple Event Simulation: <code>               POST               http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed              </code> </li> </ul> Editing a simulation configuration <ul> <li>Single Event Simulation : <code>               PUT               http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single              </code></li> <li>Multiple Event Simulation: <code>               PUT               http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed              </code></li> </ul> Deleting a simulation configuration <ul> <li>Single Event Simulation : <code>               DELETE               http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single              </code></li> <li>Multiple Event Simulation: <code>               DELETE\u00a0http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed              </code></li> </ul> Retrieving a simulation configuration <ul> <li>Single Event Simulation : <code>               GET               http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single              </code></li> <li>Multiple Event Simulation: <code>               GET\u00a0http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed              </code></li> </ul> Uploading a CSV file <code>                           POST\u00a0http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed             </code> Editing and uploaded CSV file <code>                           PUT -F 'file=@/{path to csv file}' http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/files/{fileName}?fileName={fileName}             </code> Deleting an uploaded CSV file <code>             DELETE http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/files/{fileName}            </code> Pausing an event simulation <code>             POST             http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/             simulation/feed/{simulationName}/?action=pause            </code> Resuming an event simulation <code>             POST             http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/             simulation/feed/{simulationName}/?action=resume            </code> Stopping an event simulation <code>             DELETE http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed/{simulationName}            </code> <p>The following sections cover how events can be simulated.</p> <ul> <li>Saving a simulation     configuration</li> <li>Editing a simulation     configuration</li> <li>Deleting a simulation     configuration</li> <li>Retrieving a simulation     configuration</li> <li>Uploading a CSV file</li> <li>Editing an uploaded CSV     file</li> <li>Deleting an uploaded CSV     file</li> <li>Pausing an event     simulation</li> <li>Resuming an event     simulation</li> <li>Stopping an event     simulation</li> </ul>"},{"location":"develop/simulating-Events/#saving-a-simulation-configuration","title":"Saving a simulation configuration","text":"<p>To simulate events for WSO2 SP, you should first save the event simulator configuration in the <code>&lt;SP_HOME&gt;/deployment/simulator/simulationConfigs</code> directory by sending a POST request to a REST API as described below.</p>"},{"location":"develop/simulating-Events/#rest-api","title":"REST API","text":"<p>The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below.</p> Event Simulation Type REST API Simulating a single event <code>POST             http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single</code> Simulating a multiple events <code>POST             http://             &lt;SP_HOST&gt;:&lt;API_PORT&gt;             /simulation/feed/</code>"},{"location":"develop/simulating-Events/#sample-curl-command","title":"Sample cURL command","text":"<pre><code>    curl -X POST \\\nhttp://localhost:9390/simulation/feed/ \\\n-H 'content-type: text/plain' \\\n-d '{\n\"properties\": {\n\"simulationName\": \"simulationPrimitive\",\n\"startTimestamp\": \"\",\n\"endTimestamp\": \"\",\n\"noOfEvents\": \"\",\n\"description\": \"\",\n\"timeInterval\": \"1000\"\n},\n\"sources\": [\n{\n\"siddhiAppName\": \"TestExecutionPlan\",\n\"streamName\": \"FooStream\",\n\"timestampInterval\": \"1000\",\n\"simulationType\": \"RANDOM_DATA_SIMULATION\",\n\"attributeConfiguration\": [\n{\n\"type\": \"PRIMITIVE_BASED\",\n\"primitiveType\": \"STRING\",\n\"length\": \"5\"\n},\n{\n\"type\": \"PRIMITIVE_BASED\",\n\"primitiveType\": \"INT\",\n\"min\": \"0\",\n\"max\": \"999\"\n}\n]\n}\n]\n}'\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output","title":"Sample output","text":"<pre><code>{\n  \"status\": \"CREATED\",\n  \"message\": \"Successfully uploaded simulation configuration 'simulationPrimitive'\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response","title":"REST API response","text":"<ul> <li>200 if the simulation configuration is successfully saved.</li> <li>409 if a simulation configuration with the specified name already     exists.</li> <li>400 if the configuration provided is not in a valid JSON format.</li> </ul> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"develop/simulating-Events/#editing-a-simulation-configuration","title":"Editing a simulation configuration","text":"<p>To edit a simulation configuration that is already saved, a PUT request should be sent to a REST API as explained below.</p>"},{"location":"develop/simulating-Events/#rest-api_1","title":"REST API","text":"<p>The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below.</p> Event Simulation Type REST API Simulating a single event <code>PUT             http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single</code> Simulating a multiple events <code>PUT             http://&lt;SP_HOST&gt;:                           &lt;API_PORT&gt;</code> /simulation/feed/{feed name}"},{"location":"develop/simulating-Events/#sample-curl-command_1","title":"Sample cURL command","text":"<pre><code>    curl -X PUT \\\nhttp://localhost:9390/simulation/feed/simulationPrimitive \\\n-H 'content-type: text/plain' \\\n-d '{\n\"properties\": {\n\"simulationName\": \"updatedSimulationPrimitive\",\n\"startTimestamp\": \"\",\n\"endTimestamp\": \"\",\n\"noOfEvents\": \"10\",\n\"description\": \"Updating the simulation configuration\",\n\"timeInterval\": \"1000\"\n},\n\"sources\": [\n{\n\"siddhiAppName\": \"TestExecutionPlan\",\n\"streamName\": \"FooStream\",\n\"timestampInterval\": \"1000\",\n\"simulationType\": \"RANDOM_DATA_SIMULATION\",\n\"attributeConfiguration\": [\n{\n\"type\": \"PRIMITIVE_BASED\",\n\"primitiveType\": \"STRING\",\n\"length\": \"5\"\n},\n{\n\"type\": \"PRIMITIVE_BASED\",\n\"primitiveType\": \"INT\",\n\"min\": \"0\",\n\"max\": \"999\"\n}\n]\n}\n]\n\n}'\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_1","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"Successfully updated simulation configuration 'simulationPrimitive'.\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_1","title":"REST API response","text":"<ul> <li>200 if the simulation configuration is successfully updated.</li> <li>404 if the file specified does not exist in\u00a0the     <code>&lt;SP_HOME&gt;/wso2/editor/deployment/simulation-configs</code>     directory.</li> <li>400 if the file specified is not a CSV file, or if the file does not     exist in the path specified.</li> <li>403 if the size of the file specified exceeds the maximum size     allowed.</li> </ul> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"develop/simulating-Events/#deleting-a-simulation-configuration","title":"Deleting a simulation configuration","text":"<p>To delete an event simulation file that is already saved in\u00a0the <code>&lt;SP_HOME&gt;/wso2/editor/deployment/simulation-configs</code> directory, a DELETE request should be sent to a REST API as explained below.</p>"},{"location":"develop/simulating-Events/#rest-api_2","title":"REST API","text":"<p>The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below.</p> Event Simulation Type REST API Simulating a single event <code>DELETE             http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single</code> Simulating a multiple events <code>DELETE             http://                           &lt;SP_HOST&gt;</code> : <code>&lt;API_PORT&gt;</code> /simulation/feed/"},{"location":"develop/simulating-Events/#sample-curl-command_2","title":"Sample cURL command","text":"<pre><code>    curl -X DELETE 'http://localhost:9390/simulation/feed/simulationPrimitive'\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_2","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"Successfully deleted simulation configuration 'simulationPrimitive'\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_2","title":"REST API response","text":"<ul> <li>200 if the simulation configuration is successfully deleted.</li> <li>404 if the file specified does not exist in\u00a0the     <code>&lt;SP_HOME&gt;/wso2/editor/deployment/simulation-configs</code>     directory.</li> </ul> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"develop/simulating-Events/#retrieving-a-simulation-configuration","title":"Retrieving a simulation configuration","text":"<p>To view a simulation configuration saved in the <code>&lt;SP_HOME&gt;/wso2/editor/deployment/simulation-configs</code> directory via the CLI, a GET request\u00a0should be sent to a REST API as explained below.</p>"},{"location":"develop/simulating-Events/#rest-api_3","title":"REST API","text":"<p>The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below.</p> Event Simulation Type REST API Simulating a single event <code>GET             http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/single</code> Simulating a multiple events <code>GET             http://                           &lt;SP_HOST&gt;:&lt;API_PORT&gt;</code> /simulation/feed/"},{"location":"develop/simulating-Events/#sample-curl-command_3","title":"Sample cURL command","text":"<pre><code>    curl -X GET 'http://localhost:9390/simulation/feed/simulationPrimitive'\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_3","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"{\\\"Simulation configuration\\\":{\\\"sources\\\":[{\\\"timestampInterval\\\":\\\"1000\\\",\\\"simulationType\\\":\\\"RANDOM_DATA_SIMULATION\\\",\\\"attributeConfiguration\\\":[{\\\"length\\\":\\\"5\\\",\\\"type\\\":\\\"PRIMITIVE_BASED\\\",\\\"primitiveType\\\":\\\"STRING\\\"},{\\\"min\\\":\\\"0\\\",\\\"max\\\":\\\"999\\\",\\\"type\\\":\\\"PRIMITIVE_BASED\\\",\\\"primitiveType\\\":\\\"INT\\\"}],\\\"streamName\\\":\\\"FooStream\\\",\\\"siddhiAppName\\\":\\\"TestExecutionPlan\\\"}],\\\"properties\\\":{\\\"simulationName\\\":\\\"simulationPrimitive\\\",\\\"description\\\":\\\"Updating the simulation configuration\\\",\\\"timeInterval\\\":\\\"1000\\\",\\\"endTimestamp\\\":\\\"\\\",\\\"startTimestamp\\\":\\\"\\\",\\\"noOfEvents\\\":\\\"10\\\"}}}\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_3","title":"REST API Response","text":"<ul> <li>200 if the simulation configuration is successfully retrieved.</li> <li>404 if the file specified does not exist in\u00a0the     <code>&lt;SP_HOME&gt;/wso2/editor/deployment/simulation-configs</code>     directory.</li> </ul> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"develop/simulating-Events/#uploading-a-csv-file","title":"Uploading a CSV file","text":"<p>To simulate events from a CSV file, the required CSV file needs to exist in the \\&lt;SP_HOME&gt;/wso2/editor/deployment/csv-files directory.</p>"},{"location":"develop/simulating-Events/#rest-api_4","title":"REST API","text":"<p>A POST request should be sent to the following API.</p> <p><code>POST\u00a0http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed</code></p>"},{"location":"develop/simulating-Events/#sample-curl-command_4","title":"Sample cURL command","text":"<pre><code>    curl -X POST \\\nhttp://localhost:9390/simulation/feed/ \\\n-H 'content-type: text/plain' \\\n-d '{\n\"properties\": {\n\"simulationName\": \"FeedSimulationTest\",\n\"startTimestamp\": \"\",\n\"endTimestamp\": \"\",\n\"noOfEvents\": \"\",\n\"description\": \"\",\n\"timeInterval\": \"1000\"\n},\n\"sources\": [\n{\n\"siddhiAppName\": \"TestExecutionPlan\",\n\"streamName\": \"FooStream\",\n\"timestampInterval\": \"1000\",\n\"simulationType\": \"CSV_SIMULATION\",\n\"fileName\": \"myEvents.csv\",\n\"delimiter\": \",\",\n\"isOrdered\": true,\n\"indices\": \"0,1\"\n}\n]\n}'\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_4","title":"Sample output","text":"<pre><code>{\n  \"status\": \"CREATED\",\n  \"message\": \"Successfully uploaded simulation configuration 'FeedSimulationTest'\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_4","title":"REST API response","text":"<ul> <li>200 if the CSV file is successfully uploaded.</li> <li>409 if a CSV file with the file name specified already exists in\u00a0the     \\&lt;SP_HOME&gt;/wso2/editor/deployment/csv-files directory.</li> <li>400 if the specified file is not a CSV file or if the specified file     path is not valid.</li> <li>403 if the size of the file specified exceeds the maximum file size     allowed.</li> </ul> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"develop/simulating-Events/#editing-an-uploaded-csv-file","title":"Editing an uploaded CSV file","text":"<p>This section explains how to edit a CSV file that is already uploaded to the <code>&lt;SP_HOME&gt;/wso2/editor/deployment/csv-files</code> directory.</p>"},{"location":"develop/simulating-Events/#rest-api_5","title":"REST API","text":"<p>A PUT request should be sent to the following API.</p> <p><code>PUT -F 'file=@/{path to csv file}' http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/files/{fileName}?fileName={fileName}</code></p>"},{"location":"develop/simulating-Events/#sample-curl-command_5","title":"Sample cURL command","text":"<pre><code>    curl -X PUT -F 'file=@/home/nadeeka/Desktop/editedMyEvents.csv' http://localhost:9390/simulation/files/myEvents.csv?fileName=myEvents.csv\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_5","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"Successfully updated CSV file 'myEvents.csv' with file ' editedMyEvents.csv'.\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_5","title":"REST API response","text":"<ul> <li>200 if the CSV file is successfully updated.</li> <li>404 if the specified CSV file does not exist in\u00a0the     <code>&lt;SP_HOME&gt;/deployment/simulator/csvFiles</code>     directory.</li> </ul> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"develop/simulating-Events/#deleting-an-uploaded-csv-file","title":"Deleting an uploaded CSV file","text":"<p>This section explains how to delete a CSV file that is already uploaded to the <code>&lt;SP_HOME&gt;/wso2/editor/deployment/csv-files</code> directory.</p>"},{"location":"develop/simulating-Events/#rest-api_6","title":"REST API","text":"<p>A DELETE request should be sent to the following API.</p> <p><code>DELETE http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/files/{fileName}</code></p>"},{"location":"develop/simulating-Events/#sample-curl-command_6","title":"Sample cURL command","text":"<pre><code>    curl -X DELETE http://localhost:9390/simulation/files/myEvents.csv\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_6","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"Successfully deleted file 'myEvents.csv'\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_6","title":"REST API response","text":"<ul> <li>200 if the CSV file is successfully deleted.</li> <li>404 if the specified CSV file does not exist in\u00a0the     <code>&lt;SP_HOME&gt;/wso2/editor/deployment/csv-files</code>     directory.</li> </ul>"},{"location":"develop/simulating-Events/#pausing-an-event-simulation","title":"Pausing an event simulation","text":"<p>This section explains how to pause an event simulation that has already started.</p>"},{"location":"develop/simulating-Events/#rest-api_7","title":"REST API","text":"<p>A POST request should be sent to the following API.</p> <p><code>POST http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed/{simulationName}/?action=pause</code></p>"},{"location":"develop/simulating-Events/#sample-curl-command_7","title":"Sample cURL command","text":"<pre><code>    curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=pause\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_7","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"Successfully paused event simulation 'simulationPrimitive'.\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_7","title":"REST API response","text":"<ul> <li>200 if the event simulation is successfully paused.</li> <li>409 if the event simulation is already paused.</li> </ul>"},{"location":"develop/simulating-Events/#resuming-an-event-simulation","title":"Resuming an event simulation","text":"<p>This section explains how to resume an event simulation that has already paused.</p>"},{"location":"develop/simulating-Events/#rest-api_8","title":"REST API","text":"<p>A POST request should be sent to the following API</p> <p><code>POST http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed/{                   simulationName</code> }/?action=resume</p>"},{"location":"develop/simulating-Events/#sample-curl-command_8","title":"Sample cURL command","text":"<pre><code>    curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=resume\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_8","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"Successfully resumed event simulation 'simulationPrimitive'.\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_8","title":"REST API response","text":"<ul> <li>200 if the event simulation is successfully resumed.</li> </ul>"},{"location":"develop/simulating-Events/#stopping-an-event-simulation","title":"Stopping an event simulation","text":"<p>This section explains how to stop an event simulation.</p>"},{"location":"develop/simulating-Events/#rest-api_9","title":"REST API","text":"<p>A POST request should be sent to the following API</p> <p><code>POST http://&lt;SP_HOST&gt;:&lt;API_PORT&gt;/simulation/feed/{                   simulationName</code> }/?action=stop</p>"},{"location":"develop/simulating-Events/#sample-curl-command_9","title":"Sample cURL command","text":"<pre><code>    curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=stop\n</code></pre>"},{"location":"develop/simulating-Events/#sample-output_9","title":"Sample output","text":"<pre><code>{\n  \"status\": \"OK\",\n  \"message\": \"Successfully stopped event simulation 'simulationPrimitive'.\"\n}</code></pre>"},{"location":"develop/simulating-Events/#rest-api-response_9","title":"REST API response","text":"<ul> <li>200 if the event simulation is successfully stoped.</li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/","title":"Streaming Integrator Tooling Overview","text":"<p>Info</p> <p>This page describes the latest publicly-available version of Streaming Integrator Tooling. If you cannot see some of the user interface elements described here, update your Streaming Integrator Tooling installation.</p> <p>The Streaming Integrator Tooling is a developer tool that is shipped with the Streaming Integrator to develop Siddhi applications. It allows provides three interfaces to develop Siddhi applications</p> <ul> <li> <p>Source View : This allows you to write Siddhi applications     in the Siddhi Query Language. This supports auto-completion and     tracking syntax errors.</p> </li> <li> <p>Design View :This interface visualizes the event flow of a     Siddhi application, and allows you to compose the applications by     dragging and dropping Siddhi components to a graph.</p> </li> <li> <p>Wizard View: This is a wizard with a page for each component of a Siddhi application that displays the relevant configuration parameters as fields. This wizard can be directly accessed from the Welcome Page. Only ETL (Extract, Transform, Load) applications can be created/viewed in this interface. For a Siddhi application to be considered an ETL application, it must include all of the following components: - A source configuration - A sink configuration (the sink type can be any of the supported sink types other than <code>log</code>) - A Siddhi query that performs a transformation.  For more information, see Creating an ETL Application via SI Tooling tutorial.</p> </li> </ul> <p>Once a Siddhi application is created, you can simulate events via the Streaming Integrator Tooling to test whether it works as expected.</p>"},{"location":"develop/streaming-integrator-studio-overview/#starting-streaming-integrator-tooling","title":"Starting Streaming Integrator Tooling","text":"<p>To start and access the Streaming Integrator Tooling, follow the steps below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling by issuing one of the following     commands from the <code>&lt;SI_HOME&gt;/bin</code> directory.</p> <ul> <li>For Windows: <code>streaming-integrator-tooling.bat</code></li> <li>For Linux:\u00a0<code>./streaming-integrator-tooling.sh</code></li> </ul> </li> <li> <p>Access the Streaming Integrator Tooling via the <code>http://localhost:/editor</code>     URL. The Streaming Integrator Tooling opens as shown below.</p> <p>Info</p> <p>The default URL is <code>http://localhost:9390/editor</code>. If required, you can change the host name (i.e., <code>localhost</code>) or the web UI application name (i.e., <code>editor</code>). For instructions, see Changing the Host Name and Context Path of SI Tooling.</p> </li> </ol>"},{"location":"develop/streaming-integrator-studio-overview/#welcome-page","title":"Welcome Page","text":"<p>The Welcome to the Streaming Integrator Tooling Tour Guide is open by default. You can take a tour by following the instructions on the dialog box, or close it and proceed to explore the Streaming Integrator Tooling on your own. You can also access this dialog box by clicking Tools -&gt; Tour Guide . Once you close the dialog box, you can try the following:</p> <ul> <li> <p>New     Click this to open a new untitled Siddhi file.</p> </li> <li> <p>Open     Click this to open a Siddhi file that is already saved in the <code>workspace</code> directory of the\u00a0Streaming Integrator      Tooling. If the file is already opened in a new tab, clicking Open does not\u00a0open it again. The default path to      the <code>workspace</code> directory is <code>&lt;SI_Home&gt;/wso2/server/deployment</code>.</p> </li> <li> <p>New ETL Flow     Click this to open a wizard with which you can create a Siddhi application with ETL functionality by entering values for the required parameters instead of constructing Siddhi queries by writing code or via a graphical interface. To understand how this is done, follow the Creating an ETL Application via SI Tooling tutorial.</p> </li> <li> <p>Try out samples     The pre-created samples provided out of the box are listed in this     section. When you click on a sample, it opens in a new tab without a     title.</p> </li> <li> <p>More Samples     Click this to view the complete list of samples in the samples     directory. This allows you to access samples other than the ones that     are displayed by default is the Try out samples section.\u00a0When     you click on a sample, it opens in a new tab without a title.</p> </li> <li> <p>Quick links     This section provides links to more resources.</p> </li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/#menu-items","title":"Menu items","text":"<p>This section explains the options that are available in the File , Edit and Run menus.</p>"},{"location":"develop/streaming-integrator-studio-overview/#file-menu-items","title":"File menu Items","text":"<p>The File menu includes the following options.</p> <p></p> <ul> <li> <p>New     Click this to open a new untitled Siddhi file. For more information, see the Creating an ETL Application via SI Tooling tutorial.</p> </li> <li> <p>New ETL Flow     Click this to create a new ETL application via the ETL wizard. For instructions, </p> </li> <li> <p>Open File      Click this to open a Siddhi file that is already saved in the <code>workspace</code> directory of the\u00a0Streaming Integrator      Tooling. If the file is already opened in a new tab, clicking Open does not\u00a0open it again. The default path to      the <code>workspace</code> directory is <code>&lt;SI_Home&gt;/wso2/server/deployment</code>.         ` .  </p> <p>When a Siddhi file is opened, its source view is displayed by default.  To view a design view where the elements of the Siddhi application are graphically represented, click  Design View. As a result, a graphical view of the Siddhi application is displayed as shown in the following  example. </p> </li> <li> <p>Import Sample     Click this to import a sample from the samples diretory to a new tab. The sample opens in an untitled Siddhi file.      Once you save it, it can be accessed from the <code>workspace</code> directory.  </p> </li> <li> <p>Save     Click this to save an edited or new file to the <code>workspace</code> directory.  </p> </li> <li> <p>Save As     Click this if you want to save an existing saved file with a different name. If you click this for an untitled      Siddhi file, the normal save operation\u00a0is executed\u00a0(i.e., same operation carried out when you click Save ). </p> </li> <li> <p>Import File     Click this to open a file from a system location. This file is opened in a new tab in the saved state with the same      file name with which it is imported. </p> </li> <li> <p>Export File      Click this to export a saved file to a system location. This is only applicable to Siddhi application tabs that are      in a saved state.</p> </li> <li> <p>Close File     Click this to close a currently active Siddhi application that is already open in a tab.  </p> </li> <li> <p>Close All Files      Click this to close all the Siddhi files that are currently open. </p> </li> <li> <p>Delete File     Click this to delete the currently active Siddhi file from the <code>workspace</code> directory. Only Siddhi files that are     already saved can be deleted.</p> </li> <li> <p>Settings     Click this to change the theme and the font size used in the Streaming Integrator Tooling. The default theme is Twilight .</p> </li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/#edit-menu-items","title":"Edit menu Items","text":"<p>The Edit menu includes the following options.</p> <p></p> <ul> <li> <p>Undo     Click this to undo the last edit made to the Siddhi application that     you are currently editing. Only unsaved edits can be undone.</p> </li> <li> <p>Redo     Click this to redo the edit that was last undone in the Siddhi     application that you are currently editing. The redo operation can     be carried out only if you have not saved the Siddhi application     after you undid the change.</p> </li> <li> <p>Find      Click this to search for a specific string in the currently     active Siddhi application tab.</p> </li> <li> <p>Find and Replace     Click this to search for a specific string in the currently     active Siddhi application tab, and replace it with another string.</p> </li> <li> <p>Reformat Code     Click this to reformat the Siddhi queries in the Siddhi     application that you are currently creating/editing in the source view.</p> <p>Info</p> <p>This menu option is only visible when you are working in the source view.</p> </li> <li> <p>Auto-Align      Click this to horizontally align all the Siddhi components in a     Siddhi application that you are creating/editing in the design view.</p> <p>Info</p> <p>This menu option is only visible when you are working in the design view.</p> </li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/#run-menu-items","title":"Run menu Items","text":"<p>The Run menu includes the following options.</p> <p></p> <ul> <li> <p>Run     Click this to start the Siddhi application in the Run mode. Only     saved Siddhi applications can be run.</p> <p>Info</p> <p>This menu option is enabled only when a Siddhi application is being created/edited in the source view.</p> </li> <li> <p>Stop     Click this to stop a Siddhi application that is already running.</p> </li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/#tools-menu-items","title":"Tools menu items","text":"<p>The Tools menu provides access to the following tools that are shipped with the Streaming Integrator Tooling.</p> <p></p> <ul> <li> <p>File Explorer </p> <p>The file explorer. This is also avaible in the Side Panel.</p> </li> <li> <p>Extension Installer</p> <p>This opens the Extension Installer dialog box (shown below) where you can search for the required extension and install/uninstall it by clicking Install or Uninstall as appropriate. Once you install/uninstall an extension, you need to restart the Streaming Integrator Tooling. For detailed instructions, see Installing Siddhi Extensions.</p> <p> </p> </li> <li> <p>Event Simulator </p> <p>Simulation can be carried out in two ways:</p> <ul> <li>Single Simulation</li> <li>Feed Simulation</li> </ul> <p>For detailed information about event simulation, see Simulating Events. The event simulator can also be accessed from the Side Panel.</p> </li> <li> <p>Error Store Explorer</p> <p>This opens the Error Store Explorer in which you can view, correct and replay the streaming events with errors that are stored in the error store.</p> <p>In order to use it, first you need to connect it to the SI server by clicking Connect to Server and then entering the server configurations in the dialog box that appears.</p> <p></p> <p>Once the Error Store Explorer is connected to an SI server, you can get an overview of events with errors as shown in the example below.</p> <p></p> <p>Here, you can do the following:</p> <ul> <li> <p>Fetch Errors: At a given time, the Error Store Explorer displays the number of events with errors from the time it was last fetched/refreshed. To get the latest errors, select the required Siddhi application in the Siddhi app field and click Fetch.</p> </li> <li> <p>View details: To view details of a specific error, click Detailed Info for the relevant event. Then the details of the error are displayed in the Error Entry dialog box as shown in the example below.        </p> </li> <li> <p>Replay events: This can be done in one of the following three methods:</p> <ul> <li> <p>To replay all the events for the selected Siddhi application without making any changes, click Replay All.</p> </li> <li> <p>To replay a single event without making any changes, click Replay for the specific event.</p> </li> <li> <p>To make any changes to the event before replaying, click Detailed Info for the relevant event and open the Error Entry dialog box. Then make the required changes to the event (which is displayed in an editable field) and click Replay.</p> <p>Info</p> <p>Note that only mapping errors can be corrected before replaying.</p> <p></p> </li> </ul> </li> <li> <p>Discard events: To discard all the erroneous events for the selected Siddhi application, click Discard All. To discard a specific erroneous event, click Discard for the relevant event.</p> </li> <li> <p>Purge Events: To periodically purge events, click Purge. This opens the Purge Error Store dialog box. In the Retention Period (days) field, enter the number of past days for which you want to retain the erroneous events in the store and then click Purge. As a result, all the events except the ones stored in the specified number of past days are removed from the error store.</p> </li> </ul> <p>To understand how to save messages with errors in the error store, see Handling Errors.</p> <p>To understand how to view and analyze events with errors in the Error Store Explorer, see Handling Requests with Errors.</p> </li> <li> <p>Console</p> <p>This is an output console that provides feedback on various user activities carried out on the Streaming Integration Tooling. It is accessible from the Side Panel.</p> </li> <li> <p>Sample Event Generator</p> <p>This opens the Sample Event Generator as follows.  Here, you can generate sample events for a selected stream within a selected Siddhi application in a specified format.  </p> </li> <li> <p>On-Demand Query</p> <p>This opens the On-Demand Query dialog box.</p> <p> Here, you can select a Siddhi application, and then enter a query to manipulate the store in which that Siddhi Application saves data. You can enter queries that can update record, insert/update records, retrieve records and delete records. For more information about actions you can carry out for stores, see Storage Integration - Performing CRUD operations via REST API.</p> </li> <li> <p>Tour Guide</p> <p>This opens a dialog box named Welcome to the Streaming Integrator Tooling Tour Guide which guides you to understand Streaming Integrator Tooling. When you start the Streaming Integrator Tooling and access it, this dialog box is open by default.</p> </li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/#deploy-menu-items","title":"Deploy menu items","text":"<p>The Deploy menu has the following option to select one or more Siddhi applications and deploy them to one or more  Streaming Integrator servers. For more information, see Deploying Siddhi Applications.</p> <p></p>"},{"location":"develop/streaming-integrator-studio-overview/#export-menu-items","title":"Export menu items","text":"<p>The Export menu has the following options that allow you to export Siddhi applications in a format that can be deployed in a containerized environment.</p> <p></p> <ul> <li> <p>For Docker     This opens the Export Siddhi Apps for Docker image wizard. For more information, see Exporting Siddhi Applications - Exporting Siddhi applications as a Docker Image.</p> </li> <li> <p>For Kubernetes     This opens the Export Siddhi Apps For Kubernetes CRD wizard. For more information, see Exporting Siddhi Applications - Exporting Siddhi Applications for Kubernetes.</p> </li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/#side-panel","title":"Side Panel","text":"<p>File Explorer</p> <p></p> <p>This provides a view of all the files saved as shown in the example above.</p>"},{"location":"develop/streaming-integrator-studio-overview/#event-simulator","title":"Event Simulator","text":"<p>Simulation can be carried out in two ways:</p> <ul> <li>Single Simulation</li> <li>Feed Simulation</li> </ul> <p>For detailed information about event simulation, see Simulating Events.</p>"},{"location":"develop/streaming-integrator-studio-overview/#output-console","title":"Output Console","text":"<p>This provides feedback on various user activities carried out on the Streaming Integrator.</p>"},{"location":"develop/streaming-integrator-studio-overview/#operator-finder","title":"Operator Finder","text":"<p>Click the Operator Finder icon to search for the Siddhi extensions that you want to use in your Siddhi applications.</p> <ul> <li>For the complete list of Siddhi extensions that you can search for     via the Operator Finder, see Siddhi Extensions.</li> </ul>"},{"location":"develop/streaming-integrator-studio-overview/#template-variables","title":"Template Variables","text":"<p>Click this icon to open the Templated Variables side panel (shown below) where you can open the templated attributes in the currently saved Siddhi applications and provide variables to be applied when running the Siddhi applications.</p> <p></p> <p>For more information about templated variables, see Siddhi Documentation - Siddhi as a Local Microservice - Running with environmental/system variables.</p>"},{"location":"develop/streaming-integrator-studio-overview/#toolbar","title":"Toolbar","text":"<ul> <li> <p>Run icon     Click this to start a currently open Siddhi application in the Run     mode. This icon is enabled only for saved Siddhi applications.  </p> </li> <li> <p>Stop icon     Click this to stop a Siddhi application that is currently running.</p> </li> <li> <p>Revert icon     Click this to revert the unsaved changes in the Siddhi application     that is currently being created/edited.</p> </li> </ul>"},{"location":"develop/testing-a-Siddhi-Application/","title":"Testing Siddhi Applications","text":"<p>The Streaming Integrator allows the following tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run in an actual production environment.</p> <ul> <li>Validate Siddhi applications that are written in the Streaming Integrator Tooling.</li> <li>Run Siddhi applications that were written in the Streaming Integrator Tooling.</li> <li>Simulate events to test the Siddhi applications and analyze events     that are received and sent. This allows you to analyze the status of     each query within a Siddhi application at different execution     points.</li> </ul>"},{"location":"develop/testing-a-Siddhi-Application/#validating-a-siddhi-application","title":"Validating a Siddhi application","text":"<p>To validate a Siddhi application, follow the procedure below:</p> <ol> <li> <p>Start and access the Streaming Integrator Tooling. For detailed     instructions, see Starting Streaming Integrator Tooling.</p> </li> <li> <p>In this example, let's use an existing sample as an example. Click     on the ReceiveAndCount sample to open it.</p> </li> <li> <p>Sample opens in a new tab. This sample does not have errors, and     therefore, no errors are displayed\u00a0in the editor. To create an error     for demonstration purposes, change the <code>count()</code> function in the <code>query1</code> query to     <code>countNew()</code> as shown below.</p> <pre><code>@info(name='query1') from SweetProductionStream select countNew() as totalCount insert into TotalCountStream;      `\n</code></pre> <p>Now, the editor indicates that there is a syntax error. If you move the cursor over the error icon, it indicates that <code>countNew</code> is an invalid function name as shown below. </p> </li> </ol>"},{"location":"develop/testing-a-Siddhi-Application/#running-a-siddhi-application","title":"Running a Siddhi application","text":"<p>You can run a Siddhi application to verify whether the logic you have written is correct. To start a Siddhi application, follow the procedure below:</p> <ol> <li> <p>Start and access the Streaming Integrator Tooling. For detailed     instructions, see Starting Stream Integration Tooling.</p> </li> <li> <p>For this example, click the existing sample ReceiveAndCount. It opens in a new untitled tab.</p> </li> <li> <p>Save the Siddhi file so that you can run it. To save it, click File -&gt; Save. Once the file is saved,     you can see the Run -&gt; Run menu option enabled as shown     below.</p> <p></p> <p>To start the application, click the Run menu option. This logs the following output in the console.</p> <pre><code>![Run log](https://wso2.github.io/docs-si/images/Testing-Siddhi-Applications/Run-Log.png)</code></pre> </li> <li> <p>To create an error for demonstration purposes, change the <code>count()</code> function in the     <code>query1</code> query to <code>countNew()</code>, and save. Then click Run -&gt; Run. As a result, the     following output is logged in the console.</p> <p></p> </li> </ol> <p></p>"},{"location":"develop/testing-a-Siddhi-Application/#simulating-events","title":"Simulating events","text":"<p>This section demonstrates how to test a Siddhi application via event simulation. Event simulation involves simulating  predefined event streams.\u00a0These event stream definitions have stream attributes. You can use event simulator to create  events by assigning values to the defined stream attributes and send them as events. This\u00a0is useful for testing Siddhi  applications in order to evaluate whether they function as expected</p> <p>Events can be simulated in the following methods:</p> <ul> <li>Simulating a single event</li> <li>Simulating multiple events via CSV files</li> <li>Simulating multiple events via databases</li> <li>Generating random events</li> </ul> <p>Tip</p> <p>Before you simulate events for a Siddhi application, you need to run it. Therefore, before you try this section, see Running a Siddhi application.</p>"},{"location":"develop/testing-a-Siddhi-Application/#simulating-a-single-event","title":"Simulating a single event","text":"<p>This section demonstrates how to simulate a single event to be processed via the Streaming Integrator.</p> <p>Tip</p> <p>Before simulating events, a Siddhi application should be deployed.</p> <p>To simulate a single event, follow the steps given below.</p> <ol> <li> <p>Access the Streaming Integrator Tooling via the <code>http://localhost:/editor</code>     URL. The Streaming Integrator Tooling opens as shown below.</p> <p>Info</p> <p>The default URL is<code>http://localhost:9090/editor</code>.</p> <p></p> </li> <li> <p>Click the Event Simulator icon in the left pane of the editor to     open the Single Simulation panel.</p> <p></p> <p>It opens the left panel for event simulation as follows.</p> <p></p> </li> <li> <p>Enter Information in the Single Simulation panel as described     \u00a0below.</p> <ol> <li> <p>In the Siddhi App Name field, select a currently deployed Siddhi application.</p> </li> <li> <p>In the Stream Name field, select the event stream for which you want to simulate events. The list displays all the event streams defined in the selected Siddhi application.</p> </li> <li> <p>If you want to simulate the event for a specific time different to the current time, enter a valid timestamp in the Timestamp field. To select a timestamp, click the time and calendar icon next to the Timestamp field.</p> <p> </p> <p>Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered.\u00a0If you want to select the current time, you can click Now.</p> </li> <li> <p>Enter values for the attributes of the selected stream.</p> </li> </ol> </li> <li> <p>Click Send to start sending the event. The simulated event is logged similar to the sample log given below.</p> <p></p> </li> </ol>"},{"location":"develop/testing-a-Siddhi-Application/#simulating-multiple-events-via-csv-files","title":"Simulating multiple events via CSV files","text":"<p>This section explains how to generate multiple events via CSV files to be analyzed via the Streaming Integrator.</p> <p>Tip</p> <p>Before simulating events, a Siddhi application should be deployed.</p> <p>To simulate multiple events from a CSV file, follow the steps given below.</p> <ol> <li> <p>Access the Streaming Integrator Tooling via the <code>http://localhost:/editor</code>    URL. The Streaming Integrator Tooling opens as shown below.</p> <p>Info</p> <p>The default URL is<code>http://localhost:9090/editor</code>.</p> <p></p> </li> <li> <p>Click the Event Simulator icon in the left pane of the editor.</p> <p></p> </li> <li> <p>In the event simulation left panel that opens, click on the Feed Simulation tab.</p> <p> </p> </li> <li> <p>To create a new simulation, click Create . This opens the     following panel.</p> <p></p> </li> <li> <p>Enter values for the displayed fields as follows.</p> <ol> <li> <p>In the Simulation Name field, enter a name for the event simulation.</p> </li> <li> <p>In the Description field, enter a description for the event simulation.</p> </li> <li> <p>If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field.</p> </li> <li> <p>Click Advanced Configurations if you want to enter detailed specifications to filter events from the CSV file. Then enter information as follows.</p> <ol> <li> <p>If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time \u00a0in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively.\u00a0To select a timestamp, click the time and calendar icon next to the field.</p> <p></p> <p>Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered.\u00a0If you want to select the current time, you can click Now.</p> </li> <li> <p>If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field.</p> </li> </ol> </li> <li> <p>In the Simulation Source field, select CSV File.</p> </li> <li> <p>Click Add Simulation Source to open the following section.</p> <p></p> <p>In the Siddhi App Name field, select the required Siddhi application. Then more fields as shown below.</p> <p></p> <p>Enter details as follows:</p> <ol> <li> <p>In the Stream Name field, select the stream for which you want to simulate events. All the streams defined in the Siddhi application you selected are available in the list.</p> </li> <li> <p>In the CSV File field, select an available CSV file. If no CSV files are currently uploaded, select Upload File from the list. This opens the Upload File dialog box.</p> <p></p> <p>Click Choose File and browse for the CSV file you want to upload. Then click Upload.</p> </li> <li> <p>In the Delimiter field, enter the character you want to use in order to separate the attribute values in each row of the CSV file.</p> </li> <li> <p>If you want to enter more detailed specifications, click Advanced Configuration. Then enter details as follows.</p> <ol> <li> <p>To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index.</p> </li> <li> <p>If you want to increase the value of the timestamp for each new event, select the Increment event time by(ms) option.\u00a0Then enter the number of milliseconds by which you want to increase the timestamp of each event.</p> </li> <li> <p>If you want the events to arrive in order based on the timestamp, select Yes under the Timestamp Interval option.</p> </li> </ol> </li> <li> <p>Click Save to save the information relating to the CSV file. The name os the CSV file appears in the Feed Simulation tab in the left panel.</p> </li> </ol> </li> </ol> </li> <li> <p>To simulate a CSV file that is uploaded and visible in the Feed Simulation tab in the left panel, click on the arrow to its right. The simulated events are logged in the output console.</p> </li> </ol>"},{"location":"develop/testing-a-Siddhi-Application/#simulating-multiple-events-via-databases","title":"Simulating multiple events via databases","text":"<p>This section explains how to generate multiple events via databases to be analyzed via the Streaming Integrator.</p> <p>Tip</p> <p>Before simulating events via databases: -   A Siddhi application must be created. -   The database from which you want to simulate events must be already configured for the Streaming Integrator.</p> <p>To simulate multiple events from a database, follow the procedure below:</p> <ol> <li> <p>Access the Streaming Integrator Tooling via the <code>http://localhost:/editor</code> URL. The Streaming Integrator Tooling opens as shown below.</p> <p>Info</p> <p>The default URL is<code>http://localhost:9090/editor</code>.</p> <p></p> </li> <li> <p>Click the Event Simulator icon in the left pane of the editor.</p> <p></p> </li> <li> <p>Click the Feed tab to open the Feed Simulation panel.</p> <p></p> </li> <li> <p>To create a new simulation, click Create. This opens the following panel.</p> <p></p> </li> <li> <p>Enter values for the displayed fields as follows.</p> <ol> <li> <p>In the Simulation Name field, enter a name for the event simulation.</p> </li> <li> <p>In the Description field, enter a description for the event simulation.</p> </li> <li> <p>If you want to simulate events at time intervals of a specific length, enter that length in milliseconds in the Time Interval(ms) field.</p> </li> <li> <p>If you want to enter more advanced conditions to simulate the events, click Advanced Configurations. As a result, the following section is displayed.</p> <p></p> <p>Then enter details as follows:</p> <ol> <li> <p>If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time \u00a0in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively.\u00a0To select a timestamp, click the time and calendar icon next to the field.</p> <p></p> <p>Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered.\u00a0If you want to select the current time, you can click Now.</p> </li> <li> <p>If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field.</p> </li> </ol> </li> <li> <p>In the Simulation Source field, select Database. To connect to a new database, click Add Simulation Source to open the following section.</p> <p></p> <p>Enter information as follows:</p> Field Description Siddhi App Name Select the Siddhi Application in which the event stream for which you want to simulate events is defined. Stream Name Select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. Data Source The JDBC URL to be used to access the required database. Driver Class The driver class name of the selected database. Username The username that must be used to access the database. Password The password that must be used to access the database. </li> <li> <p>Once you have entered the above information, click Connect to Database. If the datasource is correctly configured, the following is displayed to indicate that Streaming Integrator can successfully connect to the database.</p> <p></p> </li> <li> <p>To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want the vents in the CSV file to be sorted based on the timestamp, select the Yes option under CSV File is Ordered by Timestamp.</p> </li> <li> <p>To increase the timestamp of the published events, select the Timestamp Interval option. Then enter the number of milliseconds by which you want to increase the timestamp of each event.</p> </li> </ol> </li> <li> <p>Click Save. This adds the fed simulation you created as an active simulation in the Feed Simulation tab of the left panel as shown below.</p> <p></p> </li> <li> <p>Click on the play button of this simulation to open the Start the Siddhi Application dialog box.</p> <p></p> </li> <li> <p>Click Start Simulation. A message appears to inform you that the feed simulation started successfully. Similarly, when the simulation is completed, a message appears to inform you that the event simulation has finished.</p> </li> </ol>"},{"location":"develop/testing-a-Siddhi-Application/#generating-random-events","title":"Generating random events","text":"<p>This section explains how to generate random data to be analyzed via the Streaming Integrator.</p> <p>Tip</p> <p>Before simulating events, a Siddhi application should be deployed.</p> <p>To simulate random events, follow the steps given below:</p> <ol> <li> <p>Access the Streaming Integrator Tooling via the <code>http://localhost:/editor</code>    URL. The Streaming Integrator Tooling opens as shown below.</p> <p>Info</p> <p>The default URL is<code>http://localhost:9090/editor</code>.</p> <p></p> </li> <li> <p>Click the Event Simulator icon in the left pane of the editor.</p> <p></p> </li> <li> <p>Click the Feed tab to open the Feed Simulation panel.</p> <p></p> </li> <li> <p>To create a new simulation, click Create . This opens the following panel.</p> <p></p> </li> <li> <p>Enter values for the displayed fields as follows.</p> <ol> <li> <p>In the Simulation Name field, enter a name for the event simulation.</p> </li> <li> <p>In the Description field, enter a description for the event simulation.</p> </li> <li> <p>If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time \u00a0in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively.\u00a0To select a timestamp, click the time and calendar icon next to the field.</p> <p></p> <p>Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered.\u00a0If you want to select the current time, you can click Now.</p> </li> <li> <p>If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field.</p> </li> <li> <p>If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field.</p> </li> <li> <p>In the Simulation Source field, select Random.</p> </li> <li> <p>If the random simulation source from which you want to simulate events does not already exist in the Feed Simulation pane, click Add New to open the following section.</p> <p></p> </li> <li> <p>Enter information relating to the random source as follows:</p> <ol> <li>In the Siddhi App Name field, s elect the name of the     Siddhi App with the event stream for which the events are     simulated.</li> <li>In the Stream Name field, select the event stream for     which you want to simulate events. All the streams defined     in the Siddhi Application you selected are available to be     selected.</li> <li>In the Timestamp Interval field, enter the number of     milliseconds by which you want to increase the timestamp of     each event.</li> <li> <p>To enter values for the stream attributes, follow     the\u00a0instructions below.</p> <ul> <li>To enter a custom value for a stream attribute, select     Custom data based from the list. When you select     this value, data field in which the required value can     be entered appears as shown in the example below. {width=\"235\"     height=\"88\"}</li> <li> <p>To enter a primitive based value, select Primitive     based from the list. The information to be entered     varies depending on the data type of the attribute. The     following table explains the information you need to     enter when you select Primitive based for each data     type.  </p> <p> Data Type Values to enter <code>                     STRING                    </code> Specify a length in the Length field that appears. This results in a value of the specified length being auto-generated. <code>                     FLOAT                    </code> or <code>                     DOUBLE                    </code> The value generated for the attribute is based on the following values specified. <ul> <li>Min : The minimum value.</li> <li>Max : The maximum value.</li> <li>Precision : The precise value. The number of decimals included in the auto-generated values are the same as that of the value specified here.</li> </ul> <code>                     INT                    </code> or <code>                     LONG                    </code> The value generated for the attribute is based on the following values specified. <ul> <li>Min : The minimum value.</li> <li>Max : The maximum value.</li> </ul> <code>                     BOOL                    </code> No further information is required because <code>                     true                    </code> and <code>                     false                    </code> values are randomly generated. </p> </li> <li> <p>To randomly assign values based on a pre-defined set of meaningful values,\u00a0select Property based from the list. When you select this value, a field in which the set of available values are listed appears as shown in the example below.</p> <p></p> </li> <li> <p>To assign a regex value, select Regex based from the list.</p> </li> </ul> </li> </ol> </li> </ol> </li> <li> <p>Click Save to save the simulation information. The saved random simulation appears in the Feed tab of the left panel.</p> </li> <li> <p>To simulate events, click the arrow to the right of the saved simulation (shown in the example below).</p> <p></p> <p>The simulated events are logged in the CLI as shown in the extract below.</p> <p> </p> </li> </ol>"},{"location":"develop/working-with-the-Design-View/","title":"Working with the Design View","text":"<p>This section provides an overview of the\u00a0design view of the Streaming Integrator Tooling.</p>"},{"location":"develop/working-with-the-Design-View/#accesing-the-design-view","title":"Accesing the Design View","text":"<p>To open the design view of the Streaming Integrator Tooling:</p> <ol> <li>Start the Streaming Integrator Tooling and log in with your credentials.</li> <li>Click New and open a new Siddhi file, or click Open and open an existing Siddhi file.</li> <li>Click Design View to open the Design View.      The design view opens as shown in the example below. It consists of     a grid to which you can drag and drop the Siddhi components     represented by icons displayed in the left panel to design a Siddhi     application. </li> </ol>"},{"location":"develop/working-with-the-Design-View/#adding-siddhi-components","title":"Adding Siddhi components","text":"<p>To add a Siddhi component to the Siddhi application that you are creating/editing in the design view, click on the  relevant icon in the left pane, and then drag and drop it to the grid as demonstrated in the example below.</p> <p></p> <p>Once you add a Siddhi component, you can configure it as required. To configure a Siddhi component, click the settings icon on the component. This opens a form with parameters related to the relevant component.</p> <p></p> <p>The following is the complete list of Siddhi components that you can add to the grid of the design view when you create a Siddhi application.</p>"},{"location":"develop/working-with-the-Design-View/#stream","title":"Stream","text":"Icon Description A stream represents a logical series of events ordered in time. For a detailed description of streams, see Siddhi Query Guide - Stream. Form <p>To configure the stream, click the settings icon on the stream component you added to the grid. Then enter values as follows:</p> <p>Stream Name : A unique name for the stream. This should be specified in title caps, and without spaces (e.g., <code>ProductionDataStream</code> ).</p> <p>Attributes : Attributes of streams are specified as name and type pairs in the Attributes table.</p> <p>If you want to generate the the stream from a file or a database, click Generate Stream. However, note that you need to create the relevant file or set up the database and the tables beforehand.</p> <p></p> <p>The Generate Stream form opens as follows</p> <p></p> <p>To generate the stream from a file:                 <ol> <li>In the Generate Stream form, select From File.</li> <li><p>Then click Choose File and browse for the file from which you want to generate the stream.  the supported file types are <code>CSV</code>, <code>JSON</code>, and <code>XML</code>. If you select a file that is not of one of these types, the Select File Type field is enabled as shown in the example below.</p> <p></p> <p>You are required to select the appropriate file type in this field in order to proceed to generate the stream from the selected file.</p> <p>The rest of the fields that appear in the dialog box differ based on the file type as explained below. If required, change the default values that appear in them as required</p> <ul> <li>If the file type is CSV:</li> <ul> <li>Stream Name: A name for the stream that you are generating.</li> <li>Delimiter: A blank space, comma, or any other character/symbol that indicates the beginning or the end of a character string, word, or a data item.</li> <li>Is Header Exists: If this is set to true, a header exists for the purpose of identifying the attribute names and the data types of the data values in the file.</li> </ul> <li>If the file type is JSON:</li> <ul> <li>Stream Name: A name for the stream that you are generating.</li> <li>Enclosing Element: The symbol/element used to enclose the JSON object.</li> </ul> <li>If the file type is XML:</li> <ul> <li>Stream Name: A name for the stream that you are generating.</li> <li>Namespace: This is an optional field to enter an XML namespace.</li> <li>Enclosing Element: The symbol/element used to enclose the XML object.</li> </ul> </ul> </li> <li>Click Generate. The Stream Configuration form is populated with the values in the file you selected.</li> </ol> </p> <p>To generate the stream from a database:                 <ol> <li>In the Generate Stream form, select From Database.</li> <li>If you want to provide the data source definition inline, select Inline Configuration. If not, select Provide Datasource to select a data source that is already defined externally.</li> <li> Enter details relating to the data source as follows.</li> <ul> <li>If you are defining the data source configuration inline, enter information as follows:</li> <p></p> <ul> <li>Stream Name: A name for the stream that you are generating.</li> <li>Database URL: The URL via which you can connect to the database.</li> <li>Username: The username via which you access the database.</li> <li>Password: The password via which you access the database.</li> <li>Table Name: The name of the database table from which you are generating the stream. To make the available tables appear in this field as a list so that you can select one, enter the relevant information in the previous fields and click Retrieve Tables.</li> </ul> <li>If the data source you are using is already defined externally, enter information as follows:</li> <p></p> <ul> <li>Stream Name: A name for the stream that you are generating.</li> <li>Datasource Name: The name of the data source from which you are generating the stream.</li> <li>Table Name: The name of the database table from which you are generating the stream. To make the available tables appear in this field as a list so that you can select one, enter the relevant data source in the Datasource Name field and click Retrieve Tables.</li> </ul> <li>Click Generate to generate the stream.</li> Example <p></p> <p>The details entered in the above  form creates a stream configuration as follows:</p> <pre><code>define stream SweetProductionStream (amount double, name string);</code></pre> Source <ul> <li>Sources</li> <li>Projection queries</li> <li>Filter queries</li> <li>Window queries</li> <li>Join queries</li> </ul> Target <ul> <li>Sinks</li> <li>Projection queries</li> <li>Filter queries</li> <li>Window queries</li> <li>Join queries</li> </ul>"},{"location":"develop/working-with-the-Design-View/#source","title":"Source","text":"Icon Description A source receives events in the specified transport and in the specified format. For more information, see Siddhi Query Guide - Source. Form <p>To configure the source, click the settings icon on the source component you added to the grid. This opens a form where you can enter the following information:</p>                    To access the form in which you can configure a source, you must first connect the source as the source (input) object to a stream component.                <ul> <li>Source Type : This specifies the transport type via which the events are received. The value should be entered in lower case (e.g., <code>tcp</code> ). The other parameters displayed for the source depends on the source type selected.</li> <li>Map Type : This specifies the format in which you want to receive the events (e.g., <code>xml</code> ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs.</li> <li> <p>Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box.</p> </li> </ul> Example <p></p> <p>The details entered in the above  form creates a source configuration as follows:</p> <pre><code>@source(type = 'tcp',     @map(type = 'json',\n        @attributes(name =\"$.sweet\", amount = \"$.batch.count\")))</code></pre> Source No connection can start from another Siddhi component and link to a source because a source is the point from which events selected into the event flow of the Siddhi application start. Target <p>Streams</p>"},{"location":"develop/working-with-the-Design-View/#sink","title":"Sink","text":"Icon Description A sink publishes events via the specified transport and in the specified format. For more information, see Siddhi Query Guide - Sink. Form <p>To configure the sink, click the settings icon on the sink component you added to the grid.</p>                !!! info  To access the form in which you can configure a sink, you must first connect the sink as the target object to a stream component.                <ul> <li>Sink Type : This specifies the transport via which the sink publishes processed events. The value should be entered in lower case (e.g., <code>log</code> ).</li> <li>Map Type : This specifies the format in which you want to publish the events (e.g., <code>passThrough</code> ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs.</li> <li> <p>Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box.</p> </li> </ul> Example <p></p> <p>The details entered in the above  form creates a sink configuration as follows:</p> <pre><code>&gt;@sink(type = 'log', prefix = \"Sweet Totals:\"</code></pre> Source Streams Target <p>N/A</p> <p>A sink cannot be followed by another Siddhi component because it represents the last stage of the event flow where the results of the processing carried out by the Siddhi application are communicated via the required interface.</p>"},{"location":"develop/working-with-the-Design-View/#table","title":"Table","text":"Icon Description A table is a stored version of an stream or a table of events. For more information, see Siddhi Query Guide - Table. Form <p>To configure the table, click the settings icon on the table component you added to the grid.</p> <p>Name : This field specifies unique name for the table. This should be specified in title caps, and without spaces (e.g., <code>              ProductionDataTable             </code> ).</p> <p>Attributes : Attributes of tables are specified as name and type pairs in the Attributes table. To add a new attribute, click +Attribute.</p> <p>Store Type : This specifies the specific database type in which you want to stopre data or whether the data is to be stored in-memory. Once the store type is selected, select an option to indicate whether the datastore needs to be defined inline, whether you want to use a datasource defined in the <code>              &lt;SP_HOME&gt;/conf/worker/deployment.yaml             </code> file, or connected to a JNDI resource. For more information, see Defining Tables for Physical Stores. The other parameters configured under Store Type depend on the store type you select.</p> <p>Annotations : This section allows you to specify the table attributes you want to use as the primary key and indexes via the <code>              @primarykey             </code> and <code>              @index             </code> annotations. For more information, see Defining Data Tables. If you want to add any other custom annotations to your table definition, click +Annotation to define them.</p> Example <p></p> <p>The details entered in the above  form creates a table definition as follows:</p> <pre><code>@store(type = 'rdbms', datasource = \"SweetProductionDB\")\ndefine table ShipmentDetails (name string, supplier string, amount double);</code></pre> Source <ul> <li>Projection queries</li> <li>Window queries</li> <li>Filter queries</li> <li>Join queries</li> </ul> Target <ul> <li>Projection queries</li> <li>Window queries</li> <li>Filter queries</li> <li>Join queries</li> </ul>"},{"location":"develop/working-with-the-Design-View/#window","title":"Window","text":"Icon Description This icon represents a window definition that can be shared across multiple queries. For more information, see Siddhi Query Guide - (Defined) Window. Form <p>To configure the window, click the settings icon on the window component you added to the grid, and update the following information.</p> <ul> <li>Name : This field specifies a unique name for the window. <code>               PascalCase              </code> is used for window names as a convention.</li> <li>Attributes : Attributes of windows are specified as name and type pairs in the Attributes table.</li> <li>Window Type : This specifies the function of the window (i.e., the window type such as <code>               time              </code> , <code>               length              </code> , <code>               frequent              </code> etc.). The window types supported include time , timeBatch , timeLength , length , lengthBatch , sort , frequent , lossyFrequent , cron , externalTime , externalTimeBatch.</li> <li>Parameters : This section allows you to define one or more parameters for the window definition based on the window type you entered in the Window Type field.</li> <li>Annotations : If you want to add any other custom annotations to your window definition, click +Annotation to define them.</li> </ul> Example <p></p> <p>The details entered in the above form creates a window definition as follows:</p> <pre><code>define window FiveMinTempWindow (roomNo int, temp double) time(5 min) output all events;</code></pre> Source <ul> <li>Projection queries</li> <li>Window queries</li> <li>Filter queries</li> <li>Join queries</li> </ul> Target <ul> <li>Projection queries</li> <li>Window queries</li> <li>Filter queries</li> <li>Join queries</li> </ul>"},{"location":"develop/working-with-the-Design-View/#trigger","title":"Trigger","text":"Icon Description A trigger allows you to generate events periodically. For more information, see Siddhi Query Guide - Trigger. Form <p>To configure the trigger, click the settings icon on the trigger component you added to the grid, and update the following information.</p> <ul> <li>Name : A unique name for the trigger</li> <li>Trigger Criteria : This specifies the criteria based on which the trigger is activated. Possible values are as follows:                     <ul> <li>start : Select this to trigger events when the Streaming Integrator server has started.</li> <li>every : Select this to specify a time interval at which events should be triggered.</li> <li>cron-expression : Select this to enter a cron expression based on which the events can be triggered. For more information about cron expressions, see the quartz-scheduler.</li> </ul> </li> </ul> Example <p></p> <p>The details entered in the above form creates a trigger definition as follows:</p> <pre><code>define trigger FiveMinTriggerStream at every 5;</code></pre> <p></p> Source N/A Target <ul> <li>Projection queries</li> <li>Window queries</li> <li>Filter queries</li> <li>Join queries</li> </ul> Icon <p></p> Description A trigger allows you to generate events periodically. For more information, see Siddhi Query Guide - Trigger. Form <p>To configure the trigger, click the settings icon on the trigger component you added to the grid, and update the following information.</p> <ul> <li>Name : A unique name for the trigger.</li> <li> Trigger Criteria : This specifies the criteria based on which the trigger is activated. Possible values are as follows:                      <ul> <li>start : Select this to trigger events when the Streaming Integrator server has started.</li> <li>every : Select this to specify a time interval at which events should be triggered.</li> <li>cron-expression : Select this to enter a cron expression based on which the events can be triggered. For more information about cron expressions, see the  Example <p></p> <p>The details entered in the above orm creates a trigger definition as follows:</p> <pre><code>define trigger FiveMinTriggerStream at every 5;</code></pre> <p></p> Source N/A Target <ul> <li>Projection queries</li> <li>Window queries</li> <li>Filter queries</li> <li>Join queries</li> </ul>"},{"location":"develop/working-with-the-Design-View/#aggregation","title":"Aggregation","text":"Icon Description <p>Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. For more information, see Siddhi Query Guide - Incremental Aggregation.</p>                !!! tip                Before you add an aggregation, make sure that you have already added the stream with the events to which the aggregation is applied is already defined.              Form <p>To configure the aggregation, click the settings icon on the aggregation component you added to the grid, and update the following information.</p> <ul> <li>Aggregation Meta Information : In this section, define a unique name for the aggregation in the Name field, and specify the stream from which the input information is taken to perform the aggregations. You can also select the optional annotations you want to use in the aggregation definition by selecting the relevant check boxes. For more information about configuring the annotations once you select them, see Incremental Analysis.</li> <li>Projection : This section specifies the attributes to be included in the aggregation query. In the Select field, you can select All attributes to perform the aggregation for all the attributes of the stream specified under Input , or select User Defined Attributes to select specific attributes. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output.</li> <li>Aggregation Criteria : Here, you can specify the time values based on which the aggregates are calculated.</li> </ul> Example <p></p> <p>The details entered in the above form creates an aggregation definition as follows:</p> <pre><code>define aggregation TradeAggregation\nfrom TradeStream\nselect symbol, avg(price) as avgPrice, sum(price) as total\n    group by symbol\n    aggregateby timestamp every seconds...years;</code></pre> Source N/A Target Join queries"},{"location":"develop/working-with-the-Design-View/#function","title":"Function","text":"Icon Description The function icon represents Script in Siddhi Query Language. It allows you to write functions in other programming languages and execute them within Siddhi queries. A function component in a Siddhi application is not connected to ther Siddhi components in the design UI. However, the configuration of one or more Query components can include a reference to it. Form <p>To configure the function, click the settings icon on the function component you added to the grid, and update the following information.</p> <ul> <li>Name : A unique name for the function.</li> <li>Script Type : The language in which the function is written.</li> <li>Return Value : The data format of the value that is generated as the output via the function.</li> <li>Script Body : This is a free text field to write the function in the specified script type.</li> </ul> Example <p></p> <p>The details entered in the above form creates a function definition as follows:</p> <pre><code>define function concatFN[JAVASCRIPT] return string {\n    var str1 = ata[0];\n    var str2 = data[1];\n    var str3= data[2];\n    var responce = str1 + str2 + str3;\n    return responce;\n};</code></pre>"},{"location":"develop/working-with-the-Design-View/#projection-query","title":"Projection Query","text":"Icon Description                 !!! tip                <p>Before you add a projection query:</p> <p>You need to add and configure the following:</p> <ul> <li>The input stream with the events to be processed by the query.</li> <li>The output stream to which the events processed by the query are directed.</li> </ul> <p>This icon represents a query to project the events in an input stream to an output stream. This invoves selectng the attributes to be included in the output, renaming attributes, introducing constant values, and using mathematical and/or logical expressions. For more information, see Siddhi Query Guide - Query Projection.</p> Form <p>Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the projection query, click the settings icon on the projection query component you added to the grid, and update the following information.</p> <ul> <li>Query Meta Information : This section specifies the stream to be considered as the input stream with the events to which the query needs to be applied. The input stream connected to the query as the source is automatically displayed.</li> <li>Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output.</li> <li> Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows:                      <ul> <li>Operation : This field specifies the operation to be performed on the generated output event (e.g., <code>                  Insert                 </code> to insert events to a selected stream/table/window).</li> <li>Into : This field specifies the stream/table/window in which the operation specified need to be performed.</li> <li>Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.|</li> </ul> </li> </ul> Example <p></p> <p>The details entered in the above form creates a query as follows:</p> <pre><code>from TradeStream\nselect symbol, avg(price) as averagePrice, sum(volume) a total\ninsert all events into OutputStream;</code></pre> Source <ul> <li>Streams</li> <li>Tables</li> <li>Triggers</li> <li>Windows</li> </ul> Target <ul> <li>Streams</li> <li>Tables</li> <li>Windows</li> </ul>"},{"location":"develop/working-with-the-Design-View/#filter-query","title":"Filter Query","text":"Icon Description  !!! tip <p>Before you add a filter query:</p> <p>You need to add and configure the following:</p> <ul> <li>The input stream with the events to be processed by the query.</li> <li>The output stream to which the events processed by the query are directed.</li> </ul> <p>A filter query filters information in an input stream based on a given condition. For more information, see Siddhi Query Guide - Filters.</p> Form <p>Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the filter query, click the settings icon on the filter query component you added to the grid, and update the following information.</p> <ul> <li><p>By default, the Stream Handler check box is selected, and a stream handler of the <code>filter</code> type is available under it to indicate that the query is a filter. Expand this stream handler, and enter the condition based on which the information needs to be filtered.</p> <p>!!! info</p> <p>A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler. Multiple functions, filters and windows can be defined within the same form as stream handlers.</p> </li> <li>Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output.</li> <li>Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: <ul> <li>Operation : This field specifies the operation to be performed on the generated output event (e.g., <code>Insert</code> to insert events to a selected stream/table/window).</li> <li>Into : This field specifies the stream/table/window in which the operation specified need to be performed.</li> <li>Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.</li> </ul></li> </ul> Example <p></p> <p>The details entered in the above form creates a query with a filter as follows:</p> <pre><code>from TradeStream[sum(amount)&gt; 10000]\nselect symbol, avg(price) as averagePrice, sum(amount) as total\ninsert all events into OutputStream;\nSource\n<ul>\n<li>Streams</li>\n<li>Tables</li>\n<li>Triggers</li>\n<li>Windows</li>\n</ul>\nTarget\n<ul>\n<li>Streams</li>\n<li>Tables</li>\n<li>Windows</li>\n</ul>"},{"location":"develop/working-with-the-Design-View/#window-query","title":"Window Query","text":"Icon\n         Description\n         \n               !!! tip\n               <p>Before you add a window query:</p>\n               <p>You need to add and configure the following:</p>\n               <ul>\n                  <li>The input stream with the events to be processed by the query.</li>\n                  <li>The output stream to which the events processed by the query are directed.</li>\n               </ul>\n               <p>Window queries include a window to select a subset of events to be processed based on a specific criterion. For more information, see Siddhi Query Guide - (Defined) Window.</p>\n            Form\n         <p>Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the window query, click the settings icon on the window query component you added to the grid, and update the following information.</p>\n               <ul>\n                  <li>\n                     <p>By default, the Stream Handler check box is selected, and a stream handler of the <code>window</code> type is available under it to indicate that the query is a filter. Expand this stream handler, and enter details to determine the window including the window type and the basis on which the subset of events considered by the window is determined (i.e., based on the window type selected).</p>\n                     <p>!!! info</p>\n                     <p>A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler. Multiple functions, filters and windows can be defined within the same form as stream handlers.</p>\n                     </li>\n                  <li>Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output.</li>\n                  <li>\n                     Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows:\n                     <ul>\n                        <li>Operation : This field specifies the operation to be performed on the generated output event (e.g., <code>Insert</code> to insert events to a selected stream/table/window).</li>\n                        <li>Into : This field specifies the stream/table/window in which the operation specified need to be performed.</li>\n                        <li>Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.</li>\n                     </ul>\n                  </li>\n               </ul>\n            Example\n         <p></p>\n               <p>The details entered in the above Query Configuration form creates a query with a window as follows:</p>\n               <pre><code>from TradeStream#window.time(1 month)\nselect symbol, avg(price) as averagePrice, sum(amount) as total\ninsert all events into OutputStream;</code></pre>\n                     Source\n         \n               !!! info\n               <p>A window query can have only one source at a given time.</p>\n               <ul>\n                  <li>Streams</li>\n                  <li>Tables</li>\n                  <li>Triggers</li>\n                  <li>Windows</li>\n               </ul>\n            Target\n         <ul>\n               <li>Streams</li>\n               <li>Tables</li>\n               <li>Windows</li>\n            </ul>"},{"location":"develop/working-with-the-Design-View/#join-query","title":"Join Query","text":"Icon\n         Description\n         A join query derives a combined result from two streams in real-time based on a specified condition. For more information, see Siddhi Query Guide - Join.\n      Form\n         <p>Once you connect two Siddhi components to the join query as sources and another Siddhi component as the target, you can configure the join query. To configure the join query, click the settings icon on the join query component you added to the grid and update the following information.</p>\n               <ul>\n                  <li>Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The <code>                @dist               </code> annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations.</li>\n                  <li>Input : Here, you can specify the input sources, the references, the join type, join condition, and stream handlers for the left source and right source of the join. For a detailed explanation of the join concept, see Siddhi Query Guide - Joins.</li>\n                  <li>Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output.</li>\n                  <li>\n                     Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows:\n                     <ul>\n                        <li>Operation : This field specifies the operation to be performed on the generated output event (e.g., <code>                  Insert                 </code> to insert events to a selected stream/table/window).</li>\n                        <li>Into : This field specifies the stream/table/window in which the operation specified need to be performed.</li>\n                        <li>Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.</li>\n                     </ul>\n                  </li>\n               </ul>\n            Example\n         <p>A join query is configured as follows:</p>\n               <p>\n                   The above configurations result in creating the following join query.\n               </p>\n               <pre><code>from TempStream[temp &gt; 30.0]#window.time(1 min) as T\n  join RegulatorStream[isOn == false]#window.length(1) as R\n  on T.roomNo == R.roomNo\nselect T.roomNo, R.deviceID, 'start' as action\ninsert into RegulatorActionStream;</code></pre>\n                     Source\n         \n               !!! info\n               A join query must always be connected to two sources, and at least one of them must be a defined stream/trigger/window.\n               <ul>\n                  <li>Streams</li>\n                  <li>Tables</li>\n                  <li>Aggregations</li>\n                  <li>Windows</li>\n               </ul>\n            Target\n         \n               !!! info\n               A join query must always be connected to a single target.\n               <ul>\n                  <li>Streams</li>\n                  <li>Tables</li>\n                  <li>Windows</li>\n               </ul>"},{"location":"develop/working-with-the-Design-View/#pattern-query","title":"Pattern Query","text":"Icon\n         Description\n         \n               !!! tip\n               <p>Before you add a pattern query:</p>\n               <p>You need to add and configure the following:</p>\n               <ul>\n                  <li>The input stream with the events to be processed by the query.</li>\n                  <li>The output stream to which the events processed by the query are directed.</li>\n               </ul>\n               <p>A pattern query detects patterns in events that arrive overtime. For more information, see Siddhi Query Guide - Patterns.</p>\n            Form\n         <p>Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the pattern query, click the settings icon on the pattern query component you added to the grid and update the following information.</p>\n               <ul>\n                  <li>Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The <code>                @dist               </code> annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations.</li>\n                  <li>Input : This section defines the conditions based on which patterns are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Pattern concept, see Siddhi Query Guide - Patterns.</li>\n                  <li>Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output.</li>\n                  <li>\n                     Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows:\n                     <ul>\n                        <li>Operation : This field specifies the operation to be performed on the generated output event (e.g., <code>                  Insert                 </code> to insert events to a selected stream/table/window).</li>\n                        <li>Into : This field specifies the stream/table/window in which the operation specified need to be performed.</li>\n                        <li>Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.</li>\n                     </ul>\n                  </li>\n               </ul>\n            Example\n         <p></p>\n               <p>The above configuration results in creating the following query.</p>\n               <pre><code>from every (e1=MaterialSupplyStream) -&gt; not MaterialConsumptionStream[name == e1.name and amount == e1.amount]\n    for 15 sec\nselect e1.name, e1.amount\ninsert into ProductionDelayAlertStream;</code></pre>\n                     Source\n         <ul>\n               <li>Streams</li>\n               <li>Tables</li>\n               <li>Triggers</li>\n               <li>Windows</li>\n            </ul>\n         Target\n         <ul>\n               <li>Streams</li>\n               <li>Tables</li>\n               <li>Windows</li>\n            </ul>"},{"location":"develop/working-with-the-Design-View/#sequence-query","title":"Sequence Query","text":"Icon\n         Description\n         \n               !!! tip\n               <p>Before you add a sequence query:</p>\n               <p>You need to add and configure the following:</p>\n               <ul>\n                  <li>The input stream with the events to be processed by the query.</li>\n                  <li>The output stream to which the events processed by the query are directed.</li>\n               </ul>\n               <p>A sequence query detects sequences in event occurrences over time. For more information, see Siddhi Query Guide - Sequence.</p>\n            Form\n         <p>Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the sequence query, click the settings icon on the sequence query component you added to the grid and update the following information.</p>\n               <ul>\n                  <li>Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The <code>                @dist               </code> annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations.</li>\n                  <li>Input : This section defines the conditions based on which sequences are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Sequence concept, see Siddhi Query Guide - Sequences.</li>\n                  <li>Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output.</li>\n                  <li>\n                     Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows:\n                     <ul>\n                        <li>Operation : This field specifies the operation to be performed on the generated output event (e.g., <code>                  Insert                 </code> to insert events to a selected stream/table/window).</li>\n                        <li>Into : This field specifies the stream/table/window in which the operation specified need to be performed.</li>\n                        <li>Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.</li>\n                     </ul>\n                  </li>\n               </ul>\n            Example\n         <p></p>\n               <p>The above configuration results in creating the following query.</p>\n               <pre><code>&lt;from every e1=SweetProductionStream,\ne2=SweetProductionStream[e1.amount &gt; amount and (timestamp - e1.timestamp) &lt; 10 * 6000]*,\ne3=SweetProductionStream[timestamp - e1.timestam &gt; &lt;10 * 60000 and e1.amount &gt; amount]\nselect e1.name, e1.amount as initialAmount, e2.amount as finalAmount, e2.timestamp\ninsert into DecreasingTrendAlertStream;</code></pre>\n                     Source\n         <ul>\n               <li>Streams</li>\n               <li>Tables</li>\n               <li>Triggers</li>\n               <li>Windows</li>\n            </ul>\n         Target\n         <ul>\n               <li>Streams</li>\n               <li>Tables</li>\n               <li>Windows</li>\n            </ul>"},{"location":"develop/working-with-the-Design-View/#partitions","title":"Partitions","text":"Icon\n         Description\n         \n               !!! tip\n               <p>Before you add a partition:</p>\n               <p>You need to add the stream to be partitioned.</p>\n               <p>Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. For more information, see Siddhi Query Guide - Partition.</p>\n            Form\n         <p>Once the stream to be partitioned is connected as a source to the partition, you can configure the partition. In order to do so, move the cursor over the partition and click the settings icon on the partition component. This opens the Partition Configuration form. In this form, you can enter expressions to convert the attributes of the stream that is selected to be partitioned.</p>\n            Example\n         <p></p>\n               <p>The above configuration creates the following partition query.</p>\n               <pre><code>                 roomNo &lt; &lt;1030 and roomNo &gt;= 330 as 'officeRoom' or\n                 roomNo &lt; 330 as 'lobby' of TempStream)\nbegin\n    from TempStream#window.time(10 min)\n    select roomNo, deviceID, avg(temp) as avgTemp\n    insert into AreaTempStream\nend\n                     Source\n         Streams\n      Target\n         N/A"},{"location":"develop/working-with-the-Design-View/#connecting-siddhi-components","title":"Connecting Siddhi components","text":"<p>In order to define how the Siddhi components in a Siddhi application\ninteract with each other to process events, you need to define\nconnections between Siddhi components. A connection is defined by\ndrawing an arrow from one component to another by dragging the cursor as\ndemonstrated below.</p>\n<p></p>"},{"location":"develop/working-with-the-Design-View/#saving-and-running-siddhi-applications","title":"Saving and running Siddhi applications","text":"<p>To save a Siddhi application that you created in the design view, you need to switch to the source view. You also need to switch to the source view to run a Siddhi application. For more information, see Streaming Integrator Tiooling Overview.</p>"},{"location":"examples/creating-etl-application-via-tooling/","title":"Creating an ETL Application via SI Tooling","text":""},{"location":"examples/creating-etl-application-via-tooling/#introduction","title":"Introduction","text":"<p>ETL (Extract, Transform, Load) is a form of data processing that involves performing the following functions in the given order:</p> <ol> <li> <p>Extract: Obtaining input data from a specific source such as a file or a database.</p> </li> <li> <p>Transform: Converting the data obtained to a different form.</p> </li> <li> <p>Load Writing the data extracted and transformed into another destination.</p> </li> </ol> <p>Tutorials such as Performing Real-time ETL with Files and Performing Real-time ETL with MySQL show how the WSO2 Streaming Integrator can perform ETL for streaming data by writing and deploying Siddhi applications with ETL functionality. If you need to create such a Siddhi application without writing code, you can use the ETL Flow wizard in Streaming Integrator Tooling.</p> <p>In this tutorial, let's create the same Siddhi application created in Performing Real-time ETL with MySQL using the Streaming Integrator Tooling.</p> <p>Before you begin:</p> <ul> <li>You need to have access to a MySQL instance.</li> <li>Enable binary logging in the MySQL server. For detailed instructions, see Debezium documentation - Enabling the binlog.         If you are using MySQL 8.0, use the following query to check the binlog status: <code>SELECT variable_value as \"BINARY LOGGING STATUS (log-bin) ::\"         FROM performance_schema.global_variables WHERE variable_name='log_bin';</code></li> <li>Add the MySQL JDBC driver into the <code>&lt;SI_HOME&gt;/lib</code> directory as follows:<ol> <li>Download the MySQL JDBC driver from the MySQL site.</li> <li>Unzip the archive.</li> <li>Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> to the <code>&lt;SI_HOME&gt;/lib</code> directory.</li> <li>Start the SI server by issuing the appropriate command based on your operating system.<ul> <li>For Windows: <code>streaming-integrator.bat</code></li> <li>For Linux:  <code>sh server.sh</code> </li> </ul> </li> </ol> </li> <li>Once you install MySQL and start the MySQL server, create the database and the database table you require as follows:<ol> <li>Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. <code>CREATE SCHEMA production;</code></li> <li>Create a new user by executing the following SQL query. <code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2';</code></li> <li>Switch to the <code>production</code> database and create a new table, by executing the following queries: <code>use production;</code> <code>CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));</code> </li> </ol> </li> <li>Download <code>productions.csv</code> file from here and save it in a location of your choice. (e.g., in <code>/Users/foo</code>). </li> <li>Download and install Streaming Integrator Tooling </li> <li>Download and install the siddhi-io-cdc extension. For instructions, see Downloading and Instaling Siddhi Connectors.</li> </ul>"},{"location":"examples/creating-etl-application-via-tooling/#tutorial-steps","title":"Tutorial steps","text":""},{"location":"examples/creating-etl-application-via-tooling/#step-1-design-the-siddhi-application-with-etl-functionality","title":"Step 1: Design the Siddhi application with ETL functionality","text":"<p>To design the Siddhi application with ETL functionality via the Streaming Integrator Tooling, follow the steps below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling by navigating to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issuing one of the following commands as appropriate, based on your operating system:</p> <ul> <li> <p>For Windows: <code>streaming-integrator-tooling.bat</code></p> </li> <li> <p>For Linux: <code>./streaming-integrator-tooling.sh</code></p> </li> </ul> <p>Then Access the Streaming Integrator Tooling via the URL that appears in the start up log with the text <code>Editor Started on:</code>.</p> </li> <li> <p>In the Welcome screen, click New ETL Flow.</p> <p></p> <p>This opens the wizard to create ETL task flows as follows.</p> <p></p> </li> <li> <p>Change the title of the ETL task flow from <code>UntitledETLTaskFlow</code> to <code>SweetFactoryETLTaskFlow</code>.</p> </li> <li> <p>In Step 1 Configure Source, enter information relating to the data source as follows:</p> <ol> <li> <p>Under Transport Properties, select CDC as the source. Then enter values for the properties relating to the CDC source as follows.</p> <p></p> Property Value url <code>jdbc:mysql://localhost:3306/production</code> username <code>wso2si</code> password <code>wso2</code> table.name <code>SweetProductionTable</code> operation <code>insert</code> <p>Then click Next.</p> </li> <li> <p>In the Configure Schema section, do the following to define the schema of the events you expect to receive as input data:</p> <ol> <li> <p>Click the tick (\u2713) for the Add log sink for testing parameter.</p> </li> <li> <p>Under Enter input stream name, enter <code>InsertSweetProductionStream</code>. Then add two attributes as follows:</p> <p></p> <ol> <li> <p>Move the cursor over the + sign next to input stream attributes and select STRING. As a result, a new row is created for the attribute. Enter <code>name</code> as the attribute name.</p> </li> <li> <p>Move the cursor over the + sign again, and then select DOUBLE. Then enter <code>amount</code> as the attribute name.</p> </li> <li> <p>Click Next. </p> </li> </ol> </li> </ol> </li> <li> <p>In the Configure Input Mapping section, select keyvalue as the source mapper type.</p> <p></p> <p>Then click Next.</p> </li> </ol> </li> <li> <p>In this scenario, let's do a simple conversion where the names that are received in simple case are converted to upper case when they are published in the file. This is a mapping-related conversion. Therefore, in Step 2 Process Input Data, click Next without making any change.</p> </li> <li> <p>In Step 3 Configure Destination, enter information on how you want the output to be published. In this scenario, let's publish the output in a CSV file named <code>productioninserts.csv</code>.</p> <ol> <li> <p>Under Transport Properties, select file as the sink type. Then enter the path to the <code>productioninserts.csv</code> file which you saved as an empty CSV file (in this example, <code>/Users/foo/productioninserts.csv</code>).</p> <p></p> <p>Then click Next.</p> </li> <li> <p>In the Configure Schema section, enter information as follows to create an output stream that defines the schema of the outgoing events.</p> <p></p> <ol> <li> <p>Click the tick (\u2713) for the Add log sink for testing parameter in order to log the output events in the console.</p> </li> <li> <p>Under Enter output stream name, enter <code>ProductionUpdatesStream</code>.</p> </li> <li> <p>Move the cursor over the + sign next to output stream attributes and select STRING. As a result, a new row is created for the attribute. Enter <code>name</code> as the attribute name.</p> </li> <li> <p>Move the cursor over the + sign again, and then select DOUBLE. Then enter <code>amount</code> as the attribute name.</p> </li> <li> <p>Click Next.</p> </li> </ol> </li> <li> <p>In the Configure Output Mapping section, select text as the sink mapper type.</p> <p></p> <p>Then click Next.</p> </li> </ol> </li> <li> <p>In Step 4 Process Output Data, move the cursor over the + sign under Group Output by Fields, and then click name. This groups the output events by the name of the product.</p> <p></p> <p>Then click Next.</p> </li> <li> <p>In Step 5 Data Mapping, follow the procedure below to do the required configurations for the data transformation to be done by your ETL Siddhi application.</p> <ol> <li> <p>Click the following button to map all the attributes.</p> <p></p> <p>As a result, the attributes in the input stream and the output stream are joined together by lines as shown below.</p> <p></p> <p>This indicates that the value for each input attribute is directed to the output stream without any further processing to be published. However, since you need to do a simple conversion for the <code>name</code> attribute. Therefore, remove the matching for that attribute by clicking the following icon for it under Output Attributes. Move the cursor to the right of the attribute to make this icon appear.</p> <p></p> </li> <li> <p>Click on name under Output Attributes.</p> <p></p> <p>This opens a dialog box named Create expression for name of ProductionUpdatesStream. </p> </li> <li> <p>In the Create expression for name of ProductionUpdatesStream dialog box, click Function. Scroll to find the str.upper() function, and then click on it to select it.</p> <p></p> <p>Once select the function, it is displayed as follows. Click on the selected function again.</p> <p></p> </li> <li> <p>When the function is added as shown below, click on it again.</p> <p></p> <p>Another bar appears below the selected function with the function expression in the clickable mode between the brackets.</p> </li> <li> <p>To specify the attribute to which the function should be applied, click on the dots between the brackets.</p> <p></p> </li> <li> <p>Click on name attribute to select it as the attribute to which the function applies.</p> <p></p> </li> <li> <p>Once the <code>name</code> attribute is selected and displayed, click the arrow pointing upwards to the right of the attribute. This adds the <code>name</code> attribute to the function expression.</p> <p></p> </li> <li> <p>Once the function is displayed with both the expression and the attribute, click the arrow pointing upwards to the right of it. This completes the function configuration.</p> <p></p> </li> <li> <p>Click Submit.</p> <p>Now both the attributes appear matched again, and the function expression is displayed for the name attribute.</p> <p></p> </li> <li> <p>Click Save.</p> </li> </ol> </li> <li> <p>In Step 6 Finalize, deploy the Siddhi application you just completed by clicking Deploy to Worker.</p> <p></p> <p>This opens the Deploy Siddhi Apps to Server dialog box.</p> <ol> <li> <p>In the Add New Server section, enter the host, port, user name and the password of your Streaming Integrator server as shown below.</p> <p></p> <p>Then click Add.</p> </li> <li> <p>In the Siddhi Apps to Deploy section, select the checkbox for the SweetFactoryETLTaskFlow.siddhi application. In the Servers section, select the check box for the server you added. Then click Deploy.</p> <p></p> <p>The following message appears in the Deploy Siddhi Apps to Server dialog box.</p> <p><code>SweetFactoryETLTaskFlow.siddhi was successfully deployed to 0.0.0.0:9444</code></p> </li> </ol> </li> </ol>"},{"location":"examples/creating-etl-application-via-tooling/#step-2-test-the-siddhi-application","title":"Step 2: Test the Siddhi application","text":"<p>To test the Siddhi application, insert a record to the <code>SweetProductionTable</code> MySQL table by issuing the following command in your MySQL console.</p> <p><code>insert into SweetProductionTable values('chocolate',100.0);</code></p> <p>The following log appears in the Streaming Integrator console.</p> <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : logStream : Event{timestamp=1563200225948, data=[chocolate, 100.0], isExpired=false}\n</code></pre> <p>If you open the <code>/Users/foo/productions.csv</code> file, the <code>Chocalate, 100.0</code> record is displayed as shown below.</p> <p></p>"},{"location":"examples/creating-etl-application-via-tooling/#whats-next","title":"What's Next?","text":"<p>Once you develop an ETL application, you may need to carry out following tasks:</p> <ul> <li> <p>Error Handling: To understand how to handle errors that may occur when carrying out ETL operations, try the Managing Streaming Data with Errors tutorial.</p> </li> <li> <p>Monitoring ETL Statistics: For instructions to set up pre-configured dashboards provided with WSO2 Streaming Integrator and visualize statistics related to your ETL flows, see Monitoring ETL Statistics with Grafana.</p> </li> </ul>"},{"location":"examples/exposing-processed-data-as-api/","title":"Siddhi Query API","text":""},{"location":"examples/exposing-processed-data-as-api/#introduction","title":"Introduction","text":"<p>Siddhi query API is the REST API exposed by the Streaming Integrator (SI). It gives you a set of APIs to perform all of the essential operations relating to Siddhi applications, including developing, testing, and querying them.</p> <p>Siddhi query API provides APIs related to: - Siddhi application management (such as creating, updating, deleting a Siddhi application; Listing all running Siddhi applications etc.) - Event simulation - Authentication and Permission management - Health check - Siddhi Store operations</p> <p>For a comprehensive reference on the Siddhi query API, see Streaming Integration REST API Guide.</p> <p>This tutorial demonstrates how you can use the Siddhi query API to perform essential operations in SI, using simple examples.</p>"},{"location":"examples/exposing-processed-data-as-api/#preparing-the-server","title":"Preparing the server","text":"<p>Before you begin:</p> <ul> <li>You need to have access to a MySQL instance.</li> <li>Save the MySQL JDBC driver in the <code>&lt;SI_HOME&gt;/lib</code> directory as follows:</li> <li>Download the MySQL JDBC driver from the MySQL site.</li> <li>Unzip the archive.</li> <li>Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> to the <code>&lt;SI_HOME&gt;/lib</code> directory.</li> <li>Start the SI server.</li> </ul> <ol> <li> <p>Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query.     <pre><code>CREATE SCHEMA production;\n</code></pre></p> </li> <li> <p>Create a new user by executing the following SQL query.     <pre><code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2';\n</code></pre></p> </li> <li> <p>Switch to the <code>production</code> database and create a new table, by executing the following queries:     <pre><code>use production;\n</code></pre></p> <pre><code>CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));\n</code></pre> </li> </ol>"},{"location":"examples/exposing-processed-data-as-api/#creating-a-siddhi-application","title":"Creating a Siddhi application","text":"<ol> <li> <p>Open a text file and copy-paste following application to it.</p> <pre><code>@App:name(\"SweetProduction-Store\")\n\n@App:description('Receive events via HTTP and persist the received data in the store.')\n\n@Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',\n    @map(type='json'))\ndefine stream insertSweetProductionStream (name string, amount double);\n\n@Store(type=\"rdbms\",\n       jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\",\n       username=\"wso2si\",\n       password=\"wso2\" ,\n       jdbc.driver.name=\"com.mysql.jdbc.Driver\")\ndefine table SweetProductionTable (name string, amount double);\n\nfrom insertSweetProductionStream\nupdate or insert into SweetProductionTable\non SweetProductionTable.name == name;\n</code></pre> <p>Here the <code>jdbc.url</code> parameter has the value <code>jdbc:mysql://localhost:3306/production?useSSL=false</code>. Change it to point to your MySQL server. Similarly change <code>username</code> and <code>password</code> parameters as well.</p> </li> <li> <p>Save this file as <code>SweetProduction-Store.siddhi</code> in a location of your choice in the local file system.</p> </li> <li> <p>Now you need to execute a <code>CURL</code> command and deploy this Siddhi application. In the command line, navigate to the location where you saved the Siddhi application in the previous step, and execute following command:     <pre><code>curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @SweetProduction-Store.siddhi -u admin:admin -k\n</code></pre></p> </li> </ol> <p>Upon successful deployment, the following response is logged for the <code>CURL</code> command you just executed.     <pre><code>{\"type\":\"success\",\"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\"}\n</code></pre></p> <p>In addition to that, the following is logged in the SI console.     <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App SweetProduction-Store deployed successfully\n</code></pre></p> <pre><code>!!!info\n    Next, you are going to send a few events to `insertSweetProductionStream` stream via a `CURL` command.</code></pre> <ol> <li> <p>Execute following <code>CURL</code> command in the console:     <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\"  http://localhost:8006/productionStream --header \"Content-Type:application/json\"\n</code></pre></p> <p>Info</p> <p>You have written the Siddhi application to insert a new record from the <code>insertSweetProductionStream</code> stream into the <code>SweetProductionTable</code> table, or to update the record if it already exists in the table. As a result, above event is now inserted into the <code>SweetProductionTable</code>.</p> </li> <li> <p>To verify whether above event is inserted into <code>SweetProductionTable</code>, execute following <code>SQL</code> query in the SQL console:     <pre><code>SELECT * FROM SweetProductionTable;\n</code></pre></p> <p>The following table appears to indicate that the record has been inserted into the table. <pre><code>+---------------+--------+\n| name          | amount |\n+---------------+--------+\n| Almond cookie | 100.00 |\n+---------------+--------+\n</code></pre></p> </li> </ol>"},{"location":"examples/exposing-processed-data-as-api/#running-a-siddhi-store-api-query","title":"Running a Siddhi Store API query","text":"<p>You can use <code>Siddhi Store Query API</code> to execute queries on Siddhi Store tables.</p> <p>In this section shows you how to execute a simple store query via the REST API in order to fetch all records from the <code>SweetProductionTable</code> Siddhi Store table. To learn about other types of queries, see Streaming Integrator REST API Guide.</p> <p>Execute following <code>CURL</code> command on the console: <pre><code>curl -k -X POST http://localhost:7070/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select *\" }'\n</code></pre></p> <p>The following output is logged in the console: <pre><code>{\"records\":[[\"Almond cookie\",100.0]]}\n</code></pre></p>"},{"location":"examples/exposing-processed-data-as-api/#fetching-the-status-of-a-siddhi-application","title":"Fetching the status of a Siddhi Application","text":"<p>Now let's fetch the status of the Siddhi application you just deployed.</p> <p>Execute following <code>CURL</code> command, in the console: <pre><code>curl -X GET \"http://localhost:9090/siddhi-apps/SweetProduction-Store/status\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre></p> <p>The following output appears in the command line: <pre><code>{\"status\":\"active\"}\n</code></pre></p>"},{"location":"examples/exposing-processed-data-as-api/#taking-a-snapshot-of-a-siddhi-application","title":"Taking a snapshot of a Siddhi Application","text":"<p>In this section, deploy a stateful Siddhi application and use the REST API to take a snapshot of it. To do this, follow the procedure below:</p> <ol> <li> <p>To enable the state persistence feature in SI server, open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file in a text editor and locate the <code>state.persistence</code> section, and then update it as follows.</p> <pre><code>  # Periodic Persistence Configuration\nstate.persistence:\n  enabled: true\n  intervalInMin: 5\n  revisionsToKeep: 2\n  persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore\n  config:\n    location: siddhi-app-persistence\n</code></pre> <p>As shown above, set <code>enabled</code> parameter to <code>true</code> and set the <code>intervalInMin</code> to <code>5</code>. Then save the file.</p> </li> <li> <p>Restart the Streaming Integrator server for the above change to be effective.</p> </li> <li> <p>Open a text file and copy-paste following application into it.     <pre><code>@App:name(\"CountProductions\")\n\n@App:description(\"Siddhi application to count the total number of orders.\")\n\n@Source(type = 'http', receiver.url='http://localhost:8007/productionStream', basic.auth.enabled='false',\n    @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (totalProductions double);\n\n@info(name = 'query')\nfrom SweetProductionStream\nselect sum(amount) as totalProductions\ninsert into LogStream;\n</code></pre></p> </li> <li> <p>Save this file as <code>CountProductions.siddhi</code> in a location of your choice in the local file system.</p> </li> <li> <p>Now execute a <code>CURL</code> command and deploy this Siddhi application. To do this, use the command line to navigate to the location where you saved the Siddhi application in above step, and then execute following command.     <pre><code>curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @CountProductions.siddhi -u admin:admin -k\n</code></pre></p> </li> </ol> <p>Upon successful deployment, the following response is logged for the <code>CURL</code> command you just executed.     <pre><code>{\"type\":\"success\",\"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\"}\n</code></pre></p> <p>In addition to that, the following log appears in the SI console.     <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully.\n</code></pre></p> <ol> <li> <p>Now let's send  two sweet production events using <code>CURL</code> by issuing tghe following two <code>CURL</code> commands via the command line:     <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\"  http://localhost:8007/productionStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\"  http://localhost:8007/productionStream --header \"Content-Type:application/json\"\n</code></pre></p> </li> <li> <p>As a result, the following logs appears on the SI console:     <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288572024, data=[100.0], isExpired=false}\nINFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288596336, data=[120.0], isExpired=false}\n</code></pre>     Note that the current productions count is <code>120</code>.</p> </li> <li> <p>Now you can invoke the Siddhi Query API to take a snapshot of the Siddhi application. To do this, execute following <code>CURL</code> command on the command line:     <pre><code>curl -X POST \"https://localhost:9443/siddhi-apps/CountProductions/backup\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre></p> <p>An output similar to the following is logged. <pre><code>{\"revision\":\"1566293390654__CountProductions\"}\n</code></pre></p> <p>Info</p> <p><code>1566293390654__CountProductions</code> is the revision number of the Siddhi application snapshot that you requested via the REST API. You can store this revision number and later use it in order to restore the Siddhi application to the state at which you took the snapshot.</p> </li> </ol>"},{"location":"examples/exposing-processed-data-as-api/#restoring-a-siddhi-application-via-a-snapshot","title":"Restoring a\u00a0Siddhi Application via a snapshot","text":"<p>In the previous section, you took a snapshot of the <code>CountProductions</code> Siddhi application when the productions count was <code>120</code>. In this section, you can increase the count further by sending a few more production events, and then restore the Siddhi application to the state you backed up. To do this, follow the procedure below</p> <ol> <li>Send following two sweet production events:     <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\"  http://localhost:8007/productionStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Doughnut\\\",\\\"amount\\\": 500.0}}\"  http://localhost:8007/productionStream --header \"Content-Type:application/json\"\n</code></pre></li> </ol> <p>As a result, the following two lines of log appear in the SI console:     <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288572024, data=[420.0], isExpired=false}\nINFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566288596336, data=[920.0], isExpired=false}\n</code></pre>    Note that the current productions count is <code>920</code>.</p> <ol> <li> <p>Now you can invoke the Siddhi Query API to restore the snapshot that you obtained in step 10 of the Taking a snapshot of a Siddhi Application section of this tutorial.</p> <p>In this example, the revision number obtained is <code>1566293390654__CountProductions</code> (see step 10 in Taking a snapshot of a Siddhi Application section.). When restoring the state, use the exact revision number that you obtained.</p> <p>Note</p> <p>Replace <code>1566293390654__CountProductions</code> with the revision number that you obtained when taking the Siddhi application snapshot.</p> <p>The response you receive is as follows: <pre><code>{\"type\":\"success\",\"message\":\"State restored to revision 1566293390654__CountProductions for Siddhi App :CountProductions\"}\n</code></pre></p> <p>In addition to that, the following log is printed in the SI console: <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - State loaded for CountProductions revision 1566293390654__CountProductions from the file system.\n</code></pre></p> </li> <li> <p>Now send another sweet production event by executing the following <code>CURL</code> command:     <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Danish pastry\\\",\\\"amount\\\": 100.0}}\"  http://localhost:8007/productionStream --header \"Content-Type:application/json\"\n</code></pre></p> </li> </ol> <p>As a result, the following log appears in the SI console:     <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1566293520176, data=[220.0], isExpired=false}\n</code></pre>     Note that the productions count is <code>220</code>. This is because the count was reset to <code>120</code> when you restored the snapshot.</p>"},{"location":"examples/handling-requests-with-errors/","title":"Error Handling with Data Streams","text":""},{"location":"examples/handling-requests-with-errors/#introduction","title":"Introduction","text":"<p>In this tutorial, let's learn how you can handle streaming data that has errors (e.g., events that do not have values for certain attributes). WSO2 Streaming Integrator allows you to log such events, direct them to a separate stream or store them in a data store. If these errors occur at the time of publishing (e.g., due to a connection error), WSO2 SI also provides the option to wait and then resume to publish once the connection is stable again. For detailed information about different ways to handle errors, see the Handling Errors guide.</p> <p>In this scenario, you are handling erroneous events by directing them to a MySQL store.</p> <p>Before you begin:</p> <p>In order to save streaming data with errors in a MySQL store, complete the following prerequisites.  - Start the SI server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory and issuing one of the following commands as appropriate, based on your operating system:        - For Windows: <code>streaming-integrator.bat</code>        - For Linux:  <code>sh server.sh</code>    The following log appears in the Streaming Integrator console once you have successfully started the server.  <code>INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 4.240 sec</code>  - You need to have access to a MySQL instance.</p>"},{"location":"examples/handling-requests-with-errors/#tutorial-steps","title":"Tutorial steps","text":""},{"location":"examples/handling-requests-with-errors/#step-1-create-the-data-store","title":"Step 1: Create the data store","text":"<p>Let's create the MySQL data store in which the events with errors can be saved. To do this, follow the steps below:</p> <ol> <li> <p>Download the MySQL JDBC driver from the MySQL site.</p> </li> <li> <p>Unzip the archive.</p> </li> <li> <p>Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> to the <code>&lt;SI_HOME&gt;/lib</code> directory.</p> </li> <li> <p>Start the MySQL server as follows:</p> <p><code>mysql -u &lt;USERNAME&gt; -p &lt;PASSWORD&gt;</code></p> </li> <li> <p>Create a new database named <code>use siddhierrorstoredb;</code> by issuing the following command in the MySQL console.</p> <p><code>mysql&gt; create database siddhierrorstoredb;</code></p> </li> <li> <p>To switch to the new database, issue the following command.</p> <p><code>mysql&gt; use siddhierrorstoredb;</code></p> </li> </ol>"},{"location":"examples/handling-requests-with-errors/#step-2-enable-the-error-store","title":"Step 2: Enable the error store","text":"<p>To enable the error store, open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file and add a configuration as follows:</p> <pre><code>error.store:\n  enabled: true\n  bufferSize: 1024\n  dropWhenBufferFull: true\n  errorStore: org.wso2.carbon.streaming.integrator.core.siddhi.error.handler.DBErrorStore\n  config:\n    datasource: SIDDHI_ERROR_STORE_DB\n    table: SIDDHI_ERROR_STORE_TABLE\n</code></pre> <p>This configuration refers to a data source named <code>Error_Store_DB</code>. Define this data source as follows under <code>Data sources</code> in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>- name: SIDDHI_ERROR_STORE_DB\n  description: The datasource used for Siddhi error handling feature\n  jndiConfig:\n    name: jdbc/SiddhiErrorStoreDB\n  definition:\n    type: RDBMS\n    configuration:\n      jdbcUrl: 'jdbc:mysql://localhost:3306/siddhierrorstoredb?useSSL=false'\n      username: root\n      password: root\n      driverClassName: com.mysql.jdbc.Driver\n      minIdle: 5\n      maxPoolSize: 50\n      idleTimeout: 60000\n      connectionTestQuery: SELECT 1\n      validationTimeout: 30000\n      isAutoCommit: false\n</code></pre>"},{"location":"examples/handling-requests-with-errors/#step-3-create-and-deploy-the-siddhi-application","title":"Step 3: Create and deploy the Siddhi application","text":"<p>To create and deploy a Siddhi application, follow the steps below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling by navigating to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issuing one of the following commands as appropriate, based on your operating system:</p> <ul> <li> <p>For Windows: <code>streaming-integrator-tooling.bat</code></p> </li> <li> <p>For Linux: <code>./streaming-integrator-tooling.sh</code></p> </li> </ul> <p>Then Access the Streaming Integrator Tooling via the URL that appears in the start up log with the text <code>Editor Started on:</code>.</p> </li> <li> <p>Copy paste the following three Siddhi applications to three separate new files and save.     <pre><code>    @App:name(\"MappingErrorTest\")\n\n    @Source(type = 'http',\n             receiver.url='http://localhost:8006/productionStream',\n             basic.auth.enabled='false',\n         @map(type='json', @attributes(name='name', amount='amount')))\n    define stream ProductionStream(name string, amount double);\n\n    @sink(type='log', prefix='Successful mapping: ')\n    define stream LogStream(name string, amount double);\n\n    from ProductionStream\n    select *\n    insert into LogStream;\n</code></pre> <pre><code>   @App:name(\"SinkTransportErrorTest\")\n\n   @sink(type = 'http', on.error='STORE', blocking.io='true', \n         publisher.url = \"http://localhost:8090/unavailableEndpoint\", \n         method = \"POST\", @map(type = 'json'))\n   define stream TestPublisherStream (name string, amount double);\n\n   @Source(type = 'http', receiver.url='http://localhost:8007/testUnavailableEP', \n           basic.auth.enabled='false', \n           @map(type='json', enclosing.element='$.event', \n               @attributes(name='name', amount='amount')))\n   define stream TestInput(name string, amount double);\n\n   from TestInput#log('Sending to unavailableEndpoint: ')\n   select name, amount\n   insert into TestPublisherStream;\n</code></pre> <pre><code>   @App:name(\"ReceiveAndCount\")\n\n   @App:description('Receive events via HTTP transport and view the output on the console')\n\n   @Source(type = 'http',\n           receiver.url='http://localhost:8090/unavailableEndpoint',\n           basic.auth.enabled='false',\n           @map(type='json'))\n   define stream SweetProductionStream (name string, amount double);\n\n   @sink(type='log')\n   define stream TotalCountStream (totalCount long);\n\n   -- Count the incoming events\n   @info(name='query1')\n   from SweetProductionStream\n   select count() as totalCount\n   insert into TotalCountStream;\n</code></pre></p> </li> <li> <p>To deploy the Siddhi file, follow the procedure below:</p> <ol> <li> <p>Click the Deploy menu and then click Deploy to Server. This opens the Deploy Siddhi Apps to Server dialog box.</p> </li> <li> <p>In the Add New Server section, enter the host, port, user name and the password of your Streaming Integrator server as shown below.</p> <p>Tip</p> <p>To check the port of your streaming integrator server, open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml file</code> and search for <code>wso2.transport.http</code> -&gt; <code>Listener Configurations</code>. The port of your WSO2 Streaming Integrator port is specified under the listener configuration that has <code>msf4j-https</code> as the ID as shown in the extract below. In this example, it is <code>9443</code>. <pre><code>listenerConfigurations:\n    -\n      id: \"default\"\n      host: \"0.0.0.0\"\n      port: 9090\n    -\n      id: \"msf4j-https\"\n      host: \"0.0.0.0\"\n      port: 9443\n      scheme: https\n      keyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\"\n      keyStorePassword: wso2carbon\n      certPass: wso2carbon\n</code></pre></p> <p></p> <p>Then click Add.</p> </li> <li> <p>In the Siddhi Apps to Deploy section, select the check boxes for the MappingErrorTest.siddhi and SinkTransportErrorTest.siddhi applications. In the Servers section, select the check box for the server you added. Then click Deploy.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"examples/handling-requests-with-errors/#step-4-connect-the-error-store-explorer-to-the-si-server","title":"Step 4: Connect the Error Store Explorer to the SI server","text":"<p>The Error Store Explorer is a tool that allows you to view, correct and replay events with errors. It order to use it, it needs to be connected to the SI server.</p> <p>To connect the Error Store Explorer to the SI server, follow the procedure below:</p> <ol> <li> <p>Start the Streaming Integrator Tooling server by navigating to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issuing one of the following commands as appropriate, based on your operating system:</p> <ul> <li> <p>For Windows: <code>streaming-integrator-tooling.bat</code></p> </li> <li> <p>For Linux: <code>./streaming-integrator-tooling.sh</code></p> </li> </ul> <p>Then Access the Streaming Integrator Tooling via the URL that appears in the start up log with the text <code>Editor Started on:</code>.</p> </li> <li> <p>To open the Error Store Explorer, click Tools and then click Error Store Explorer.</p> <p>The Error Store Explorer opens as shown below. </p> <p></p> </li> <li> <p>Click Connect to Server. Then enter information as follows:    To check the port of the Streaming Integrator Server, Open /conf/server/deployment.yaml file. Under Listener Configurations of wso2.transport.http, locate the listener configuration with msf4j-https as the ID and specify its port as shown in the extract below. <p></p> Parameter Value Host <code>localhost</code> Port <code>9444</code> Username <code>admin</code> Password <code>admin</code> <p>Then click Connect.</p>"},{"location":"examples/handling-requests-with-errors/#step-5-test-the-event-mapping-failing-scenario","title":"Step 5: Test the event mapping failing scenario","text":""},{"location":"examples/handling-requests-with-errors/#step-51-publish-an-event-with-a-mapping-error-in-mappingerrortest-siddhi-application","title":"Step 5.1: Publish an event with a mapping error in MappingErrorTest Siddhi application","text":"<p>Send an event to the <code>ProductionStream</code> stream of the <code>MappingErrorTest</code>' Siddhi application by issuing the following CURL command.</p> <p><pre><code>curl --location --request POST 'http://localhost:8006/productionStream' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n                \"foo\": \"Cake\",\n                \"amount\": 20.12\n            }'\n</code></pre> The event causes an error referred to as <code>MappingFailedException</code>. This is because the <code>ProductionStream</code> expects an event in the following format which is specified via a custom mapping. <pre><code>{\n    \"name\": \"Cake\",\n    \"amount\": 20.12\n}\n</code></pre></p>"},{"location":"examples/handling-requests-with-errors/#step-52-manage-the-error-in-the-error-store-explorer","title":"Step 5.2: Manage the error in the Error Store Explorer","text":"<p>To manage the error in the Error Store Explorer, follow the procedure below:</p> <ol> <li> <p>To open the Error Store Explorer, open Streaming Integrator Tooling, click Tools and then click Error Store Explorer.</p> <p></p> <p>The Error Store Explorer opens as shown below.</p> <p></p> <p>The single error displayed is the mapping error that you previously simulated.</p> </li> <li> <p>To view details of the error, click Detailed Info. The following is displayed.</p> <p></p> </li> <li> <p>You can correct the mapping and replay the event. To do this, change <code>foo</code> in the event to <code>name</code> and click <code>replay</code>.</p> <p></p> <p>As a result, the Error Entry dialog box closes, and the Error Store Explorer dialog box is displayed with no errors.</p> <p>At the same time the following is logged for the Streaming Integrator Server.</p> <pre><code>    INFO {io.siddhi.core.stream.output.sink.LogSink} - Successful mapping:  : Event{timestamp=1595574091411, data=[Cake, 20.02], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"examples/handling-requests-with-errors/#step-6-test-the-event-failing-scenario-at-sink-level","title":"Step 6: Test the event failing scenario at sink level","text":""},{"location":"examples/handling-requests-with-errors/#step-61-trigger-an-event-flow-that-publishes-an-event-to-the-sinktransporterrortest-siddhi-application","title":"Step 6.1: Trigger an event flow that publishes an event to the SinkTransportErrorTest Siddhi application","text":"<p>Send an HTTP event to the <code>TestInput</code> stream of the <code>SinkTransportErrorTest</code> Siddhi application by issuing the following CURL command.</p> <pre><code>curl --location --request POST 'http://localhost:8007/testUnavailableEP' --header 'Content-Type: application/json' --data-raw ' { \"event\": { \"name\": \"Cake2\", \"amount\": 20.222 } }'\n</code></pre> <p>The following is logged in the Streaming Integrator Server console <pre><code>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - SinkTransportErrorTest: Sending to unavailableEndpoint: , StreamEvent{ timestamp=1597853565942, beforeWindowData=null, onAfterWindowData=null, outputData=[Cake2, 20.222], type=CURRENT, next=null}\n</code></pre> However, because the <code>http://localhost:8007/testUnavailableEP</code> is unavailable, the event is dropped at the sink level and then stored in the ErrorStore.</p>"},{"location":"examples/handling-requests-with-errors/#step-62-start-service-via-the-receiveandcount-siddhi-application","title":"Step 6.2: Start service via the ReceiveAndCount Siddhi application","text":"<p>In this step, let's start the service at <code>http://localhost:8007/testUnavailableEP</code> via the <code>ReceiveAndCount</code> Siddhi application</p> <p>In the Siddhi Apps to Deploy section, select the check box for the ReceiveAndCount.siddhi application. In the Servers section, select the check box for the server you added. Then click Deploy.</p> <p></p> <p>The following log is displayed in the Streaming Integrator console.</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ReceiveAndCount deployed successfully\n</code></pre>"},{"location":"examples/handling-requests-with-errors/#step-63-manage-the-error-in-the-error-store-explorer","title":"Step 6.3: Manage the error in the Error Store Explorer","text":"<p>To manage the error in the Error Store Explorer, follow the procedure below:</p> <ol> <li> <p>To open the Error Store Explorer, open Streaming Integrator Tooling, click Tools and then click Error Store Explorer.</p> <p></p> </li> <li> <p>In the Siddhi app field, select SinkTransportErrorTest Siddhi application and then click Fetch.</p> <p>As a result, an error is displayed as follows.</p> <p></p> <p>This indicates that the event was dropped because the end point was not available.</p> </li> <li> <p>To view details of the error, click Detailed Info. The following is displayed.</p> <p></p> <p>Previously, you deployed the <code>ReceiveAndCount</code> Siddhi application. Therefore, you can send the event again by clicking Replay.</p> <p></p> <p>As a result, the Error Entry dialog box closes, and the Error Store Explorer dialog box is displayed with no errors.</p> <p>At the same time the following is logged for the Streaming Integrator Server which is logged by the ReceiveAndCount Siddhi application.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveAndCount : TotalCountStream : Event{timestamp=1597857170244, data=[1], isExpired=false}    \n</code></pre> </li> </ol>"},{"location":"examples/integrating-stores/","title":"Integrating Data Stores in Streaming Integration","text":""},{"location":"examples/integrating-stores/#introduction","title":"Introduction","text":"<p>WSO2 Streaming Integrator allows you to incorporate data stores when performing various streaming integration activities. The methods in which this is done include:</p> <ul> <li>Change data capture</li> <li>Storing received data/processed data in data stores</li> <li>Performing CRUD operations in data stores</li> </ul> <p>[Performing Real-time Change Data Capture with MySQL] tutorial covers how to perform change data capture in details. Therefore, in this tutorial, let's learn how WSO2 Streaming Integrator can incorporate data stores in streaming operations by performing CRUD operations.</p>"},{"location":"examples/integrating-stores/#scenario","title":"Scenario","text":"<p>Let's consider the example of a Sweet Factory that stores the following information in three different databases:</p> <ul> <li>Records of material purchases to be used in production</li> <li>Records of material dispatches for production</li> <li>The current stock of materials</li> </ul> <p>In order to manage the material stock and to maintain the required records, the Factory Manager needs to do the following activities:</p> <ul> <li>Record each material purchase in the store for purchases.</li> <li>Record each dispatch of material for production in the store for dispatches.</li> <li>Update the store with current stock for each material after each purchase and dispatch to keep it up to date.</li> </ul> <p>Recording purchases and dispatches involve inserting new records into data stores. To maintain the current stock records, the Factory Manager needs to retrieve information about both purchases and dispatches, calculate the impact of both on the current stock and then perform an insert/update operation to the store with the stock records. </p> <p>To understand how the WSO2 Streaming Integrator performs these operations, follow the steps below.</p>"},{"location":"examples/integrating-stores/#tutorial-steps","title":"Tutorial steps","text":"<p>Before you begin:</p> <p>You need to complete the following prerequisites before you begin: - You need to have access to a MySQL instance. - Install <code>rdbms-mysql</code> extension in WSO2 Streaming Integrator as follows:     1. Start WSO2 Streaming Integrator by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory and issuing the appropriate command based on your operating system.         - For Linux  : <code>./server.sh</code>         - For Windows: <code>server.bat --run</code>     2. To install the <code>rdbms-mysql</code> extension, navigate to the to the <code>&lt;SI_HOME&gt;/bin</code> directory and issue the appropriate command based on your operating system:         - For Linux  : <code>./extension-installer.sh</code>         - For Windows: <code>extension-installer.bat --run</code>     3. Restart the WSO2 Streaming Integrator server. - Install the <code>rdbms-mysql</code> extension in WSO2 Streaming Integrator as follows.     1. Start WSO2 Streaming Integrator Tooling by navigating to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issuing the appropriate command based on your operating system.         - For Linux  : <code>./tooling.sh</code>         - For Windows: <code>tooling.bat --run</code>     2. Access Streaming Integrator Tooling. Then click Tools -&gt; Extension Installer to open the Extension Installer dialog box.     3. In the Extension Installer dialog box, click Install for the RDBMS-MYSQL extension. Then click Install in the message that appears to confirm whether you want to proceed.     4. Restart WSO2 Streaming Integrator Tooling. - Start the MySQL server. - Create three MySQL databases by issuing the following commands. <code>CREATE SCHEMA purchases;</code><code>CREATE SCHEMA dispatches;</code><code>CREATE SCHEMA closingstock;</code></p>"},{"location":"examples/integrating-stores/#connect-a-siddhi-application-to-data-stores","title":"Connect a Siddhi application to data stores","text":"<p>In this section, let's learn the different ways in which you can connect a Siddhi application to a data store.</p> <p>In Streaming Integrator Tooling, open a new file and start creating a new  Siddhi Application named <code>StockManagementApp</code>.</p> <pre><code>```\n@App:name(\"StockManagementApp\")\n@App:description(\"Managing Raw Materials\")\n```</code></pre> <p>Now let's connect to the data stores (i.e., databases) you previously created to the Siddhi application. There are three methods in which this can be done. To learn them, let's connect each of the three databases in a different method.</p>"},{"location":"examples/integrating-stores/#connect-to-a-store-via-a-data-source","title":"Connect to a store via a data source","text":"<p>To connect to the <code>closingstock</code> database via a data source, follow the steps below:</p> <ol> <li> <p>Define a data source in the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file as follows:</p> <p><pre><code>  - name: Stock_DB\n    description: The datasource used for stock records\n    jndiConfig:\n      name: jdbc/closingstock\n    definition:\n      type: RDBMS\n      configuration:\n        jdbcUrl: 'jdbc:mysql://localhost:3306/closingstock?useSSL=false'\n        username: root\n        password: root\n        driverClassName: com.mysql.jdbc.Driver\n        minIdle: 5\n        maxPoolSize: 50\n        idleTimeout: 60000\n        connectionTestQuery: SELECT 1\n        validationTimeout: 30000\n        isAutoCommit: false\n</code></pre>    The above data source connects to the <code>closingstock</code> database you previously created via the <code>jdbcUrl</code> specified.</p> </li> <li> <p>Now include the following table definition in the <code>StockManagementApp</code> Siddhi application that you started creating.</p> <pre><code>@store(type = 'rdbms', datasource = \"Stock_DB\")\n@primaryKey('name' )\ndefine table StockTable (name string, amount double);\n</code></pre> <p>In the above table definition:</p> <ul> <li> <p>The table has the two attributes <code>name</code> and <code>amount</code> to match the <code>closingstock</code> database you previously created.</p> </li> <li> <p>The <code>@store</code>annotation specifies the database type as <code>rdbms</code> and connects the table to the <code>Stock_DB</code> data source you configured. Thus, you are connected to the <code>closingstock</code> database via the data source.</p> </li> <li> <p>The <code>@primaryKey</code> annotation specifies <code>name</code> as the primary key of the table, requiring each record in the table to have a unique value for <code>name</code>.</p> </li> </ul> </li> </ol>"},{"location":"examples/integrating-stores/#refer-to-an-externally-defined-store","title":"Refer to an externally defined store","text":"<p>To connect to the <code>purchases</code> database via a reference, follow the steps below:</p> <ol> <li> <p>In the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file, add a subsection for refs, and then add a ref as shown below:</p> <p><pre><code>siddhi:\n  refs:\n    -\n      ref:\n        name: 'purchases'\n        type: 'rdbms'\n        properties:\n          jdbc.url: \"jdbc:mysql://localhost:3306/purchases?useSSL=false\"\n          username: 'root'\n          password: 'root'\n          jdbc.driver.name: 'com.mysql.jdbc.Driver'\n</code></pre>    The above reference connects to the <code>purchases</code> database that you previously created.</p> </li> <li> <p>Now include the following table definition in the <code>StockManagementApp</code> Siddhi application.</p> <pre><code>@store(type = 'rdbms', ref = \"purchases\")\ndefine table PurchasesTable (timestamp long, name string, amount double);\n</code></pre> </li> </ol>"},{"location":"examples/integrating-stores/#configure-the-data-store-inline","title":"Configure the data store inline","text":"<p>You can define the data store configuration for the <code>dispatches</code> database by adding a table definition in the <code>StockManagementApp</code> Siddhi application as follows:</p> <p><pre><code>@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/dispatches?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine table DispatchesTable (timestamp long, name string, amount double);\n</code></pre> Here, you are configuring the data store configuration in the Siddhi application itself. The Siddhi application connects to the <code>dispatches</code> database via the specified JDBC URL.</p>"},{"location":"examples/integrating-stores/#perform-crud-operations-via-siddhi-queries","title":"Perform CRUD operations via Siddhi queries","text":"<p>In this section, let's complete the <code>StockManagementApp</code> Siddhi application by adding the streams and queries to perform CRUD operations.</p> <ol> <li> <p>First, let's define the streams that receive information about material purchases and dispatches as follows.</p> <ul> <li> <p>For purchases:</p> <pre><code>define stream MaterialPurchasesStream (timestamp long, name string, amount double);\n</code></pre> </li> <li> <p>For dispatches:</p> <p><pre><code>define stream MaterialDispatchesStream (timestamp long, name string, amount double);\n</code></pre> Now let's write Siddhi queries to perform different CRUD operations as follows:</p> </li> </ul> </li> </ol>"},{"location":"examples/integrating-stores/#insert-records","title":"Insert records","text":"<p>To insert values into <code>purchases</code> and <code>dispatches</code> databases, let's write two queries as follows:</p> <ul> <li> <p>For purchases:</p> <pre><code>@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n</code></pre> </li> <li> <p>For dispatches:</p> <p><pre><code>@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n</code></pre> To try out these queries, simulate events for the streams via the Event Simulator as follows:</p> </li> <li> <p>Save the Siddhi application.</p> <p>The complete Siddhi application looks as follows:</p> <pre><code>@App:name('StockManagementApp')\n\n@App:description('Managing Raw Materials')\n\ndefine stream MaterialDispatchesStream (timestamp long, name string, amount double);\n\ndefine stream MaterialPurchasesStream (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/dispatches?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine table DispatchesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', ref = \"purchases\")\ndefine table PurchasesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', datasource = \"Stock_DB\")\n@primaryKey(\"name\")\ndefine table StockTable (name string, amount double);\n\n@info(name = 'Save material dispatch records')\nfrom MaterialDispatchesStream \nselect * \ninsert into DispatchesTable;\n\n@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n</code></pre> <p>Then start it by clicking the play icon for it in the top panel.   </p> </li> <li> <p>Click the Event Simulator icon to open the event simulator.</p> <p></p> <p>It opens the left panel for event simulation as follows.</p> <p></p> </li> <li> <p>To simulate purchase events, select <code>StockManagementApp</code> for the Siddhi App Name field, and <code>MaterialPurchasesStream</code> for the Stream Name field.</p> <p>Then enter values for the attribute fields as follows and click Send.        </p> timestamp name amount <code>1608023646000</code> <code>honey</code> <code>150</code> </li> <li> <p>To simulate an event for material dispatches, select <code>StockManagementApp</code> for the Siddhi App Name field, and <code>MaterialDispatchesStream</code> for the Stream Name field. </p> <p>Then enter values for the attribute fields as follows and click Send.  </p> timestamp name amount <code>1608023646000</code> <code>honey</code> <code>100</code> </li> <li> <p>To check whether the above insertions were successful, issue the following MySQL commands in the terminal in which you are running the MySQL server.</p> <ul> <li> <p>For the <code>purchases</code> database:</p> <pre><code>use purchases;\n</code></pre> <pre><code>select * from PurchasesTable;\n</code></pre> </li> </ul> <p>The following table is displayed.</p> <p></p> <ul> <li> <p>For the <code>dispatches</code> database:</p> <pre><code>use dispatches;\n</code></pre> <pre><code>select * from DispatchesTable;\n</code></pre> </li> </ul> <p>The following table is displayed.</p> <p></p> </li> </ul>"},{"location":"examples/integrating-stores/#retrieve-records","title":"Retrieve Records","text":"<p>Assume that the Factory Manager needs to view all the purchase records for honey. This can be done by following the steps below:</p> <ol> <li> <p>To receive the record retrieval requests as input events, define an input stream as follows:</p> <pre><code>define stream PurchaseRecordRetrievalStream (name string);\n</code></pre> </li> </ol> <p>This stream only has the <code>name</code> attribute because only the name is needed to filter the search results</p> <ol> <li> <p>To present the retrieved records, define an output stream as follows:</p> <pre><code>@sink(type = 'log', prefix = \"Search Results\",\n    @map(type = 'passThrough'))\ndefine stream SearchResultsStream (timestamp long, name string, amount double);\n</code></pre> <p>The <code>SearchResultsStream</code> output stream has all the attributes of the <code>PurchasesTable</code> table to retrieve the complete record. Also, the <code>@sink</code> annotation connects this stream to a log sink so that the search results can be logged.</p> </li> <li> <p>Now lets add a join query to join the <code>PurchaseRecordRetrievalStream</code> and the <code>PurchasesTable</code> table.</p> <pre><code>@info(name = 'Retrieve purchase records')\nfrom PurchaseRecordRetrievalStream as s \njoin PurchasesTable as p \n    on s.name == p.name \nselect p.timestamp as timestamp, s.name as name, p.amount as amount \n    group by p.name \ninsert into SearchResultsStream;\n</code></pre> </li> </ol> <p>Note the following about the above join query.</p> <ul> <li> <p>The stream is assigned the short name <code>s</code> and the table is assigned the short name <code>p</code>.</p> </li> <li> <p>Based on the previous point, <code>on s.name == p.name</code> condition specifies that a matching event is identified when the <code>PurchasesTable</code> has a record where the value for the <code>name</code> attribute is the same as that of the stream.</p> </li> <li> <p>The <code>select</code> clause the query specifies that when such a matching event is identified, attribute values for the output event should be selected as follows:</p> <pre><code>- The timestamp from the table\n- The name from the stream\n- The amount from the table</code></pre> </li> <li> <p>The <code>insert into</code> clause specifies that the output events derived as stated above should be inserted into the <code>SearchResultsStream</code>.</p> </li> <li> <p>Save the Siddhi application. The complete Siddhi application after the above changes looks as follows:</p> <p><pre><code>@App:name('StockManagementApp')\n@App:description('Managing Raw Materials')\n\ndefine stream MaterialDispatchesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Search Results\",\n    @map(type = 'passThrough'))\ndefine stream SearchResultsStream (timestamp long, name string, amount double);\n\ndefine stream MaterialPurchasesStream (timestamp long, name string, amount double);\n\ndefine stream PurchaseRecordRetrievalStream (name string);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/dispatches?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine table DispatchesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', ref = \"purchases\")\ndefine table PurchasesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', datasource = \"Stock_DB\")\n@primaryKey(\"name\")\ndefine table StockTable (name string, amount double);\n\n@info(name = 'Save material dispatch records')\nfrom MaterialDispatchesStream \nselect * \ninsert into DispatchesTable;\n\n@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n\n@info(name = 'Retrieve purchase records')\nfrom PurchaseRecordRetrievalStream as s \njoin PurchasesTable as p \n    on s.name == p.name \nselect p.timestamp as timestamp, s.name as name, p.amount as amount \n    group by p.name \ninsert into SearchResultsStream;\n</code></pre> 5. Open the Event Simulator and simulate an event for the <code>PurchaseRecordRetrievalStream</code> stream of the <code>StockManagementApp</code> Siddhi application with <code>honey</code> as the value for the name attribute.</p> <p>The following is logged in the terminal.</p> <p></p> </li> </ul>"},{"location":"examples/integrating-stores/#update-or-insert-records","title":"Update or insert records","text":"<p>The <code>Stock Table</code> table at any given time contains a single record per product, showing the current closing stock for the relevant product. When you send a new event reporting a stock value to the table, the outcome is one of the following:</p> <ul> <li>If a record with the same value for <code>name</code> already exists, the event updates the value for the <code>amount</code> attribute in that record.</li> <li>If a record with the same value for <code>name</code> does not exist, the new event is inserted into the table as a new record.</li> </ul> <p>To try this, follow the steps below:</p> <ol> <li> <p>Add a new stream as follows:</p> <pre><code>define stream LatestStockStream (name string, amount double);\n</code></pre> </li> <li> <p>Now add a query to update or insert values into the <code>StockTable</code> stream as follows:</p> <pre><code>@info(name = 'Update or Record Stock')\nfrom LatestStockStream\nselect name, amount\nupdate or insert into StockTable\n set LatestStockStream.amount = amount\n on StockTable.name == name \n</code></pre> <p>Here, the Streaming Integrator checks whether an event in the <code>LatestStockStream</code> has a matching record in the <code>StockTable</code> table where the value for the <code>name</code> attribute is the same. If such a record exists, the value for the <code>amount</code> attribute in that record is set to the amount reported via the stream event. If no matching event exists, the stream event is inserted as a new event</p> </li> <li> <p>Save the Siddhi application. The complete Siddhi application is as follows:</p> <pre><code>@App:name('StockManagementApp')\n\n@App:description('Managing Raw Materials')\n\ndefine stream MaterialDispatchesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Search Results\",\n    @map(type = 'passThrough'))\ndefine stream SearchResultsStream (timestamp long, name string, amount double);\n\ndefine stream MaterialPurchasesStream (timestamp long, name string, amount double);\n\ndefine stream PurchaseRecordRetrievalStream (name string);\n\ndefine stream LatestStockStream (name string, amount double);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/dispatches?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine table DispatchesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', ref = \"purchases\")\ndefine table PurchasesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', datasource = \"Stock_DB\")\n@primaryKey(\"name\")\ndefine table StockTable (name string, amount double);\n\n@info(name = 'Save material dispatch records')\nfrom MaterialDispatchesStream \nselect * \ninsert into DispatchesTable;\n\n@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n\n\n@info(name = 'Retrieve purchase records')\nfrom PurchaseRecordRetrievalStream as s \njoin PurchasesTable as p \n    on s.name == p.name \nselect p.timestamp as timestamp, s.name as name, p.amount as amount \n    group by p.name \ninsert into SearchResultsStream;\n\n@info(name = ''Update or Record Stock'')\nfrom LatestStockStream\nselect name, amount\nupdate or insert into StockTable\n set LatestStockStream.amount = amount\n on StockTable.name == name \n</code></pre> </li> <li> <p>Simulate events as follows:</p> <ol> <li> <p>In the event simulator, select StockManagementApp for the Siddhi App Name field, and select LatestStockStream for the Stream Name field.</p> </li> <li> <p>Enter the following values for the attribute fields and send the event.</p> name amount <code>flour</code> <code>150</code> </li> <li> <p>Execute the following MySQL queries:</p> <pre><code>use closing stock\n</code></pre> <pre><code>select * from StockTable\n</code></pre> <p>The following is  displayed.</p> <p></p> <p>Here, the single record displayed is the event you sent. This event is inserted as a new record because the <code>StockTable</code> table did not have any records.</p> </li> <li> <p>Now simulate another event for the same stream with the following attribute values:</p> name amount <code>flour</code> <code>200</code> </li> <li> <p>Execute the following MySQL queries:</p> <pre><code>use closing stock\n</code></pre> <pre><code>select * from StockTable\n</code></pre> <p>The following is  displayed.</p> <p></p> <p>Again, a single record  is displayed. Although value for the <code>name</code> attribute is the same, the value for the <code>amount</code> attribute has been updated from <code>150</code> to <code>200</code>. This is because <code>name</code> is the primary key of the <code>StockTable</code> table and at any given time, there can be only one record with a specific name for the <code>name</code> attribute. Therefore, because you simulated two events with the same value for the <code>name</code> attribute, the second event updated the first one.</p> </li> </ol> </li> </ol>"},{"location":"examples/integrating-stores/#update-records","title":"Update records","text":"<p>To update the <code>StockTable</code> table via streams, follow the steps below:</p> <ol> <li> <p>Add a new stream as follows:</p> <pre><code>define stream UpdateStockStream (name string, amount double);\n</code></pre> </li> <li> <p>Now add a query to update the values in the <code>StockTable</code> stream as follows:</p> <p><pre><code>@info(name = 'Update Stock')\nfrom UpdateStockStream\nselect name, amount\nupdate StockTable\n set UpdateStockStream.amount = amount\n on StockTable.name == name;\n</code></pre> Here, the Streaming Integrator checks whether an event in the <code>UpdateStockStream</code> has a matching record in the <code>StockTable</code> table where the value for the <code>name</code> attribute is the same. If such a record exists, the value for the <code>amount</code> attribute in that record is set to the amount reported via the stream event.</p> </li> <li> <p>Save the Siddhi application. The complete Siddhi application is as follows:</p> <pre><code>@App:name('StockManagementApp')\n\n@App:description('Managing Raw Materials')\n\ndefine stream MaterialDispatchesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Search Results\",\n    @map(type = 'passThrough'))\ndefine stream SearchResultsStream (timestamp long, name string, amount double);\n\ndefine stream MaterialPurchasesStream (timestamp long, name string, amount double);\n\ndefine stream PurchaseRecordRetrievalStream (name string);\n\ndefine stream LatestStockStream (name string, amount double);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/dispatches?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine table DispatchesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', ref = \"purchases\")\ndefine table PurchasesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', datasource = \"Stock_DB\")\n@primaryKey(\"name\")\ndefine table StockTable (name string, amount double);\n\n@info(name = 'Save material dispatch records')\nfrom MaterialDispatchesStream \nselect * \ninsert into DispatchesTable;\n\n@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n\n\n@info(name = 'Retrieve purchase records')\nfrom PurchaseRecordRetrievalStream as s \njoin PurchasesTable as p \n    on s.name == p.name \nselect p.timestamp as timestamp, s.name as name, p.amount as amount \n    group by p.name \ninsert into SearchResultsStream;\n\n@info(name = ''Update or Record Stock'')\nfrom LatestStockStream\nselect name, amount\nupdate or insert into StockTable\n set LatestStockStream.amount = amount\n on StockTable.name == name \n\n@info(name = 'Update Stock')\nfrom UpdateStockStream\nselect name, amount\nupdate StockTable\n set UpdateStockStream.amount = amount\n on StockTable.name == name;\n</code></pre> </li> <li> <p>Simulate events as follows:</p> <ol> <li> <p>In the event simulator, select StockManagementApp for the Siddhi App Name field, and select UpdateStockStream for the Stream Name field.</p> </li> <li> <p>Enter the following values for the attribute fields and send the event.</p> name amount <code>flour</code> <code>129</code> </li> <li> <p>Execute the following MySQL queries:</p> <pre><code>use closing stock\n</code></pre> <pre><code>select * from StockTable\n</code></pre> </li> </ol> <p>The following is  displayed.</p> <p></p> <p>Here, the single record displayed is the event you sent. This event is inserted as a new record because the <code>StockTable</code> table did not have any records. </p> </li> </ol>"},{"location":"examples/integrating-stores/#delete-records","title":"Delete records","text":"<p>To delete records in the <code>StockTable</code> table via streams, follow the steps below:</p> <ol> <li> <p>Add a new stream as follows:</p> <pre><code>define stream DeleteStream (name string, amount double);\n</code></pre> </li> <li> <p>Now add a query to update the values in the <code>StockTable</code> stream as follows:</p> <p><pre><code>@info(name = 'Delete Stock')\nfrom DeleteStream\nselect name, amount\ndelete StockTable \n  on StockTable.name == name;\n</code></pre> Here, the Streaming Integrator checks whether an event in the <code>DeleteStream</code> has a matching record in the <code>StockTable</code> table where the value for the <code>name</code> attribute is the same. If such a record exists, it is deleted.</p> </li> <li> <p>Save the Siddhi application. The complete Siddhi application is as follows:</p> <pre><code>@App:name('StockManagementApp')\n\n@App:description('Managing Raw Materials')\n\ndefine stream MaterialDispatchesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Search Results\",\n    @map(type = 'passThrough'))\ndefine stream SearchResultsStream (timestamp long, name string, amount double);\n\ndefine stream MaterialPurchasesStream (timestamp long, name string, amount double);\n\ndefine stream PurchaseRecordRetrievalStream (name string);\n\ndefine stream LatestStockStream (name string, amount double);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/dispatches?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine table DispatchesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', ref = \"purchases\")\ndefine table PurchasesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', datasource = \"Stock_DB\")\n@primaryKey(\"name\")\ndefine table StockTable (name string, amount double);\n\n@info(name = 'Save material dispatch records')\nfrom MaterialDispatchesStream \nselect * \ninsert into DispatchesTable;\n\n@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n\n\n@info(name = 'Retrieve purchase records')\nfrom PurchaseRecordRetrievalStream as s \njoin PurchasesTable as p \n    on s.name == p.name \nselect p.timestamp as timestamp, s.name as name, p.amount as amount \n    group by p.name \ninsert into SearchResultsStream;\n\n@info(name = ''Update or Record Stock'')\nfrom LatestStockStream\nselect name, amount\nupdate or insert into StockTable\n set LatestStockStream.amount = amount\n on StockTable.name == name \n\n@info(name = 'Update Stock')\nfrom UpdateStockStream\nselect name, amount\nupdate StockTable\n set UpdateStockStream.amount = amount\n on StockTable.name == name;\n\n@info(name = 'Delete Stock')\nfrom DeleteStream\nselect name, amount\ndelete StockTable \n  on StockTable.name == name;\n</code></pre> </li> <li> <p>Simulate events as follows:</p> <ol> <li> <p>In the event simulator, select StockManagementApp for the Siddhi App Name field, and select DeleteStream for the Stream Name field.</p> </li> <li> <p>Enter the following values for the attribute fields and send the event.</p> name amount <code>flour</code> <code>129</code> </li> <li> <p>Execute the following MySQL queries:</p> <pre><code>use closing stock\n</code></pre> <pre><code>select * from StockTable\n</code></pre> </li> </ol> <p>The <code>StockTable</code> is displayed as an empty set. This is because the event you sent to the <code>DeleteStream</code> stream matched the record in the table, and as a result, the record was deleted by the <code>Delete Stock</code> query.</p> </li> </ol>"},{"location":"examples/integrating-stores/#perform-crud-operations-via-rest-api","title":"Perform CRUD operations via REST API","text":"<p>In this section, let's perform CRUD operations via the Store API</p>"},{"location":"examples/integrating-stores/#insert-records_1","title":"Insert records","text":"<p>To insert a record into the <code>StockTable</code> table, issue the following CURL command:</p> <pre><code>curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockManagementApp\", \"query\" : \"select \\\"sugar\\\" as name, 200.0 as amount insert into StockTable;\" }' -k\n</code></pre> <p>Then issue the following commands in the terminal where you are running the MySQL server.</p> <pre><code>use closingstock;\n</code></pre> <pre><code>select * from StockTable;\n</code></pre> <p>The following is displayed:</p> <p></p>"},{"location":"examples/integrating-stores/#retrieve-records_1","title":"Retrieve records","text":"<p>To retrieve a record from the <code>StockTable</code> table, issue the following CURL command:</p> <pre><code>curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockManagementApp\", \"query\" : \"from StockTable on name == \\\"sugar\\\" select name, amount;  \" }' -k\n</code></pre> <p>This returns the following response:</p> <pre><code>{\"records\":[[\"sugar\",200.0]]\n</code></pre>"},{"location":"examples/integrating-stores/#update-or-inserts-records","title":"Update or inserts records","text":"<p>First, let's send an event that has the same value for the <code>name</code> attribute as the existing record in the <code>StockTable</code> table. To do this, issue the following command:</p> <pre><code>curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockManagementApp\", \"query\" : \"select \\\"sugar\\\" as name, 260.0 as amount update or insert into StockTable  set amount = amount  on StockTable.name == name;\" }' -k\n</code></pre> <p>Then issue the following commands in the terminal where you are running the MySQL server.</p> <pre><code>use closingstock;\n</code></pre> <pre><code>select * from StockTable;\n</code></pre> <p>The following is displayed:</p> <p></p> <p>Now let's send an event where the value for the <code>name</code> attribute is different to that of the existing value in the <code>StockTable</code> table as follows:</p> <pre><code>curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockManagementApp\", \"query\" : \"select \\\"vanilla\\\" as name, 100.0 as amount update or insert into StockTable  set amount = amount  on StockTable.name == name;\" }' -k\n</code></pre> <p>Then issue the following commands in the terminal where you are running the MySQL server.</p> <pre><code>use closingstock;\n</code></pre> <pre><code>select * from StockTable;\n</code></pre> <p>The following is displayed:</p> <p></p>"},{"location":"examples/integrating-stores/#update-records_1","title":"Update records","text":"<p>To update an existing record in the <code>StockTable</code> table, issue the following CURL command:</p> <pre><code>curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockManagementApp\", \"query\" : \"select \\\"vanilla\\\" as name, 127.0 as amount update StockTable  set amount = amount on StockTable.name == name;\" }' -k\n</code></pre> <p>Then issue the following commands in the terminal where you are running the MySQL server.</p> <pre><code>use closingstock;\n</code></pre> <pre><code>select * from StockTable;\n</code></pre> <p>The following is displayed:</p> <p></p>"},{"location":"examples/integrating-stores/#delete-records_1","title":"Delete records","text":"<p>To delete an existing record in the <code>StockTable</code> table, issue the following CURL command:</p> <pre><code>curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockManagementApp\", \"query\" : \"select \\\"vanilla\\\" as name delete StockTable on StockTable.name == name;\" }' -k\n</code></pre> <p>Then issue the following commands in the terminal where you are running the MySQL server.</p> <pre><code>use closingstock;\n</code></pre> <pre><code>select * from StockTable;\n</code></pre> <p>The following is displayed:</p> <p> </p>"},{"location":"examples/integrating-stores/#manipulate-data-in-stores-via-sql-queries","title":"Manipulate data in stores via SQL queries","text":"<p>You can execute SQL queries via WSO2 Streaming Integrator to manipulate data in data stores. This is supported via the siddhi-store-rdbms extension.</p> <p>Before you begin:</p> <p>To allow Streaming Integrator Tooling to perform CRUD operations, open <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file, and add an extract as shown below with the <code>perform.CRUD.operations</code> parameter set to <code>true</code> as shown below: <code>yaml     siddhi:       extensions:         -           extension:             name: cud             namespace: rdbms             properties:               perform.CUD.operations: true</code></p> <p>To perform CRUD operations in multiple tables via WSO2 Streaming Integrator, follow the steps below:</p> <p>To start creating the Siddhi application with the required tables, follow the steps below:</p> <ol> <li> <p>In WSO2 Streaming Integrator Tooling, open the <code>StockManagementApp</code> that you previously created.</p> </li> <li> <p>Define a new stream in it named <code>StockStream</code> as follows.</p> <p><code>define stream TriggerStream (name string, amount double);</code></p> </li> <li> <p>Add a query as follows:</p> <p><pre><code>from TriggerStream#rdbms:cud(\"Stock_DB\", \"UPDATE StockTable SET name='sugarsyrup' where name='sugar'\")\nselect name, amount\ninsert into OutputStream\n</code></pre> This query updates the record in the <code>StockTable</code> table where the value for the <code>name</code> attribute is <code>sugar</code> by changing that same value to <code>sugarsyrup</code>.</p> </li> <li> <p>Save the Siddhi application. The complete Siddhi application is now as follows:</p> <pre><code>@App:name('StockManagementApp')\n\ndefine stream MaterialDispatchesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Search Results\",\n    @map(type = 'passThrough'))\ndefine stream SearchResultsStream (timestamp long, name string, amount double);\n\ndefine stream MaterialPurchasesStream (timestamp long, name string, amount double);\n\ndefine stream PurchaseRecordRetrievalStream (name string);\n\ndefine stream LatestStockStream (name string, amount double);\n\ndefine stream UpdateStockStream (name string, amount double);\n\ndefine stream DeleteStream (name string, amount double);\n\ndefine stream TriggerStream (name string, amount double);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/dispatches?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine table DispatchesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', ref = \"purchases\")\ndefine table PurchasesTable (timestamp long, name string, amount double);\n\n@store(type = 'rdbms', datasource = \"Stock_DB\")\n@primaryKey(\"name\")\ndefine table StockTable (name string, amount double);\n\n@info(name = 'Save material dispatch records')\nfrom MaterialDispatchesStream \nselect * \ninsert into DispatchesTable;\n\n@info(name = 'Save purchase records')\nfrom MaterialPurchasesStream \nselect * \ninsert into PurchasesTable;\n\n\n@info(name = 'Retrieve purchase records')\nfrom PurchaseRecordRetrievalStream as s \njoin PurchasesTable as p \n    on s.name == p.name \nselect p.timestamp as timestamp, s.name as name, p.amount as amount \n    group by p.name \ninsert into SearchResultsStream;\n\n@info(name = 'Update or Record Stock')\nfrom LatestStockStream\nselect name, amount\nupdate or insert into StockTable\n set LatestStockStream.amount = amount\n on StockTable.name == name;\n\n@info(name = 'Update Stock')\nfrom UpdateStockStream\nselect name, amount\nupdate StockTable\n set UpdateStockStream.amount = amount\n on StockTable.name == name;\n\n@info(name = 'Delete Stock')\nfrom DeleteStream\nselect name, amount\ndelete StockTable \n  on StockTable.name == name;\n\nfrom TriggerStream#rdbms:cud(\"Stock_DB\", \"UPDATE StockTable SET name='sugarsyrup' where name='sugar'\")\nselect name, amount\ninsert into OutputStream\n</code></pre> </li> <li> <p>Simulate an event for the <code>TriggerStream</code> stream of the <code>StockManagementApp</code> Siddhi application. You can enter any values of your choice as the attribute values.</p> </li> <li> <p>To check the database table, issue the following MySQL commands.</p> <pre><code>use closingstock;\n</code></pre> <pre><code>select * from StockTable;\n</code></pre> </li> </ol> <p>The following is displayed:</p> <p></p>"},{"location":"examples/manage-stored-data-via-rest-api/","title":"Manage Stored Data via Rest API","text":""},{"location":"examples/manage-stored-data-via-rest-api/#introduction","title":"Introduction","text":"<p>The Siddhi Store Query API is the REST API exposed by the Streaming Integrator (SI) in order to perform actions on the stored data. </p> <p>Stored data includes, - Siddhi Stores - Siddhi Aggregations - Siddhi Windows,     that you have defined in Siddhi applications. </p> <p>You can perform actions such as, - inserting - searching - updating - deleting on those stored data, using the Rest API. </p> <p>For a comprehensive reference on the Siddhi query API, see Streaming Integration REST API Guide.</p> <p>This tutorial demonstrates how you can use the Siddhi Store Query API to perform a few essential operations in SI, using simple examples.</p>"},{"location":"examples/manage-stored-data-via-rest-api/#preparing-the-server","title":"Preparing the server","text":"<p>!!!tip\"Before you begin:\"     - You need to have access to a MySQL instance.     - Save the MySQL JDBC driver in the <code>&lt;SI_HOME&gt;/lib</code> directory as follows:       1. Download the MySQL JDBC driver from the MySQL site.       2. Unzip the archive.       3. Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> to the <code>&lt;SI_HOME&gt;/lib</code> directory.       4. Start the SI server.</p> <ol> <li> <p>Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query.     <pre><code>CREATE SCHEMA production;\n</code></pre></p> </li> <li> <p>Create a new user by executing the following SQL query.     <pre><code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2';\n</code></pre></p> </li> <li> <p>Switch to the <code>production</code> database and create a new table, by executing the following queries:     <pre><code>use production;\n</code></pre></p> <pre><code>CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));\n</code></pre> </li> </ol>"},{"location":"examples/manage-stored-data-via-rest-api/#managing-siddhi-store-data","title":"Managing Siddhi Store Data","text":""},{"location":"examples/manage-stored-data-via-rest-api/#preparing-an-rdbms-store","title":"Preparing an RDBMS Store","text":"<p>First let's create a Siddhi application with an RDBMS Store so that we can try out certain operations on it. </p> <ol> <li> <p>Open a text file and copy-paste following application to it.</p> <pre><code>@App:name(\"SweetProduction-Store\")\n\n@App:description('Receive events via HTTP and persist the received data in the store.')\n\n@Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',\n    @map(type='json'))\ndefine stream insertSweetProductionStream (name string, amount double);\n\n@Store(type=\"rdbms\",\n       jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\",\n       username=\"wso2si\",\n       password=\"wso2\" ,\n       jdbc.driver.name=\"com.mysql.jdbc.Driver\")\ndefine table SweetProductionTable (name string, amount double);\n\nfrom insertSweetProductionStream\nupdate or insert into SweetProductionTable\non SweetProductionTable.name == name;\n</code></pre> <p>Here the <code>jdbc.url</code> parameter has the value <code>jdbc:mysql://localhost:3306/production?useSSL=false</code>. Change it to point to your MySQL server. Similarly change <code>username</code> and <code>password</code> parameters as well.</p> </li> <li> <p>Save this file as <code>SweetProduction-Store.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> </li> </ol>"},{"location":"examples/manage-stored-data-via-rest-api/#inserting-records","title":"Inserting records","text":"<p>To insert a record into the <code>SweetProductionTable</code>, execute following <code>CURL</code> command: <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Almond cookie\\\" as name, 100.0 as amount insert into SweetProductionTable;\" }' -k\n</code></pre> On successful execution of the command, you will get following response on the terminal: <pre><code>{\"records\":[]}\n</code></pre></p> <p>Let's insert a few more records by executing following <code>CURL</code> command: <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Baked alaska\\\" as name, 20.0 as amount insert into SweetProductionTable;\" }' -k\n</code></pre> <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Cup cake\\\" as name, 30.0 as amount insert into SweetProductionTable;\" }' -k\n</code></pre> <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Doughnut\\\" as name, 500.0 as amount insert into SweetProductionTable;\" }' -k\n</code></pre></p> <pre><code>!!!info\n    Above `CURL` commands, insert the following records into the `SweetProductionTable` table. In the next section, we will retrieve these record from the table.\n        +---------------+--------+\n        | name          | amount |\n        +---------------+--------+\n        | Almond cookie | 100.00 |\n        +---------------+--------+\n        | Baked alaska  |  20.00 |\n        +---------------+--------+\n        | Cup cake      |  30.00 |\n        +---------------+--------+\n        | Doughnut      | 500.00 |\n        +---------------+--------+</code></pre>"},{"location":"examples/manage-stored-data-via-rest-api/#searching-records","title":"Searching records","text":"<p>To obtain all of the records from the <code>SweetProductionTable</code>, execute following <code>CURL</code> command:</p> <p><pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k\n</code></pre> On successful execution of the command, you will get following response on the terminal: <pre><code>{\"records\":[[\"Almond cookie\",100.0],[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]}\n</code></pre></p> <p>Now let's obtain all of the records which has <code>amount</code> greater than <code>25</code> and order those by <code>amount</code>.  <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable on amount &gt; 25 select name, amount order by amount; \" }' -k\n</code></pre> On successful execution of the command, you will get following response on the terminal: <pre><code>{\"records\":[[\"Cup cake\",30.0],[\"Almond cookie\",100.0],[\"Doughnut\",500.0]]}\n</code></pre></p> <p>To get the top two records from above ouput, let's use the <code>limit</code> condition on the Store Query: <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable on amount &gt; 25 select name, amount order by amount limit 2; \" }' -k\n</code></pre> On successful execution of the command, you will get following response on the terminal: <pre><code>{\"records\":[[\"Cup cake\",30.0],[\"Almond cookie\",100.0]]}\n</code></pre></p> <pre><code>!!! info\n    For more information on search queries, refer [Store APIs: Streaming Integration REST API Guide](https://ei.docs.wso2.com/en/latest/streaming-integrator/ref/si-rest-api-guide/#store-apis).</code></pre>"},{"location":"examples/manage-stored-data-via-rest-api/#updating-records","title":"Updating records","text":"<p>Now let's update the following record in the <code>SweetProductionTable</code>. We will set the <code>name</code> to <code>Almond Cookie</code> and <code>amount</code> to <code>150.0</code>. <pre><code>            +---------------+--------+\n            | name          | amount |\n            +---------------+--------+\n            | Almond cookie | 100.00 |\n            +---------------+--------+\n</code></pre> In order to update above record, execute following <code>CURL</code> command: <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select \\\"Almond Cookie\\\" as name, 150.0 as amount update or insert into SweetProductionTable  set SweetProductionTable.name = name, SweetProductionTable.amount = amount  on SweetProductionTable.name == \\\"Almond cookie\\\";\"}' -k\n</code></pre> On successful execution of the command, you will get following response on the terminal: <pre><code>{\"records\":[]}\n</code></pre>     !!! tip         To verify whether the update is successful, you can execute following <code>CURL</code> command:         <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k\n</code></pre>         On successful execution of the command, you will get following response on the terminal:         <pre><code>{\"records\":[[\"Almond Cookie\",150.0],[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]}\n</code></pre>         Notice that the <code>[\"Almond cookie\",100.0]</code> has being changed to <code>[\"Almond Cookie\",150.0]</code> </p>"},{"location":"examples/manage-stored-data-via-rest-api/#deleting-records","title":"Deleting records","text":"<p>Let's delete the entry <code>[\"Almond Cookie\",150.0]</code> from the <code>SweetProductionTable</code> by executing following <code>CURL</code> command: <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"select 150.0 as amount delete SweetProductionTable on SweetProductionTable.amount == amount;\" }' -k\n</code></pre> On successful execution of the command, you will get following response on the terminal: <pre><code>{\"records\":[]}\n</code></pre>     !!! tip         To verify whether the delete is successful, you can execute following <code>CURL</code> command:         <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Store\", \"query\" : \"from SweetProductionTable select name, amount; \" }' -k\n</code></pre>         On successful execution of the command, you will get following response on the terminal:         <pre><code>{\"records\":[[\"Baked alaska\",20.0],[\"Cup cake\",30.0],[\"Doughnut\",500.0]]}\n</code></pre>         Notice that the <code>[\"Almond Cookie\",150.0]</code> record has being deleted. </p>"},{"location":"examples/manage-stored-data-via-rest-api/#managing-data-in-a-siddhi-aggregation","title":"Managing Data in a Siddhi Aggregation","text":"<p>First let's create a Siddhi application with an Aggregation, so that we can try out search operations on it later. </p> <ol> <li> <p>Open a text file and copy-paste following application to it.</p> <pre><code>@App:name(\"AggregateDataIncrementally\")\n\n@App:description('Aggregates values every second until year and gets statistics')\n\n@Source(type = 'http', receiver.url='http://localhost:8006/rawMaterialStream', basic.auth.enabled='false',\n    @map(type='json'))\ndefine stream RawMaterialStream (name string, amount double);\n\n@store( type=\"rdbms\",\n        jdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\",\n        username=\"wso2si\",\n        password=\"wso2\" ,\n        jdbc.driver.name=\"com.mysql.jdbc.Driver\")\ndefine aggregation RawMaterialAggregation\nfrom RawMaterialStream\nselect name, avg(amount) as avgAmount, sum(amount) as totalAmount\ngroup by name\naggregate every sec...year;\n</code></pre> <p>Here the <code>jdbc.url</code> parameter has the value <code>jdbc:mysql://localhost:3306/production?useSSL=false</code>. Change it to point to your MySQL server. Similarly change <code>username</code> and <code>password</code> parameters as well.</p> </li> <li> <p>Save this file as <code>AggregateDataIncrementally.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> </li> <li> <p>Let's insert a few records into the <code>RawMaterialStream</code> so that those data will be summarized and you can query for the summary later.</p> <p>Info</p> <p>Unlike RDBMS Stores, you cannot insert records into a Aggregation table straight away. In order to put records, you need to insert data into the event stream which is associated to the Aggregation. In this example, the <code>RawMaterialAggregation</code> aggregation table is associated to the event stream 'RawMaterialStream<code>. Therefore, in order to insert into the aggregation table, you need to insert into the 'RawMaterialStream</code>.  </p> <p>Execute following <code>CURL</code> command on a terminal:  <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 20.5}}\"  http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\"  http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 30.0}}\"  http://localhost:8006/rawMaterialStream --header \"Content-Type:application/json\"\n</code></pre></p> </li> <li> <p>Now let's use the Store Query API, in order to find out the average and total amount of <code>Almond cookie</code> productions. </p> <p>Info</p> <p>Above, you have executed following Store Query: <pre><code>from RawMaterialAggregation on name==\"Almond cookie\" \nwithin \"2019-**-** **:**:** +05:30\" \nper \"hours\" \nselect AGG_TIMESTAMP, name, avgAmount, totalAmount\" \n</code></pre> This query retrieves the average and total <code>Almond cookie</code> productions happened within the year <code>2019</code>. The average and total is calculated for every hour.   </p> </li> <li> <p>You will get following response:     <pre><code>{\"records\":[[1571234400000,\"Almond cookie\",50.166666666666664,150.5]]}\n</code></pre>     !!! info         The value <code>1571234400000</code> indicates the Unix timestamp for which the result set belong. In this example, the result set belongs to <code>October 16, 2019 7:30:00 PM GMT+05:30</code>.          <code>50.166666666666664</code> is the average amount of <code>Almond cookie</code> productions happened within the hour, while <code>150.5</code> is the total amount of <code>Almond cookie</code> productions happened within the hour.  </p> </li> </ol>"},{"location":"examples/manage-stored-data-via-rest-api/#managing-data-in-a-siddhi-window","title":"Managing Data in a Siddhi Window","text":"<p>Let's create a Siddhi application with a Window and then query the status of the Window, using a Store query. </p> <ol> <li> <p>Open a text file and copy-paste following application to it.</p> <pre><code>@App:name(\"SweetProduction-Window\")\n\n@Source(type = 'http', receiver.url='http://localhost:8008/productionStream', basic.auth.enabled='false',\n    @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\ndefine window LastFourProductions (name string, amount double) lengthBatch(4);\n\n@sink(type='log')\ndefine stream LogStream (name string, sumAmount double);\n\n@info(name = 'query1')\nfrom SweetProductionStream\nselect name, amount\ninsert into LastFourProductions;\n\n@info(name = 'query2')\nfrom LastFourProductions\nselect name,sum(amount) as sumAmount\ninsert into LogStream;\n</code></pre> </li> </ol> <p>Info</p> <p>The above Siddhi application calculates the sum of last four productions (in batches). The last four productions are retained in the <code>LastFourProductions</code> window which is a <code>lengthBatch</code> window of size four. <code>query1</code> inserts all of the incoming sweet productions into the <code>LastFourProductions</code> window. <code>query2</code> calculates the sum of the batch of four productions, within the window. </p> <ol> <li> <p>Save this file as <code>SweetProduction-Window.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> </li> <li> <p>Let's insert four events into <code>SweetProductionStream</code>. Execute following <code>CURL</code> commands on the terminal:     <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\"  http://localhost:8008/productionStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\"  http://localhost:8008/productionStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\"  http://localhost:8008/productionStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Doughnut\\\",\\\"amount\\\": 500.0}}\"  http://localhost:8008/productionStream --header \"Content-Type:application/json\"\n</code></pre>     Once you send the fourth event, a batch of four productions completes; hence the following log appears on the SI console. The log prints the sum of amounts of the four productions.     <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - SweetProduction-Window : LogStream : Event{timestamp=1571675148391, data=[Doughnut, 920.0], isExpired=false}\n</code></pre></p> </li> <li> <p>Now, using the Store Query API, you will be querying the contents in the <code>LastFourProductions</code> window. </p> <p>Let's select all events in the window by executing following <code>CURL</code> command: <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Window\", \"query\" : \"from LastFourProductions select *\" }' -k\n</code></pre> On successful execution of the command, you get following response on the terminal. The output shows the last four sweet productions.  <pre><code>{\"records\":[[\"Almond cookie\",100.0],[\"Baked alaska\",20.0],[\"Cup cake\",300.0],[\"Doughnut\",500.0]]}\n</code></pre></p> </li> <li> <p>Next, using the Store Query API, you will be querying for the maximum production amount, among the four productions that are in the <code>LastFourProductions</code> window.</p> <p>Execute following <code>CURL</code> command on the terminal: <pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"SweetProduction-Window\", \"query\" : \"from LastFourProductions select max(amount) as maxAmount\" }' -k\n</code></pre> On successful execution of the command, you get following response on the terminal. The output shows the maximum amount of the four sweet productions.  <pre><code>{\"records\":[[500.0]]}\n</code></pre></p> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-files/","title":"Performing Real-time ETL with Files","text":""},{"location":"examples/performing-real-time-etl-with-files/#introduction","title":"Introduction","text":"<p>The Streaming Integrator (SI) allows you to perform real-time ETL with data that is stored in files.</p> <p>This tutorial takes you through the different modes and options you could use, in order to perform real-time ETL with files using the SI.</p> <p>Before you begin:</p> <ul> <li>Start the SI server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory and issuing one of the following commands as appropriate, based on your operating system: <ul> <li>For Windows: <code>streaming-integrator.bat</code> </li> <li>For Linux:  <code>sh server.sh</code>  The following log appears in the Streaming Integrator console once you have successfully started the server.  <code>INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 4.240 sec</code> </li> </ul> </li> </ul>"},{"location":"examples/performing-real-time-etl-with-files/#tutorial-steps","title":"Tutorial steps","text":""},{"location":"examples/performing-real-time-etl-with-files/#extracting-data-from-a-file","title":"Extracting data from a file","text":"<p>In this section of the tutorial, you are exploring the different ways in which you could extract data from a specific file.</p>"},{"location":"examples/performing-real-time-etl-with-files/#tailing-a-text-file-line-by-line","title":"Tailing a text file line by line","text":"<p>In this scenario, you are tailing a text file, line by line, in order to extract data from it. Each line is extracted as an event that undergoes a simple transformation thereafter. Let's write a simple Siddhi application to do this.</p> <ol> <li> <p>Download <code>productions.csv</code> file from here and save it in a location of your choice.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('TailFileLineByLine')\n\n@App:description('Tails a file line by line and does a simple transformation.')\n\n@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/productions.csv',\n    tailing='true',\n    @map(type='csv'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (name string, amount double);\n\nfrom SweetProductionStream\nselect str:upper(name) as name, amount\ninsert into LogStream;\n</code></pre> <p>Change the  value of the <code>file.uri</code> parameter in the above Siddhi application to the file path to which you downloaded <code>productions.csv</code> file in step 1.</p> </li> <li> <p>Save this file as <code>TailFileLineByLine.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.   </p> <p>Info</p> <p>This Siddhi application tails the file <code>productions.csv</code> line by line. Each line is converted to an event in the <code>SweetProductionStream</code> stream. After that, a simple transformation is carried out for the sweet production runs. The transformation involves converting the value for the <code>name</code> attribute to upper case. Finally, the output is logged in the Streaming Integrator console.</p> <p>Upon successful deployment, following log appears in the SI console:</p> <p><pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TailFileLineByLine deployed successfully\n</code></pre> 4. To install the extensions required for the <code>TailFileLineByLine</code> Siddhi application you deployed, open a new terminal window and navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory and issue one of the following commands as appropriate, based on your operating system:  - For Windows: <code>extension-installer.bat</code>  - For Linux:  <code>sh extension-installer.sh</code> </p> </li> <li> <p>Now the Siddhi application starts to process the <code>productions.csv</code> file. The file contains the following entries.</p> <pre><code>Almond cookie,100.0\nBaked alaska,20.0\n</code></pre> <p>As a result, the following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490830652, data=[ALMOND COOKIE, 100.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490830657, data=[BAKED ALASKA, 20.0], isExpired=false}\n</code></pre> </li> <li> <p>Now append the following line to <code>productions.csv</code> file and save the file.</p> <pre><code>Cup cake,300.0\n</code></pre> </li> <li> <p>The following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEventsFromFile : LogStream : Event{timestamp=1564490869579, data=[CUP CAKE, 300.0], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-files/#tailing-a-text-file-using-a-regular-expression","title":"Tailing a text file using a regular expression","text":"<p>In this scenario, you are using a regular expression to extract data from the file. After data is extracted, a simple transformation is performed on them. Finally, the transformed event is logged in the SI console. Let's write a simple Siddhi application to do this.</p> <ol> <li> <p>Download <code>noisy_data.txt</code> file from here and save it in a location of your choice.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('TailFileRegex')\n\n@App:description('Tails a file using a regex and does a simple transformation.')\n\n@source(type='file', mode='REGEX',\n    file.uri='file:/Users/foo/noisy_data.txt',\n    begin.regex='\\&lt;', end.regex='\\&gt;',\n    tailing='true',\n    @map(type='text', fail.on.missing.attribute = 'false', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B')))\ndefine stream StockStream (symbol string, price float, volume long);\n\n@sink(type = 'log')\ndefine stream LogStream (symbol string, price float, volume long);\n\nfrom StockStream[NOT(symbol is null)]\nselect str:upper(symbol) as symbol, price, volume  \ninsert into LogStream;\n</code></pre> <p>Change the  value of the <code>file.uri</code> parameter in the above Siddhi application to the file path to which you downloaded <code>noisy_data.txt</code> file in step 1.</p> </li> <li> <p>Save this file as <code>TailFileRegex.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application tails the <code>noisy_data.txt</code> file to find matches according to the regular expressions given: <code>begin.regex</code> and <code>end.regex</code>. Each match is converted to an event in the <code>StockStream</code> stream. After that, a simple transformation is carried out on the <code>StockStream</code> stream where the value for the <code>symbol</code> attribute from the event is converted to upper case. Finally, the output is logged in the SI console.</p> <p>Once the Siddhi application is successfully deployed, the following log appears in the SI console.</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TailFileRegex deployed successfully\n</code></pre> </li> <li> <p>Now the Siddhi application starts to process the <code>noisy_data.txt</code> file. </p> <p>As a result, the following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564583307974, data=[WSO2, 75.0, 100], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564583307975, data=[ORCL, 95.0, 200], isExpired=false}\n</code></pre> </li> <li> <p>Now append the following text to <code>noisy_data.txt</code> file and save the file.</p> <pre><code>IBM &lt;ibm 88 volume 150&gt; 1 New Orchard Rd\u00a0 Armonk, NY\u00a010504 \nPhone Number: (914) 499-1900\nFax Number: (914) 765-6021\n</code></pre> </li> <li> <p>The following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - TailFileRegex : LogStream : Event{timestamp=1564588585214, data=[IBM, 88.0, 150], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-remote-text-file-and-moving-it-after-processing","title":"Reading a remote text file and moving it after processing","text":"<p>In the previous scenarios, you tailed a file and each file generated multiple events. In this scenario, you are reading the complete file to build a single event.</p> <p>Furthermore, to try out the capability of processing remote files, you are processing a remote file instead of a file located in the local file system.</p> <ol> <li> <p>Download <code>portfolio.txt</code> file from here and upload it into an FTP server.</p> </li> <li> <p>Create a directory on the FTP server.  The <code>portfolio.txt</code> file is moved to this folder after the processing is complete.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('TextFullFileProcessing')\n\n@App:description('Reads a text file and moves it after processing.')\n\n@source(type='file', mode='TEXT.FULL',\n    file.uri=\"ftp://&lt;username&gt;:&lt;password&gt;@&lt;ftp_hostname&gt;:&lt;ftp_port&gt;/Users/foo/portfolio.txt\",\n    action.after.process='MOVE', move.after.process=\"ftp://&lt;username&gt;:&lt;password&gt;@&lt;ftp_hostname&gt;:&lt;ftp_port&gt;/Users/foo/move.after.process\", \n    @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\")))\ndefine stream StockStream (symbol string, price float, volume long);\n\n@sink(type = 'log')\ndefine stream LogStream (symbol string, price float, volume long);\n\nfrom StockStream\nselect str:upper(symbol) as symbol, price, volume   \ninsert into LogStream;\n</code></pre> <p>Change the value of the <code>file.uri</code> parameter in the above Siddhi application to the remote file path to which you uploaded the <code>portfolio.txt</code> file in step 1. In addition to that, change <code>move.after.process</code> so that it points to the remote folder you created in step 2. When configuring both of the above parameters, change the values for <code>&lt;username&gt;</code>, <code>&lt;password&gt;</code>, <code>&lt;ftp_hostname&gt;</code>, and <code>&lt;ftp_port&gt;</code> parameters accordingly.</p> </li> <li> <p>Save this file as <code>TextFullFileProcessing.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application reads the complete <code>portfolio.txt</code> remote file to create a <code>StockStream</code> event. After that, a simple transformation is carried out on the <code>StockStream</code> stream where the value for the <code>symbol</code> attribute in each event is converted ito upper case. Finally, the output is logged in the SI console.</p> <p>Once the Siddhi application is successfully deployed, following log appears in the SI console:</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App TextFullFileProcessing deployed successfully\n</code></pre> </li> <li> <p>Now the Siddhi application starts to process the <code>portfolio.txt</code> file. </p> <p>As a result, the following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - TextFullFileProcessing :  LogStream : Event{timestamp=1564660443519, data=[WSO2, 55.6, 100], isExpired=false} \n</code></pre> </li> </ol> <p>Info</p> <p>In this scenario, you moved the file after processing. To delete a file after processing, remove the <code>action.after.process</code> and <code>move.after.process</code> parameters from the Siddhi application. For other configuration options, see Siddhi File Source documentation.</p>"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-binary-file-and-moving-it-after-processing","title":"Reading a binary file and moving it after processing","text":"<p>In the previous scenarios, you processed text files in order to extract data. In this scenario, you are reading a binary file. The content of the file generates a single event.</p> <ol> <li> <p>Download <code>wso2.bin</code> file from here and save it in a location of your choice.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('BinaryFullFileProcessing')\n\n@App:description('Reads a binary file and moves it after processing.')\n\n@source(type='file', mode='TEXT.FULL',\n    file.uri='file:/Users/foo/wso2.bin',\n    action.after.process='MOVE', move.after.process='file:/Users/foo/move.after.process', \n    @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\")))\ndefine stream StockStream (symbol string, price float, volume long);\n\n@sink(type = 'log')\ndefine stream LogStream (symbol string, price float, volume long);\n\nfrom StockStream\nselect str:upper(symbol) as symbol, price, volume   \ninsert into LogStream;\n</code></pre> <p>In the above Siddhi application, change the value for the <code>file.uri</code> parameter to the file path to which you downloaded the <code>wso2.bin</code> file in step 1.</p> </li> <li> <p>Save this file as <code>BinaryFullFileProcessing.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application reads the file <code>wso2.bin</code> fully to create a <code>StockStream</code> event. After that, a simple transformation is carried out for the <code>StockStream</code> stream where the value for the <code>symbol</code> attribute is converted to upper case. Finally, the output is logged in the SI console.</p> <p>Once the Siddhi application is successfully deployed, following log appears in the SI console.</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App BinaryFullFileProcessing deployed successfully\n</code></pre> </li> <li> <p>Now the Siddhi application starts to process the <code>wso2.bin</code> file.</p> <p>As a result, the following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - BinaryFullFileProcessing :  LogStream : Event{timestamp=1564660553623, data=[WSO2, 55.6, 100], isExpired=false} \n</code></pre> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-file-line-by-line-and-delete-it-after-processing","title":"Reading a file line by line and delete it after processing","text":"<p>In this scenario, you are reading a text file completely, and then deleting it after  processing. In other words, the file is not tailed. You read the file line by line where each line generates an event.</p> <ol> <li> <p>Download <code>productions.csv</code> file from here and save it in a location of your choice.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('ReadFileLineByLine')\n\n@App:description('Reads a file line by line and does a simple transformation.')\n\n@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/productions.csv',\n    tailing='false',\n    @map(type='csv'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (name string, amount double);\n\nfrom SweetProductionStream\nselect str:upper(name) as name, amount\ninsert into LogStream;\n</code></pre> <p>In the above Siddhi application, change the value for the <code>file.uri</code> parameter to the file path to which you downloaded the <code>productions.csv</code> file in step 1.</p> </li> <li> <p>Save this file as <code>ReadFileLineByLine.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.   </p> <p>Info</p> <p>This Siddhi application tails the <code>productions.csv</code> file line by line. Each line is converted to an event in the <code>SweetProductionStream</code> stream. After that, a simple transformation is carried out for the sweet production runs where the value for the <code>name</code> attribute from the event is converted into upper case. Finally, the output is logged in the SI console.</p> <p>Once the Siddhi application is successfully deployed, the following log appears in the SI console:</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ReadFileLineByLine deployed successfully\n</code></pre> </li> <li> <p>Now the Siddhi application starts to process the <code>productions.csv</code> file. The file has below two entries:</p> <pre><code>Almond cookie,100.0\nBaked alaska,20.0\n</code></pre> <p>As a result, the following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564490867341, data=[ALMOND COOKIE, 100.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564490867341, data=[BAKED ALASKA, 20.0], isExpired=false}\n</code></pre> </li> <li> <p>Note that <code>productions.csv</code> file is not present in the <code>file.uri</code> location.</p> </li> <li> <p>Next, create a new <code>productions.csv</code> file in the <code>file.uri</code> location that includes the latest set of productions. Download <code>productions.csv</code> file from here and save it in the <code>file.uri</code> location.</p> </li> <li> <p>Now the Siddhi application starts to process the new set of production runs in the <code>productions.csv</code> file. The file has the following two entries.</p> <pre><code>Cup cake,300.0\nDoughnut,500.0\n</code></pre> <p>As a result, the following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564902130543, data=[CUP CAKE, 300.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileLineByLine : LogStream : Event{timestamp=1564902130543, data=[DOUGHNUT, 500.0], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-files/#reading-a-file-using-a-regular-expression-and-deleting-it-after-processing","title":"Reading a file using a regular expression and deleting it after processing","text":"<p>In this scenario, you are using a regular expression to extract data from the content of the file. Here, you do not tail the file.  Instead, you read the full content of the file and generate a single event. After this is done, the file is deleted. To generate an event stream, you can keep re-creating the file with new data.</p> <ol> <li> <p>Download <code>noisy_data.txt</code> file from here and save it in a location of your choice.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('ReadFileRegex')\n\n@App:description('Reads a file using a regex and does a simple transformation.')\n\n@source(type='file', mode='REGEX',\n    file.uri='file:/Users/foo/noisy_data.txt',\n    begin.regex='\\&lt;', end.regex='\\&gt;',\n    tailing='false',\n    @map(type='text', fail.on.missing.attribute = 'false', regex.A='(\\w+)\\s([-0-9]+)',regex.B='volume\\s([-0-9]+)', @attributes(symbol = 'A[1]',price = 'A[2]',volume = 'B')))\ndefine stream StockStream (symbol string, price float, volume long);\n\n@sink(type = 'log')\ndefine stream LogStream (symbol string, price float, volume long);\n\nfrom StockStream[NOT(symbol is null)]\nselect str:upper(symbol) as symbol, price, volume  \ninsert into LogStream;\n</code></pre> <p>In the above Siddhi application, change the value of the <code>file.uri</code> parameter to the file path to which you downloaded the <code>noisy_data.txt</code> file in step 1.</p> </li> <li> <p>Save this file as <code>ReadFileRegex.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application tails the <code>noisy_data.txt</code> file to find matches based on the <code>begin.regex</code> and <code>end.regex</code> regular expressions. Each match is converted to an event in the <code>StockStream</code> stream. After that, a simple transformation is carried out for the <code>StockStream</code> stream where value for the <code>symbol</code> attribute converted to upper case. Finally, the output is logged in the SI console.</p> <p>Once the Siddhi application is successfully deployed, following log appears in the SI console:</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ReadFileRegex deployed successfully\n</code></pre> </li> <li> <p>Now the Siddhi application starts to process the <code>noisy_data.txt</code> file. </p> <p>As a result, the following log appears in the SI console.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileRegex : LogStream : Event{timestamp=1564906475623, data=[WSO2, 75.0, 100], isExpired=false}\n</code></pre> <p>Note that <code>noisy_data.txt</code> file is not present in the <code>file.uri</code> location.</p> </li> <li> <p>Next, let's create a new <code>noisy_data.txt</code> file in the <code>file.uri</code> location that includes the latest set of productions. Download <code>noisy_data.txt</code> file from here and save it in the <code>file.uri</code> location.</p> <p>Now the Siddhi application starts to process the new content in the <code>noisy_data.txt</code> file. The file has the following content.</p> <pre><code>Oracle\u00a0Corporation &lt;orcl 95 volume 200&gt; 500\u00a0Oracle\u00a0Parkway.\nRedwood Shores\u00a0CA, 94065.\nCorporate\u00a0Phone: 650.506.7000.\nHQ-Security: 650.506.5555\n</code></pre> <p>As a result, the following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReadFileRegex : LogStream : Event{timestamp=1564906713176, data=[ORCL, 95.0, 200], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-files/#extracting-data-from-a-folder","title":"Extracting data from a folder","text":""},{"location":"examples/performing-real-time-etl-with-files/#processing-all-files-in-the-folder","title":"Processing all files in the folder","text":"<p>In this scenario, you extract data from a specific folder. All of the files are processed sequentially, where each file generates a single event.</p> <ol> <li> <p>Download <code>productions.zip</code> file from here and extract it. Now you have a folder named <code>productions</code>. Place it in a location of your choice.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('ProcessFolder')\n\n@App:description('Process all files in the folder and delete files after processing.')\n\n@source(type='file', mode='text.full',\n    dir.uri='file:/Users/foo/productions',  \n    @map(type='json', enclosing.element=\"$.portfolio\", @attributes(symbol = \"stock.company.symbol\", price = \"stock.price\", volume = \"stock.volume\")))\ndefine stream StockStream (symbol string, price float, volume long);\n\n@sink(type = 'log')\ndefine stream LogStream (symbol string, price float, volume long);\n\nfrom StockStream\nselect str:upper(symbol) as symbol, price, volume    \ninsert into LogStream;\n</code></pre> <p>In the above Siddhi application, change the value for the <code>dir.uri</code> parameter so that it points to the <code>productions</code> folder you created in step 1.</p> </li> <li> <p>Save this file as <code>ProcessFolder.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application processes each file in <code>productions</code> folder. Each file generates an event in the <code>StockStream</code> stream. After that, a simple transformation is carried out for the <code>StockStream</code> stream where the value for the <code>symbol</code> attribute is converted to upper case. Finally, the output is logged in the SI console.</p> <p>Once the Siddhi application is successfully deployed, following log appears in the SI console:</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App ProcessFolder deployed successfully\n</code></pre> <p>Now the Siddhi application starts to process each file in the <code>productions</code> directory.</p> <p>As a result, the following logs appear in the SI console: <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[WSO2, 75.0, 100], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[ORCL, 95.0, 200], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ProcessFolder : LogStream : Event{timestamp=1564932255417, data=[IBM, 88.0, 150], isExpired=false}\n</code></pre></p> </li> </ol> <p>Info</p> <p>In this scenario, you deleted each file in the folder after processing. You can choose to move the files instead of deleting them. To do this, set the <code>action.after.process</code> parameter to <code>MOVE</code> and specify the directory to which the files should be moved via the <code>move.after.process</code> parameter. For more information about these parameters, see Siddhi File Source documentation.</p>"},{"location":"examples/performing-real-time-etl-with-files/#loading-data-into-a-file","title":"Loading data into a file","text":"<p>In this section of the tutorial, you are exploring the different ways in which you could load data into a file.  </p>"},{"location":"examples/performing-real-time-etl-with-files/#appending-or-over-writing-events-to-a-file","title":"Appending or over-writing events to a file","text":"<p>In this scenario, you are appending a stream of events to the end of a file.</p> <ol> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('AppendToFile')\n\n@App:description('Append incoming events in to a file.')\n\n@Source(type = 'http', receiver.url='http://localhost:8006/SweetProductionStream', basic.auth.enabled='false',\n        @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='file', @map(type='json'), file.uri='/Users/foo/low_productions.txt')\ndefine stream LowProductionStream (name string, amount double);\n\n-- Query to filter productions which have amount &lt; 500.0\n@info(name='query1') \nfrom SweetProductionStream[amount &lt; 500.0]\nselect *\ninsert into LowProductionStream;\n</code></pre> <p>Create an empty file and specify the location of the file as the value for the <code>file.uri</code> parameter. If this file does not exist, it is created at runtime.</p> </li> <li> <p>Save this file as <code>AppendToFile.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application filters incoming <code>SweetProductionStream</code> events, selects the production runs of which the value for the <code>amount</code> attribute is less than <code>500.0</code>, and inserts the results into the <code>LowProductionStream</code>. Finally, all the events in the <code>LowProductionStream</code> events are appended to the file specified via the <code>file.uri</code> parameter in the Siddhi application.</p> <p>Once the Siddhi application is successfully deployed, the following log appears in the SI console:</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App AppendToFile deployed successfully\n</code></pre> </li> <li> <p>To insert a few events into <code>SweetProductionStream</code>,  let's issue the following <code>CURL</code> commands:</p> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Almond cookie\\\",\\\"amount\\\": 100.0}}\"  http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Baked alaska\\\",\\\"amount\\\": 20.0}}\"  http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\"\n</code></pre> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"Cup cake\\\",\\\"amount\\\": 300.0}}\"  http://localhost:8006/SweetProductionStream --header \"Content-Type:application/json\"\n</code></pre> </li> <li> <p>Now open the file that you specified via the <code>file.uri</code> parameter. Note that the file has following content.</p> <pre><code>{\"event\":{\"name\":\"Almond cookie\",\"amount\":100.0}}\n{\"event\":{\"name\":\"Baked alaska\",\"amount\":20.0}}\n{\"event\":{\"name\":\"Cup cake\",\"amount\":300.0}}\n</code></pre> </li> </ol> <p>Info</p> <p>Instead of appending each event to the end of the file, you can configure your Siddhi application to over-write the file. To do this, set the <code>append='false'</code> configuration in the Siddhi application as shown in the sample <code>file</code> sink configuration below.</p> <pre><code>@sink(type='file', append='false',  @map(type='json'), file.uri='/Users/foo/low_productions.txt')\n   define stream LowProductionAlertStream (name string, amount double);\n</code></pre> <p>For other configuration options, see Siddhi File Sink documentation.</p>"},{"location":"examples/performing-real-time-etl-with-files/#preserving-the-state-of-the-application-through-a-system-failure","title":"Preserving the state of the application through a system failure","text":"<p>Let's try out a scenario where you deploy a Siddhi application to count the total number of production runs of a sweet factory.</p> <p>The production data is updated in a file and therefore you have to keep tailing this file, in order to get updates on the productions. </p> <p>Info</p> <p>In this scenario, the Streaming Integrator server needs to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator.</p> <ol> <li> <p>Enable state persistence feature in SI server as follows. Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file on a text editor and locate the <code>state.persistence</code> section.  </p> <pre><code>  # Periodic Persistence Configuration\nstate.persistence:\n  enabled: true\n  intervalInMin: 1\n  revisionsToKeep: 2\n  persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore\n  config:\n    location: siddhi-app-persistence\n</code></pre> <p>Set <code>enabled</code> parameter to <code>true</code> and save the file. </p> </li> <li> <p>To enable the state persistence debug logs, open the <code>&lt;SI_HOME&gt;/conf/server/log4j2.xml</code> file on a text editor and locate following line in it.</p> <pre><code> &lt;Logger name=\"com.zaxxer.hikari\" level=\"error\"/&gt;\n</code></pre> <p>Then add following <code>&lt;Logger&gt;</code> element below that line.</p> <pre><code>&lt;Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/&gt;\n</code></pre> <p>Save the file.</p> </li> <li> <p>Restart the Streaming Integrator server for above change to be effective.</p> </li> <li> <p>Download <code>productions.csv</code> file from here and save it in a location of your choice.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('CountProductions')\n\n@App:description('Siddhi application to count the total number of orders.')\n\n@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/productions.csv',\n    tailing='true',\n    @map(type='csv'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (totalProductions double);\n\n-- Following query counts the number of sweet productions.\n@info(name = 'query')\nfrom SweetProductionStream\nselect sum(amount) as totalProductions\ninsert into LogStream;\n</code></pre> <p>Change the <code>file.uri</code> parameter in the above Siddhi application to the file path to which you downloaded the <code>productions.csv</code> file in step 4.</p> </li> <li> <p>Save this file as <code>CountProductions.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.   </p> <p>Info</p> <p>This Siddhi application tails the file <code>productions.csv</code> line by line. Each line is converted to an event in the <code>SweetProductionStream</code> stream. After that, a simple transformation is carried out for the sweet production runs. This transformation involves converting the value for the <code>name</code> attribute to upper case. Finally, the output is logged in the SI console.</p> <p>Once the Siddhi application is successfully deployed, the following log appears in the SI console.</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully\n</code></pre> </li> <li> <p>Now the Siddhi application starts to process the <code>productions.csv</code> file. The file two entries as follows.</p> <pre><code>Almond cookie,100.0\nBaked alaska,20.0\n</code></pre> <p>As a result, the following log appears in the SI console.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097506866, data=[100.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097506866, data=[120.0], isExpired=false}\n</code></pre> <p>These logs print the sweet production count. Note that the current count of sweet productions is being printed as <code>120</code> in the second log. This is because the factory has so far produced <code>120</code> sweets: <code>100</code> Almond cookies and <code>20</code> Baked alaskas.</p> </li> <li> <p>Now wait for following log to appear in the SI console.</p> <pre><code>DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully\n</code></pre> <p>This log indicates that the current state of the Siddhi application is successfully persisted. The Siddhi application state is persisted every minute. Therefore, you can see this log appearing every minute.</p> <p>Next, let's append two sweet production entries into the <code>productions.csv</code> file and shutdown the SI server before the state persistence happens (i.e., before the above log appears).</p> <p>Tip</p> <p>It is better to start appending the records immediately after the state persistence log appears so that you have plenty of time to append the records and shutdown the server before next log appears.</p> </li> <li> <p>Now append following content into the <code>productions.csv</code> file:</p> <pre><code>Croissant,100.0\nCroutons,100.0\n</code></pre> </li> <li> <p>Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count.</p> <p>Info</p> <p>Here, the SI server crashes before the state is persisted. Therefore, the Streaming Integrator server cannot persist the latest count (which includes the last two production runs that produced <code>100</code> Croissants and <code>100</code> Croutons). The good news is, the <code>File source</code> source replays the last two messages, allowing the Streaming Integrator to successfully recover from the server crash.</p> </li> <li> <p>Restart the SI server and wait for about one minute.</p> <p>The following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097846807, data=[220.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1565097846812, data=[320.0], isExpired=false}\n</code></pre> </li> </ol> <p>Note that the <code>File source</code> has replayed the last two messages. This indicates that the sweet productions count has been correctly restored.</p>"},{"location":"examples/performing-real-time-etl-with-mysql/","title":"Performing Real-time Change Data Capture with MySQL","text":""},{"location":"examples/performing-real-time-etl-with-mysql/#introduction","title":"Introduction","text":"<p>The Streaming Integrator (SI) allows you to capture changes to a database table, in a streaming manner, enabling you to perform ETL operations.</p> <p>This tutorial takes you through the different modes and  options you could use to perform Change Data Capturing (CDC) using the SI. In this tutorial, you are using a MySQL datasource.</p> <p>Info</p> <p>To use a different database other than MySQL, see dependencies for CDC and add the corresponding driver jar. In addition to that, modify the JDBC URL accordingly, in <code>url</code> parameter in all Siddhi applications given in this tutorial.</p> <p>Listening mode and Polling mode</p> <p>There are two modes in which you could perform CDC using the SI: Listening mode and Polling mode.</p> <ul> <li> <p>Polling mode: In the polling mode, the data source is periodically polled for capturing the changes. The polling period can be configured.</p> </li> <li> <p>Listening mode: In listening mode, the SI keeps listening to the Change Log of the database and notifies if a change takes place. Here, unlike the polling mode, you are notified about the change immediately.</p> </li> </ul> <p>Type of events captured</p> <p>You can capture following type of changes done to a database table:</p> <ul> <li> <p>Insert operations</p> </li> <li> <p>Update operations</p> </li> <li> <p>Delete operations (available for Listening mode only)</p> </li> </ul>"},{"location":"examples/performing-real-time-etl-with-mysql/#tutorial-steps","title":"Tutorial steps","text":""},{"location":"examples/performing-real-time-etl-with-mysql/#listening-mode","title":"Listening mode","text":"<p>Before you begin:</p> <ul> <li>You need to have access to a MySQL instance.</li> <li> <p>Enable binary logging in the MySQL server. For detailed instructions, see Debezium documentation - Enabling the binlog.</p> <p>Note</p> <p>If you are using MySQL 8.0, use the following query to check the binlog status: <pre><code>SELECT variable_value as \"BINARY LOGGING STATUS (log-bin) ::\"\nFROM performance_schema.global_variables WHERE variable_name='log_bin';\n</code></pre></p> </li> <li> <p>Add the MySQL JDBC driver into the <code>&lt;SI_HOME&gt;/lib</code> directory as follows:</p> <ol> <li>Download the MySQL JDBC driver from the MySQL site.</li> <li>Unzip the archive.</li> <li>Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> to the <code>&lt;SI_HOME&gt;/lib</code> directory.</li> <li>Start the SI server.</li> </ol> </li> <li>Once you install MySQL and start the MySQL server, create the database and the database table you require as follows:<ol> <li>Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query. <code>CREATE SCHEMA production;</code></li> <li>Create a new user by executing the following SQL query. <code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2';</code></li> <li>Switch to the <code>production</code> database and create a new table, by executing the following queries: <code>use production;</code> <code>CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));</code> </li> </ol> </li> <li>Download and install the siddhi-io-cdc extension. For instructions, see Downloading and Installing Siddhi Connectors.</li> </ul>"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-inserts","title":"Capturing inserts","text":"<p>Now you can write a simple Siddhi application to monitor the <code>SweetProductionTable</code> for insert operations.</p> <ol> <li> <p>Open a text file and copy-paste following application into it.</p> <p><pre><code>@App:name('CDCListenForInserts')\n\n@App:description('Capture MySQL Inserts using CDC listening mode.')\n\n@source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert',\n    @map(type = 'keyvalue'))\ndefine stream InsertSweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (name string, amount double);\n\n@info(name = 'query')\nfrom InsertSweetProductionStream\nselect *\ninsert into LogStream;\n</code></pre> Here the <code>url</code> parameter has the value <code>jdbc:mysql://localhost:3306/production</code>. Change it to point to your MySQL server.</p> </li> <li> <p>Save this file as <code>CDCListenForInserts.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application captures all the inserts made to the <code>SweetProductionTable</code> database table and logs them.</p> </li> <li> <p>To install the extensions required for the <code>CDCListenForInserts</code> Siddhi application you deployed, open a new terminal window and navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory and issue one of the following commands as appropriate, based on your operating system:     </p> <ul> <li>For Windows: <code>extension-installer.bat</code> </li> <li>For Linux:  <code>sh extension-installer.sh</code> </li> </ul> </li> <li> <p>Now let's perform an insert operation on the MySQL table by executing the following MySQL query on the database:</p> <pre><code>insert into SweetProductionTable values('chocolate',100.0);\n</code></pre> <p>The following log appears in the SI console:</p> <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : logStream : Event{timestamp=1563200225948, data=[chocolate, 100.0], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-updates","title":"Capturing updates","text":"<p>Now you can write a Siddhi application to monitor the <code>SweetProductionTable</code> for update operations.</p> <ol> <li> <p>Open a text file and copy-paste following application into it.</p> <pre><code>@App:name('CDCListenForUpdates')\n\n@App:description('Capture MySQL Updates using CDC listening mode.')\n\n@source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'update',\n    @map(type = 'keyvalue'))\ndefine stream UpdateSweetProductionStream (before_name string, name string, before_amount double, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (before_name string, name string, before_amount double, amount double);\n\n@info(name = 'query')\nfrom UpdateSweetProductionStream\nselect *\ninsert into LogStream;\n</code></pre> </li> <li> <p>Save this file as <code>CDCListenForUpdates.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application captures all the updates to the <code>SweetProductionTable</code> database table and logs them.</p> </li> <li> <p>Now let's perform an update operation on the MySQL table. For this, execute following MySQL query on the database:</p> <pre><code>update SweetProductionTable SET name = 'Almond cookie' where name = 'chocolate';\n</code></pre> <p>As a result, you can see the following log in the SI console.</p> <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : updateSweetProductionStream : Event{timestamp=1563201040953, data=[chocolate, Almond cookie, 100.0, 100.0], isExpired=false}\n</code></pre> <p>Info</p> <p>Here, the <code>before_name1</code> attribute indicates the value of the <code>name</code> attribute before the update was made (<code>chocolate</code> in this case), and the <code>name</code> attribute has the current name after the update (i.e., <code>almond cookie</code>).</p> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-deletes","title":"Capturing deletes","text":"<p>Now you can write a Siddhi application to monitor the <code>SweetProductionTable</code> for delete operations.</p> <ol> <li> <p>Open a text file and copy-paste following application into it.</p> <pre><code>@App:name('CDCListenForDeletes')\n\n@App:description('Capture MySQL Deletes using CDC listening mode.')\n\n@source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'delete',\n    @map(type = 'keyvalue'))\ndefine stream DeleteSweetProductionStream (before_name string, before_amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (before_name string, before_amount double);\n\n@info(name = 'query')\nfrom DeleteSweetProductionStream\nselect *\ninsert into LogStream;\n</code></pre> </li> <li> <p>Save this file as <code>CDCListenForDeletes.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application captures all the delete operations carried out for the <code>SweetProductionTable</code> database table and logs them.</p> </li> <li> <p>Now let's perform a delete operation for the MySQL table. To do this, execute following MySQL query on the database:</p> <pre><code>delete from SweetProductionTable where name = 'Almond cookie';\n</code></pre> <p>The following log appears in the SI console:</p> <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithListeningMode : DeleteSweetProductionStream : Event{timestamp=1563367367098, data=[Almond cookie, 100.0], isExpired=false}\n</code></pre> <p>Info</p> <p>Here, the <code>before_name</code> attribute indicates the name of the sweet in the deleted record (i.e., <code>Almond cookie</code> in this case). Similarly, the <code>before_amount</code> indicates the amount in the deleted record.</p> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-mysql/#preserving-state-of-the-application-through-a-system-failure","title":"Preserving State of the application through a system failure","text":"<p>Let's try out a scenario in which you are going to deploy a Siddhi application to count the total number of productions.</p> <p>Info</p> <p>In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator.</p> <ol> <li> <p>Enable state persistence feature in SI server as follows. Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file on a text editor and locate the <code>state.persistence</code> section.</p> <pre><code>  # Periodic Persistence Configuration\nstate.persistence:\n  enabled: true\n  intervalInMin: 1\n  revisionsToKeep: 2\n  persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore\n  config:\n    location: siddhi-app-persistence\n</code></pre> <p>Set <code>enabled</code> parameter to <code>true</code> and save the file.</p> </li> <li> <p>Enable state persistence debug logs as follows. Open the <code>&lt;SI_HOME&gt;/conf/server/log4j2.xml</code> file on a text editor and locate the following line in it.</p> <pre><code> &lt;Logger name=\"com.zaxxer.hikari\" level=\"error\"/&gt;\n</code></pre> <p>Add following <code>&lt;Logger&gt;</code> element below that.</p> <pre><code>&lt;Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/&gt;\n</code></pre> <p>Save the file.</p> </li> <li> <p>Restart the Streaming Integrator server for above change to be effective.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name('CountProductions')\n\n@App:description('Siddhi application to count the total number of orders.')\n\n@source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2si', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert',\n        @map(type = 'keyvalue'))\ndefine stream InsertSweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (totalProductions double);\n\n@info(name = 'query')\nfrom InsertSweetProductionStream\nselect sum(amount) as totalProductions\ninsert into LogStream;\n</code></pre> </li> <li> <p>Save this file as <code>CountProductions.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory. When the    Siddhi application is successfully deployed, the following <code>INFO</code> log appears in the Streaming Integrator console.</p> <pre><code>INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully\n</code></pre> </li> <li> <p>Now let's perform a few insert operations on the MySQL table. Execute following MySQL queries on the database:</p> <pre><code>insert into SweetProductionTable values('Almond cookie',100.0);\n</code></pre> <pre><code>insert into SweetProductionTable values('Baked alaska',20.0);\n</code></pre> <p>Now you can see following logs on the SI console.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151034866, data=[100.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151037870, data=[120.0], isExpired=false}\n</code></pre> <p>These logs print the sweet production count. Note that the current count of sweet productions is being printed as <code>120</code> in the second log. This is because the factory has so far produced <code>120</code> sweets: <code>100</code> Almond cookies and <code>20</code> Baked alaskas.</p> </li> <li> <p>Now wait for following log to appear on the SI console</p> <pre><code>DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully\n</code></pre> <p>This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute. Therefore, you can see this log appearing every minute.</p> <p>Next, let's insert two sweet productions into the <code>SweetProductionTable</code> and shutdown the SI server before the state persistence happens (in other words, before the above log appears).</p> <p>Tip</p> <p>It is better to start inserting records immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server before next log appears.</p> </li> <li> <p>Now insert following sweets into the <code>SweetProductionTable</code> by executing following queries on the database :</p> <pre><code>insert into SweetProductionTable values('Croissant',100.0);\n</code></pre> <pre><code>insert into SweetProductionTable values('Croutons',100.0);\n</code></pre> </li> <li> <p>Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count.</p> <p>Info</p> <p>Here, the SI server crashes before the state is persisted. Therefore the SI server cannot persist the latest count (which should include the last two productions <code>100</code> Croissants and <code>100</code> Croutons). The good news is, <code>CDC source</code> replays the last two messages, allowing the Streaming Integrator to recover successfully from the server crash.</p> </li> <li> <p>Restart the SI server and wait for about one minute.</p> </li> <li> <p>The following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151078607, data=[220.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : LogStream : Event{timestamp=1564151078612, data=[320.0], isExpired=false}\n</code></pre> </li> </ol> <p>Note that the <code>CDC source</code> has replayed the last two messages. As a result, the sweet production runs count has being correctly restored.</p>"},{"location":"examples/performing-real-time-etl-with-mysql/#polling-mode","title":"Polling mode","text":"<p>Before you begin:</p> <p>You are required to have access to a MySQL instance. Create the required database and the database table in the MySQL instance as follows: 1. Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, issue the following command. <code>CREATE SCHEMA production_pol;</code> 2. Switch to the production database and create a new table by executing following queries. <code>use production_pol;</code> <code>CREATE TABLE SweetProductionTable (last_update TIMESTAMP, name VARCHAR(20),amount double(10,2));</code> 3. If you have not already created a user under Listening Mode, create a new user by executing the following SQL query. <code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2';</code> 4. If you have not already added the MySQL JDBC driver into <code>&lt;SI_HOME&gt;/lib</code> under Listening Mode, add it as follows:         a. Download the MySQL JDBC driver from the MySQL site.         b. Unzip the archive.         c. Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> to the <code>&lt;SI_HOME&gt;/lib</code> directory.</p>"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-inserts_1","title":"Capturing inserts","text":"<p>Now you can write a simple Siddhi application to monitor the <code>SweetProductionTable</code> table for insert operations.</p> <ol> <li> <p>Open a text file and copy-paste the following application into it.</p> <pre><code>@App:name(\"CDCPolling\")\n\n@App:description(\"Capture MySQL changes, using CDC source - polling mode.\")\n\n@source(type = 'cdc',\n    url = 'jdbc:mysql://localhost:3306/production_pol?useSSL=false',\n    mode = 'polling',\n    jdbc.driver.name = 'com.mysql.jdbc.Driver',\n    polling.column = 'last_update',\n    polling.interval = '10',\n    username = 'wso2si',\n    password = 'wso2',\n    table.name = 'SweetProductionTable',\n    @map(type = 'keyvalue' ))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (name string, amount double);\n\n@info(name = 'query')\nfrom SweetProductionStream\nselect *\ninsert into LogStream;\n</code></pre> <p>Here the <code>url</code> parameter currently specifies the URL <code>jdbc:mysql://localhost:3306/production_pol</code>. Change it to point to your MySQL server.</p> </li> <li> <p>Save this file as <code>CDCPolling.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>Info</p> <p>This Siddhi application polls the database periodically, captures the changes made to the <code>SweetProductionTable</code>  database table during the polled interval and logs them. The polling interval is specified via the <code>polling.interval</code>  parameter in the Siddhi application when defining the CDC source. In this example, the polling interval is 10 seconds.</p> </li> <li> <p>Now let's perform an insert operation on the MySQL table. To do this, execute following MySQL query on the database.</p> <pre><code>insert into SweetProductionTable(name,amount) values('chocolate',100.0);\n</code></pre> <p>The following log appears in the SI console:</p> <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithPollingMode : LogStream : Event{timestamp=1563378804914, data=[chocolate, 100.0], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"examples/performing-real-time-etl-with-mysql/#capturing-updates_1","title":"Capturing Updates","text":"<p>For capturing updates, you can use the same <code>CDCPolling.siddhi</code> Siddhi application that you deployed in the Capturing inserts section.</p> <p>Let's perform an update operation on the MySQL table. To do this, execute the following MySQL query on the database:</p> <pre><code>update SweetProductionTable SET name = 'Almond cookie' where name = 'chocolate';\n</code></pre> <p>The following log appears in the SI console:</p> <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - CDCWithPollingMode : logStream : Event{timestamp=1563436388530, data=[Almond cookie, 100.0], isExpired=false}\n</code></pre>"},{"location":"examples/performing-real-time-etl-with-mysql/#preserving-state-of-the-application-through-a-system-failure_1","title":"Preserving State of the application through a system failure","text":"<p>Let's try out a scenario in which you deploy a Siddhi application to count the total number of production runs.</p> <p>Info</p> <p>In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero. To achieve this, you can use the state persistence capability in the Streaming Integrator.</p> <ol> <li> <p>Enable state persistence feature in SI server as follows. Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file on a text editor and locate the <code>state.persistence</code> section.</p> <pre><code>  # Periodic Persistence Configuration\nstate.persistence:\n  enabled: true\n  intervalInMin: 1\n  revisionsToKeep: 2\n  persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore\n  config:\n    location: siddhi-app-persistence\n</code></pre> <p>Set <code>enabled</code> parameter to <code>true</code> and save the file.</p> </li> <li> <p>Enable state persistence debug logs as follows. Open the <code>&lt;SI_HOME&gt;/conf/server/log4j2.xml</code> file on a text editor and locate the following line in it.</p> <pre><code> &lt;Logger name=\"com.zaxxer.hikari\" level=\"error\"/&gt;\n</code></pre> <p>Add following <code>&lt;Logger&gt;</code> element below that.</p> <pre><code>&lt;Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/&gt;\n</code></pre> <p>Save the file.</p> </li> <li> <p>Restart the Streaming Integrator server for above change to be effective.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name(\"CountProductions_pol\")\n\n@App:description(\"Siddhi application to count the total number of orders.\")\n\n@source(type = 'cdc',\n    url = 'jdbc:mysql://localhost:3306/production_pol?useSSL=false',\n    mode = 'polling',\n    jdbc.driver.name = 'com.mysql.jdbc.Driver',\n    polling.column = 'last_update',\n    polling.interval = '10',\n    username = 'wso2si',\n    password = 'wso2',\n    table.name = 'SweetProductionTable',\n    @map(type = 'keyvalue' ))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream LogStream (totalProductions double);\n\n@info(name = 'query')\nfrom SweetProductionStream\nselect sum(amount) as totalProductions\ninsert into LogStream;\n</code></pre> </li> <li> <p>Save this file as <code>CountProductions_pol.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory. When the    Siddhi application is successfully deployed, the following <code>INFO</code> log appears in the Streaming Integrator console.</p> <pre><code>INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions_pol deployed successfully\n</code></pre> </li> <li> <p>Now let's perform a few insert operations on the MySQL table. Execute following MySQL queries on the database:</p> <pre><code>insert into SweetProductionTable(name,amount) values('Almond cookie',100.0);\n</code></pre> <pre><code>insert into SweetProductionTable(name,amount) values('Baked alaska',20.0);\n</code></pre> <p>Now you can see following logs on the SI console.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564385971323, data=[100.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386011344, data=[120.0], isExpired=false}\n</code></pre> <p>These logs print the sweet production count. Note that the current count of sweet production runs is being printed as <code>120</code> in the second log. This is because we have so far produced <code>120</code> sweets: <code>100</code> Almond cookies and <code>20</code> Baked alaskas.</p> </li> <li> <p>Now wait for following log to appear on the SI console.</p> <pre><code>DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions_pol persisted successfully\n</code></pre> <p>This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute, therefore you can see this log appearing every minute.</p> <p>Next, you are going to insert two sweet production runs into the <code>SweetProductionTable</code> and shutdown the SI server before state persistence happens (in other words, before above log appears).</p> <p>Tip</p> <p>It is better to start pushing messages immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server before next log appears.</p> </li> <li> <p>Now insert following sweets into the <code>SweetProductionTable</code> by executing following queries on the database :</p> <pre><code>insert into SweetProductionTable(name,amount) values('Croissant',100.0);\n</code></pre> <pre><code>insert into SweetProductionTable(name,amount) values('Croutons',100.0);\n</code></pre> </li> <li> <p>Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count.</p> <p>Info</p> <p>Here, the SI server crashes before the state is persisted. Therefore, the SI server cannot persist the latest count (which should include the last two production runs <code>100</code> Croissants and <code>100</code> Croutons). The good news is, the <code>CDC source</code> replays the last two messages, allowing the Streaming Integrator to successfully recover from the server crash.</p> </li> <li> <p>Restart the SI server and wait for about one minute.</p> </li> <li> <p>The following log appears in the SI console:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386179998, data=[220.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions_pol : LogStream : Event{timestamp=1564386180004, data=[320.0], isExpired=false}\n</code></pre> </li> </ol> <p>Note that the <code>CDC source</code> has replayed the last two messages. This indicates that the sweet production run count is correctly restored.</p>"},{"location":"examples/running-si-with-docker-and-kubernetes/","title":"Running the Streaming Integrator in Containerized Environments","text":""},{"location":"examples/running-si-with-docker-and-kubernetes/#running-the-streaming-integrator-with-docker","title":"Running the Streaming Integrator with Docker","text":"<p>This section shows you how to run Streaming Integrator in Docker. This involves installing Docker, running the Streaming Integrator in Docker and then deploying and running a Siddhi application in the Docker environment.</p> <p>Before you begin:</p> <ul> <li>The system requirements are as follows:<ul> <li>3 GHz Dual-core Xeon/Opteron (or latest)</li> <li>8 GB RAM</li> <li>10 GB free disk space</li> </ul> </li> <li>Install Docker by following the instructions provided in here.</li> <li>Save the following Siddhi application as a <code>.siddhi</code> file in a preferred location in your machine. <pre><code>@App:name('MySimpleApp')\n@App:description('Receive events via HTTP transport and view the output on the console')\n@Source(type = 'http', receiver.url='http://0.0.0.0:8006/productionStream', basic.auth.enabled='false',\n   @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n@sink(type='log')\ndefine stream TransformedProductionStream (nameInUpperCase string, amount double);\n-- Simple Siddhi query to transform the name to upper case.\nfrom SweetProductionStream\nselect str:upper(name) as nameInUpperCase, amount\ninsert into TransformedProductionStream;\n</code></pre>      Note the following about this Siddhi application.<ul> <li>The Siddhi application operates in Docker. Therefore, the HTTP source configured in it uses a receiver URL where the host number is <code>0.0.0.0</code>.</li> <li>The <code>8006</code> port of the receiver URL is the same HTTP port that you previously exposed via Docker.</li> </ul> </li> </ul>"},{"location":"examples/running-si-with-docker-and-kubernetes/#starting-the-streaming-integrator-in-docker","title":"Starting the Streaming Integrator in Docker","text":"<p>In this scenario, you are downloading and installing the Streaming Integrator via Docker.</p> <p>WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2.</p> <p>To run the Streaming Integrator in the  open source image that is available for it</p> <ol> <li> <p>To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command.</p> <p><code>docker pull -it wso2/streaming-integrator:1.0.0</code></p> </li> <li> <p>Expose the required ports via docker when running the docker container. In this scenario, you need to expose the following ports:</p> <ul> <li> <p>The 9443 port where the Streaming Integrator server is run.</p> </li> <li> <p>The 8006 HTTP port from which Siddhi application you are deploying in this scenario receives messages.</p> </li> </ul> <p>To expose these ports, issue the following command.</p> <p><code>docker run -p 9443:9443 -p 8006:8006 wso2/streaming-integrator/1.0.0 -v &lt;local-absolute-siddhi-file-path&gt;/MySimpleApp.siddhi:/apps/MySimpleApp.siddhi siddhiio/siddhi-runner-alpine -Dapps=/apps/MySimpleApp.siddhi</code></p> <p>Info</p> <p>In the above command, you are mounting the location where you have saved the <code>MySimpleApp.siddhi</code> file so that the Streaming Integrator can locate it and run it when it starts in Docker. Therefore, replace <code>&lt;local-absolute-siddhi-file-path&gt;</code> with the path in which you saved the Siddhi application in your machine.</p> </li> <li> <p>If you did not mount the location to the <code>MySimpleApp.siddhi</code> file when issuing the command to start the Streaming Integrator, you can deploy the Siddhi application via the Streaming Integrator tool.</p> Click here for detailed instructions.<ol> <li>Start and access the Streaming Integrator Tooling. Open a new file and copy-paste the <code>MySimpleApp.siddhi</code> Siddhi application in the Source View.     Then save the Siddhi application.</li> <li>To deploy the Siddhi application, click the Deploy menu option and then click Deploy to Server. The Deploy Siddhi Apps to Server dialog box opens as shown in the example below. <ol> <li>In the Add New Server section, enter information as follows:      Then click Add.</li> <li>Select the check boxes for the MySimpleApp Siddhi application and the server you added as shown below. </li> <li>Click Deploy.     When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box.      The following is logged in the console in which you started the Streaming Integrator in Docker. </li> </ol> </li> </ol> </li> </ol> <p>Now the Streaming Integrator has started in the Docker environment.</p>"},{"location":"examples/running-si-with-docker-and-kubernetes/#creating-and-deploying-the-siddhi-application","title":"Creating and deploying the Siddhi application","text":"<p>Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then logs it in the SI console.</p>"},{"location":"examples/running-si-with-docker-and-kubernetes/#trying-out-the-siddhi-application","title":"Trying-out the Siddhi application","text":"<p>To try out the <code>MySimpleApp</code> Siddhi application you deployed in Docker, issue the following CURL command.</p> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\"  http://0.0.0.0:8006/productionStream --header \"Content-Type:application/json\"\n</code></pre> <p>The following output appears in the console in which you started the Streaming Integrator in Docker.</p> <p></p>"},{"location":"examples/running-si-with-docker-and-kubernetes/#running-the-streaming-integrator-with-kubernetes","title":"Running the Streaming Integrator with Kubernetes","text":"<p>In this section, you get to start and run the Streaming Integrator in a Kubernetes cluster in 5 minutes.</p> <p>Before you begin:</p> <ul> <li>Create a Kubernetes cluster. In this quick start guide, you can do this via Minikube as follows.<ol> <li>Install Minikube  and start a cluster by following the Minikube Documentation.</li> <li>Enable ingress on Minikube by issuing the following command. <code>minikube addons enable ingress</code></li> </ol> </li> <li>Make sure that you have admin privileges to install the Siddhi operator.</li> </ul>"},{"location":"examples/running-si-with-docker-and-kubernetes/#installing-the-siddhi-operator-for-the-streaming-integrator","title":"Installing the Siddhi Operator for the Streaming Integrator","text":"<p>To install the Siddhi Operator, follow the procedure below:</p> <ol> <li> <p>To install the Siddhi Kubernetes operator for streaming integrator issue the following commands:</p> <p><code>kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml</code></p> <p><code>kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml</code></p> </li> <li> <p>To verify whether the Siddhi operator is successfully installed, issue the following command.</p> <p><code>kubectl get deployment</code></p> <p>If the installation is successful, the following deployments should be running in the Kubernetes cluster.</p> <p></p> </li> </ol>"},{"location":"examples/running-si-with-docker-and-kubernetes/#deploying-siddhi-applications-in-kubernetes","title":"Deploying Siddhi applications in Kubernetes","text":"<p>You can deploy multiple Siddhi applications in one or more selected containers via Kubernetes. In this example, let's deploy just one Siddhi application in one container for the ease of understanding how to run the Streaming Integrator in a Kubernetes cluster.</p> <ol> <li> <p>First, let's design a simple Siddhi application that consumes events via HTTP to detect power surges. It filters events for a specific device type (i.e., dryers) and that also report a value greater than 600 for <code>power</code>.</p> <pre><code>    @App:name(\"PowerSurgeDetection\")\n@App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")\n/*\n        Input: deviceType string and powerConsuption int(Watt)\n        Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.\n    */\n\n\n@source(\ntype='http',\nreceiver.url='${RECEIVER_URL}',\nbasic.auth.enabled='false',\n@map(type='json')\n)\n\ndefine stream DevicePowerStream(deviceType string, power int);\n\n@sink(type='log', prefix='LOGGER')\ndefine stream PowerSurgeAlertStream(deviceType string, power int);\n\n@info(name='surge-detector')\nfrom DevicePowerStream[deviceType == 'dryer' and power &gt;= 600]\nselect deviceType, power\ninsert into PowerSurgeAlertStream;\n</code></pre> </li> <li> <p>The above Siddhi application needs to be deployed via a YAML file. Therefore, enter basic information for the YAML file and include the Siddhi application in a section named <code>spec</code> as shown below.</p> <pre><code>apiVersion: siddhi.io/v1alpha2\nkind: SiddhiProcess\nmetadata:\n  name: streaming-integrator\nspec:\n  apps:\n    - script: |\n        @App:name(\"PowerSurgeDetection\")\n        @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")\n        /*\n            Input: deviceType string and powerConsuption int(Watt)\n            Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.\n        */\n\n        @source(\n          type='http',\n          receiver.url='${RECEIVER_URL}',\n          basic.auth.enabled='false',\n          @map(type='json')\n        )\n        define stream DevicePowerStream(deviceType string, power int);\n        @sink(type='log', prefix='LOGGER')\n        define stream PowerSurgeAlertStream(deviceType string, power int);\n        @info(name='surge-detector')\n        from DevicePowerStream[deviceType == 'dryer' and power &gt;= 600]\n        select deviceType, power\n        insert into PowerSurgeAlertStream;\n</code></pre> </li> <li> <p>Add a section named `container' and and parameters with values to configure the container in which the Siddhi application is to be deployed.</p> <pre><code>container:\nenv:\n  -\n    name: RECEIVER_URL\n    value: \"http://0.0.0.0:8080/checkPower\"\n  -\n    name: BASIC_AUTH_ENABLED\n    value: \"false\"\n</code></pre> <p>Here, you are specifying that Siddhi applications running within the container should receive events to the <code>http://0.0.0.0:8080/checkPower</code> URL and basic authentication is not enabled for them.</p> </li> <li> <p>Add a <code>runner</code> section and add configurations related to authorization such as users and roles. For this example, you can configure this section as follows.</p> <pre><code>runner: |\nauth.configs:\n  type: 'local'        # Type of the IdP client used\n  userManager:\n    adminRole: admin   # Admin role which is granted all permissions\n    userStore:         # User store\n      users:\n      -\n        user:\n          username: root\n          password: YWRtaW4=\n          roles: 1\n      roles:\n      -\n        role:\n          id: 1\n          displayName: root\n  restAPIAuthConfigs:\n    exclude:\n      - /simulation/*\n      - /stores/*\n</code></pre> To view the complete file, click here.<pre><code>apiVersion: siddhi.io/v1alpha2\nkind: SiddhiProcess\nmetadata:\n  name: streaming-integrator-app\nspec:\n  apps:\n    - script: |\n        @App:name(\"PowerSurgeDetection\")\n        @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")\n        /*\n            Input: deviceType string and powerConsuption int(Watt)\n            Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.\n        */\n\n        @source(\n          type='http',\n          receiver.url='${RECEIVER_URL}',\n          basic.auth.enabled='false',\n          @map(type='json')\n        )\n        define stream DevicePowerStream(deviceType string, power int);\n        @sink(type='log', prefix='LOGGER')\n        define stream PowerSurgeAlertStream(deviceType string, power int);\n        @info(name='surge-detector')\n        from DevicePowerStream[deviceType == 'dryer' and power &gt;= 600]\n        select deviceType, power\n        insert into PowerSurgeAlertStream;\n  container:\n    env:\n      -\n        name: RECEIVER_URL\n        value: \"http://0.0.0.0:8080/checkPower\"\n      -\n        name: BASIC_AUTH_ENABLED\n        value: \"false\"\n\n  runner: |\n    auth.configs:\n      type: 'local'        # Type of the IdP client used\n      userManager:\n        adminRole: admin   # Admin role which is granted all permissions\n        userStore:         # User store\n          users:\n          -\n            user:\n              username: root\n              password: YWRtaW4=\n              roles: 1\n          roles:\n          -\n            role:\n              id: 1\n              displayName: root\n      restAPIAuthConfigs:\n        exclude:\n          - /simulation/*\n          - /stores/*\n</code></pre> </li> <li> <p>Save the file as <code>siddhi-process.yaml</code> in a preferred location</p> </li> <li> <p>To apply the configurations in this YAML file to the Kubernetes cluster, issue the following command.</p> <p><code>kubectl apply -f &lt;PATH_to_siddhi-process.yaml&gt;</code></p> <p>Info</p> <p>This file overrules the configurations in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> </li> </ol>"},{"location":"examples/running-si-with-docker-and-kubernetes/#invoking-the-siddhi-application","title":"Invoking the Siddhi application","text":"<p>To invoke the <code>PowerSurgeDetection</code> Siddhi application that you deployed in the Kubernetes cluster, follow the steps below.</p> <ol> <li> <p>First, get the external IP of minikube by issuing the following command.</p> <p><code>minikube ip</code></p> <p>Add the IP it returns to the <code>/etc/hosts</code> file in your machine.</p> </li> <li> <p>Issue the following CURL command to invoke the <code>PowerSurgeDetection</code> Siddhi application.</p> <pre><code>curl -X POST \\\n  http://siddhi/streaming-integrator-0/8080/checkPower \\\n    -H 'Accept: */*' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Host: siddhi' \\\n    -d '{\n        \"deviceType\": \"dryer\",\n        \"power\": 600\n        }'\n</code></pre> </li> <li> <p>To monitor the associated logs for the above siddhi application, get a list of the available pods by issuing the following command.</p> <p>`kubectl get pods'</p> <p>This returns the list of pods as shown in the example below.</p> <pre><code>NAME                                        READY    STATUS    RESTARTS    AGE\nstreaming-integrator-app-0-b4dcf85-npgj7     1/1     Running      0        165m\nstreaming-integrator-5f9fcb7679-n4zpj        1/1     Running      0        173m\n</code></pre> </li> <li> <p>To monitor the logs for the required pod, issue a command similar to the following. In this example, the pod to be monitored is <code>streaming-integrator-app-0-b4dcf85-npgj7</code>.</p> <p><code>streaming-integrator-app-0-b4dcf85-npgj7</code></p> </li> </ol>"},{"location":"examples/summarizing-and-aggregating-data/","title":"Summarizing and Aggregating Data","text":""},{"location":"examples/summarizing-and-aggregating-data/#short-term-summarization","title":"Short-term summarization","text":""},{"location":"examples/summarizing-and-aggregating-data/#time-based-summarization","title":"Time-based summarization","text":""},{"location":"examples/summarizing-and-aggregating-data/#length-based-summarization","title":"Length-based summarization","text":""},{"location":"examples/summarizing-and-aggregating-data/#long-term-summarization","title":"Long-term summarization","text":""},{"location":"examples/triggering-integrations-via-micro-integrator/","title":"Triggering Integration Flows via the Micro Integrator","text":""},{"location":"examples/triggering-integrations-via-micro-integrator/#introduction","title":"Introduction","text":"<p>In this tutorial, lets look at how the Streaming Integrator generates an alert based on the events received, and how that particular alert can trigger an integration flow in the Micro Integrator, and get a response back to the Streaming Integrator for further processing.</p> <p>To understand this, consider a scenario where the Streaming Integrator receives production data from a factory, and triggers an integration flow if it detects a per minute production average that exceeds 100.</p> <p>Before you begin:</p> <ul> <li>Start WSO2 Streaming Integrator server.</li> <li>Start Streaming Integrator Tooling. </li> <li>Install the <code>grpc</code> Siddhi extension in Streaming Integrator Tooling. To do this, access Streaming Integrator Tooling, click Tools -&gt; Extension Installer to open the Extension Installer dialog box, and then click Install for the gRPC extension. Restart Streaming Integrator Tooling for the installation to be effective. For detailed instructions, see Installing Siddhi Extensions.</li> <li>To install the <code>grpc</code> Siddhi extension in WSO2 Streaming Integrator, navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory and issue the appropriate command based on your operating system.<ul> <li>For Windows     : <code>extension-installer.bat install grpc</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh install grpc</code>    Then restart WSO2 Streaming Integrator for the installation to be effective. For detailed instructions to install a Siddhi extension, see Downloading and Installing Siddhi Extensions.</li> </ul> </li> </ul>"},{"location":"examples/triggering-integrations-via-micro-integrator/#configuring-the-streaming-integrator","title":"Configuring the Streaming Integrator","text":"<p>Let's design a Siddhi application that triggers an integration flow and deploy it by following the procedure below:</p> <ol> <li> <p>In Streaming Integrator Tooling, click New to open a new application.</p> </li> <li> <p>Add a name and a description for your new Siddhi application as follows:</p> <pre><code>@App:name(\"grpc-call-response\")\n@App:description(\"This application triggers integration process in the micro integrator using gRPC calls\")\n</code></pre> </li> <li> <p>Let's add an input stream to define the schema of input production events, and connect a source of the <code>http</code> type to to receive those events.</p> <pre><code>@source(type = 'http',\n        receiver.url='http://localhost:8006/InputStream',\n        basic.auth.enabled='false',\n        @map(type='json'))\ndefine stream InputStream(symbol string, amount double);\n</code></pre> <p>Here, the Streaming Integrator receives events to the <code>http://localhost:8006/InputStream</code> in the JSON format. Each event reports the product name (via the <code>symbol</code> attribute) and the amount produced.</p> </li> <li> <p>Now, let's add the configurations to publish an alert in the Micro Integrator to trigger an integration flow, and then receive a response back into the Streaming Integrator.</p> <pre><code>@sink(\n        type='grpc-call',\n        publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq',\n        sink.id= '1', headers='Content-Type:json',\n        @map(type='json', @payload(\"\"\"{\"symbol\":\"{{ symbol }}\",\"avgAmount\":{{ avgAmount }}}\"\"\"))\n    )\ndefine stream FooStream (symbol string, avgAmount double);\n\n@source(type='grpc-call-response', sink.id= '1', @map(type='json'))\ndefine stream BarStream (symbol string, avgAmount double);\n</code></pre> <p>Note the following in the above configuration:</p> <ul> <li> <p>Each output event that represents an alert that is published to the Micro Integrator reports the product name and the average production (as per the schema of the <code>FooStream</code> stream.</p> </li> <li> <p>The <code>grpc-call</code> sink connected to the <code>FooStream</code> stream gets the two attributes from the stream and generates the output events as JSON messages before they are published to the Micro Integrator.  The value for the <code>publisher.url</code> parameter in the sink configuration contains <code>process</code> and <code>inSeq</code> which means that the Streaming Integrator calls the process method of the gRPC Listener server in the Micro Integrator, and injects the message to the <code>inSeq</code> which then sends a response back to the client.</p> </li> <li> <p>The <code>grpc-call-response source</code> connected to the <code>BarStream</code> input stream retrieves a response from the Micro Integrator and publishes it as a JSON message in the Streaming Integrator. As specified via the schema of the <code>BarStream</code> input stream, this response comprises of a single JSON message.</p> </li> </ul> </li> <li> <p>To publish the messages received from the Micro Integrator as logs in the terminal, let's define an output stream named <code>LogStream</code>, and connect a sink of the <code>log</code> type to it as shown below.</p> <pre><code>@sink(type='log', prefix='response_from_mi: ')\ndefine stream LogStream (symbol string, avgAmount double);\n</code></pre> </li> <li> <p>Let's define Siddhi queries to calculate the average production per minute, filter production runs where the average production per minute is greater than 100, and direct the logs to be published to the output stream.</p> <p>a. To calculate the average per minute, add a Siddi query named <code>CalculateAverageProductionPerMinute</code> as follows:</p> <pre><code>```\n@info(name = 'CalculateAverageProductionPerMinute')\nfrom InputStream#window.timeBatch(1 min)\nselect avg(amount) as avgAmount, symbol\ngroup by symbol\ninsert into AVGStream;\n```</code></pre> <p>This query applies a time batch window to the <code>InputStream</code> stream so that events within each minute is considered a separate subset to be calculations in the query are applied. The minutes are considered in a tumbling manner because it is a batch window. Then the <code>avg()</code> function is applied to the <code>amount</code> attribute of the input stream to derive the average production amount. The results are then inserted into an inferred stream named <code>AVGStream</code>.</p> <p>b. To filter events from the <code>AVGStream</code> stream where the average production is greater then 100, add a query named <code>FilterExcessProduction</code> as follows.</p> <pre><code>```\n@info(name = 'FilterExcessProduction')\nfrom AVGStream[avgAmount &gt; 100]\nselect symbol, avgAmount\ninsert into FooStream;\n```</code></pre> <p>Here, the <code>avgAmount &gt; 100</code> filter is applied to filter only events that report an average production amount greater than 100. The filtered events are inserted into the <code>FooStream</code> stream.</p> <p>c. To select all the responses from the Micro Integrator to be logged, add a new query named <code>LogResponseEvents</code>.</p> <pre><code>```\n@info(name = 'LogResponseEvents')\nfrom BarStream\nselect *\ninsert into LogStream;\n```</code></pre> <p>The responses received from the Micro Integrator are directed to the <code>BarStream</code> input stream. This query gets them all these events from the <code>BarStream</code> stream and inserts them into the <code>LogStream</code> stream that is connected to a <code>log</code> stream so that they can be published as logs in the terminal.</p> <p>The Siddhi application is now complete.</p> Click here to view the complete Siddhi application.<pre><code>@App:name(\"grpc-call-response\")\n@App:description(\"This application triggers integration process in the micro integrator using gRPC calls\")\n\n@source(type = 'http',\n            receiver.url='http://localhost:8006/InputStream',\n            basic.auth.enabled='false',\n            @map(type='json'))\ndefine stream InputStream(symbol string, amount double);\n\n@sink(\n    type='grpc-call',\n    publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq',\n    sink.id= '1', headers='Content-Type:json',\n    @map(type='json', @payload(\"\"\"{\"symbol\":\"{{ symbol }}\",\"avgAmount\":{{ avgAmount }}}\"\"\"))\n)\n@sink(type='log')\ndefine stream FooStream (symbol string, avgAmount double);\n\n@source(type='grpc-call-response', sink.id= '1', @map(type='json'))\ndefine stream BarStream (symbol string, avgAmount double);\n\n@sink(type='log', prefix='response_from_mi: ')\ndefine stream LogStream (symbol string, avgAmount double);\n\n@info(name = 'CalculateAverageProductionPerMinute')\nfrom InputStream#window.timeBatch(5 sec)\nselect avg(amount) as avgAmount, symbol\ngroup by symbol\ninsert into AVGStream;\n\n@info(name = 'FilterExcessProduction')\nfrom AVGStream[avgAmount &gt; 100]\nselect symbol, avgAmount\ninsert into FooStream;\n\n@info(name = 'LogResponseEvents')\nfrom BarStream\nselect *\ninsert into LogStream;\n</code></pre> </li> <li> <p>Save the Siddhi application. As a result, it is saved in the <code>&lt;SI_TOOLING_HOME&gt;/wso2/server/deployment/workspace</code> directory.</p> </li> <li> <p>Click the Deploy menu option and then click Deploy to Server. The Deploy Siddhi Apps to Server dialog box opens as shown in the example below.</p> <p></p> <ol> <li> <p>In the Add New Server section, enter information as follows:</p> Field Value Host Your host Port <code>9443</code> User Name <code>admin</code> Password <code>admin</code> <p></p> <p>Then click Add.</p> </li> <li> <p>Select the check boxes for the grpc-call-response.siddhi Siddhi application and the server you added as shown below.</p> <p></p> </li> <li> <p>Click Deploy.</p> <p>When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box.</p> <p></p> </li> </ol> <p>As a result, the <code>grpc-call-response.siddhi</code> Siddhi application is saved in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> </li> </ol>"},{"location":"examples/triggering-integrations-via-micro-integrator/#configuring-micro-integrator","title":"Configuring Micro integrator","text":"<p>After doing the required configurations in the Streaming Integrator, let's configure the Micro Integrator to receive the excess production alert from the Streaming Integrator as a gRPC event and send back a response.</p> <ol> <li> <p>Start the gRPC server in the Micro Integrator server to receive the Streaming Integrator event. To do this, save the following inbound endpoint configuration as <code>GrpcInboundEndpoint.xml</code> in the <code>&lt;MI_Home&gt;/repository/deployment/server/synapse-configs/default/inbound-endpoints</code> directory.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\" name=\"GrpcInboundEndpoint\" sequence=\"inSeq\" onError=\"fault\" protocol=\"grpc\" suspend=\"false\"&gt;\n&lt;parameters&gt;\n&lt;parameter name=\"inbound.grpc.port\"&gt;8888&lt;/parameter&gt;\n&lt;/parameters&gt;\n&lt;/inboundEndpoint&gt;\n</code></pre> <p>This configuration has a configuration parameter to start the gRPC server, and specifies the default sequence to inject messages accordingly.</p> <p>Info</p> <p>Currently, WSO2 Integration Studio does not support GRPC Inbound Endpoint. This capability will be available in a future release.  For now, you need to create the inbound endpoint manually as an XML file.</p> </li> <li> <p>Deploy the following sequence by saving it as <code>inSeq.xml</code> file in the <code>&lt;MI_Home&gt;/repository/deployment/server/synapse-configs/default/sequences</code> directory.</p> <p>Info</p> <p>Note that the name of the sequence is <code>inSeq</code>. This is referred to in the <code>gRPC</code> sink configuration in the <code>grpc-call-response</code> Siddhi application you previously created in the Streaming Integrator.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"&gt;\n&lt;log level=\"full\"/&gt;\n&lt;respond/&gt;\n&lt;/sequence&gt;\n</code></pre> </li> </ol> <p>This sequence does the following:</p> <ul> <li> <p>Calls the REST endpoint that returns a JSON object.</p> </li> <li> <p>Logs the response.</p> </li> <li> <p>Sends the response back to the gRPC client.</p> </li> <li> <p>Start the Micro Integrator by issuing the appropriate command out of the following, depending on your operating system.</p> <ul> <li>For Linux/MacOS: <code>./micro-integrator.sh</code></li> <li>For Windows: <code>micro-integrator.bat --run</code></li> </ul> </li> </ul>"},{"location":"examples/triggering-integrations-via-micro-integrator/#executing-and-getting-results","title":"Executing and getting results","text":"<p>To send an event to the defines <code>http</code> source hosted in <code>http://localhost:8006/InputStream</code>, issue the following sample CURL command.</p> <p><code>curl -X POST -d \"{\\\"event\\\":{\\\"symbol\\\":\\\"soap\\\",\\\"amount\\\":110.23}}\" http://localhost:8006/InputStream --header \"Content-Type:application/json\"</code></p> <p>In the SI console an output similar to following will be printed after 1 minute (if the average of the amount is larger than 100)</p> <p><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - response_from_mi:  : Event{timestamp=1573711436547, data=[soap, 110.23], isExpired=false}</code></p>"},{"location":"examples/tutorials-overview/","title":"Tutorials Overview","text":"<p>The following table lists the tutorials available for different user scenarios.</p> User Scenario Tutorials Performing ETL - Performing Real-time ETL with Files - Performing Real-time ETL with MySQL - Creating an ETL Application via SI Tooling Receiving and Publishing Data Working with Kafka Working with API Exposing Processed Data as API Working with the Micro Integrator Triggering Integration Flows via MI Working in Containerized Environments Running the Streaming Integrator in Containerized Environments Handling Errors Managing Streaming Data with Errors"},{"location":"examples/working-with-kafka/","title":"Working with Kafka","text":""},{"location":"examples/working-with-kafka/#introduction","title":"Introduction","text":"<p>The Streaming Integrator can consume from a Kafka topic as well as to publish to a Kafka topic in a streaming manner.</p> <p>This tutorial takes you through consuming from a Kafka topic, processing the messages, and finally, publishing output to a Kafka topic.</p> <p>Before you begin:</p> <p>Prepare the server to consume from or to publish to Kafka, follow the steps below:      1. Download the Kafka broker from the Apache site and extract it.     From here onwards, this directory is referred to as <code>&lt;KAFKA_HOME&gt;</code>.       2. Create a directory named <code>Source</code> in a preferred location in your machine and copy the following JARs to it from the <code>&lt;KAFKA_HOME&gt;/libs</code> directory.          - <code>kafka_2.12-2.3.0.jar</code>          - <code>kafka-clients-2.3.0.jar</code>          - <code>metrics-core-2.2.0.jar</code>          - <code>scala-library-2.12.8.jar</code>          - <code>zkclient-0.11.jar</code>          - <code>zookeeper-3.4.14.jar</code>      3. Create another directory named <code>Destination</code> in a preferred location in your machine.      4. To convert the Kafka JARS you copied to the <code>Source</code> directory, issue the following command: <code>sh &lt;SI_HOME&gt;/bin/jartobundle.sh &lt;{Source}_Directory_Path&gt; &lt;{Destination}_Directory_Path&gt;</code>      5. Copy all the jars from the <code>Destination</code> directory to the <code>&lt;SI_HOME&gt;/lib</code> directory.</p>"},{"location":"examples/working-with-kafka/#tutorial-steps","title":"Tutorial steps","text":""},{"location":"examples/working-with-kafka/#consuming-data-from-kafka","title":"Consuming data from Kafka","text":""},{"location":"examples/working-with-kafka/#start-kafka","title":"Start Kafka","text":"<ol> <li> <p>Navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and start a zookeeper node by issuing the following command.</p> <p><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties</code></p> </li> <li> <p>Navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and start Kafka server node by issuing the following command.</p> <p><code>sh bin/kafka-server-start.sh config/server.properties</code></p> </li> </ol>"},{"location":"examples/working-with-kafka/#start-the-streaming-integrator","title":"Start the Streaming Integrator","text":"<p>Navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory and issue the following command.  <code>sh server.sh</code></p> <p>The following log appears on the SI console when the server is started successfully.</p> <pre><code>INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 4.240 sec\n</code></pre>"},{"location":"examples/working-with-kafka/#consume-from-a-kafka-topic","title":"Consume from a Kafka topic","text":"<p>Let's create a basic Siddhi application to consume messages from a Kafka topic.</p> <ol> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name(\"HelloKafka\")\n\n@App:description('Consume events from a Kafka Topic and log the messages on the console.')\n\n@source(type='kafka',\n        topic.list='productions',\n        threading.option='single.thread',\n        group.id=\"group1\",\n        bootstrap.servers='localhost:9092',\n        @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream OutputStream (name string, amount double);\n\n-- Query to transform the name to upper case.\nfrom SweetProductionStream\nselect str:upper(name) as name, amount\ninsert into OutputStream;\n</code></pre> </li> <li> <p>Save this file as <code>HelloKafka.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>The following log appears on the SI console.</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App HelloKafka deployed successfully\n</code></pre> </li> </ol> <p>Info</p> <p>You just created a Siddhi application that listens to a Kafka topic named <code>productions</code> and logs any incoming messages. When logging, the name attribute of the message is converted to upper case. However, you have still not created this Kafka topic or published any messages to it. To do this, proceed to the next section.</p>"},{"location":"examples/working-with-kafka/#generate-kafka-messages","title":"Generate Kafka messages","text":"<p>Now let's generate some Kafka messages that the Streaming Integrator can receive. </p> <ol> <li> <p>First, let's create a topic named <code>productions</code> in the Kafka server. To do this, navigate to <code>&lt;KAFKA_HOME&gt;</code> and run following command:</p> <pre><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic productions\n</code></pre> </li> <li> <p>Now let's run the Kafka command line client to push a few messages to the Kafka server.</p> <pre><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic productions\n</code></pre> </li> <li> <p>Now you are prompted to type messages in the console. Type the following in the command prompt:</p> <pre><code>{\"event\":{ \"name\":\"Almond cookie\", \"amount\":100.0}}\n</code></pre> </li> </ol> <p>This pushes a message to the Kafka Server. Then, the Siddhi application you deployed in the Streaming Integrator consumes this message. As a result, the Streaming Integrator log displays the following:</p> <pre><code>```\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562069868006, data=[ALMOND COOKIE, 100.0], isExpired=false}\n```</code></pre> <p>You may notice that the output message has an uppercase name: <code>ALMOND COOKIE</code>. This is because of the simple message transformation done in the Siddhi application.   </p>"},{"location":"examples/working-with-kafka/#consuming-with-an-offset","title":"Consuming with an offset","text":"<p>Previously, you consumed messages from the <code>productions</code> topic without specifying an offset. In other words, the Kafka offset was zero. In this section, instead of consuming with a zero offset, you specify an offset value and consume messages from that offset onwards.</p> <p>For this purpose, you can configure the <code>topic.offsets.map</code> parameter. Let's modify our previous Siddhi application to specify an offset value. Specify an offset value <code>2</code> so that the Siddhi application consumes messages with index <code>2</code> and above. </p> <ol> <li> <p>Open the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files/HelloKafka.siddhi</code> file and add the following new configuration parameter.</p> <pre><code>topic.offsets.map='productions=2'\n</code></pre> <p>Now the complete Siddhi application is as follows.</p> <pre><code>@App:name(\"HelloKafka\")\n\n@App:description('Consume events from a Kafka Topic and log the messages on the console.')\n\n@source(type='kafka',\n        topic.list='productions',\n        threading.option='single.thread',\n        group.id=\"group1\",\n        bootstrap.servers='localhost:9092',\n        topic.offsets.map='productions=2',\n        @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream OutputStream (name string, amount double);\n\nfrom SweetProductionStream\nselect str:upper(name) as name, amount\ninsert into OutputStream;\n</code></pre> </li> <li> <p>Save the file.</p> </li> <li> <p>Push the following message to the Kafka server.</p> <pre><code>{\"event\":{ \"name\":\"Baked alaska\", \"amount\":20.0}}\n</code></pre> <p>Note that this is the second message that you pushed (hence bearing index 1), and therefore it is not consumed by the Streaming Integrator.</p> </li> <li> <p>Let's push another message (bearing index 2) to the Kafka server.</p> <pre><code>{\"event\":{ \"name\":\"Cup cake\", \"amount\":300.0}}\n</code></pre> </li> </ol> <p>Now you can see the following log in the Streaming Integrator Studio console.</p> <pre><code>```\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562676477785, data=[CUP CAKE, 300.0], isExpired=false}\n```</code></pre> <p>As you configured your Siddhi application to consume messages with offset <code>2</code>, all messages bearing index <code>2</code> or above are consumed.</p>"},{"location":"examples/working-with-kafka/#adding-more-consumers-to-the-consumer-group","title":"Adding more consumers to the consumer group","text":"<p>In our <code>HelloKafka</code> Siddhi application, note the <code>group.id</code> parameter. This parameter defines the Kafka consumer group. </p> <p>Let's add another Siddhi application <code>HelloKafka_2</code>, to add another Kafka consumer to the same consumer group.</p> <ol> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>@App:name(\"HelloKafka_2\")\n\n@App:description('Consume events from a Kafka Topic and log the messages on the console.')\n\n@source(type='kafka',\n        topic.list='productions',\n        threading.option='single.thread',\n        group.id=\"group1\",\n        bootstrap.servers='localhost:9092',\n        @map(type='json'))        \ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream OutputStream (name string, amount double);\n\nfrom SweetProductionStream\nselect str:upper(name) as name, amount   \ninsert into OutputStream;\n</code></pre> </li> <li> <p>Save this file as <code>HelloKafka_2.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory. When the Siddhi application is successfully deployed, the following <code>INFO</code> log appears in the Streaming Integrator console.</p> <pre><code>INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App HelloKafka_2 deployed successfully\n</code></pre> </li> <li> <p>Navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and run following command.</p> <pre><code>bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 2 --topic productions\n</code></pre> </li> </ol> <p>This adds another partition to the <code>productions</code> Kafka topic. </p> <ol> <li> <p>Push following messages to the Kafka server using the Kafka Console Producer.</p> <pre><code>{\"event\":{ \"name\":\"Doughnut\", \"amount\":500.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Danish pastry\", \"amount\":200.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Eclair\", \"amount\":400.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Eclair toffee\", \"amount\":100.0}} \n</code></pre> </li> </ol> <p>Now observe the logs on the SI console.     <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka_2 : OutputStream : Event{timestamp=1562759480019, data=[DOUGHNUT, 500.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562759494710, data=[DANISH PASTRY, 200.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka_2 : OutputStream : Event{timestamp=1562759506252, data=[ECLAIR, 400.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562759508757, data=[ECLAIR TOFFEE, 100.0], isExpired=false}\n</code></pre></p> <p>You can see that the events are being received by the two consumers in a Round Robin manner. Events received by the first consumer are logged by Siddhi application <code>HelloKafka</code>, whilst events received by the second consumer are logged by the <code>HelloKafka_2</code> Siddhi application.</p>"},{"location":"examples/working-with-kafka/#assigning-consumers-to-partitions","title":"Assigning consumers to partitions","text":"<p>In the previous scenario, you had two partitions for the Kafka topic and two consumers. Instead of assigning the consumers to the partitions, you allowed Kafka do the assignments. Optionally, you can assign consumers to partitions.</p> <p>This option is useful if you have multiple consumers with different performance speeds, and you need to balance the load among the consumers.</p> <p>Let's alter your topic to have three partitions. After that, you can assign two partitions to <code>consumer-1</code>, and the remaining partition to <code>consumer-2</code>.</p> <ol> <li> <p>Navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue following command.</p> <pre><code>bin/kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 3 --topic productions\n</code></pre> </li> </ol> <p>This adds another partition to the <code>productions</code> Kafka topic. Now there are three partitions in total. </p> <ol> <li> <p>To assign partitions to the consumers, add the <code>partition.no.list</code> parameter as shown below.</p> <pre><code>@App:name(\"HelloKafka\")\n\n@App:description('Consume events from a Kafka Topic and log the messages on the console.')\n\n-- consumer-1\n@source(type='kafka',\n        topic.list='productions',\n        threading.option='single.thread',\n        group.id=\"group1\",\n        bootstrap.servers='localhost:9092',\n        partition.no.list='0,1',\n        @map(type='json'))\ndefine stream SweetProductionStream1 (name string, amount double);\n\n-- consumer-2\n@source(type='kafka',\n        topic.list='productions',\n        threading.option='single.thread',\n        group.id=\"group1\",\n        bootstrap.servers='localhost:9092',\n        partition.no.list='2',\n        @map(type='json'))\ndefine stream SweetProductionStream2 (name string, amount double);\n\n@sink(type='log')\ndefine stream OutputStream (name string, amount double, id string);\n\nfrom SweetProductionStream1\nselect str:upper(name) as name, amount, 'consumer-1' as id\ninsert into OutputStream;\n\nfrom SweetProductionStream2\nselect str:upper(name) as name, amount, 'consumer-2' as id\ninsert into OutputStream;\n</code></pre> <p>Note that <code>consumer-1</code> is assigned partitions <code>0</code> and <code>1</code>, while <code>consumer-2</code> is assigned partition <code>2</code>. </p> </li> <li> <p>Now let's publish some messages as follows, and see how the load is distributed among the consumers with the new partition assignments.</p> <pre><code>{\"event\":{ \"name\":\"Fortune cookie\", \"amount\":100.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Frozen yogurt\", \"amount\":350.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Gingerbread\", \"amount\":450.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Hot-fudge sundae\", \"amount\":150.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Hot-chocolate pudding\", \"amount\":200.0}} \n</code></pre> <pre><code>{\"event\":{ \"name\":\"Ice cream cake\", \"amount\":250.0}} \n</code></pre> </li> <li> <p>Now observe the Streaming Integrator logs. The following is displayed.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851086792, data=[FORTUNE COOKIE, 100.0, consumer-1], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851092100, data=[FROZEN YOGURT, 350.0, consumer-1], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851094459, data=[GINGERBREAD, 450.0, consumer-2], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851096434, data=[HOT-FUDGE SUNDAE, 150.0, consumer-1], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851098328, data=[HOT-CHOCOLATE PUDDING, 200.0, consumer-1], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - HelloKafka : OutputStream : Event{timestamp=1562851100309, data=[ICE CREAM CAKE, 250.0, consumer-2], isExpired=false}\n</code></pre> </li> </ol> <p>You can observe a pattern where the load is distributed among <code>consumer-1</code> and <code>consumer-2</code> in the 2:1 ratio. This is because you assigned two partitions to <code>consumer-1</code> and assigned only one partition to <code>consumer-2</code>.</p>"},{"location":"examples/working-with-kafka/#publish-to-a-kafka-topic","title":"Publish to a Kafka topic","text":"<p>Now let's create a new Siddhi application to consume from the <code>productions</code> topic, filter the incoming messages based on a condition, and then publish those filtered messages to another Kafka topic.</p> <ol> <li> <p>First, let's create a new topic named <code>bulk-orders</code> in the Kafka server.</p> </li> <li> <p>To publish the filtered messages to the <code>bulk-orders</code> Kafka topic you created, issue the following command.</p> <pre><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic bulk-orders\n</code></pre> </li> <li> <p>Next, let's create the Siddhi application. Open a text file, and copy-paste following Siddhi application into it.</p> <pre><code>    @App:name(\"PublishToKafka\")\n\n    @App:description('Consume events from a Kafka Topic, do basic filtering and publish filtered messages to a Kafka topic.')\n\n    @source(type='kafka',\n            topic.list='productions',\n            threading.option='single.thread',\n            group.id=\"group2\",\n            bootstrap.servers='localhost:9092',\n            @map(type='json'))\n    define stream SweetProductionStream (name string, amount double);\n\n    @sink(type='kafka',\n          topic='bulk-orders',\n          bootstrap.servers='localhost:9092',\n          partition.no='0',\n          @map(type='json'))\n    define stream BulkOrdersStream (name string, amount double);\n\n    from SweetProductionStream[amount &gt; 50]\n    select *\n    insert into BulkOrdersStream;\n</code></pre> </li> <li> <p>Save this file as <code>PublishToKafka.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory. When the     Siddhi application is successfully deployed, the following <code>INFO</code> log appears in the Streaming Integrator console.</p> <pre><code>INFO {org.wso2.carbon.streaming.integrator.core.internal.StreamProcessorService} - Siddhi App PublishToKafka deployed successfully\n</code></pre> <p>Info</p> <p>The <code>PublishToKafka</code> Siddhi application consumes all the messages from the <code>productions</code> topic and populates the <code>SweetProductionStream</code> stream. All the sweet production runs where the amount is greater than 100 are inserted into the <code>BulkOrdersStream</code> stream. These events are pushed to the <code>bulk-orders</code> Kafka topic.</p> </li> <li> <p>To observe the messages in the <code>bulk-orders</code> topic, run a Kafka Console Consumer. Then navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command.     <pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic bulk-orders --from-beginning\n</code></pre>    You can see the following message in the Kafka Consumer log. These indicate the production runs of which the amount is greater than 50.</p> <pre><code>{\"event\":{ \"name\":\"Almond cookie\", \"amount\":100.0}}\n</code></pre> </li> </ol>"},{"location":"examples/working-with-kafka/#preserving-the-state-of-the-application-through-a-system-failure","title":"Preserving the state of the application through a system failure","text":"<p>Let's try out a scenario in which you deploy a Siddhi application to count the total number of productions.</p> <p>Info</p> <p>In this scenario, the SI server is required to remember the current count through system failures so that when the system is restored, the count is not reset to zero.</p> <p>To achieve this, you can use the state persistence capability in the Streaming Integrator.</p> <ol> <li> <p>Enable state persistence feature in SI server as follows. Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file on a text editor and locate the <code>state.persistence</code> section.  </p> <pre><code>  # Periodic Persistence Configuration\nstate.persistence:\n  enabled: true\n  intervalInMin: 1\n  revisionsToKeep: 2\n  persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore\n  config:\n    location: siddhi-app-persistence\n</code></pre> <p>Set <code>enabled</code> parameter to <code>true</code> and save the file. </p> </li> <li> <p>Enable state persistence debug logs as follows. Open the <code>&lt;SI_HOME&gt;/conf/server/log4j2.xml</code> file on a text editor and locate following line in it.</p> <pre><code> &lt;Logger name=\"com.zaxxer.hikari\" level=\"error\"/&gt;\n</code></pre> <p>Add following <code>&lt;Logger&gt;</code> element below that.</p> <pre><code>&lt;Logger name=\"org.wso2.carbon.streaming.integrator.core.persistence\" level=\"debug\"/&gt;\n</code></pre> <p>Save the file.</p> </li> <li> <p>Restart the Streaming Integrator server for above change to be effective.</p> </li> <li> <p>Let's create a new topic named <code>sandwich_productions</code> in the Kafka server. To do this, navigate to <code>&lt;KAFKA_HOME&gt;</code> and run following command:</p> <pre><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic sandwich_productions\n</code></pre> </li> <li> <p>Open a text file and copy-paste following Siddhi application to it.</p> <pre><code>    @App:name(\"CountProductions\")\n\n    @App:description('Siddhi application to count the total number of orders.')\n\n    @source(type='kafka',\n            topic.list='sandwich_productions',\n            threading.option='single.thread',\n            group.id=\"group3\",\n            bootstrap.servers='localhost:9092',\n            partition.no.list='0',\n            @map(type='json'))\n    define stream SandwichProductionStream (name string, amount double);\n\n    @sink(type='log')\n    define stream OutputStream (totalProductions double);\n\n    from SandwichProductionStream\n    select sum(amount) as totalProductions\n    insert into OutputStream;\n</code></pre> </li> <li> <p>Save this file as <code>CountProductions.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory. When the Siddhi application is successfully deployed, the following <code>INFO</code> log appears in the Streaming Integrator console.</p> <pre><code>INFO {org.wso2.carbon.stream.processor.core.internal.StreamProcessorService} - Siddhi App CountProductions deployed successfully\n</code></pre> </li> <li> <p>Now let's run the Kafka command line client to push a few messages to the Kafka server. Navigate to <code>&lt;KAFKA_HOME&gt;</code> and run following command:</p> <pre><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sandwich_productions\n</code></pre> </li> <li> <p>Now you are prompted to type the messages in the console. Type following in the command prompt:</p> <pre><code>{\"event\":{ \"name\":\"Bagel\", \"amount\":100.0}}\n</code></pre> <pre><code>{\"event\":{ \"name\":\"Buterbrod\", \"amount\":100.0}} \n</code></pre> <p>Now the following logs appear on the SI console.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563903034768, data=[100.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563903034768, data=[200.0], isExpired=false}\n</code></pre> <p>These logs print the sandwich production count. Note that the current count of sandwich productions is being printed as <code>200</code> in the second log. This is because the production count up to now is <code>200</code> sandwiches: <code>100</code> bagels and <code>100</code> buterbrods.</p> </li> <li> <p>Now wait for following log to appear on the SI console     <pre><code>DEBUG {org.wso2.carbon.streaming.integrator.core.persistence.FileSystemPersistenceStore} - Periodic persistence of CountProductions persisted successfully\n</code></pre></p> <p>This log indicates that the current state of the Siddhi application is successfully persisted. Siddhi application state is persisted every minute. Therefore, you can notice this log appearing every minute.</p> <p>Next, let's push two sandwich production messages to the Kafka server and shutdown the SI server before state persistence happens (i.e., before the above log appears).</p> <p>Tip</p> <p>It is better to start pushing messages immediately after the state persistence log appears, so that you have plenty of time to push messages and shutdown the server, until next log appears.</p> </li> <li> <p>Now push following messages to the Kafka server using the Kafka Console Producer:</p> <pre><code>{\"event\":{ \"name\":\"Croissant\", \"amount\":100.0}}\n</code></pre> <pre><code>{\"event\":{ \"name\":\"Croutons\", \"amount\":100.0}} \n</code></pre> </li> <li> <p>Shutdown SI server. Here you are deliberately creating a scenario where the server crashes before the SI server could persist the latest production count.</p> <p>Info</p> <p>Here the SI server crashes before the state is persisted. Therefore the SI server cannot persist the latest count (which should include the last two productions <code>100</code> Croissants and <code>100</code> Croutons). The good news is, the Kafka source replays the last two messages, thereby allowing the Streaming Integrator to successfully recover from the server crash.</p> </li> <li> <p>Restart the SI server and wait for about one minute to observe the following logs.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563904912073, data=[300.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - CountProductions : OutputStream : Event{timestamp=1563904912076, data=[400.0], isExpired=false}\n</code></pre> </li> </ol> <p>Note that the Kafka source has replayed the last two messages. As a result, the sandwich productions count is correctly restored.</p>"},{"location":"guides/cleansing-data/","title":"Cleansing Data","text":"<p>When you receive input data via the Streaming Integrator, it may consist of data that is not required to generate the required output, null values for certain attributes, etc. Cleansing data refers to refining the input data received by assigning values where there are missing values (if there are applicable values), filtering out the data that is not required, etc.</p>"},{"location":"guides/cleansing-data/#filtering-data-based-on-conditions","title":"Filtering data based on conditions","text":"<p>You can filter data based on the exact match of attribute, based on a regex pattern or based on multiple criteria</p> <p>To understand this, consider a scenario where you receive the temperature of multiple rooms in a streaming manner. </p> <ul> <li> <p>Filtering based on exact match of attribute:</p> <p>If you want the temperature readings only for a specific room, you can add a query with a filter as follows.</p> <p><pre><code>from TempStream [roomNo=='2233']\nselect *\ninsert into RoomAnalysisStream;\n</code></pre> With the <code>roomNo=='2233'</code> filter, you are filtering the temperature readings for the room number <code>2233</code>. These readings are then inserted into a separate stream named <code>RoomAnalysisStream</code>.</p> </li> <li> <p>Filtering based on regex pattern</p> <p>In the example of processing temperature readings, assume that you need to filter the temperature readings for a specific range of devices located in the Southern wing and used for purpose B. Also assume that this can be derived from the device ID because the first three characters of the device ID represent the wing, and the eighth character represents the purpose. e.g., in device ID <code>SOU5438B765</code>, the first three characters <code>SOU</code> represent the Southern wing, and the eighth character <code>B</code> represents purpose B.</p> <p>You can achieve this by adding a filter with a regex pattern as follows:</p> <p><pre><code>@info(name = 'FilteredRoomRange')\nfrom TempStream\nselect regex.find(SOU*B*) as deviceID, roomNo, temp\ninsert into FilteredResultsStream;\n</code></pre>    The <code>regex.find(SOU*B*)</code> function filters room IDs that start with the three characters <code>SOU</code> and has the character <code>B</code> with one or more characters before it as well as after it. </p> </li> <li> <p>Filtering based on multiple criteria</p> <p>In the example of processing temperature readings, assume that you need to filter the readings for a range of rooms (e.g., rooms 100-210) where the temperature is greater than 40. For this, you can add a filter as follows.</p> <pre><code>from TempStream [(roomNo &gt;= 100 and roomNo &lt; 210) and temp &gt; 40]\nselect *\ninsert into RoomAnalysisStream;\n</code></pre> </li> </ul>"},{"location":"guides/cleansing-data/#try-it-out","title":"Try it out","text":"<p>To try out the query used in the above example, let's include it in a Siddhi Application and run it.</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file. Then add and save the following Siddhi application.</p> <p><pre><code>@App:name('TemperatureApp')\n\ndefine stream TempStream (deviceID string, roomNo int, temp double);\n\n@sink(type = 'log', prefix = \"FilteredResult\",\n    @map(type = 'passThrough'))\ndefine stream RoomAnalysisStream (deviceID string, roomNo int, temp double);\n\n@info(name = 'Filtering2233')\nfrom TempStream[roomNo == '2233'] \nselect * \ninsert into RoomAnalysisStream;\n</code></pre> 3. Open the event simulator and simulate three events for the <code>TempStream</code> input stream of the <code>TemperatureApp</code> Siddhi application with the values for the attributes as given below. For instructions to simulate single events, see Testing Siddhi Applications - Simulating a single event.</p> Event deviceID roomNo temp 1 <code>SOU5438B765</code> <code>2233</code> <code>30</code> 2 <code>WES1827A876</code> <code>1121</code> <code>27</code> 3 <code>NOR1633B231</code> <code>0899</code> <code>28</code> <p>Only the first event is logged in the terminal as follows. This is because only the first event has <code>2233</code> as the value for the <code>roomNo</code> attribute.</p> <p><pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - FilteredResult : Event{timestamp=1604494352744, data=[SOU5438B765, 2233, 30.0], isExpired=false} \n</code></pre> 4. Now remove the <code>Filtering2233</code> query and replace it with the following query that filters based on multiple criteria.</p> <p><pre><code>from TempStream [(roomNo &gt;= 100 and roomNo &lt; 210) and temp &gt; 40]\nselect *\ninsert into RoomAnalysisStream;\n</code></pre>    The complete Siddhi application is as follows:</p> </li> </ol> <pre><code> @App:name('TemperatureApp')\n\n define stream TempStream (deviceID string, roomNo int, temp double);\n\n @sink(type = 'log', prefix = \"FilteredResult\",\n     @map(type = 'passThrough'))\n define stream RoomAnalysisStream (deviceID string, roomNo int, temp double);\n\n from TempStream [(roomNo &gt;= 100 and roomNo &lt; 210) and temp &gt; 40]\n select *\n insert into RoomAnalysisStream;\n</code></pre> <ol> <li> <p>Open the event simulator and simulate three events for the <code>TempStream</code> input stream of the <code>TemperatureApp</code> Siddhi application with the values for the attributes as given below. For instructions to simulate single events, see Testing Siddhi Applications - Simulating a single event.</p> Event deviceID roomNo temp 1 <code>SOU5438B765</code> <code>183</code> <code>30</code> 2 <code>WES1827A876</code> <code>136</code> <code>42</code> 3 <code>NOR1633B231</code> <code>899</code> <code>41</code> <p>Only the second event is logged because only that event matched both the criteria. The event is logged as follows:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - FilteredResult : Event{timestamp=1604557083556, data=[WES1827A876, 136, 42.0], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"guides/cleansing-data/#modifying-removing-and-replacing-attributes","title":"Modifying, removing, and replacing attributes","text":"<p>The input data may include attributes that are not required in order to generate the required output, attributes with values that need to be updated or replaced before further processing.</p> <p>Assume that in the previous example, you do not need the device ID for further processing, and you need to present the room numbers as string values instead of integer values. To do this, follow the procedure below:</p> <p><pre><code>@info(name = 'CleaningData')\nfrom FilteredResultsStream\nselect cast(roomNo string) as roomNo, temp\ninsert into CleansedDataStream;\n</code></pre> Here, the <code>cast()</code> function presents the value for the <code>roomNo</code> attribute as a string value although it is received as an integer value. The <code>select</code> clause excludes the <code>deviceID</code> attribute.</p>"},{"location":"guides/cleansing-data/#try-it-out_1","title":"Try it out","text":"<p>To try out the above example, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file. Then add and save the following Siddhi application.</p> <p><pre><code>@App:name('TemperatureApp')\n\ndefine stream TempStream (deviceID string, roomNo int, temp double);\n\n@sink(type = 'log', prefix = \"CleanedData\",\n    @map(type = 'passThrough'))\ndefine stream CleansedDataStream (roomNo string, temp double);\n\n@info(name = 'CleaningData')\nfrom TempStream\nselect cast(roomNo, \"string\") as roomNo, temp\ninsert into CleansedDataStream;\n</code></pre>    In this Siddhi application, the <code>Temp Stream</code> has an attribute named <code>deviceID</code>, but it is not selected to be included in the output events. The <code>roomNo</code>attribute is cast as an string value via <code>cast(roomNo, \"string\")</code>. This means although the value for this attribute is received as an integer, it is presented as a string value in the output.</p> </li> <li> <p>Open the event simulator and simulate an event for the <code>TempStream</code> input stream of the <code>TemperatureApp</code> Siddhi application with the values for the attributes as given below. For instructions to simulate single events, see Testing Siddhi Applications - Simulating a single event.</p> deviceID roomNo temp <code>SOU5438B765</code> <code>183</code> <code>30</code> <p>The output is logged as follows:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - CleanedData : Event{timestamp=1604578130314, data=[183, 30.0], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"guides/cleansing-data/#handling-attributes-with-null-values","title":"Handling attributes with null values","text":"<p>In the example of processing temperature readings, assume that some events arrive with null values for the <code>deviceID</code> attribute, and you want to assign the value <code>unknown</code> in such scenarios. This can be achieved by writing a query as follows:</p> <pre><code>@info(name = 'AddingMissingValues')\nfrom FilteredResultsStream\nselect ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp\ninsert into CleansedDataStream\n</code></pre>"},{"location":"guides/cleansing-data/#try-it-out_2","title":"Try it out.","text":"<p>To try out the above example, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file. Then add and save the following Siddhi application.</p> <p><pre><code>@App:name(\"TemperatureApp\")\n@App:description(\"Description of the plan\")\n\ndefine stream TempStream (deviceID string, roomNo string, temp double);\n\n@sink(type = 'log', prefix = \"Cleansed Data\",\n    @map(type = 'passThrough'))\ndefine stream CleansedDataStream (deviceID string, roomNo string, temp double);\n\n@info(name = 'AddingMissingValues')\nfrom TempStream\nselect ifThenElse(deviceID is null, \"UNKNOWN\", deviceID) as deviceID, roomNo, temp\ninsert into CleansedDataStream;\n</code></pre>    In this Siddhi application, the <code>Temp Stream</code> has an attribute named <code>deviceID</code>, but it is not selected to be included in the output events. The <code>roomNo</code>attribute is cast as an string value via <code>cast(roomNo, \"string\")</code>. This means although the value for this attribute is received as an integer, it is presented as a string value in the output.</p> </li> <li> <p>Open the event simulator and simulate an event for the <code>TempStream</code> input stream of the <code>TemperatureApp</code> Siddhi application with the values for the attributes as given below. For instructions to simulate single events, see Testing Siddhi Applications - Simulating a single event.</p> deviceID roomNo temp Select the Is Null check box for this sttribute <code>183</code> <code>30</code> <p>The output is logged as follows:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Cleansed Data : Event{timestamp=1604581209943, data=[UNKNOWN, 183, 30.0], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"guides/correlating-data/","title":"Correlating Data","text":"<p>The streaming integrator can correlate data in order to detect patterns and trends in streaming data. Correlating can be done via patterns as well as sequences.</p> <p></p> <p>The difference between patterns and sequence is that sequences require all the matching events to arrive consecutively to  match the sequence condition, whereas patterns identify events that match the pattern condition irrespective of the order in which they arrive.</p>"},{"location":"guides/correlating-data/#identifying-patterns","title":"Identifying patterns","text":"<p>A pattern identifies a correlation between two events that may or may not arrive in a specific sequence. </p>"},{"location":"guides/correlating-data/#combine-several-patterns-logically-and-match-events","title":"Combine several patterns logically and match events","text":"<p>Logical patterns involve  combining several patterns logically and matching events. </p> <p>To understand this type of pattern, consider an example where you need to calculate the average shelf life of a production batch by calculating the amount of time it takes to sell the total amount in a batch. For this purpose, let's assume that the products are sold on a FIFO (First In First Out) basis.</p> <p>The above requirement can be addressed by the following Siddhi application.</p> <p><pre><code>@App:name('ShelfLifeApp')\n\ndefine stream ProductionStream (timestamp long, name string, amount double);\n\ndefine stream SalesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Shelf Life\",\n    @map(type = 'json'))\ndefine stream ShelfLifeStream (name string, days long);\n\n@info(name = 'Calculate Sales Total')\nfrom SalesStream \nselect timestamp, name, sum(amount) as total \ninsert into SalesTotalsStream;\n\n@info(name = 'Calculate Shelf Life')\nfrom every e1 = ProductionStream -&gt; e2 = SalesTotalsStream[name == e1.name and total &gt;= e1.amount] \nselect e1.name as name, time:dateDiff(e1.timestamp, e2.timestamp) as days \ninsert into ShelfLifeStream;\n</code></pre> First, the <code>Calculate Sales Total</code> query calculates the total sales from the sales stream that reports each sales transaction, and inserts the total into the <code>SalesTotalsStream</code> stream. </p> <p>In the <code>Calculate Shelf Life</code> query, <code>every e1 = ProductionStream -&gt; e2 = SalesTotalsStream[name == e1.name and total &gt;= e1.amount]</code> means every event in the <code>ProductionStream</code> stream is compared with the first subsequent event in the <code>SalesTotalsStream</code> stream, of which the value for the <code>total</code> attribute is greater than or equal to that of the <code>amount</code> attribute of the <code>ProductionStream</code> stream. When such an event exists in the <code>SalesTotalsStream</code> stream, the <code>time:dateDiff()</code> function is applied to calculate the time difference between the two events. The result is inserted into the <code>ShelfLifeStream</code> stream, and logged with the <code>Shelf Life</code> prefix via the log sink connected to this stream.</p>"},{"location":"guides/correlating-data/#try-it-out","title":"Try it out","text":"<p>To try out the Siddhi application given in the sample above, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file, add the following content to it and save.</p> <pre><code>@App:name('ShelfLifeApp')\n\ndefine stream ProductionStream (timestamp long, name string, amount double);\n\ndefine stream SalesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Shelf Life\",\n    @map(type = 'json'))\ndefine stream ShelfLifeStream (name string, days long);\n\n@info(name = 'Calculate Sales Total')\nfrom SalesStream \nselect timestamp, name, sum(amount) as total \ninsert into SalesTotalsStream;\n\n@info(name = 'Calculate Shelf Life')\nfrom every e1 = ProductionStream -&gt; e2 = SalesTotalsStream[name == e1.name and total &gt;= e1.amount] \nselect e1.name as name, time:dateDiff(e1.timestamp, e2.timestamp) as days \ninsert into ShelfLifeStream;\n</code></pre> </li> <li> <p>Simulate events for the <code>ShelfLifeApp</code> application as follows. For instructions to simulate events, see Testing Siddhi Applications - Simulating Events.</p> <ul> <li> <p>For the <code>ProductionStream</code> stream</p> timestamp name amount <code>1602323450000</code> <code>cake</code> 10 </li> <li> <p>For the <code>SalesStream</code> stream</p> timestamp name amount <code>1602327050000</code> <code>cake</code> <code>5</code> <code>1602413450000</code> <code>cake</code> <code>6</code> </li> </ul> <p>As a result, the following is logged in the terminal.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Shelf Life : {\"event\":{\"name\":\"cake\",\"days\":-1}} (Encoded) \n</code></pre> </li> </ol>"},{"location":"guides/correlating-data/#count-and-match-multiple-events-for-a-given-pattern-condition","title":"Count and match multiple events for a given pattern condition","text":"<p>Counting patterns involve counting and matching multiple events for a given pattern condition. </p> <p>To understand this type of patterns, consider an example where the manager of a Sweet Factory needs to count the number of times number of items sold within a time period one hour exceeds 90% of the latest stock amount during that same period.</p> <p>The above requirement can be addressed by the following Siddhi application.</p> <pre><code>@App:name('DetectLowStockApp')\n\ndefine stream LatestStockStream (timestamp long, name string, amount double);\n\ndefine stream SalesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Low Stock Alert!\",\n    @map(type = 'json'))\ndefine stream LowStockLevelAlertStream (name string);\n\nfrom e1=LatestStockStream -&gt; e2=SalesStream[e1.name == e2.name and e2.amount &gt; e1.amount]&lt;3:&gt; within 1 hour\nselect e1.name as name\ninsert into LowStockLevelAlertStream;\n</code></pre>"},{"location":"guides/correlating-data/#try-it-out_1","title":"Try it out","text":"<p>To try out the Siddhi application given in the sample above, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file, add the following content to it and save.</p> <pre><code>@App:name('DetectLowStockApp')\n\ndefine stream LatestStockStream (timestamp long, name string, amount double);\n\ndefine stream SalesStream (timestamp long, name string, amount double);\n\n@sink(type = 'log', prefix = \"Low Stock Alert!\",\n    @map(type = 'json'))\ndefine stream LowStockLevelAlertStream (name string);\n\nfrom e1=LatestStockStream -&gt; e2=SalesStream[e1.name == e2.name and e2.amount &gt; e1.amount]&lt;3:&gt; within 1 hour\nselect e1.name as name\ninsert into LowStockLevelAlertStream;\n</code></pre> </li> <li> <p>Simulate events for the <code>DetectingLowStockApp</code> application as follows. For instructions to simulate events, see Testing Siddhi Applications.</p> <ul> <li> <p>For the <code>LatestStockStream</code> stream</p> timestamp name amount <code>1602410450000</code> <code>eclairs</code> <code>10</code> <code>1602411290000</code> <code>eclairs</code> <code>18</code> <code>1602412310000</code> <code>eclairs</code> <code>20</code> </li> <li> <p>For the <code>SalesStream</code> stream</p> timestamp name amount <code>1602410570000</code> <code>eclairs</code> <code>10</code> <code>1602411350000</code> <code>eclairs</code> <code>17</code> <code>1602412850000</code> <code>eclairs</code> <code>19</code> </li> </ul> <p>As a result, the following is logged in the terminal.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Low Stock Alert! : {\"event\":{\"name\":\"eclairs\"}} (Encoded) \n</code></pre> </li> </ol>"},{"location":"guides/correlating-data/#find-non-occurrence-of-events","title":"Find non-occurrence of events","text":"<p>To understand how to detect the non occurrence of events, consider a scenario where the production manager of sa sweet factory needs to count the number of times a wastage occurred due to some of the items in a production batch not being sold within three days since it was produced and having to be scrapped as a result.  For this purpose, let's assume that the products are sold on a FIFO (First In First Out) basis.</p> <p>The above requirement can be addressed by the following Siddhi application.</p> <p><pre><code>@App:name('DetectWastageApp')\n\ndefine stream ProductionStream (name string, amount double);\n\n@sink(type = 'log', prefix = \"WastageAlert!\",\n    @map(type = 'json'))\ndefine stream WastageAlertStream (name string);\n\ndefine stream SalesStream (name string, amount double);\n\n@info(name = 'Calculate Sales Total')\nfrom SalesStream \nselect name, sum(amount) as total \ninsert into SalesTotalsStream;\n\n@info(name = 'Detect Wastage')\nfrom e1 = ProductionStream -&gt; not SalesTotalsStream[name == e1.name and total &gt;= e1.amount] for 3 days \nselect e1.name as name \ninsert into WastageAlertStream;\n</code></pre> First, the <code>Calculate Sales Total</code> query calculates the total sales from the sales stream that reports each sales transaction, and inserts the total into the <code>SalesTotalsStream</code> stream. </p> <p>In the <code>Detect Wastage</code> query, the <code>from</code> clause detects a pattern where the sales total for a specific product has not equalled or exceeded the amount of the last production batch reported for that product within a time period of 3 days. When this pattern condition is met, the resulting output is inserted into the <code>WastageAlertStream;</code> and logged in the terminal via the connected log sink.</p>"},{"location":"guides/correlating-data/#try-it-out_2","title":"Try it out","text":"<p>To try out the Siddhi application given in the sample above, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file, add the following content to it and save.</p> <p><pre><code>@App:name('DetectWastageApp')\n\ndefine stream ProductionStream (name string, amount double);\n\n@sink(type = 'log', prefix = \"WastageAlert!\",\n    @map(type = 'json'))\ndefine stream WastageAlertStream (name string);\n\ndefine stream SalesStream (name string, amount double);\n\n@info(name = 'Calculate Sales Total')\nfrom SalesStream \nselect name, sum(amount) as total \ninsert into SalesTotalsStream;\n\n@info(name = 'Detect Wastage')\nfrom e1 = ProductionStream -&gt; not SalesTotalsStream[name == e1.name and total &gt;= e1.amount] for 3 seconds \nselect e1.name as name \ninsert into WastageAlertStream;\n</code></pre>    For testing purposes, the above Siddhi application detects thye non occurrence of a matching event within three seconds instead of three days.</p> </li> <li> <p>Simulate events for the <code>ProductionStream</code> stream of the <code>DetectWastageApp</code> application as follows. For instructions to simulate events, see Testing Siddhi Applications.</p> name amount <code>eclairs</code> <code>100</code> <p>The following is logged after three seconds.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - WastageAlert! : {\"event\":{\"name\":\"eclairs\"}} (Encoded) \n</code></pre> </li> </ol>"},{"location":"guides/correlating-data/#correlating-events-to-find-a-trendsequence","title":"Correlating events to find a trend(sequence)","text":"<p>Sequences are identified by observing trends in events that occur consecutively.</p>"},{"location":"guides/correlating-data/#combine-several-trends-logically-and-match-events","title":"Combine several trends logically and match events","text":"<p>Logical sequences are trends observed when consecutively occurring events match a given condition.</p> <p>To understand logical sequences, consider a production manager identifying a decreasing trend in production when he/she observes a continuous decrease within three consecutive production batches.</p> <p>The above requirement can be addressed by the following Siddhi application.</p> <pre><code>@App:name('DecreasingProductionAlertApp')\n\ndefine stream ProductionStream (name string, amount double);\n\n@sink(type = 'log', prefix = \"Decreasing Production Alert\",\n    @map(type = 'json'))\ndefine stream DecreasingProductionAlertStream (name string);\n\n@info(name = 'query')\nfrom every e1 = ProductionStream , e2 = ProductionStream[e1.name == name and e1.amount &gt; amount] , e3 = ProductionStream[e2.name == name and e2.amount &gt; amount] \nselect e1.name as name \ninsert into DecreasingProductionAlertStream;\n</code></pre> <p>The above Siddhi application compares  three events (i.e., e1, e2 and e3) that occur consecutively in the <code>ProductionStream</code>. If the second event reports a lower production amount than the first event, and then the third event reports a lower production amount than the second event, an output event is inserted into the <code>DecreasingProductionTrendAlertStream</code> stream. This output event is then logged via the log sink connected to the <code>DecreasingProductionTrendAlertStream</code> stream.</p>"},{"location":"guides/correlating-data/#try-it-out_3","title":"Try it out","text":"<p>To try out the Siddhi application given in the sample above, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file, add the following content to it and save.</p> <pre><code>@App:name('DecreasingProductionAlertApp')\n\ndefine stream ProductionStream (name string, amount double);\n\n@sink(type = 'log', prefix = \"Decreasing Production Alert\",\n    @map(type = 'json'))\ndefine stream DecreasingProductionAlertStream (name string);\n\n@info(name = 'query')\nfrom every e1 = ProductionStream , e2 = ProductionStream[e1.name == name and e1.amount &gt; amount] , e3 = ProductionStream[e2.name == name and e2.amount &gt; amount] \nselect e1.name as name \ninsert into DecreasingProductionAlertStream;\n</code></pre> </li> <li> <p>Simulate three events for the <code>ProductionStream</code> stream of the <code>DecreasingProductionAlertApp</code> application as follows. For instructions to simulate events, see Testing Siddhi Applications.</p> name amount <code>gingerbread</code> <code>100</code> <code>gingerbread</code> <code>90</code> <code>gingerbread</code> <code>80</code> <p>As a result, the following is logged in the terminal.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Decreasing Production Alert : {\"event\":{\"name\":\"gingerbread\"}} (Encoded) \n</code></pre> </li> </ol>"},{"location":"guides/correlating-data/#count-and-match-multiple-events-for-a-given-trend","title":"Count and match multiple events for a given trend","text":"<p>Counting sequences involves counting and matching multiple consecutively occurring events that match a given condition.</p> <p>To understand this, consider a scenario where the productivity of a production bot changes since it is started. To use them in an optimum way, the production manager wants to identify the peaks and slumps in the production by observing every six production runs.</p> <p>The above requirement can be addressed by the following Siddhi application.</p> <p><pre><code>@App:name('ObserveProductionTrendsApp')\n\ndefine stream ProductionStream (name string, amount double);\n\n@sink(type = 'log', prefix = \"Production Peaks\",\n    @map(type = 'json'))\ndefine stream DecreasingProductionAlertStream (initialAmount double, peakAmount double, firstReducedAmount double);\n\n@info(name = 'query')\nfrom every e1 = ProductionStream , e2 = ProductionStream[ifThenElse(e2[last].amount is null, e1.amount &lt;= amount, e2[last].amount &lt;= amount)] + , e3 = ProductionStream[e2[last].amount &gt; amount] \nselect e1.amount as initialAmount, e2[last].amount as peakAmount, e3.amount as firstReducedAmount \ninsert into DecreasingProductionAlertStream;\n</code></pre> The above Siddhi application matches every event in the <code>ProductionStream</code> stream. It first checks whether the value for the <code>amount</code> attribute of the second event is greater than that of the first event. Then for every event, it checks whetherv the value for the <code>amount</code> attribute is greater or equal to that of the previous event (i.e., via <code>e2[last].temp</code>). If an event occurs with a value for the <code>amount</code> attribute that is less than that of the preceding event, an output event is generated in the <code>DecreasingProductionAlertStream</code>.</p>"},{"location":"guides/correlating-data/#try-it-out_4","title":"Try it out","text":"<p>To try out the Siddhi application given in the sample above, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file, add the following content to it and save.</p> <pre><code>@App:name('ObserveProductionTrendsApp')\n\ndefine stream ProductionStream (name string, amount double);\n\n@sink(type = 'log', prefix = \"Production Peaks\",\n    @map(type = 'json'))\ndefine stream DecreasingProductionAlertStream (initialAmount double, peakAmount double, firstReducedAmount double);\n\n@info(name = 'query')\nfrom every e1 = ProductionStream , e2 = ProductionStream[ifThenElse(e2[last].amount is null, e1.amount &lt;= amount, e2[last].amount &lt;= amount)] + , e3 = ProductionStream[e2[last].amount &gt; amount] \nselect e1.amount as initialAmount, e2[last].amount as peakAmount, e3.amount as firstReducedAmount \ninsert into DecreasingProductionAlertStream;\n</code></pre> </li> <li> <p>Simulate events for the <code>ProductionStream</code> steeam of the <code>ObserveProductionTrendsApp</code> application as follows. For instructions to simulate events, see Testing Siddhi Applications - Simulating Events.</p> name amount <code>gingerbread</code> <code>100</code> <code>gingerbread</code> <code>105</code> <code>gingerbread</code> <code>117</code> <code>gingerbread</code> <code>121</code> <code>gingerbread</code> <code>115</code> <p>As a result, the following is logged in the terminal.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Production Peaks : {\"event\":{\"initialAmount\":100.0,\"peakAmount\":121.0,\"firstReducedAmount\":115.0}} (Encoded) \n</code></pre> </li> </ol>"},{"location":"guides/enriching-data/","title":"Enriching Data","text":"<p>Enriching data involves integrated the data received into a streaming integration flow with data from other medium such as a data store, another data stream, or an external service to derive an expected result.</p>"},{"location":"guides/enriching-data/#integrating-data-streams-and-static-data","title":"Integrating data streams and static data","text":"<p>This involves enriching a data stream by joining it with a data store.</p> <p>For example, consider a scenario where a sweet factory reports its production data in a streaming manner after each run. To update the stock with the latest amount produced, you need to add the latest production amounts to the stock amounts saved in a database.</p> <p>To address this, you can write a query as follows.</p> <pre><code>from ProductionStream as p \njoin StocksTable as s \n    on p.name == s.name \nselect p.name as name, sum(p.amount) + s.amount as amount \n    group by p.name \ninsert into UpdateStockwithProductionStream;\n</code></pre> <p>Here, the <code>ProductionStream</code> stream that has the production amounts for sweets after each production run is assigned the short name <code>p</code>. The <code>StocksTable</code> that has the current stock for each product before the latest production runs is given the short name <code>s</code>. This allows you to uniquely identify the attributes in each. The matching condition is <code>p.name == s.name</code>, which means that a match is identified when an event in the <code>ProductionStream</code> stream has the same value for the <code>name</code> attribute as a record in the <code>StockTable</code> table.  <code>sum(p.amount)</code> calculates the total production per product. This total production amount for a product is then added to the stock amount of the product (i.e., <code>s.amount</code>). The resulting output is inserted into the <code>UpdateStockwithProductionStream</code> stream.</p>"},{"location":"guides/enriching-data/#integrating-multiple-data-streams","title":"Integrating multiple data streams","text":"<p>This involves enriching a data stream by joining it with another data stream.</p> <p>To understand this, consider the example you used in the previous Integrating data streams and static data section where you directed the stock amounts updated with the latest production amounts into a stream. Assume another stream reports the sale of stock in a streaming manner. To further update the stock by deducting the amounts sold, you need to join the data stream that has the latest stock amounts with the data stream that has the sales amounts.</p> <p>To address the above requirement, you can write a query as follows.</p> <pre><code>from UpdateStockwithProductionStream as u \njoin SalesStream as s \n    on u.name == s.name \nselect u.name as name, sum(u.amount) - sum(s.amount) as amount \ninsert into LatestStockStream;\n</code></pre> <p>Here, <code>u</code> is the short name for the <code>UpdateStockwithProductionStream</code> stream that has the total stock amounts for each product updated with the latest production amounts. <code>s</code> is the short name for the <code>SalesStream</code> stream that reports the sales for all the products in a streaming manner. Matching events are the events with the same value for the <code>name</code> attribute. To calculate the latest stock amount for each product, the total sales amount is deducted from the total stock amount updated with production amounts. The resulting output is inserted into the <code>LatestStockStream</code>.</p>"},{"location":"guides/enriching-data/#integrating-data-streams-with-external-services","title":"Integrating data streams with external services","text":"<p>This involves enriching a data stream by incorporating information received from an external service to it.</p> <p>To understand this, consider that in order to value the stock, a Sweet Factory obtains the value of one unit of a product from an external application named <code>StockValuingApp</code>. When you submit the name of the product, it returns the unit value. To value the stock based on this information, you can create a Siddhi application as follows:</p> <pre><code>@App:name(\"StockValuingApp\")\n\n@sink(type='http-request',publisher.url='http://localhost:5005/CheckProductValueEP',method='POST', headers=\"'Content-Type:application/x-www-form-urlencoded'\",\nsink.id=\"unitvalueSink\",\n@map(type='keyvalue', @payload(Sweet='{{ name }}')))\ndefine stream CheckUnitValueStream (name string);\n\n@source(type='http-response' ,sink.id='unitvalueSink',    \n@map(type='xml', namespaces = \"xmlns=http://localhost:5005/CheckProductValueEP/\",    \n@attributes(name = 'trp:name',unitValue = \".\")))        \ndefine stream StoreUnitValueStream (name string,unitValue double);\n\nfrom StoreUnitValueStream\nselect *\nupdate or insert into ProductValueTable;\n</code></pre> <p>In the above application, events in the <code>CheckUnitValueStream</code> stream are published to the <code>http://localhost:5005/CheckProductValueEP</code> URL via the connected <code>http-request</code> sink to invoke a service that returns the unit value for the name of the product sent. WSO2 Streaming Integrator captures this response (i.e., unit value) in the <code>StoreUnitValueStream</code> stream via the <code>http-response</code> source connected to the stream. In orcder to allow WSO2 Streaming Integrator to identify the response as the result of the request it previously sent, the same value is specified for the <code>sink.id</code> parameter in both the source configuration and the sink configuration. To store the unit values obtained for further processing, all the events in the <code>StoreUnitValueStream</code> stream are inserted into a table named <code>ProductValueTable</code>.</p>"},{"location":"guides/enriching-data/#enriching-data-with-built-in-extensions","title":"Enriching data with built-in extensions","text":"<p>The following is a list of Siddhi extensions with which you can enrich data.</p> <ul> <li>Siddhi-execution-streamingml</li> <li>Siddhi-execution-env</li> <li>Siddhi-execution-math</li> <li>Siddhi-execution-string</li> <li>Siddhi-execution-time</li> <li>Siddhi-execution-json</li> </ul>"},{"location":"guides/enriching-data/#try-it-out","title":"Try it out","text":"<p>To try out the examples given above, follow the steps below.</p> <ol> <li> <p>Set up your database as follows:</p> <ol> <li> <p>Download and install MySQL. Then start the MySQL server and create a new database in it by issuing the following command:</p> <p><code>CREATE SCHEMA stock;</code></p> </li> <li> <p>Create a table in the <code>stock</code> database you created by issuing the following two commands.</p> <p><code>use stock;</code></p> <p><code>CREATE TABLE StockTable (name VARCHAR(20),amount double(10,2));</code></p> </li> <li> <p>Insert two records into the <code>StockTable</code> you created by issuing the following commands.</p> <p><code>insert into StockTable values('gingerbread',8.0);</code></p> <p><code>insert into StockTable values('coffee cake',6.0);</code> </p> </li> <li> <p>Then open the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file and add the following data source configuration under <code>datasources</code>.</p> <pre><code>  - name: Stock_DB\n    description: The datasource used for Stock Valuation\n    jndiConfig:\n      name: jdbc/stock\n    definition:\n      type: RDBMS\n      configuration:\n        jdbcUrl: 'jdbc:mysql://localhost:3306/stock?useSSL=false'\n        username: root\n        password: root\n        driverClassName: com.mysql.jdbc.Driver\n        minIdle: 5\n        maxPoolSize: 50\n        idleTimeout: 60000\n        connectionTestQuery: SELECT 1\n        validationTimeout: 30000\n        isAutoCommit: false\n</code></pre> </li> </ol> </li> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file in Streaming Integrator Tooling. Then add and save the following Siddhi application.</p> <pre><code>@App:name('StockValuingApp')\n@App:description('Description of the plan')\n\ndefine stream ProductionStream (name string, amount double);\n\n@source(type = 'http-response', sink.id = \"unitvalueSink\",\n    @map(type = 'xml', namespaces = \"xmlns=http://localhost:5005/CheckProductValueEP/\",\n        @attributes(name = \"trp:name\", unitValue = \".\")))\ndefine stream GetUnitValueStream (name string, unitValue double);\n\n@sink(type = 'log', prefix = \"Stock Update After Production\",\n    @map(type = 'text'))\n\ndefine stream UpdateStockwithProductionStream (name string, amount double);\n\ndefine stream SalesStream (name string, amount double);\n\n@sink(type = 'http-request', publisher.url = \"http://localhost:5005/CheckProductValueEP\", method = \"POST\", headers = \"'Content-Type:application/x-www-form-urlencoded'\", sink.id = \"unitvalueSink\",\n    @map(type = 'keyvalue'))\ndefine stream CheckUnitValueStream (name string);\n\n@sink(type = 'log', prefix = \"Latest Stock\",\n    @map(type = 'text'))\ndefine stream LatestStockStream (name string, amount double);\n\n@sink(type = 'log', prefix = \"Stock Value\",\n    @map(type = 'text'))\ndefine stream StockValueStream (name string, value double);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/stock?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\n@primaryKey(\"name\")\ndefine table StockTable (name string, amount double);\n\n@info(name = 'UpdateStockwithProduction')\nfrom ProductionStream as p \njoin StockTable as s \n    on p.name == s.name \nselect p.name as name, sum(p.amount) + s.amount as amount \n    group by p.name \ninsert into UpdateStockwithProductionStream;\n\n@info(name='UpdateStockwithSales') \nfrom UpdateStockwithProductionStream#window.time(5 min) as u \njoin SalesStream as s \n    on u.name == s.name \nselect u.name as name, sum(u.amount) - sum(s.amount) as amount \ninsert into LatestStockStream;\n\n\n@info(name='CalculateStockValue') \nfrom LatestStockStream as l \njoin GetUnitValueStream as g \n    on l.name == g.name \nselect l.name as name, g.unitValue * l.amount as value \ninsert into StockValueStream;  \n</code></pre> </li> </ol> <p>The above Siddhi application does the following:</p> <ol> <li> <p>Updates the stock amount for each product by adding the total production amount to the stock amount. This is done by joining the <code>StockTable</code> table with the <code>ProductionStream</code> stream. Then it logs the result with the <code>Stock Update After Production</code> prefix.</p> </li> <li> <p>Calculates the latest stock by deducting the total sales for a product from the updated stock derived by adding the total production amount with the current stock amount. This is done by joining the <code>UpdateStockwithProductionStream</code> stream with the <code>SalesStream</code> stream. The result is logged with the <code>Latest Stock</code> prefix.</p> </li> <li> <p>Sends requests with the product name to an external service with the <code>http://localhost:5005/CheckProductValueEP</code> endpoint and receives the unit value of the submitted product name as a response. This response is captured in the <code>GetUnitValueStream</code> stream.</p> </li> <li> <p>Calculates the stock value by multiplying the latest stock with the unit value obtained from the external service. This is done by joining the <code>GetUnitValueStream</code> stream with the <code>LatestStockStream</code> stream. The result is then logges with the <code>Stock Value</code> prefix.</p> </li> <li> <p>In Streaming Integrator Tooling, create a new Siddhi application as follows, save it, and then start it.</p> <p><pre><code>@App:name('ReturnUnitValueApp')\n\n@source(type='http-service' , source.id='unitValue', receiver.url='http://localhost:5005/CheckProductValueEP/',\n    @map(type = 'json')) \ndefine stream RequestStream (name string);\n\n@sink(type='http-service-response', source.id='unitValue',\n      message.id='{{ name }}', @map(type = 'json'))\ndefine stream ResultStream (name string, unitValue double);\n\nfrom RequestStream\nselect name, ifThenElse(name == 'gingerbread', 10.0, 20.0) as unitValue\ninsert into ResultStream;\n</code></pre> This application functions as the external service for testing purposes.</p> </li> <li> <p>Simulate events for the <code>StockValuingApp</code> application as follows. For instructions to simulate events, see Testing Siddhi Applications - Simulating Events.</p> <ol> <li> <p>First, simulate two events for the <code>ProductionStream</code> stream with the following values.</p> Name Amount <code>gingerbread</code> <code>10</code> <code>coffee cake</code> <code>10</code> <p>As a result, the following logs appear in the terminal.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Stock Update After Production : name:\"gingerbread\",\namount:18.0 (Encoded) \n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Stock Update After Production : name:\"coffee cake\",\namount:16.0 (Encoded) \n</code></pre> </li> </ol> <p>Here, the <code>StockUpdateAfterProduction</code> value is calculated by adding the production value to the stock value saved in the database.</p> <ol> <li> <p>Now simulate the following two events to the <code>SalesStream</code> stream.</p> Name Amount <code>gingerbread</code> <code>12</code> <code>coffee cake</code> <code>13</code> <p>As a result, the following logs appear in the terminal.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Latest Stock : name:\"gingerbread\",\namount:6.0 (Encoded) \n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Latest Stock : name:\"coffee cake\",\namount:3.0 (Encoded) \n</code></pre> </li> <li> <p>Now simulate two events to the <code>CheckUnitValueStream</code> stream. Enter <code>gingerbread</code> and <code>coffee cake</code> as the <code>name</code> for the first and the second event respectively.</p> <p>As a result, the following logs appear in the terminal.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Stock Value : name:\"gingerbread\",\nvalue:60.0 (Encoded) \n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Stock Value : name:\"coffee cake\",\nvalue:60.0 (Encoded) \n</code></pre> </li> </ol> </li> </ol>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/","title":"Extracting Data from Static Sources in Real Time","text":"<p>WSO2 Streaming Integrator can extract data from static sources such as databases, files and cloud storages in real-tme. </p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#consuming-data-from-rdbms-databases","title":"Consuming data from RDBMS databases","text":"<p>A database table is a stored collection of records of a specific schema. Each record can be equivalent to an event. WSO2 Streaming Integrator can integrate databases into the streaming flow by extracting records in database tables as streaming events. This can be done via change data capture or by polling a database.</p> <p></p> <p>To understand how data is extracted from a database into a streaming flow, consider a scenario where an online booking site automatically save all online bookings of vacation packages in a database. The company wants to monitor the bookings in real-time. Therefore, this data stored in the database needs to be extracted in real time. You can either capture this data as change data or poll the database. The <code>cdc</code> Siddhi extensions can be used for both methods as explained in the following subsections.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#change-data-capture","title":"Change data capture","text":"<p>Change data capture involves extracting any change that takes place in a selected database (i.e., any insert, update or a deletion) in real-time.</p> <p>To capture change data via the WSO2 Streaming Integrator Tooling, define an input stream with the appropriate schema to capture the information you require, and then connect a source of the <code>cdc</code> type as shown in the example below.</p> <p><pre><code>@source(type = 'cdc', \n    url = \"jdbc:mysql://localhost:3306/tours?useSSL=false\", \n    username = \"wso2si\", \n    password = \"wso2\", \n    table.name = \"OnlineBookingsTable\", \n    operation = \"insert\", \n    mode = \"listening\", jdbc.driver.name = \"com.mysql.jdbc.Driver\",\n    @map(type = 'keyvalue'))\ndefine stream OnlineBookingsStream (ref int, timestamp int, name string, package string, people int);\n</code></pre> Here, note that the <code>mode</code> parameter of the <code>cdc</code> source is set to <code>listening</code>. This mode involves listening to the database for the specified database operation. In this example, the <code>operation</code> parameter is set to <code>insert</code>. Therefore, the source listens to new records inserted into the <code>OnlineBookingsTable</code> table and generates an input event in the <code>OnlineBookingsStream</code> stream for each insert.</p> <p>If you want to capture updates to the records in the <code>OnlineBookingsTable</code> database table in real time, you can change the value for the <code>operation</code> parameter to <code>update</code> as shown below.</p> <p><pre><code>@source(type = 'cdc', \n    url = \"jdbc:mysql://localhost:3306/tours?useSSL=false\", \n    username = \"wso2si\", \n    password = \"wso2\", \n    table.name = \"OnlineBookingsTable\", \n    operation = \"update\", \n    mode = \"listening\", \n    jdbc.driver.name = \"com.mysql.jdbc.Driver\",\n    @map(type = 'keyvalue'))\ndefine stream OnlineBookingsStream (ref int, timestamp int, name string, package string, people int);\n</code></pre> Similarly, if you want to capture deletions in the <code>OnlineBookingsTable</code> database table in real time, you can change the value for the <code>operation</code> parameter to <code>delete</code> as shown below.</p> <pre><code>@source(type = 'cdc', \n    url = \"jdbc:mysql://localhost:3306/tours?useSSL=false\", \n    username = \"wso2si\", \n    password = \"wso2\", \n    table.name = \"OnlineBookingsTable\", \n    operation = \"delete\", \n    mode = \"listening\", \n    jdbc.driver.name = \"com.mysql.jdbc.Driver\",\n    @map(type = 'keyvalue'))\ndefine stream OnlineBookingsStream (ref int, timestamp int, name string, package string, people int);\n</code></pre>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#polling-databases","title":"Polling databases","text":"<p>This method involves periodically polling a database table to capture changes in the data. Similar to change data capture, you can  define an input stream with the appropriate schema to capture the information you require, and then connect a source of the <code>cdc</code> type as shown in the example below. However, for polling, the value for the <code>mode</code> parameter must be <code>polling</code></p> <p><pre><code>@source(type = 'cdc',\n    url = 'jdbc:mysql://localhost:3306/tours?useSSL=false',\n    mode = 'polling',\n    jdbc.driver.name = 'com.mysql.jdbc.Driver',\n    polling.column = 'timestamp',\n    polling.interval = '10',\n    username = 'wso2si',\n    password = 'wso2',\n    table.name = 'OnlineBookingsTable',\n    @map(type = 'keyvalue' ))\ndefine stream OnlineBookingsStream (ref int, timestamp long, name string, package string, people int);\n</code></pre> The above source polls the <code>OnlineBookingsTable</code>table every 10 seconds and captures all inserts and updates to the database table that take place during that time interval. An input event is generated in the <code>OnlineBookingsStream</code> stream for each insert and update.</p> <p>Tip</p> <ul> <li>The <code>polling</code> mode only captures insert and update operations. Unlike in the <code>listening</code> mode, you do not need to specify the operation.</li> </ul>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#try-out-an-example","title":"Try out an example","text":"<p>Let's try out the example where you want to view the online bookings saved in a database in real time. To do this, follow the steps below:</p> <ol> <li> <p>Download and install MySQL.</p> </li> <li> <p>Enable binary logging in the MySQL server. For detailed instructions, see Debezium documentation - Enabling the binlog.</p> <p>Info</p> <p>If you are using MySQL 8.0, use the following query to check the binlog status. <pre><code>SELECT variable_value as \"BINARY LOGGING STATUS (log-bin) ::\"\nFROM performance_schema.global_variables WHERE variable_name='log_bin';\n</code></pre></p> </li> <li> <p>Start the MySQL server, create the database and the database table you require as follows:</p> <ol> <li> <p>To create a new database, issue the following MySQL command.</p> <pre><code>CREATE SCHEMA tours;\n</code></pre> </li> <li> <p>Create a new user by executing the following SQL query.</p> </li> </ol> <pre><code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2';\n</code></pre> <ol> <li> <p>Switch to the <code>tours</code> database and create a new table, by executing the following queries.</p> <p><code>use tours;</code></p> <p><code>CREATE TABLE tours.tours (   ref INT NOT NULL AUTO_INCREMENT,   timestamp LONGTEXT NULL,   name VARCHAR(45) NULL,   package VARCHAR(45) NULL,   people INT NULL,   PRIMARY KEY (ref));</code></p> </li> <li> <p>Start the Streaming Integrator Tooling.</p> </li> <li> <p>Download the <code>cdc-mysql</code>Siddhi extension for Streaming Integrator Tooling. For instructions, see Installing Siddhi Extensions.</p> </li> <li> <p>In Streaming Integrator Tooling, open a new file. Copy and paste the following Siddhi application to it.</p> <pre><code>@App:name(\"VacationsApp\")\n@App:description(\"Captures cdc events from MySQL table\")\n\n@source(type = 'cdc', url = \"jdbc:mysql://localhost:3306/tours?useSSL=false\", username = \"wso2si\", password = \"wso2\", table.name = \"OnlineBookingsTable\", operation = \"insert\", mode = \"listening\", jdbc.driver.name = \"com.mysql.jdbc.Driver\",\n    @map(type = 'keyvalue'))\ndefine stream OnlineBookingsStream (ref int, timestamp long, name string, package string, people int);\n\n@sink(type = 'log')\ndefine stream LogStream (ref int, timestamp long, name string, package string, people int);\n\n@info(name = 'query')\nfrom OnlineBookingsStream\nselect *\ninsert into LogStream;\n</code></pre> <p>Then save the Siddhi application.This Siddhi application uses a <code>cdc</code> source that extracts events in the change data capturing (i.e., listening) mode and logs the captured records in the console via a <code>log</code> sink.</p> </li> <li> <p>Start the Siddhi Application by clicking the play button.</p> <p></p> </li> <li> <p>To insert a record into the <code>OnlineBookingsTable</code>, issue the following MySQL command:</p> <p><code>insert into OnlineBookingsTable(ref,timestamp,name,package,people) values('1',1602506738000,'jem','best of rome',2);</code></p> <p>The following is logged in the Streaming Integrator Tooling terminal.</p> <pre><code>INFO {org.wso2.siddhi.core.stream.output.sink.LogSink} - VacationsApp : LogStream : Event{timestamp=1563378804914, data=[1, 1602506738000, jem, best of rome, 2], isExpired=false}\n</code></pre> </li> </ol> </li> </ol>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#supported-databases","title":"Supported databases","text":"<p>siddhi-io-cdc source via which the WSO2 Steaming Integrator extracts database records supports the following database types.</p> <p>The following is a list of Siddhi extensions that support change data capturing to allow you to extract database records as input events in real time.</p> Database Type Extension Name Description Mongo DB <code>cdc-mongodb</code> Captures change data from Mongo DB databases. MS SQL <code>cdc-mssql</code> Captures change data from MS SQL databases. MySQL <code>cdc-mysql</code> Captures change data from MySQL databases. Oracle <code>cdc-oracle</code> Captures change data from Oracle databases. PostgreSQL <code>cdc-postgresql</code> Captures change data from PostgreSQL databases."},{"location":"guides/extracting-data-from-static-sources-in-real-time/#supported-mappers","title":"Supported mappers","text":"<p>Mappers determine the format in which the event is received. For information about transforming events by changing the format in which the data is received/published, see Transforming Data).</p> <p>The mapper available for extracting data from databases is Keyvalue.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#file-processing","title":"File Processing","text":"<p>File Processing involves two types of operations related to files:</p> <ul> <li> <p>Extracting data from files: This involves extracting the content of file as input data for further processing.</p> </li> <li> <p>Managed file transfer: This involves using statistics of operations carried out for files (e.g., creating, editing, moving, deleting, etc.) as input data for further processing.</p> </li> </ul> <p>e.g., In a sweet factory where the production bots publishes the production statistics in a file after each production run. Extracting the production statistics from the files for further processing can be considered reading files and extracting data. Checking whether a file is generated to indicate a completed production run, and checking whether a file is moved to a different location after its content is processed can be considered as managed file transfer.</p> <p>To understand how you can perform these file processing activities via the WSO2 Streaming Integrator, see the subtopics below.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#extracting-data-from-files","title":"Extracting data from files","text":"<p>WSO2 Streaming Integrator extracts data from files via the File Source. Once it extracts the data, it can publish it in a streaming manner so that other streaming applications that cannot read static data from files.</p> <p></p> <p>To further understand this, let's try out designing a solution for the Sweet Factory that needs to expose its production statitstics in the file generated by production bots in a streaming manner to the production manager so thyat the statistics can be viewed and analyzed in real time.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#selecting-the-files-to-read","title":"Selecting the file(s) to read","text":"<p>You can extract data from a single file or from multiple files in a directory. This is specified via the <code>file.uri</code> and <code>dir.uri</code> parameters as shown below:</p> <ul> <li> <p>Reading a single file</p> <p>In the following example, the <code>file.uri</code> parameter specifies the <code>productioninserts.csv</code> file in the <code>/Users/foo</code> directory as the file from which the source should extract data. </p> <pre><code>@source(type = 'file',file.uri = \"file:/Users/foo/productioninserts.csv\",\n    @map(type = 'csv'))\ndefine stream ProductionStream (name string, amount double);\n</code></pre> </li> <li> <p>Reading multiple files within a directory</p> <p>In the following example, the <code>dir.uri</code> parameter specifies the <code>/Users/foo/production</code> as the directory with the files from which the source extracts information. According to the following configuration, all the files in the directory are read.</p> <pre><code>@source(type = 'file',\n    dir.uri = \"file:/Users/foo/production\",\n    @map(type = 'csv'))\ndefine stream ProductionStream (name string, amount double);\n</code></pre> <p>If you want the source to read only specific files within the directory, you need to specify the required files via the <code>file.name.list</code> parameter as shown below.</p> <pre><code>@source(type = 'file', \n    dir.uri = \"file:/Users/foo/production\", \n    file.name.list = \"productioninserts.csv,AssistantFile.csv,ManagerFile.csv\",\n    @map(type = 'csv'))\ndefine stream ProductionStream (name string, amount double);\n</code></pre> </li> </ul> <p>The <code>file</code> source can read the selected file(s) in many modes. The available modes are as follows.</p> Mode Description <code>TEXT.FULL</code> Reads a text file completely in one reading. <code>BINARY.FULL</code> Reads a binary file completely in one reading. <code>BINARY.CHUNKED</code> Reads a binary file chunk by chunk. <code>LINE</code> Reads a text file line by line <code>REGEX</code> Reads a text file and extracts data using a regex. <p>You can specify the required mode via the <code>mode</code> parameter as shown in the example below.</p> <pre><code>@source(type = 'file',\n    file.uri = \"file:/Users/foo/productioninserts.csv\",\n    mode='LINE'\n    @map(type = 'csv'))\ndefine stream ProductionStream (name string, amount double);\n</code></pre>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#selecting-the-mode-in-which-the-file-is-read","title":"Selecting the mode in which the file is read","text":""},{"location":"guides/extracting-data-from-static-sources-in-real-time/#moving-or-deleting-files-after-readingfailure","title":"Moving or deleting files after reading/failure","text":"<p>If required, you can configure a <code>file</code> source to move or delete the files after they are read or after an attempt to read them has resulted in a failure. In both scenarios, the defauly action is to delete the file.</p> <p>e.g., If you want to move the <code>productioninserts.csv</code> file in the previous example after it is read, specify <code>move</code> as the value for <code>action.after.process</code>. Then add the <code>move.after.process</code> to specify the location to which the file should be moved after processing.</p> <p><pre><code>@source(type = 'file', file.uri = \"file:/Users/foo/productioninserts.csv\", \n    mode = \"line\",\n    tailing = \"false\",\n    action.after.process = \"move\", \n    move.after.process = \"file:/Users/processedfiles/productioninserts.csv\", \n    @map(type = 'csv'))\ndefine stream ProductionStream (name string, amount double);\n</code></pre> Here, you are  moving the <code>productioninserts.csv</code> file from the <code>/Users/foo</code> directory to the <code>/Users/processedfiles</code> after it is processed. </p> <p>Note that this extract also includes <code>tailing = \"false\"</code>. When tailing is enabled, the source reports any change made to the file immediately. Tailing is available only when the mode is set to <code>LINE</code> or <code>REGEX</code>, and it is enabled for these modes by default. Therefore, if you are using one of these modes and you want to set the <code>action.after.process</code> to <code>move</code> you need to disable tailing.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#supporting-siddhi-extension","title":"Supporting Siddhi extension","text":"<p>Reading content in files are supported via the file Siddhi extension.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#supporting-mappers","title":"Supporting mappers","text":"<p>The following mappers are supported for the File extension.</p> Transport Supporting Siddhi Extension <code>csv</code> csv <code>xml</code> xml <code>text</code> text"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#performing-managed-file-transfers","title":"Performing managed file transfers","text":"<p>WSO2 Streaming Integrator supports managed file transfers which involves detecting whether a file is created/modified/removed.</p> <p></p> <p>To check whether any file is created, modified or removed in a specific directory, you can configure a source of the <code>fileeventlistener</code> connected to an event stream as follows.</p> <p><pre><code>@source(type = 'fileeventlistener', \n    dir.uri = \"file:/Users/foo\",\n    @map(type = 'passThrough'))\ndefine stream FileListenerStream (filepath string, filename string, status string);\n</code></pre> The above configuration monitors whether any activity is generated for any file in the <code>/Users/foo</code> directory. If any file was created/modified/removed in the directory, an event is generated in the <code>FileListenerStream</code> stream. This event reports the name of the file, the file path, and the status of the file.</p> <p>If you want to monitor only activities generated for a specific file, you need to specify the names of the files via the  <code>file.name.list</code> parameter as shown in the example below.</p> <p><pre><code>@source(type = 'fileeventlistener', \n    dir.uri = \"file:/Users/foo\",\n    file.name.list = \"productioninserts 18.01.22.csv,materialconsumption.txt\",\n    @map(type = 'passThrough'))\ndefine stream FileListenerStream (filepath string, filename string, status string);\n</code></pre> The above source configuration generates an event in the <code>FileListenerStream</code> only when the <code>productioninserts 18.01.22.csv</code> and <code>materialconsumption.txt</code> are created, modified, and/or removed.</p> <p>If you want the directory to be monitored for file events periodically, you can specify the required monitoring interval in milliseconds via the <code>monitoring.interval</code> parameter as shown in the example below.</p> <p><pre><code>@source(type = 'fileeventlistener', \n        dir.uri = \"file:/Users/foo\", \n        monitoring.interval = \"200\", \n        file.name.list = \"productioninserts 18.01.22.csv,materialconsumption.txt\",\n        @map(type = 'passThrough'))\ndefine stream FileListenerStream (filepath string, filename string, status string);\n</code></pre> The above source configuration checks the <code>/Users/foo</code> directory every 200 milliseconds, and an event is generated in the <code>FileListenerStream</code> for each file transaction that involved creating/modifying/removing a file named <code>productioninserts 18.01.22.csv</code> or <code>materialconsumption.txt</code>.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#supporting-siddhi-extension_1","title":"Supporting Siddhi extension","text":"<p>Capturing file events is supported via the fileeventlistener Siddhi extension.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#try-out-an-example_1","title":"Try out an example","text":"<p>To try out reading the content of a file and file events, let's address the requirement of the example mentioned before of a sweet factory that publishes production details in a file. </p> <ol> <li> <p>Start and access WSO2 Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file and add the following Siddhi application. </p> <pre><code>    @App:name('LogFileEventsApp')\n    @App:description('Description of the plan')\n\n    @source(type = 'fileeventlistener', \n            dir.uri = \"file:/Users/production\", \n            @map(type = 'passThrough'))\n    define stream FileListenerStream (filepath string, filename string, status string);\n\n    @sink(type = 'log',\n        @map(type = 'passThrough'))\n    define stream LogFileEventsStream (filepath string, filename string, status string);\n\n    @info(name = 'Query')\n    from FileListenerStream \n    select * \n    insert into LogFileEventsStream;\n</code></pre> <p>Tip</p> <p>You can change the <code>Users/production</code> directory path to the path of a preferred directory in your machine.</p> <p>Then save the file as <code>LogFileEventsApp</code>.</p> <p>The above Siddhi Application monitors the <code>Users/production</code> directory and generates an event in the <code>FileListenerStream</code> if any file is created/modified/removed in it.</p> </li> <li> <p>Start the <code>LogFileEventsApp</code> Siddhi application you created by clicking on the play icon in the top panel.</p> <p></p> </li> <li> <p>Open a new file in a text editor of your choice, and save it as <code>productionstats.csv</code> in the <code>Users/production</code> directory.</p> <p>As a result, the following is logged in the Streaming Integrator Tooling terminal to indicate that the <code>productionstats.csv</code> is created in the <code>Users/production</code> directory.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - LogFileEventsApp : LogFileEventsStream : Event{timestamp=1603105747423, data=[/Users/production/productionstats.csv, productionstats.csv, created], isExpired=false} \n</code></pre> </li> <li> <p>Create and save the following Siddhi application in Streaming Integrator Tooling.</p> <pre><code>@App:name(\"FileReadingApp\")\n\n@source(type = 'file', \n    file.uri = \"file:/Users/production/productionstats.csv\", \n    mode = \"line\", \n    tailing = \"false\", \n    action.after.process = \"move\", \n    move.after.process = \"file:/Users/processedfiles/productionstats.csv\",\n    @map(type = 'csv'))\ndefine stream ProductionStream (name string, amount double);\n\n@sink(type = 'log',\n@map(type = 'passThrough'))\ndefine stream LogStream (name string, amount double);\n\n@info(name = 'Query')\nfrom ProductionStream \nselect * \ninsert into LogStream;\n</code></pre> </li> </ol> <p>This Siddhi application reads the content of the <code>/Users/production/productionstats.csv</code> file that you previously created and generates an event per row in the <code>ProductionStream</code> stream. After reading the file, the Siddhi app moves it to the <code>/Users/processedfiles</code> directory.</p> <ol> <li> <p>Start the <code>FileReadingApp</code> Siddhi application.</p> </li> <li> <p>Open the <code>/Users/production/productionstats.csv</code> file, add the following content to it, and then save the file.</p> <p><pre><code>Almond cookie,100.0\nBaked alaska,20.0\n</code></pre> The following is logged in the Streaming Integrator Tooling terminal:</p> <ul> <li> <p>For the <code>FileReadingApp</code> Siddhi application</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - FileReadingApp : LogStream : Event{timestamp=1603106006720, data=[Almond cookie, 100.0], isExpired=false} \n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - FileReadingApp : LogStream : Event{timestamp=1603106006720, data=[Baked alaska, 20.0], isExpired=false} \n</code></pre> <p>These logs show the content of the <code>productionstats.csv</code> file that is read by WSO2 Streaming Integrator.</p> </li> <li> <p>For the <code>LogFileEventsApp</code> Siddhi application</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - LogFileEventsApp : LogFileEventsStream : Event{timestamp=1603106006807, data=[/Users/production/productionstats.csv, productionstats.csv, removed], isExpired=false}\n</code></pre> <p>This log indicates that the WSO2 Streaming Integrator has detected that the 'productionstats.csv<code>file is removed from the</code>/Users/production` directory.    </p> </li> </ul> </li> </ol>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#consuming-data-from-cloud-storages","title":"Consuming data from cloud storages","text":"<p>WSO2 Streaming Integrator allows you to access data in cloud storages (such as Amazon Web Services - SQS, Amazon Web Services - S3, and Google Cloud Platform) and expose it in a streaming manner to applications that cannot access cloud storages. Cloud-based data sources generally cannot be tailed and therefore, it is challenging to expose changes to the stored data in real time. WSO2 Streaming Integrator addresses this issue by periodically polling the cloud storage, transferring the changes detected during those polling intervals to a file, and then tailing the file to expose the data in a streaming manner as illustrated in the following diagram.</p> <p></p> <p>The following is an example where the WSO2 Streaming Integrator retrieves messages from an SQS queue. A source of the <code>sqs</code>type is used for this purpose where you can provide the URL to the SQS queue that you want to subscribe to, and provide the access key and the secret to access it. the queue is polled periodically (i.e., every 5000 milliseconds). The source generates an event in the <code>InStream</code> stream for each message it retrieves.</p> <pre><code>@source(type='sqs',\n    queue='http://aws.sqs.queue.url',\n    access.key='aws.access.key',\n    secret.key='aws.secret.key',\n    region='us-east-2',\n    polling.interval='5000',\n    max.number.of.messages='10',\n    number.of.parallel.consumers='1',\n    purge.messages='true',\n    wait.time='2',\n    visibility.timeout='30',\n    delete.retry.interval='1000',\n    max.number.of.delete.retry.attempts='10',\n    @map(type='xml',enclosing.element=\"//events\",@attributes(symbol='symbol', message_id='trp:MESSAGE_ID') ))\ndefine stream InStream (symbol string, message_id string);\n</code></pre> <p>To transfer the content of the cloud storage to a file, add another stream with a sink of the <code>file</code> type as shown in the example below.</p> <p>Tip</p> <p>To learn more about publishing data to files, see Loading and Writing Data.</p> <pre><code>@sink(type = 'file', \n    file.uri = \"/Users/messages/messages.csv\",\n    @map(type = 'json'))\ndefine stream ExtractCloudDataStream (symbol string, message_id string);\n</code></pre> <p>Then write a query as follows to send all the events in the <code>InStream</code> stream to the <code>ExtractCloudDataStream</code> stream so that all the events extracted from the cloud can be transferred to the <code>/Users/messages/messages.csv</code> file.</p> <p><pre><code>@info(name = 'MoveCloudContentToFile')\nfrom InStream \nselect * \ninsert into ExtractCloudDataStream;\n</code></pre> The complete Siddhi application with the above configurations is as follows.</p> <pre><code>@App:name('CloudProcessingApp')\n@App:description('Description of the plan')\n\n@source(type = 'sqs', queue = \"http://aws.sqs.queue.url\", access.key = \"aws.access.key\", secret.key = \"aws.secret.key\", region = \"us-east-2\", polling.interval = \"5000\", max.number.of.messages = \"10\", number.of.parallel.consumers = \"1\", purge.messages = \"true\", wait.time = \"2\", visibility.timeout = \"30\", delete.retry.interval = \"1000\", max.number.of.delete.retry.attempts = \"10\",\n    @map(type = 'xml', enclosing.element = \"//events\",\n        @attributes(symbol = \"symbol\", message_id = \"trp:MESSAGE_ID\")))\ndefine stream InStream (symbol string, message_id string);\n\n@sink(type = 'file', \n    file.uri = \"/Users/messages/messages.csv\",\n    @map(type = 'json'))\n\ndefine stream ExtractCloudDataStream (symbol string, message_id string);\n\n@info(name = 'MoveCloudContentToFile')\nfrom InStream \nselect * \ninsert into ExtractCloudDataStream;\n</code></pre> <p>Now you can tail the data that is stored in the cloud by tailing the <code>/Users/messages/messages.csv</code> file. For more information about extracting information from files, see Extracting data from files.</p>"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#supported-cloud-platforms","title":"Supported cloud platforms","text":"<p>The following is a list of cloud platforms from which you can extract stored data via WSO2 Streaming Integrator.</p> Cloud Platform Extension AWS SQS SQS AWS Simple Cloud Storage (S3) S3 Google Cloud Storage GCS CosmosDB CosmosDB Azure Data Lake azuredatalake"},{"location":"guides/extracting-data-from-static-sources-in-real-time/#supported-mappers_1","title":"Supported mappers","text":"<p>Mappers determine the format in which the event is received. For information about transforming events by changing the format in which the data is received/published, see Transforming Data.</p> <p>WSO2 Streaming Integrator supports the following mappers for the cloud-based storages from which it extracts data.</p> Mapper Supporting Siddhi Extension <code>json</code> json <code>xml</code> xml <code>text</code> text <code>avro</code> avro <code>binary</code> binary <code>keyvalue</code> keyvalue <code>csv</code> csv <code>protobuf</code> protobuf"},{"location":"guides/handling-errors/","title":"Handling Errors","text":"<p>WSO2 Streaming Integrator allows you to handle any errors that may occur when handling streaming data in a graceful manner. </p> <p>The possible actions that you can take for events with errors are:</p> <ul> <li>Storing and replaying</li> <li>Logging</li> <li>Streaming</li> <li>Waiting</li> </ul> <p>This section explains the different types of errors that can occur and how they can be handled.</p>"},{"location":"guides/handling-errors/#storing-and-replaying-events-with-errors","title":"Storing and replaying events with errors","text":"<p>This involves storing the events with errors in the error store and then replaying them. </p> <p>To do this, you need to enable the error store in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file by adding the following configuration.</p> <p><pre><code>error.store:\n  enabled: true\n  bufferSize: 1024\n  dropWhenBufferFull: true\n  errorStore: org.wso2.carbon.streaming.integrator.core.siddhi.error.handler.DBErrorStore\n  config:\n    datasource: ERROR_STORE_DB\n    table: ERROR_STORE_TABLE\n</code></pre> - <code>bufferSize</code> denotes the size of the ring buffer that is used in the disruptor when publishing events to the error store. This has to be a power of two. If not, it throws an exception during initialization. The default buffer size is <code>1024</code>. - If the <code>dropWhenBufferFull</code> is set to <code>true</code>, the event is dropped when the capacity of the ring buffer is insufficient.</p> <p>Once the error store is enabled, you need to add a configuration for the data source you are connecting to the error store (in the above example <code>ERROR_STORE_DB</code>) in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file. Then you can create the database in which you want to store the events with errors and link to it from the data source.</p> <p>Note</p> <p>If you are configuring an Oracle datasource where the Oracle server version is less than 12 ,then you need to create corresponding table (e.g., <code>ERROR_STORE_TABLE</code>) with the following syntax before starting the server.     <pre><code>CREATE TABLE ERROR_STORE_TABLE (id NUMBER(10) NOT NULL, timestamp LONG, siddhiAppName VARCHAR(100), \nstreamName VARCHAR(100), event BLOB, cause VARCHAR(1000), stackTrace BLOB, originalPayload BLOB, \nerrorOccurrence VARCHAR(50), eventType VARCHAR(50), errorType VARCHAR(50)); \nALTER TABLE ERROR_STORE_TABLE ADD (CONSTRAINT err_store_pk PRIMARY KEY (id)); \nCREATE SEQUENCE err_store_seq START WITH 1; CREATE OR REPLACE TRIGGER err_store_trigger BEFORE INSERT ON ERROR_STORE_TABLE \nFOR EACH ROW BEGIN SELECT err_store_seq.NEXTVAL INTO   :new.id FROM   dual; END;\n</code></pre> You can run the queries given above separately. If the Oracle server version is 12 or more, run the query given below.     <pre><code>CREATE TABLE ERROR_STORE_TABLE (id NUMBER(10) GENERATED ALWAYS as IDENTITY(START with 1 INCREMENT by 1) NOT NULL, \ntimestamp LONG, siddhiAppName VARCHAR(100), streamName VARCHAR(100), event BLOB, cause VARCHAR(1000), stackTrace BLOB, \noriginalPayload BLOB, errorOccurrence VARCHAR(50), eventType VARCHAR(50), errorType VARCHAR(50))\n</code></pre> Alternatively, for versions 12 or greater, the fix for the query is available via WUM or Updates 2.0. If you have an updated pack you do not need to run this manually.  This capability is released as a product update on 01/10/2021. If you don't already have this update, you can get the latest updates here.   </p> <p>This can be used with the following:</p> <ul> <li> <p>Siddhi Streams</p> <p></p> <p>This on-error action can be specified for a  stream via the <code>@OnError()</code> annotation. </p> <p>The Siddhi query uses the <code>cast(\"abc\", \"double\")</code> which intentionally generates an error for testing purposes. </p> <pre><code>@OnError(action='STORE')\ndefine stream StreamA (symbol string, amount double);\n\nfrom StreamA[cast(\"abc\", \"double\") &gt; 100]\ninsert into StreamB;\n</code></pre> </li> </ul> <p>If you do not specify the on-error action for a stream  via the <code>@OnError()</code> annotation, the event is logged and dropped.</p> <ul> <li> <p>Sinks</p> <p></p> <p>You can specify an on-error action by including the <code>on-error</code> parameter within the sink configuration as shown below.</p> <pre><code>@sink(type = 'http', on.error='STORE', blocking.io='true', \n      publisher.url = \"http://localhost:8090/unavailableEndpoint\", \n      method = \"POST\", @map(type = 'json'))\ndefine stream StreamA (name string, volume long);\n</code></pre> </li> <li> <p>Source mappers</p> </li> </ul> <p>If the <code>error.store</code> is enabled in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file, mapping errors are automatically added to the error store.</p>"},{"location":"guides/handling-errors/#try-it-out","title":"Try it out","text":"<p>To understand how you can store erroneous events in a real world scenario, consider a sweet factory where production statistics are written into a file. The same file content needs to be copied into another file for the factory manager. In order to make sure that none of the production records is missed in the manager's copy, the factory foreman needs the erroneous events saved in the error store so that he can check the errors, correct than and then replay them.</p> <p>To try out storing errors in the store, follow the steps below:</p> <ol> <li> <p>To configure the error store in which you can store events, follow the steps below.</p> <ol> <li> <p>Start the MySQL server if it is not already started.</p> </li> <li> <p>Create a new database named <code>siddhierrorstoredb</code>; by issuing the following command in the MySQL console.</p> <p><code>mysql&gt; create database siddhierrorstoredb;</code></p> </li> <li> <p>To switch to the new database, issue the following command.</p> <p><code>mysql&gt; use siddhierrorstoredb;</code></p> </li> <li> <p>To enable the error store, open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file and add a configuration as follows:</p> <p><pre><code>error.store:\n  enabled: true\n  bufferSize: 1024\n  dropWhenBufferFull: true\n  errorStore: org.wso2.carbon.streaming.integrator.core.siddhi.error.handler.DBErrorStore\n  config:\n    datasource: SIDDHI_ERROR_STORE_DB\n    table: SIDDHI_ERROR_STORE_TABLE\n</code></pre>     5. The above configuration refers to a data source named <code>SIDDHI_ERROR_STORE_DB</code>. Define this data source as follows under <code>Data sources</code> in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>- name: SIDDHI_ERROR_STORE_DB\n  description: The datasource used for Siddhi error handling feature\n  jndiConfig:\n    name: jdbc/SiddhiErrorStoreDB\n  definition:\n    type: RDBMS\n    configuration:\n      jdbcUrl: 'jdbc:mysql://localhost:3306/siddhierrorstoredb?useSSL=false'\n      username: root\n      password: root\n      driverClassName: com.mysql.jdbc.Driver\n      minIdle: 5\n      maxPoolSize: 50\n      idleTimeout: 60000\n      connectionTestQuery: SELECT 1\n      validationTimeout: 30000\n      isAutoCommit: false\n</code></pre> </li> </ol> </li> <li> <p>Download the productions.csv file and save it in a location of your choice in your machine.</p> </li> <li> <p>Create a Siddhi application as follows and deploy it in the Streaming Integrator server.</p> <pre><code>@App:name(\"CopyingProductionStatsApp\")\n\n@source(type='file', mode='LINE',\n   file.uri='file:/Users/foo/productions.csv',\n   tailing='true',\n   @map(type='csv'))\n@onError(action='STORE') \ndefine stream ProductionStream (name string,amount double);\n\n@sink(type='file',\n    on.error='STORE',\n    file.uri = \"/Users/foo/manager/managercopy.csv\",\n    @map(type='csv'))\ndefine stream CopyProductionStream (name string,amount double);\n\nfrom ProductionStream\nselect *\ninsert into CopyProductionStream;\n</code></pre> </li> </ol> <p>The above Siddhi application simply copied content from one file to another. Any mapping errors generated when you run it can be stored in the error store you configured.</p> <ol> <li> <p>Start the MySQL server. If the Streaming Integrator server and Streaming Integrator Tooling are not already started, start them too.</p> </li> <li> <p>Access and open the Streaming Integrator Tooling.</p> </li> <li> <p>To open the Error Store Explorer, click Tools and then click Error Store Explorer.</p> <p>The Error Store Explorer opens as shown below. </p> <p></p> </li> <li> <p>Click Connect to Server. Then enter information as follows:</p> <p>To check the port of the Streaming Integrator Server, Open /conf/server/deployment.yaml file. Under Listener Configurations of wso2.transport.http, locate the listener configuration with msf4j-https as the ID and specify its port as shown in the extract below. <p></p> Parameter Value Host <code>localhost</code> Port <code>9443</code> Username <code>admin</code> Password <code>admin</code> <p>Then click Connect.</p> <li> <p>Now let's generate some errors for testing purposes as follows:</p> <p>Generating a sink error</p> <ol> <li> <p>Be sure  that the file path you specified in the sink configuration is not actually available. For example, in this scenario, you can make sure that the <code>manager</code> sub-directory in the <code>/Users/foo/manager/managercopy.csv</code> path is not available.</p> </li> <li> <p>Now create a file event by entering a new row in the input file (in this scenario, <code>/Users/foo/productions.csv</code>) as follows.</p> <p><code>Crossaints,90.0</code></p> </li> <li> <p>Access Streaming Integrator Tooling and click Tools -&gt; Error Store Explorer. Then in the Siddhi App section, select CopyingProductionStatsApp Siddhi application from the drop down list. The error store displays the sink error as follows.</p> <p>![Sink Error](https://wso2.github.io/docs-si/images/handling-errors/sink-error.png</p> </li> <li> <p>To correct the error and replay it, follow the procedure below:</p> <ol> <li> <p>Correct the file path. For example, in this scenario, you can add a directory named <code>manager</code> in the <code>Users/foo</code> directory so that the <code>/Users/foo/manager/</code> is a path that actually exists, enabling WSO2 Streaming Integrator to generate the <code>managercopy.csv</code> in it.</p> </li> <li> <p>In the Error Store Explorer dialog box, click Replay for the event.</p> <p>As a result, the Error Entry dialog box closes, and the Error Store Explorer dialog box does not display any errors.</p> </li> </ol> </li> </ol> <p>Generating a mapping error</p> <ol> <li> <p>Open the input file (in this scenario, <code>/Users/foo/productions.csv</code>) and enter a new row in it in the wrong format as shown below.</p> <p><code>Fudge,Gateaux,80.0</code></p> <p>The above entry is erroneous because it has two string values instead of one.</p> </li> <li> <p>Access Streaming Integrator Tooling and click Tools -&gt; Error Store Explorer. Then in the Siddhi App section, select CopyingProductionStatsApp Siddhi application from the drop down list. The error store displays the mapping error as follows.</p> <p></p> </li> <li> <p>To correct this error and replay the event, follow the procedure below:</p> <ol> <li> <p>Click Detailed Info. This opens the Error Entry dialog box where the event is displayed in a text field as shown below.</p> <p></p> </li> <li> <p>In the text field, edit the event and correct the format by removing one of the string values.</p> <p>As a result, the Error Entry dialog box closes, and the Error Store Explorer dialog box does not display any errors.</p> </li> </ol> </li> </ol> </li>"},{"location":"guides/handling-errors/#logging-events-with-errors","title":"Logging events with errors","text":"<p>This involves logging the event with details of the error and then dropping it. This can be used with the following:</p> <ul> <li> <p>Siddhi Streams</p> <p></p> <p>You can specify this on-error action for streams via the <code>@OnError</code> annotation as shown below.</p> <p><pre><code>@OnError(action='LOG')\ndefine stream StreamA (symbol string, volume long);\n</code></pre> If you do not specify the on-error action for a stream  via the <code>@OnError()</code> annotation, the event is logged and dropped.</p> </li> <li> <p>Sinks</p> <p></p> <p>You can specify this on-error action by including the <code>on-error</code> parameter within the sink configuration as shown below. <pre><code>@sink(type = 'http', on.error='LOG', blocking.io='true', \n      publisher.url = \"http://localhost:8090/unavailableEndpoint\", \n      method = \"POST\", @map(type = 'json'))\ndefine stream TestPublisherStream (symbol string, volume long);\n</code></pre>    If you do not specify the on-error action for a stream  via the <code>on.error</code> parameter, the event is logged and dropped.</p> </li> <li> <p>Source mappers </p> <p>Logging is the default on-error action for source mappers when the error store is not enabled in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> </li> </ul>"},{"location":"guides/handling-errors/#try-it-out_1","title":"Try it out","text":"<p>To try out logging events with errors, consider the same example previously used where production statistics is copied from one file to another.</p> <ol> <li> <p>In Streaming Integrator Tooling, open the <code>CopyingProductionStatsApp</code>  Siddhi application that you created in the Storing events with errors section and update it as follows. Then deploy it in the Streaming Integrator server.</p> <p><pre><code>@App:name(\"CopyingProductionStatsApp\")\n\n@source(type='file', mode='LINE',\n   file.uri='file:/Users/foo/productions.csv',\n   tailing='true',\n   @map(type='csv'))\n@onError(action='LOG') \ndefine stream ProductionStream (name string,amount double);\n\n@sink(type='file',\n    on.error='LOG',\n    file.uri = \"/Users/foo/manager/managercopy.csv\",\n    @map(type='csv'))\ndefine stream CopyProductionStream (name string,amount double);\n\nfrom ProductionStream\nselect *\ninsert into CopyProductionStream;\n</code></pre>    Here, the on error action is changed to <code>LOG</code> for both the stream and the sink.</p> </li> <li> <p>To generate a sink error, give an incorrect destination path for your output file. For example, in this scenario, be sure that the <code>manager</code> directory does not exist in the <code>/Users/foo/manager/managercopy.csv</code> path. </p> <p>Then generate an input event by adding the foillowing row in the <code>Users/foo/productions.csv</code> input file.</p> <p><code>Crossaints,90.0</code></p> <p>As a result, the following is logged in the Streaming Integrator terminal.</p> <pre><code>ERROR {io.siddhi.core.stream.output.sink.Sink} - Error on 'CopyingProductionStatsApp'. Dropping event at Sink 'file' at 'CopyProductionStream' as its still trying to reconnect!, events dropped 'Fudge,100.0\n</code></pre> </li> <li> <p>To generate a mapping error, open the input file (in this scenario, <code>/Users/foo/productions.csv</code>) and enter a new row in it in the wrong format as shown below.</p> <p><code>Fudge,Gateaux,80.0</code></p> <p>As a result, the following is logged in the Streaming Integrator console.</p> <pre><code>ERROR {io.siddhi.extension.map.csv.sourcemapper.CSVSourceMapper} - Incompatible data format. Because value of amount isGateaux and attribute type is DOUBLE in the stream ProductionStream of siddhi csv input mapper.\n</code></pre> </li> </ol>"},{"location":"guides/handling-errors/#streaming-events-with-errors","title":"Streaming events with errors","text":"<p>This can be used with the following:</p> <ul> <li> <p>Siddhi Streams</p> <p></p> <p>This on-error action can be specified for a  stream via the <code>@OnError()</code> annotation. </p> <p>In the following example, the Siddhi query uses the <code>cast(\"abc\", \"double\")</code> function that intentionally generates an error for testing purposes.</p> <p><pre><code>@OnError(action='STREAM')\ndefine stream StreamA (symbol string, amount double);\n\nfrom StreamA[cast(\"abc\", \"double\") &gt; 100]\ninsert into StreamB;\n\n-- consumes from the fault stream\nfrom !StreamA#log(\"Error Occured\")\nselect symbol, amount, _error\ninsert into tempStream;\n</code></pre> If you do not specify the on-error action for a stream  via the <code>@OnError()</code> annotation, the event is logged and dropped.</p> </li> <li> <p>Sinks</p> <p></p> <p>You can specify this on-error action by including the <code>on-error</code> parameter within the sink configuration as shown below.</p> <pre><code>@OnError(action='STREAM')\n@sink(type = 'http', on.error='STREAM', blocking.io='true', \n      publisher.url = \"http://localhost:8090/unavailableEndpoint\", \n      method = \"POST\", @map(type = 'json'))\ndefine stream StreamA (name string, volume long);\n\n-- consumes from the fault stream\nfrom !StreamA#log(\"Error Occured\")\nselect symbol, volume, _error\ninsert into tempStream;\n</code></pre> </li> </ul> <p>Note</p> <p>This on.error action is not applicable for source mappers.</p>"},{"location":"guides/handling-errors/#try-it-out_2","title":"Try it out","text":"<p>To try out streaming events with errors, follow the procedure below.</p> <ol> <li> <p>In Streaming Integrator Tooling, open the <code>CopyingProductionStatsApp</code>  Siddhi application that you created in the Storing events with errors section and update it as follows. Then deploy it in the Streaming Integrator server.</p> <p><pre><code>@App:name(\"CopyingProductionStatsApp\")\n\n@source(type='file', mode='LINE',\n   file.uri='file:/Users/foo/productions.csv',\n   tailing='true',\n   @map(type='csv'))\n@onError(action='STREAM') \ndefine stream ProductionStream (name string,amount string);\n\n@sink(type='file',\n    on.error='STREAM',\n    file.uri = \"/Users/foo/managercopy.csv\",\n    @map(type='csv'))\ndefine stream CopyProductionStream (name string,amount double);\n\n@info(name = 'FilterEvents')\nfrom ProductionStream[cast(amount, \"double\") &gt; 100]\nselect name, cast(amount, \"double\") as amount\ninsert into CopyProductionStream;\n\n@info(name = 'streamerrors')\nfrom !ProductionStream#log(\"Error Occured\")\nselect name, amount, _error\ninsert into ErrorStream;\n</code></pre>    Here, the on error action is changed to <code>STREAM</code> for both the stream and the sink. Any stream errors that occur for the <code>ProductionStream</code> are directed to an error stream named <code>!ProductionStream</code>. The events with errors that are sent to the <code>!ProductionStream</code> stream have the two attributes of the <code>ProductionStream</code> input stream, and in addition, an attribute named <code>_error</code> to capture the details of the error. A log is connected to it with the prefix <code>Error Occurred</code></p> </li> </ol> <p>The <code>ProductionStream</code> stream receives events with two string values each. The <code>FilterEvents</code> query casts value for the <code>amount</code> attribute as a value of the <code>double</code> type and filters events where the value for this field is greater than <code>100</code>. This results in an error when events are sent to this Siddhi application. </p> <ol> <li> <p>To generate an error, add the following row with two string values in the <code>Users/foo/productions.csv</code> input file.</p> <p><code>Crossaints,abc</code></p> <p>As a result, the following is logged in the Streaming Integrator terminal.</p> <pre><code>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - CopyingProductionStatsApp: Error Occured, StreamEvent{ timestamp=1604408058031, beforeWindowData=null, onAfterWindowData=null, outputData=[Crossaints, abc, java.lang.ClassCastException: class java.lang.String cannot be cast to class java.lang.Double (java.lang.String and java.lang.Double are in module java.base of loader 'bootstrap')], type=CURRENT, next=null} \n</code></pre> </li> </ol>"},{"location":"guides/handling-errors/#waiting","title":"Waiting","text":"<p>This on-error action is only applicable to errors that occur when publishing data, and therefore it can be only used with sinks. Here, the thread waits in the <code>back-off and re-trying</code> state, and reconnects once the connection is re-established.</p> <pre><code>@sink(type = 'http', on.error='WAIT', blocking.io='true', \n      publisher.url = \"http://localhost:8090/unavailableEndpoint\", \n      method = \"POST\", @map(type = 'json'))\ndefine stream StreamA (name string, volume long);\n</code></pre>"},{"location":"guides/loading-and-writing-date/","title":"Loading and Writing Data","text":"<p>Loading and writing data involves publishing the data in a destination where it can be extracted again at any given time for further processing. WSO2 Streaming Integrator supports loading and writing data to databases, files, and cloud storages.</p>"},{"location":"guides/loading-and-writing-date/#loading-data-to-databases","title":"Loading data to databases","text":"<p>WSO2 Streaming allows you to load data into databases so that the data can be available in a static manner for further processing. You can load the data received from another source unchanged or after processing it. This is achieved by defining Siddhi tables that are connected to database tables, and then writing queries to publish data into those tables so that it can be transferred to the connected database tables.</p> <p></p> <p>To understand this, consider an example of an sweet shop that needs to save all its sales records in a database table. To address this, you can write a Siddhi application as follows:</p> <ul> <li> <p>Define a table</p> <p>In this example, let's define a table named <code>SalesRecords</code> as follows:</p> <p><pre><code>@primaryKey('ref')\n@index('name')\n@store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/SalesRecordsDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\")\ndefine table SalesRecords(ref string, name string, amount int);\n</code></pre>   The above table definition captures sales records. The details captured in each sales record includes the transaction reference (<code>ref</code>), the name of the product (<code>name</code>), and sales amount (<code>amount</code>). The <code>ref</code> attribute is the primary key because there cannot be two or more records with the same transaction reference. The <code>name</code> attribute is an index attribute.</p> </li> </ul> <p>A data store named <code>SalesRecordsDB</code> is connected to this table definition via the <code>@store</code> annotation. This is the data store to which you are loading data.</p> <ul> <li> <p>Add a query</p> <p>You need to add a query to specify how events are selected to be inserted into the table you defined. You can add this query as shown below. </p> <p><pre><code>from SalesRecordsStream\nselect *\ninsert into SalesRecords;\n</code></pre> The above query gets all the input events from the <code>SalesRecordsStream</code> stream and loads them to the <code>SalesRecords</code> table.</p> </li> </ul>"},{"location":"guides/loading-and-writing-date/#try-it-out","title":"Try it out","text":"<p>To try out the example given above, follow the steps below:</p> <ol> <li> <p>Set up MySQL as follows:</p> <ol> <li> <p>Download and install MySQL.    </p> </li> <li> <p>Start the MySQL server, create the database and the database table you require as follows:</p> <ol> <li> <p>To create a new database, issue the following MySQL command.</p> <pre><code>CREATE SCHEMA sales;\n</code></pre> </li> <li> <p>Switch to the <code>sales</code> database by issuing the following command.</p> </li> </ol> <pre><code>use sales;\n</code></pre> <ol> <li> <p>Create a new table, by issuing the following command.        </p> <pre><code>CREATE TABLE sales.SalesRecords (\n  ref INT NOT NULL AUTO_INCREMENT,\n  name VARCHAR(45) NULL,\n  amount INT NULL,\n  PRIMARY KEY (ref));\n</code></pre> </li> </ol> </li> </ol> </li> <li> <p>Start and access WSO2 Streaming Integrator Tooling.</p> </li> <li> <p>Install the <code>RDBMS - MySQL</code> extension in Streaming Integrator tooling. For instructions to install an extension, see Installing Siddhi Extensions.</p> </li> <li> <p>Open a new file in WSO2 Streaming Integrator Tooling and add the following Siddhi content to it.</p> <pre><code>@App:name('SalesApp')\n\ndefine stream SalesRecordsStream (name string, amount int);\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/sales?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\n@primaryKey('ref' )\n@index('name' )\ndefine table SalesRecords (name string, amount int);\n\nfrom SalesRecordsStream \nselect * \ninsert into SalesRecords;\n</code></pre> </li> </ol> <p>Save the Siddhi application.   </p> <ol> <li> <p>Simulate an event with the following values for the <code>SalesRecordsStream</code> stream of the <code>SalesApp</code> Siddhi application. For instructions to simulate events, see Testing Siddhi Applications.</p> Attribute Value ref <code>AA000000000001</code> name <code>fruit cake</code> amount <code>100</code> </li> <li> <p>To check whether the <code>sales</code> mysql table is updated, issue the following command in the MySQL server.</p> <p><code>select * from SalesRecords;</code></p> <p>The table is displayed as follows, indicating that the event you generated is added as a record.</p> <p></p> </li> </ol>"},{"location":"guides/loading-and-writing-date/#publishing-data-on-demand-via-store-queries","title":"Publishing data on demand via store queries","text":"<p>To understand how to publish data on demand, see Correlating Data</p>"},{"location":"guides/loading-and-writing-date/#supported-databases","title":"Supported databases","text":"<p>WSO2 Streaming supports the following database types via Siddhi extensions:</p> Database Type Siddhi Extension RDBMS rdbms MongoDB mongodb Redis redis elasticsearch elasticsearch"},{"location":"guides/loading-and-writing-date/#supported-mappers","title":"Supported Mappers","text":"<p>Mappers determine the format in which the event is published. For information about transforming events by changing the format in which the data is published, see Transforming Data.</p> <p>The mapper available for loading data to databases is Keyvalue.</p>"},{"location":"guides/loading-and-writing-date/#writing-data-to-files","title":"Writing data to files","text":"<p>WSO2 Streaming allows you to write data into files so that the data can be available in a static manner for further processing. You can write the data received from another source unchanged or after processing it. This is achieved by defining an output stream and then connecting a sink of the file type.</p> <p></p> <p>To understand this, consider the example of a lab with a sensor that reports the temperature at different times. These temperature readings need to be saved in a file for reference when carrying out further analysis.  </p> <p>To address the above requirement via WSO2 Streaming Integrator, define an output stream and connect a file source to it as shown below.</p> <p><pre><code>@sink(type = 'file', \n    file.uri = \"/users/temperature/temperature.csv\",\n    @map(type = 'passThrough'))\ndefine stream TemperatureLogStream (timestamp long, temperature int);\n</code></pre> Here, any event directed to the <code>TemperatureLogStream</code> is written into the <code>/users/temperature/temperature.csv</code> file.</p>"},{"location":"guides/loading-and-writing-date/#try-it-out_1","title":"Try it out","text":"<p>To try out the above example by including the given output stream and the sink configuration in a complete Siddhi application, follow the steps below:</p> <ol> <li> <p>Start and access WSO2 Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file and copy the following Siddhi Application to it.</p> <p><pre><code>@App:name('LabTemperatureApp')\n\ndefine stream LabTemperatureStream (timestamp long, temperature int);\n\n@sink(type = 'file', \n    file.uri = \"/users/temperature/temperature.csv\",\n    @map(type = 'passThrough'))\ndefine stream TemperatureLogStream (timestamp long, temperature int);\n\nfrom LabTemperatureStream \nselect * \ninsert into TemperatureLogStream;\n</code></pre>    This Siddhi application includes the file sink from the previous example.</p> <p>Tip</p> <p>If required, you can replace the value for the <code>file.uri</code> parameter to a preferred location in your machine.</p> </li> <li> <p>Simulate an event with the following values for the <code>LabTemperatureStream</code> stream of the <code>LabTemperatureApp</code> Siddhi application. For instructions to simulate events, see Testing Siddhi Applications.  </p> Attribute Value timestamp <code>1603461542000</code> temperature <code>27</code> </li> <li> <p>Open the <code>/users/temperature/temperature.csv</code> file. It contains a line as shown below.</p> <p></p> <p>This is the event that you simulated that has been written into the file by WSO2 Streaming Integrator.</p> </li> </ol>"},{"location":"guides/loading-and-writing-date/#supported-mappers_1","title":"Supported Mappers","text":"<p>Mappers determine the format in which the event is published. For information about transforming events by changing the format in which the data is published, see Transforming Data.</p> <p>The following mappers are supported for the File extension.</p> Transport Supporting Siddhi Extension <code>csv</code> csv <code>xml</code> xml <code>text</code> text"},{"location":"guides/loading-and-writing-date/#storing-data-in-cloud-storage","title":"Storing data in Cloud storage","text":"<p>WSO2 SI allows you to store data in cloud storages in a static manner so that it can be accessed for further processing. The data you store can be the unprocessed data you received from another source or output data generated by WSO2 Streaming Integrator. This is achieved by defining an output stream and then connecting a sink of a type that links to cloud storages.</p> <p></p> <p>To understand this, To understand this, consider the example of a lab with a sensor that reports the temperature at different times. These temperature readings need to be uploaded to a cloud-based application for reference when carrying out further analysis. </p> <p>To address the above requirement with WSO2 Streaming Integrator, configure an output stream, and connect a sink of the <code>google-cloud-storage</code> type to it as shown below.</p> <pre><code>@sink(type='google-cloud-storage', credential.path='&lt;credential.path&gt;', bucket.name='temperaturelog',\n      object.name='temperature-object-{{ name }}',\n    @map(type='text'))\ndefine stream TemperatureLogStream (timestamp long, temperature int);\n</code></pre> <p>Here, all events in the <code>TemperatureLogStream</code> stream are stored in a Google cloud bucket named <code>temperaturelog</code>. The <code>credential path</code> parameter needs to specify the location in your machine where you have stored the credentials file generated by Google Cloud Service.</p>"},{"location":"guides/loading-and-writing-date/#try-it-out_2","title":"Try it out","text":"<p>To try out the above example, follow the steps below:</p> <ol> <li> <p>Set up a Google cloud as follows:</p> <ol> <li> <p>Create an account in Google Cloud.</p> </li> <li> <p>Download the credential file that is generated through the GCP console and save it in a directory of your choice. For more information, see Google Cloud Authentication Documentation.</p> </li> <li> <p>Create a bucket named <code>temperaturelog</code> in the Google Cloud Console.</p> </li> </ol> </li> <li> <p>Start and access WSO2 Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file and copy the following Siddhi Application to it.</p> <pre><code>@App:name('LabTemperatureApp')\n\ndefine stream LabTemperatureStream (timestamp long, temperature int);\n\n@sink(type='google-cloud-storage', credential.path='&lt;credential.path&gt;', bucket.name='temperaturelog',\n      object.name='temperature-object-{{ name }}',\n    @map(type='text'))\ndefine stream TemperatureLogStream (timestamp long, temperature int);\n\nfrom LabTemperatureStream \nselect * \ninsert into TemperatureLogStream;\n</code></pre> </li> </ol> <p>Save the Siddhi application. </p> <p>The above Siddhi application gets all the events in the <code>LabTemperatureStream</code> stream and inserts them into the <code>TemperatureLogStream</code> stream so that they can be stored in the <code>temperaturelog</code> bucket in the Google Cloud Console via the sink connected to the <code>TemperatureLogStream</code> stream.</p> <ol> <li> <p>Simulate an event to the <code>LabTemperatureStream</code> stream of the <code>LabTemperatureApp</code> Siddhi application with the following values via the Event Simulator tool. For instructions to simulate events, see Testing Siddhi Applications. </p> Attribute Value timestamp <code>1603461542000</code> temperature <code>27</code> <p>This generates an output event that is updated in the <code>temperaturelog</code>bucket.</p> </li> </ol>"},{"location":"guides/loading-and-writing-date/#supported-cloud-platforms","title":"Supported cloud platforms","text":"<p>The following is a list of cloud platforms in which you can store data via WSO2 Streaming Integrator.</p> Cloud Platform Extension AWS SQS SQS AWS Simple Cloud Storage (S3) S3 Google Cloud Storage GCS CosmosDB CosmosDB Azure Data Lake azuredatalake GCS GCS"},{"location":"guides/loading-and-writing-date/#supported-mappers_2","title":"Supported mappers","text":"<p>Mappers determine the format in which the event is received. For information about transforming events by changing the format in which the data is received/published, see Transforming Data.</p> <p>WSO2 Streaming Integrator supports the following mappers for the cloud-based storages in which it stores data.</p> Mapper Supporting Siddhi Extension <code>json</code> json <code>xml</code> xml <code>text</code> text <code>avro</code> avro <code>binary</code> binary <code>keyvalue</code> keyvalue <code>csv</code> csv <code>protobuf</code> protobuf"},{"location":"guides/performing-etl-tasks/","title":"Performing ETL Operations","text":"<p>ETL (Extract Transform Load) is a form of data processing that involves extracting data from one or multiple sources (typically from multiple sources), transforming data to generate the required output, and then loading that output to one or more destinations to make it available for further processing.</p> <p>The following topics explain how WSO2 Streaming Integrator performs ETL operations, and how it addresses the modern business requirements relating to ETL operations.</p>"},{"location":"guides/performing-etl-tasks/#performing-etl-in-real-time","title":"Performing ETL in real time","text":"<p>Traditional ETL used batch processing method where the ETL tasks were executed periodically on static data (e.g., database records). Due to this, the required output was also generated periodically and it involved a lot of waiting time. However, modern businesses often carry out a high volume of transactions in real time. This requires the ETL operations also to be executed in real time and to generate the results in real time so that quick decisions can be made based on this output.</p> <p>WSO2 Streaming Integrator executes ETL operations on streaming data and generates results in a streaming manner. To understand how this is done, consider a scenario where a sweet factory purchases sugar and flour from two suppliers. Each supplier publishes information about each consignment of raw material it supplies in a file. The events published by the sugar supplier does not include the product name. The flour supplier also supplies goods other than raw material. The head office needs all the purchase records saved in a database. The details of each consignment that needs to be saved as a purchase record includes the transaction number, product name, unit price and the amount.</p> <p>To maintain a database with purchase records, you can create an ETL application as follows:</p> <p><pre><code>@App:name('ManagingStocksApp')\n\n@App:description('Maintains purchase records')\n\n@source(type='file', mode='LINE',\nfile.uri='file:/Users/foo/SugarSupply.csv',\ntailing='true',\n@map(type='csv'))\ndefine stream SugarSupplyStream (transNo string, price double, amount double);\n\n@source(type='file', mode='LINE',\nfile.uri='file:/Users/foo/FlourSupply.csv',\ntailing='true',\n@map(type='csv'))\ndefine stream FlourSupplyStream (transNo string, name string, price double, amount double);\n\n@primaryKey('transNo')\n@index ('name')\n@store (type='rdbms', datasource='RAW_MATERIAL_DB')\ndefine table PurchaseRecords(transNo string, name string, price double, amount double);\n\n@info(name = 'CleaningSugarSupplyData')\nfrom SugarSupplyStream\nselect transNo, \"sugar\" as name, price, amount\nupdate or insert into PurchaseRecords\non PurchaseRecords.transNo == transNo;\n\n@info(name = 'CleaningFlourSupplyData')\nfrom FlourSupplyStream [name == \"flour\"]\nselect *\nupdate or insert into PurchaseRecords\non PurchaseRecords.transNo == transNo;\n</code></pre> The following diagram summarizes the ETL flow of the above Siddhi application.</p> <p></p> <p>Here, you are extracting the input by tailing the <code>SugarSupply.csv</code> and <code>FlourSupply.csv</code> files in which suppliers publish details of their supplies in real time. This is done via a file source.</p> <p>To transform the input data, you are performing two cleansing activities. In the first query named <code>CleaningSugarSupplyData</code>, you are introducing a new attribute named <code>name</code> with <code>sugar</code> as the value for all the events received from the sugar supplier via the <code>SugarSupply.csv</code> file. In the second query, you are filtering the events received from the flour supplier to save only details of flour purchases (i.e., because the same supplier also supplies labels).</p> <p>You are loading the output by performing <code>update or insert</code> operations to insert events from both suppliers into the <code>PurchaseRecords</code> database table after carrying out the data cleansing described above. This enables the cleansed data to be used for further processing.</p>"},{"location":"guides/performing-etl-tasks/#integrating-heterogeneous-data-sources","title":"Integrating heterogeneous data sources","text":"<p>In the previous example, you extracted information from two sources of the same type (i.e., files). In real world business scenarios, you often need to extract data from multiple sources of different types. </p> <p>To understand how this requirement can be addressed via the WSO2 Streaming Integrator, let's try out consuming events from both a file and a database at the same time.</p> <p>Assume that the Head Office of the Sweet Factory also maintains a record of the current stock of each material in a database table named <code>StockRecords</code>. To keep the stock updated, each purchase of a raw material needs to be added to the existing stock, and each dispatch to the factory needs to be deducted from the existing stock. The material dispatches are recorded in a file. To achieve this, you need to create an ETL flow as shown in the below diagram.</p> <p></p> <p>WSO2 Stream Processor needs to extract events from that file and the <code>PurchaseRecords</code> database table simultaneously to update the stock records. To do this, you can define two input streams and connect then to the relevant sources as follows:</p> <pre><code>@source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/PurchaseRecords', username = 'root', password = 'root', table.name = 'PurchaseRecords', operation = 'insert',\n    @map(type = 'keyvalue'))\ndefine stream PurchasesStream (name string, amount double);\n\n@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/MaterialDispatches.xml',\n    tailing='true',\n    @map(type='xml'))\ndefine stream MaterialDispatchesStream (name string, amount double); \n</code></pre> <p>The <code>PurchasesStream</code> uses a <code>cdc</code> source to extract all the insert records of the <code>PurchaseRecords</code> database table in real time. At the same time, the <code>MaterialDispatches</code> stream extracts all the material dispatches saved in the <code>MaterialDispatches.xml</code> file in realtime by tailing it. Here, you are receiving data via heterogeneous sources as well as in heterogeneous formats (i.e., in key value format from the database and in XML format fromn the file).</p> <p>To maintain stock records, you can define a table as follows:</p> <pre><code>@primaryKey('name')\n@store (type='rdbms', datasource='STOCKS_DB')\ndefine table StockRecords(name string, amount double);\n</code></pre> <p>Assuming that the stock records are updated per minute, the amount by which the stocks need to be updated during a minute is a difference between the total purchases and the total dispatched that took place during that minute. You can calculate this difference by adding another query as follows:</p> <pre><code>@info(name = 'CalculateStockUpdate')\nfrom PurchasesStream#window.timeBatch(1 min) as p \njoin MaterialDispatchesStream#window.timeBatch(1 min) as d \n    on p.name == d.name \nselect p.name as name, sum(p.amount) - sum(d.amount) as amount \ngroup by p.name \ninsert into StockUpdatesStream;\n</code></pre> <p>Here, we are joining the <code>PurchaseStream</code> stream and the <code>MaterialDispatchesStream</code> stream to calculate the difference. This is a transform operation that you are performing on the data you extracted from the two heterogeneous sources. To learn more about performing joins, see Siddhi Query Guide - Join.</p> <p>Now you can add the stock update to the <code>StockRecords</code> table as follows.</p> <pre><code>@info(name = 'UpdateStock')\nfrom StockUpdatesStream#window.timeBatch(1 min) as s \njoin StockRecords#window.timeBatch(1 min) as r \n    on s.name == r.name \nselect s.name as name, s.amount + r.amount as amount \ngroup by s.name \nupdate or insert into StockRecords\n    on StockRecords.name == name;\n</code></pre> <p>The above query performs a join between the <code>StockUpdatesStream</code> stream and the <code>StockRecords</code> table,  and adds the stock update calculated to the existing amount in the <code>StockRecords</code> table. Then to load the final output, the query performs an <code>update or insert into</code> operation to the <code>Stock Records</code> table. This means, if the table already has a record with a same value for the <code>name</code> field as the latest output event generated in the <code>StockUpdatesStream</code> stream, the output event overwrites the record in the table. If no such matching record is found, the output event is inserted as a new record.</p> <p>The queries above updtes the ETL flow as shown in the diagram </p> <p>Once you add all the new Siddhi queries and configurations introduced in this section to the original <code>ManagingStocksApp</code> Siddi application, it looks as follows:</p> <pre><code>@App:name('ManagingStocksApp')\n\n\n@App:description('Maintains the latest stock amounts')\n\n@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/SugarSupply.csv',\n    tailing='true',\n    @map(type='csv'))\ndefine stream SugarSupplyStream (transNo string, price double, amount double);\n\n@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/FlourSupply.csv',\n    tailing='true',\n    @map(type='csv'))\ndefine stream FlourSupplyStream (transNo string, name string, price double, amount double);\n\n@source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/RAW_MATERIAL_DB', username = 'root', password = 'root', table.name = 'PurchaseRecords', operation = 'insert',\n    @map(type = 'keyvalue'))\ndefine stream PurchasesStream (name string, amount double);\n\n@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/MaterialDispatches.csv',\n    tailing='true',\n    @map(type='csv'))\ndefine stream MaterialDispatchesStream (name string, amount double); \n\n\n@primaryKey('transNo')\n@index ('name')\n@store (type='rdbms', datasource='RAW_MATERIAL_DB')\ndefine table PurchaseRecords(transNo string, name string, price double, amount double);\n\n@primaryKey('name')\n@store (type='rdbms', datasource='STOCKS_DB')\ndefine table StockRecords(name string, amount double);\n\n\n@info(name = 'CleaningSugarSupplyData')\nfrom SugarSupplyStream\nselect transNo, \"sugar\" as name, price, amount\nupdate or insert into PurchaseRecords\n    on PurchaseRecords.transNo == transNo;\n\n@info(name = 'CleaningFlourSupplyData')\nfrom FlourSupplyStream [name == \"flour\"]\nselect *\nupdate or insert into PurchaseRecords\n    on PurchaseRecords.transNo == transNo;\n\n@info(name = 'CalculateStockUpdate')\nfrom PurchasesStream#window.timeBatch(1 min) as p \njoin MaterialDispatchesStream#window.timeBatch(1 min) as d \n    on p.name == d.name \nselect p.name as name, sum(p.amount) - sum(d.amount) as amount \ngroup by p.name \ninsert into StockUpdatesStream;\n\n@info(name = 'UpdateStock')\nfrom StockUpdatesStream#window.timeBatch(1 min) as s \njoin StockRecords#window.timeBatch(1 min) as r \n    on s.name == r.name \nselect s.name as name, s.amount + r.amount as amount \ngroup by s.name \nupdate or insert into StockRecords\n    on StockRecords.name == name;\n</code></pre>"},{"location":"guides/performing-etl-tasks/#scalability","title":"Scalability","text":"<p>When there are rapid changes and growths in business, it is necessary to scale ETL applications in an agile manner to support it. WSO2 Streaming Integrator supports the need for scalability via the Siddhi logic.  This can be observed in the previous examples where the <code>ManagingStocksApp</code> Siddhi application which only captured purchase records in the Performing ETL in real time section and with only two files and one database table (<code>SugarSupply.csv</code> file, <code>FlourSupply.csv</code> file and <code>PurchaseRecords</code> database table) in the ETL flow was scaled to perform stock updates by incorporating another file and a database (i.e., <code>MaterialDispatches.csv</code> file and <code>StockRecords</code> database table) to the ETL flow.</p> <p></p> <p>When you extended the ETL flow to perform stock updates, it involved adding more ETL tasks to the flow. You can also scale your ETL applications without adding more ETL tasks. This is done by adding only more sources to extract data for the existing tasks or adding more destinations for the existing tasks to load the output. For example, if the Sweet Factory starts purchasing another ingredient (e.g., honey), you can define another stream to consume from a new source (e.g., a new file named `HoneySupply.xml) as follows:</p> <pre><code>@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/HoneySupply.xml',\n    tailing='true',\n    @map(type='xml'))\ndefine stream HoneySupplyStream (transNo string, name string, price double, amount double);\n</code></pre> <p>Then you can update the existing <code>PurchaseRecords</code> table with information about the purchases of the new material as shown below.</p> <p><pre><code>@info(name = 'RecordingHoneySupplyData')\nfrom FlourSupplyStream\nselect *\nupdate or insert into PurchaseRecords\n    on PurchaseRecords.transNo == transNo;\n</code></pre> The following diagram depicts how the above changes scaled the ETL flow.</p> <p></p> <p>Tip</p> <p>As you scale your ETL operations, you may have all the related queries in a single application or create multiple Siddhi applications that function as components of the same ETL flow.</p>"},{"location":"guides/performing-etl-tasks/#multiple-platforms-for-etl-application-design","title":"Multiple platforms for ETL application design","text":"<p>WSO2 Streaming Integrator provides the Source View, Design View and the Wizard View for application design. For more information, see Streaming Integrator Tooling Overview.</p> <p>Out of these three views, the Wizard View is dedicated for designing ETL applications without writing many Siddhi queries. This platform mainly caters for application designers who prefer to use Siddhi constructs without writing code. Therefore, it guides you to write multiple simple Siddhi applications that contribute to the same ETL flow instead of heavy applications embodying multiple components of the ETL flow. A single Siddhi application designed using the ETL wizard can only incorporate one source and one destination to the ETL flow. </p> <p>To learn how to design an ETL application via the Wizard view, see the Creating an ETL Application via SI Tooling tutorial.</p>"},{"location":"guides/performing-etl-tasks/#visualizing-etl-performance-statistics","title":"Visualizing ETL Performance Statistics","text":"<p>WSO2 Streaming Integrator provides nine pre-configured dashboards to visualize the overall ETLS statistics for your Streaming Integrator deployment, as well as the ETL statistics per Siddhi application and per ETL-related Siddhi extension type (i.e., CDC statistics, file statistics and RDBMS statistics).</p> <p>You can set up the pre-configured dashboards in Grafana. For instructions to set up these dashboards and visualize your ETL statistics, see Monitoring ETL Statistics with Grafana.</p>"},{"location":"guides/performing-etl-tasks/#processing-high-volumes-of-data-at-high-speed","title":"Processing high volumes of data at high speed","text":"<p>In real world business scenarios, many businesses carry out about thousands of online transactions per second. This requires an ETL application performing in real time to handle a high volume of data in high speed.</p> <p>According to the latest performance statistics of the Streaming Integrator, it can process 29,000 transactions per second when performing ETL tasks. For more information about performance statistics, see the following:</p> <ul> <li>Performance Analysis Results - Performing ETL Tasks</li> <li>Streaming ETL with WSO2 Streaming Integrator article</li> </ul>"},{"location":"guides/processing-data/","title":"Stream Processing","text":"<p>Stream processing involves making changes to streaming data to generate a required output. WSO2 Streaming Integrator allows you to carry out a wide range of operations to process streaming data. These operations are supported via Siddhi Extensions. These operations can be broadly categorized into five categories as follows:</p> <ul> <li> <p>Cleansing</p> <p>Filtering only the required information for further processing out of all the extracted/received data, and other cleaning activities such as pre-processing, adding missing values, removing unnecessary attributes, etc.</p> </li> <li> <p>Transforming</p> <p>Transforming data from one format to another, performing a range of mathematical, regex, string, unit conversion, map, json, etc., and enabling the creation of scripts that perform transformations.</p> </li> <li> <p>Enriching</p> <p>Joining streams of data with each other as well as static data (such as tables) to enrich the data</p> </li> <li> <p>Summarizing</p> <p>Performing long-term time based summarizations, as well as performing summarizations for subsets of data based on time or length (number of events e.g., every 10 events). </p> </li> <li> <p>Correlating</p> <p>Identifying trends and patterns.</p> </li> </ul>"},{"location":"guides/publishing-data-to-event-stream-consumers/","title":"Publishing Data","text":"<p>This guide covers how WSO2 Streaming Integrator publishes data to destinations and messaging systems.</p>"},{"location":"guides/publishing-data-to-event-stream-consumers/#publishing-data-to-destinations","title":"Publishing data to destinations","text":"<p>Publishing to destinations involve using transports such as HTTP, TCP, email, etc., where the data is sent to an endpoint that is available to listen to messages and respond. </p> <p></p> <p>To understand this, consider a warehouse that needs to publish each stock update to a specific endpoint so that the stock can be monitored by the warehouse manager. To address this requirement via the WSO2 Streaming Integrator, you can define an output stream and connect a sink to it as shown below. In this example, let's use an HTTP sink.</p> <pre><code>@sink(type = 'http', publisher.url = 'http://stocks.com/stocks',\n      @map(type = 'json'))\ndefine stream StockStream (symbol string, price float, volume long);\n</code></pre> <p>The above sink configuration publishes all the events in the <code>StockStream</code> output stream to the <code>http://stocks.com/stocks</code> HTTP URL in JSON format.</p>"},{"location":"guides/publishing-data-to-event-stream-consumers/#try-it-out","title":"Try it out","text":"<p>To try out the above example, follow the steps below:</p> <ol> <li> <p>Start and access WSO2 Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file and copy the following Siddhi Application to it.</p> <pre><code>@App:name(\"PublishStockUpdatesApp\")  \n\ndefine stream InputStream (symbol string, price float, volume long);\n\n@sink(type = 'http', publisher.url = 'http://localhost:5005/stocks',\n      @map(type = 'json'))\ndefine stream StockStream (symbol string, price float, volume long);\n\nfrom InputStream\nselect *\ninsert into StockStream;\n</code></pre> </li> </ol> <p>Save the Siddhi application.</p> <p>This Siddhi application publishes stock updates as HTTP events via the <code>http</code> sink in the previous example.</p> <ol> <li> <p>To monitor whether the HTTP events generated via the <code>PublishStockUpdatesApp</code> Siddhi application are getting published to the <code>http://localhost:5005/stocks</code> URL as specified, create and save another Siddhi Application as follows:</p> <pre><code>@App:name('ListenToStockUpdatesApp')\n\n@source(type = 'http', receiver.url = \"http://localhost:5005/stocks\",\n    @map(type = 'json'))\ndefine stream StockStream (symbol string, price float, volume long);\n\n@sink(type = 'log', prefix = \"Stock Updates\",\n    @map(type = 'passThrough'))\ndefine stream OutputStream (symbol string, price float, volume long);\n\n@info(name = 'query1')\nfrom StockStream \nselect * \ninsert into OutputStream;\n</code></pre> <p>This Siddhi application listens for events in the <code>http://localhost:5005/stocks</code> endpoint and logs them in the Streaming Integrator Tooling console.</p> </li> <li> <p>Start both the Siddhi applications. To do this, open each siddhi application and click the Play icon.</p> <p></p> </li> <li> <p>Simulate an event with the following values for the <code>InputStream</code> stream of the <code>PublishStockUpdatesApp</code> Siddhi application. For instructions to simulate events, see Testing Siddhi Applications.</p> Attribute Value symbol <code>ABC</code> price <code>100</code> volume <code>20</code> <p>As a result, the <code>ListenToStockUpdates</code> Siddhi applications prints the following log in the Streaming Integrator Tooling Console.</p> <pre><code>[2020-10-28_10-59-20_463] INFO {io.siddhi.core.stream.output.sink.LogSink} - Stock Updates : Event{timestamp=1603862960462, data=[ABC, 100.0, 20], isExpired=false} \n</code></pre> </li> </ol>"},{"location":"guides/publishing-data-to-event-stream-consumers/#supported-transports","title":"Supported transports","text":"<p>WSO2 Streaming Integrator supports the following transport types to send messages to destinations.</p> Transport Supporting Siddhi Extension <code>http</code> http <code>tcp</code> tcp <code>email</code> email <code>grpc</code> grpc <code>wso2event</code> wso2event <code>websocket</code> websocket <code>Thrift</code>"},{"location":"guides/publishing-data-to-event-stream-consumers/#supported-mappers","title":"Supported mappers","text":"<p>Mappers determine the format in which the event is published. For information about transforming events by changing the format in which the data is published, see Transforming Data.</p> <p>The following are the supported mappers when you publish data to destinations.</p> Transport Supporting Siddhi Extension <code>json</code> json <code>xml</code> xml <code>text</code> text <code>avro</code> avro <code>binary</code> binary"},{"location":"guides/publishing-data-to-event-stream-consumers/#publishing-data-to-messaging-systems","title":"Publishing data to messaging systems","text":"<p>WSO2 Streaming Integrator allows you to publish data to messaging systems such as Kafka, JMS, NATS, GooglePubSub, etc. so that you can expose streaming data to applications that cannot read streaming data, but are able to subscribe for data in messaging systems.</p> <p></p> <p>To understand this, consider a scenario where temperature readings from a sensor are published into a Kafka topic so that other devices that need to consume that data can subscribe for it. You can address this requirement via WSO2 Streaming Integrator by defining an output stream and then connecting a sink to it as shown in the example below.</p> <pre><code>@sink(type = 'kafka', bootstrap.servers = \"localhost:9092\", topic = \"temperature\",\n    @map(type = 'json'))\ndefine stream PublishTemperatureStream (temperature int);\n</code></pre> <p>The above sink configuration of the <code>kafka</code> type publishes all the events in the <code>PublishTemperatureStream</code> stream to a Kafka topic named <code>temperature</code> running in the <code>localhost:9092</code> server. The messages are published in <code>json</code> format.</p>"},{"location":"guides/publishing-data-to-event-stream-consumers/#try-it-out_1","title":"Try it out","text":"<p>To try out the example in the previous subtopic, follow the steps below:</p> <ol> <li> <p>Download the Kafka broker from the Apache site and extract it.    This directory is referred to as <code>&lt;KAFKA_HOME&gt;</code> from here on.</p> </li> <li> <p>Start Kafka as follows:</p> <ol> <li> <p>First, start a zoo keeper node. To do this, navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command.</p> <p><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties</code></p> </li> <li> <p>Next, start a Kafka server node. To do this, issue the following command from the same directory.</p> <p><code>sh bin/kafka-server-start.sh config/server.properties</code></p> </li> <li> <p>To create a Kafka topic named <code>temperature</code>, issue the following command from the same directory.</p> <p><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic temperature</code></p> </li> </ol> </li> <li> <p>Prepare WSO2 Streaming Integrator Tooling to publish data to a Kafka topic as follows:</p> <ol> <li> <p>Start and access WSO2 Streaming Integrator Tooling. </p> </li> <li> <p>Download and install the Kafka extension to it. For instructions, see Installing Siddhi Extensions.</p> </li> <li> <p>Open a new file and add the following Siddhi application to it.</p> <p><pre><code>@App:name('TemperaturePublishingApp')\n\n@sink(type = 'kafka', bootstrap.servers = \"localhost:9092\", topic = \"temperature\",\n    @map(type = 'json'))\ndefine stream PublishTemperatureStream (temperature int);\n\ndefine stream TemperatureStream (temperature int);\n\nfrom TemperatureStream \nselect * \ninsert into PublishTemperatureStream;\n</code></pre>        Save the Siddhi application.</p> </li> </ol> <p>The above Siddhi application includes the sink configuration from the previous example. The Siddhi query takes all the input events in the <code>TemperatureStream</code> stream and inserts them into the <code>PublishTemperatureStream</code> stream so that they can be published to the <code>temperature</code> Kafka topic via the connected source.</p> <ol> <li> <p>Start the Siddhi application by clicking the Play icon in the top panel for it.</p> <p></p> </li> <li> <p>Simulate an event for the <code>TemperatureStream</code> stream of the <code>TemperaturePublishingApp</code> Siddhi application. In this example, let's enter <code>30</code> as the value for the <code>temperature</code> attribute.</p> <p>For instructions to simulate events, see Testing Siddhi Applications.</p> </li> <li> <p>To retrieve the events published to the Kafka topic, issue the following command from <code>&lt;KAFKA_HOME&gt;</code></p> <pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic temperature --from-beginning\n</code></pre> <p>You can see the following in the Kafka consumer log.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"guides/publishing-data-to-event-stream-consumers/#supported-messaging-systems","title":"Supported messaging systems","text":"<p>WSO2 Streaming Integrator allows you to publish messages to the following messaging system via Siddhi extentions.</p> Transport Supporting Siddhi Extension <code>kafka</code> kafka <code>NATS</code> NATS <code>Google Pub/Sub</code> Google Pub/Sub <code>rabbitmq</code> rabbitmq <code>jms</code> jms <code>mqtt</code> mqtt <code>sqs</code> sqs"},{"location":"guides/publishing-data-to-event-stream-consumers/#supported-mappers_1","title":"Supported mappers","text":"<p>Mappers determine the format in which the event is published. For information about transforming events by changing the format in which the data is published, see Transforming Data.</p> <p>The following are the supported mappers when you publish data to destinations.</p> Transport Supporting Siddhi Extension <code>json</code> json <code>xml</code> xml <code>text</code> text <code>avro</code> avro <code>binary</code> binary"},{"location":"guides/receiving-data-in-transit/","title":"Receiving Data in Transit","text":"<p>Data in transit, also known as data in flight refers to data that is in the process of being moved and therefore not permanently stored in a location where they can be in a static state. Streaming data, messages in a queue or a topic in a messaging system, and requests sent to a HTTP listening port are a few examples.</p> <p>The sources from which data in transit/flight are received can be classified into two categories as follows:</p> <ul> <li> <p>Data publishers: You can receive data from these sources without subscribing to receive it. (e.g., HTTP, HTTPS, TCP, email, etc.)</p> </li> <li> <p>Messaging Systems: You need to subscribe to receive data from the source. (e.g., messaging systems such as Kafka, JMS, MQTT, etc.)</p> </li> </ul>"},{"location":"guides/receiving-data-in-transit/#receiving-data-from-data-publishers","title":"Receiving data from data publishers","text":"<p>Data publishers are transports from which WSO2 SI can receive messages without subscribing for them. In a typical scenario, you are required to open a port in the WSO2 Streaming Integrator that is dedicated to listen to messages from the data publisher.</p> <p></p> <p>To receive data from a data publisher, define an input stream and connect a [source] annotation of a type that receives data from a data publisher as shown in the example below.</p> <p><pre><code>@source(type='http', \n    receiver.url='http://localhost:5005/StudentRegistrationEP', \n    @map(type = 'json'))\n\ndefine stream StudentRegistrationStream (name string, course string);\n</code></pre> In this example, an online student registration results in an HTTP request in JSON format being sent to the endpoint named <code>StudentRegistrationEP</code> to the <code>5005</code> port of the localhost. The source generates an event in the <code>StudentRegistrationStream</code> stream for each of these requests.</p>"},{"location":"guides/receiving-data-in-transit/#try-it-out","title":"Try it out","text":"<p>To try out the example given above, let's include the source configuration in a Siddhi application and simulate an event to it.</p> <ol> <li> <p>Open and access Streaming Integrator Tooling. For instructions, see Streaming Integrator Tooling Overview - Starting Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file and add the following Siddhi application to it.</p> <pre><code>@App:name('StudentRegistrationApp')\n\n@source(type = 'http', receiver.url = \"http://localhost:5005/StudentRegistrationEP\",\n    @map(type = 'json'))\ndefine stream StudentRegistrationStream (name string, course string);\n\n@sink(type = 'log', prefix = \"New Student\",\n    @map(type = 'passThrough'))\ndefine stream StudentLogStream (name string, course string, total long);\n\n\n@info(name = 'TotalStudentsQuery')\nfrom StudentRegistrationStream \nselect name, course, count() as total \ninsert into StudentLogStream;\n</code></pre> <p>Save the Siddhi application.</p> <p>This Siddhi application contains the <code>http</code> source of the previously used example. The <code>TotalStudentsQuery</code> query selects all the student registrations captured as HTTP requests and directs them to the <code>StudentLogStream</code> output stream. A log sink connected to this output stream logs these registrations in the terminal. Before logging the events the Siddhi application also counts the number of registrations via the <code>count()</code> function. This count is presented as <code>total</code> in the logs.</p> </li> <li> <p>Start the Siddhi application by clicking on the play icon in the top panel.</p> <p></p> </li> <li> <p>To simulate an event, issue the following two CURL commands.</p> <pre><code>curl -X POST \\\n  http://localhost:5005/StudentRegistrationEP \\\n  -H 'content-type: application/json' \\\n  -d '{\n  \"event\": {\n    \"name\": \"John Doe\",\n    \"course\": \"Graphic Design\"\n  }\n}'\n</code></pre> <pre><code>curl -X POST \\\n http://localhost:5005/StudentRegistrationEP \\\n -H 'content-type: application/json' \\\n -d '{\n \"event\": {\n   \"name\": \"Michelle Cole\",\n   \"course\": \"Graphic Design\"\n }\n}'\n</code></pre> </li> </ol> <p>The following is logged in the terminal.</p> <pre><code>```text\nINFO {io.siddhi.core.stream.output.sink.LogSink} - New Student : Event{timestamp=1603185021250, data=[John Doe, Graphic Design, 1], isExpired=false}\n\nINFO {io.siddhi.core.stream.output.sink.LogSink} - New Student : Event{timestamp=1603185486763, data=[Michelle Cole, Graphic Design, 2], isExpired=false}\n```</code></pre>"},{"location":"guides/receiving-data-in-transit/#supported-transports","title":"Supported transports","text":"<p>The following are the supported transports to capture data in transit from data publishers.</p> Transport Siddhi Extension <code>HTTP</code> http <code>TCP</code> tcp <code>Email</code> email <code>grpc</code> grpc <code>wso2event</code> wso2event <code>Thrift</code>"},{"location":"guides/receiving-data-in-transit/#supported-mappers","title":"Supported mappers","text":"<p>Mappers determine the format in which the event is received. For information about transforming events by changing the format in which the data is received/published, see Transforming Data.</p> <p>The following are the supported mappers when you receive data from data publishers.</p> Mapper Supporting Siddhi Extension <code>json</code> json <code>xml</code> xml <code>text</code> text <code>avro</code> avro <code>binary</code> binary"},{"location":"guides/receiving-data-in-transit/#receiving-data-from-messaging-systems","title":"Receiving data from messaging systems","text":"<p>This section explains how to receive input data from messaging systems where WSO2 Streaming Integrator needs to subscribe to specific queues/topics in order to receive the required data.</p> <p></p> <p>To receive data from a messaging system, define an input stream and connect a [source] annotation of a type that receives data from a messaging system.</p> <p>For example, consider a weather broadcasting application that publishes the temperature and humidity for each region is monitors in a separate Kafka topic. The local weather broadcasting firm of Houston wants to subscribe to receive weather broadcasts for Houston.</p> <pre><code>@source(type='kafka',\n        topic.list='houston',\n        threading.option='single.thread',\n        group.id=\"group1\",\n        bootstrap.servers='localhost:9092',\n        @map(type='json'))\ndefine stream TemperatureHumidityStream (temperature int, humidity int);\n</code></pre> <p>The above Kafka source listens at bootstrap server <code>localhost:9092</code> for messages in the kafka topic named <code>houston</code> sent in JSON format. For each message, it generates an event in the <code>TemperatureHumidityStream</code> stream.</p>"},{"location":"guides/receiving-data-in-transit/#try-it-out_1","title":"Try it out","text":"<p>To try the above example, follow the steps below.</p> <ol> <li> <p>Download the Kafka broker from the Apache site and extract it.    This directory is referred to as <code>&lt;KAFKA_HOME&gt;</code> from here on.</p> </li> <li> <p>Start Kafka as follows:</p> <ol> <li> <p>First, start a zoo keeper node. To do this, navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command.</p> <p><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties</code></p> </li> <li> <p>Next, start a Kafka server node. To do this, issue the following command from the same directory.</p> <p><code>sh bin/kafka-server-start.sh config/server.properties</code></p> </li> <li> <p>To create a Kafka topic named <code>houston</code>, issue the following command from the same directory.</p> <p><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic houston</code></p> </li> </ol> </li> <li> <p>Prepare WSO2 Streaming Integrator Tooling to consume Kafka messages as follows:</p> <ol> <li> <p>Start and access WSO2 Streaming Integrator Tooling. </p> </li> <li> <p>Download and install the Kafka extension to it. For instructions, see Installing Siddhi Extensions.</p> </li> <li> <p>Open a new file and add the following Siddhi application to it.</p> <pre><code>@App:name('TemperatureReportingApp')\n\n@source(type = 'kafka', topic.list = \"houston\", threading.option = \"single.thread\", group.id = \"group1\", bootstrap.servers = \"localhost:9092\",\n@map(type = 'json'))\ndefine stream TemperatureHumidityStream (temperature int, humidity int);\n\n@sink(type = 'log', prefix = \"Temperature Update\",\n    @map(type = 'passThrough'))\n\ndefine stream OutputStream (temperature int, humidity int);\n\n@info(name = 'query1')\nfrom TemperatureHumidityStream \nselect * \ninsert into OutputStream;\n</code></pre> </li> </ol> <p>This Siddhi application includes the Kafka source that subscribes to the <code>houston</code> kafka source and generates an event in the <code>TemperatureHumidityStream</code> stream for each message in the topic (as described in the example inj the previous section). <code>query1</code> query gets all these messages from the <code>TemperatureHumidityStream</code> stream and inserts them into the <code>OutputStream</code> stream so that they can be logged via the log sink connected to the latter.</p> <p>Save the Siddhi application.</p> <ol> <li>Start the <code>TemperatureReportingApp</code> Siddhi application that you created and saved.</li> </ol> </li> <li> <p>To generate a message in the <code>houston</code> Kafka topic, follow the steps below:</p> <ol> <li> <p>To run the Kafka command line client, issue the following command from the <code>&lt;KAFKA_HOME&gt;</code> directory.</p> <p><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic houston</code></p> </li> <li> <p>When you are prompted to type messages in the console. Type the following in the command prompt.</p> <p><code>{\"event\":{ \"temperature\":23, \"humidity\":99}}</code></p> <p>This pushes a message to the Kafka Server. Then, the Siddhi application you deployed in the Streaming Integrator consumes this message. As a result, the Streaming Integrator log displays the following:</p> </li> </ol> </li> <li> <p>Check the logs of Streaming Integrator Tooling. The Kafka message you generated is logged as follows:</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Temperature Update : Event{timestamp=1603339705244, data=[23, 99], isExpired=false}\n</code></pre> </li> </ol>"},{"location":"guides/receiving-data-in-transit/#supported-transports_1","title":"Supported transports","text":"<p>The following are the supported transports to capture data in transit from messaging systems.</p> Transport Siddhi Extension NATS nats Kafka kafka googlepubsub googlepubsub RabbitMQ rabbitmq JMS JMS MQTT MQTT SQS sqs"},{"location":"guides/receiving-data-in-transit/#supported-mappers_1","title":"Supported mappers","text":"<p>Mappers determine the format in which the event is received. For information about transforming events by changing the format in which the data is received/published, see Transforming Data.</p> <p>The following are the supported mappers when you receive data from messaging systems.</p> Mapper Supporting Siddhi Extension <code>json</code> json <code>xml</code> xml <code>text</code> text <code>avro</code> avro <code>binary</code> binary <code>protobuf</code> protobuf"},{"location":"guides/summarizing-data/","title":"Summarizing Data","text":"<p>Summarizing data refers to obtaining aggregates in an incremental manner for a specified set of time periods.</p>"},{"location":"guides/summarizing-data/#performing-clock-time-based-summarization","title":"Performing clock-time based summarization","text":"<p>Performing clock time-based summarizations involve two steps:</p> <ol> <li> <p>Calculating the aggregations for the selected time granularities and storing the results.</p> </li> <li> <p>Retrieving previously calculated aggregations for selected time granularities.</p> </li> </ol>"},{"location":"guides/summarizing-data/#calculating-the-aggregations-for-the-selected-time-granularities-and-storing-the-results","title":"Calculating the aggregations for the selected time granularities and storing the results","text":"<p>To understand this, consider a scenario where the production statistics generated by the Sweet Production Factory processed. The results need to be summarized for different time granularities and saved so that they can be later retrieved for periodical production analysis. To do this, you can create a Siddhi application as shown below.</p> <p><pre><code>@App:name('ProductionAggregatesApp')\n\ndefine stream ProductionStream (name string, amount double, timestamp long);\n\n@store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/Production\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\")\ndefine aggregation ProductionAggregation\nfrom ProductionStream\nselect name, amount, sum(amount) as total, avg(amount) as average \ngroup by name \naggregate by timestamp every sec ... year;\n</code></pre> Observe the following in the above Siddhi application:</p> <ul> <li> <p>The stream</p> <p>In addition to the <code>name</code> and <code>amount</code> attributes to capture the name of the product and the amount produced, the stream has an attribute named <code>timestamp</code> to capture the time at which the production run takes place. he aggregations are executed based on this time. This attribute's value could either be a long value (reflecting the Unix timestamp in milliseconds), or a string value adhering to one of the following formats:</p> <ul> <li> <p><code>&lt;YYYY&gt;-&lt;MM&gt;-&lt;dd&gt; &lt;HH&gt;:&lt;mm&gt;:&lt;ss&gt; &lt;Z&gt;</code>: This format can be used if the timezone needs to be specified explicitly. Here the ISO 8601 UTC offset must be provided. e.g., +05:30 reflects the India Time Zone. If time is not in GMT, this value must be provided.</p> </li> <li> <p><code>&lt;yyyy&gt;-&lt;MM&gt;-&lt;dd&gt; &lt;HH&gt;:&lt;mm&gt;:&lt;ss&gt;</code>: This format can be used if the timezone is in GMT.</p> </li> </ul> </li> <li> <p>The aggregation</p> <p>You are defining the <code>ProductionAggregation</code> aggregation to store the aggregated values. </p> <p>A store is connected to it via the <code>@store</code> annotation. If the store definition is not provided, the data is stored in-memory. The aggregations stored in-memory can be lost when the Siddhi application is stopped.</p> </li> <li> <p>Siddhi query</p> <p>The Siddhi query gets the production events from the <code>ProductionStream</code> stream, calculates the total and the average, and aggregates them every <code>sec...year</code>. This means the production total and the average is calculated per second, per minute, per hour, per day, per month, and per year.</p> </li> </ul>"},{"location":"guides/summarizing-data/#retrieving-previously-calculated-aggregations-for-selected-time-granularities","title":"Retrieving previously calculated aggregations for selected time granularities","text":"<p>To retrieve the aggregates stored via the Siddhi application in the previous section, you need to create a new stream for data retrieval and join it with the aggregation that you previously created. In this example, let's assume that you need to production statistics for the period 12th October 2020 to 16th October 2020.</p> <p>For this, you can update the <code>ProductionAggregatesApp</code> Siddhi application that you previously created as follows:</p> <ol> <li> <p>Define a stream in which you want to generate the event (request) to retrieve data as follows.</p> <pre><code>define stream ProductionSummaryRetrievalStream (name string);\n</code></pre> </li> <li> <p>Define a query that specifies the criteria for retrieving data as follows.</p> <p><pre><code>@info(name = 'RetrievingAggregates') \nfrom ProductionSummaryRetrievalStream as b join ProductionAggregation as a\non a.name == b.name \nwithin \"2020-10-12 00:00:00 +00:00\", \"2020-10-17 00:00:00 +00:00\" \nper \"days\" \nselect a.name, a.total, a.average \ninsert into ProductionSummaryStream;\n</code></pre> Observe the following in the above Siddhi query:</p> <ul> <li> <p>The join</p> <p>The above query joins the <code>ProductionsSummaryRetyrievalStream</code> stream and the <code>ProductionAggregation</code> aggregation. The <code>ProductionsSummaryRetyrievalStream</code> stream is assigned <code>b</code> as the short name, and the aggregation is assigned <code>a</code>. Therefore, <code>a.name == b.name</code> specifies that a matching event is identified when the value for the <code>name</code> attribute is the same. </p> <p>For more information about how to perform joins, see Enriching Data.</p> </li> <li> <p><code>within</code> clause </p> <p>This specifies the time interval for which the aggregates should be retrieved. You are requesting data for the period between 00.00 AM of 12th October 2020 and 00.00 AM of 17th October 2020 so that the days 12th, 13th, 14th, 15th, and the 16th of October are covered.</p> </li> <li> <p><code>per</code> clause</p> <p>This specifies that the aggregates should be summarized per day.</p> </li> <li> <p><code>select</code> clause</p> <p>This selects the <code>name</code>, <code>total</code> and <code>average</code> attributes to be selected from the aggregate to be included in the output event.</p> </li> </ul> <p>The output event is inserted into the <code>ProductionSummaryStream</code> stream.</p> </li> </ol>"},{"location":"guides/summarizing-data/#try-it-out","title":"Try it out","text":"<p>To try out the example given above, follow the procedure below:</p> <ol> <li> <p>Download and install MySQL. Then start the MySQL server and create a new database in it by issuing the following command:</p> <p><code>CREATE SCHEMA production;</code></p> <p>Then open the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file and add the following datasource configuration under <code>datasources</code>.</p> <pre><code>  - name: Production_DB\n    description: The datasource used for Production Statistics\n    jndiConfig:\n      name: jdbc/production\n    definition:\n      type: RDBMS\n      configuration:\n        jdbcUrl: 'jdbc:mysql://localhost:3306/production?useSSL=false'\n        username: root\n        password: root\n        driverClassName: com.mysql.jdbc.Driver\n        minIdle: 5\n        maxPoolSize: 50\n        idleTimeout: 60000\n        connectionTestQuery: SELECT 1\n        validationTimeout: 30000\n        isAutoCommit: false\n</code></pre> </li> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file in Streaming Integrator Tooling. Then add and save the following Siddhi application.</p> <pre><code>@App:name('ProductionAggregatesApp')\n@App:description('Description of the plan')\n\ndefine stream ProductionStream (name string, amount double, timestamp long);\n\ndefine stream ProductionSummaryRetrievalStream (name string);\n\n@sink(type = 'log', prefix = \"Production Summary\",\n    @map(type = 'text'))\ndefine stream ProductionSummaryStream (name string, total double, average double);\n\n\n@store(type = 'rdbms', jdbc.url = \"jdbc:mysql://localhost:3306/production?useSSL=false\", username = \"root\", password = \"root\", jdbc.driver.name = \"com.mysql.jdbc.Driver\")\ndefine aggregation ProductionAggregation\nfrom ProductionStream\nselect name, amount, sum(amount) as total, avg(amount) as average\n    aggregate by timestamp every seconds...years;\n\n@info(name = 'RetrievingAggregates')\nfrom ProductionSummaryRetrievalStream as b \njoin ProductionAggregation as a \n    on a.name == b.name\nwithin \"2020-10-12 00:00:00 +00:00\", \"2020-10-17 00:00:00 +00:00\"\nper \"days\" \nselect a.name as name, a.total as total, a.average as average \ninsert into ProductionSummaryStream;\n</code></pre> </li> </ol> <p>This is the complete <code>ProductionAggregatesApp</code> Siddhi application with the queries given in the examples to store and retrieve aggregates. You are annotating a sink of the <code>log</code> type to the <code>ProductionSummaryStream</code> stream to which the retrieved aggregates are sent so that you can view the retrieved information in the terminal logs.</p> <ol> <li> <p>To store aggregates, simulate five events with the following values for the <code>ProductionStream</code> stream via the Event Simulator tool. For instructions to simulate events, see Testing Siddhi Applications.</p> name amount timestamp <code>brownie</code> <code>90</code> <code>1602489419000</code> <code>brownie</code> <code>90</code> <code>1602488519000</code> <code>eclairs</code> <code>95</code> <code>1602661319000</code> <code>brownie</code> <code>100</code> <code>1602747719000</code> <code>brownie</code> <code>120</code> <code>1602834119000</code> <p>The above events are stored in the <code>production</code> database that you previously defined.</p> </li> <li> <p>To retrieve the information you stored, simulate an event for the <code>ProductionSummaryRetrievalStream</code> stream with <code>brownie</code> as the value for `name'. For instructions to simulate events, see Testing Siddhi Applications.</p> <p>The Streaming Integrator Tooling terminal displays the following logs.</p> <p></p> </li> </ol>"},{"location":"guides/summarizing-data/#supported-extensions","title":"Supported extensions","text":"<p>The following table describes the complete list of extensions that provide aggregation functionality when you perform time-based aggregations:</p> Extension Description Siddhi-execution-math Transforms data by performing mathematical operations. Siddhi-execution-streeamingml Provides streaming machine learning (clustering, classification and regression) for event streams."},{"location":"guides/summarizing-data/#performing-short-term-summarizations","title":"Performing short term summarizations","text":"<p>This section explains how to apply Siddhi logic to process a subset of events received to a stream based on time or the number of events. This is achieved via Siddi Windows. The window can apply to a batch of events or in a sliding manner. </p> <p>The following are a few examples of how short time summarizations can be performed based on time or the number of events.</p> <ul> <li> <p>Performing a time-based summarization in a sliding manner</p> <p>This involves selecting a subset of events based on a specified duration of time in a sliding manner as illustrated via an example in the diagram below.</p> <p></p> <p>For example, consider that the factory foreman of a sweet factory wants to calculate the production total and average per product every four minutes in a sliding manner. To address this, you can write a query as follows.</p> <p><pre><code>from ProductionStream#window.time(4 min)\nselect name, sum(amount) as pastFourMinTotal, avg(amount) as pastFourMinAvg\ngroup by name\ninsert into TimeSlidingOutputStream;\n</code></pre> Here, <code>#window.time(4 min)</code> represents a sliding time window of four minutes. Based on this, the total for the last four minutes is calculated and presented as <code>pastFourMinTotal</code>, and the average for the last four minutes is calculated and presented as <code>pastFourMinAvg</code>.</p> </li> <li> <p>Performing a time-based summarization in a tumbling manner</p> <p>This involves selecting a subset of events based on a specified duration of time in a tumbling manner as illustrated via an example in the diagram below.</p> <p></p> <p>For example, consider that the factory foreman of a sweet factory wants to calculate the production total and average per product every four minutes in a tumbling manner. To address this, you can write a query as follows.</p> <p><pre><code>from ProductionStream#window.timeBatch(4 min)\nselect name, sum(amount) as pastFourMinTotal, avg(amount) as pastFourMinAvg\ngroup by name\ninsert into TimeBatchOutputStream;\n</code></pre> Here, <code>#window.timeBatch(4 min)</code> represents a tumbling time window of four minutes. Based on this, the total for the last four minutes is calculated and presented as <code>pastFourMinTotal</code>, and the average for the last four minutes is calculated and presented as <code>pastFourMinAvg</code>.</p> </li> <li> <p>Performing a length-based summarization in a sliding manner</p> <p>This involves selecting a batch of events based on the number of events specified in a sliding manner as illustrated via an example in the diagram below.</p> <p></p> <p>For example, consider that the factory foreman of a sweet factory wants to calculate the production total and average per product for every three events in a sliding manner. To address this, you can write a query as follows.</p> <p><pre><code>from ProductionStream#window.length(3)\nselect name, sum(amount) as lastBatchTotal, avg(amount) as lastBatchAvg\ngroup by name\ninsert into LengthSlidingOutputStream;\n</code></pre> Here, <code>#window.length(3)</code> represents a sliding length window of 3 events. Based on this, the total for the last three events is calculated and presented as <code>lastBatchTotal</code>, and the average for the last three events is calculated and presented as <code>lastBatchAvg</code>.</p> </li> <li> <p>Performing a length-based summarization to a batch of events </p> <p>This involves selecting a batch of events based on the number of events specified in a sliding manner as illustrated via an example in the diagram below.</p> <p></p> <p>For example, consider that the factory foreman of a sweet factory wants to calculate the production total and average per product for every three events in a sliding manner. To address this, you can write a query as follows.</p> <p><pre><code>from ProductionStream#window.lengthBatch(3)\nselect name, sum(amount) as lastBatchTotal, avg(amount) as lastBatchAvg\ngroup by name\ninsert into LengthBatchOutputStream;\n</code></pre> Here, <code>#window.lengthBatch(3)</code> represents a sliding length window of 3 events. Based on this, the total for the last three events is calculated and presented as <code>lastBatchTotal</code>, and the average for the last three events is calculated and presented as <code>lastBatchAvg</code>.</p> </li> </ul>"},{"location":"guides/summarizing-data/#try-it-out_1","title":"Try it out","text":"<p>To try out the four sample queries given above, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file. Then add and save the following Siddhi application.</p> <pre><code>@App:name('ProductionSummaryApp')\n\n\n@sink(type = 'log', prefix = \"Four Minute Summary\",\n    @map(type = 'text'))\ndefine stream TimeSlidingOutputStream (name string, pastFourMinTotal double, pastFourMinAvg double);\n\n@sink(type = 'log', prefix = \"Three Production Run Summary\",\n    @map(type = 'passThrough'))\ndefine stream LengthSlidingOutputStream (name string, lastBatchTotal double, lastBatchAvg double);\n\ndefine stream ProductionStream (name string, amount double, timestamp long);\n\n@sink(type = 'log', prefix = \"Four Minute Summary - Batch\",\n    @map(type = 'text'))\ndefine stream TimeBatchOutputStream (name string, pastFourMinTotal double, pastFourMinAvg double);\n\n@sink(type = 'log', prefix = \"Three Production Run Summary - Batch\",\n    @map(type = 'passThrough'))\ndefine stream LengthBatchOutputStream (name string, lastBatchTotal double, lastBatchAvg double);\n\n@info(name = 'query1')\nfrom ProductionStream#window.time(4 min) \nselect name, sum(amount) as pastFourMinTotal, avg(amount) as pastFourMinAvg \n    group by name \ninsert into TimeSlidingOutputStream;\n\n@info(name = 'query2')\nfrom ProductionStream#window.timeBatch(4 min) \nselect name, sum(amount) as pastFourMinTotal, avg(amount) as pastFourMinAvg \n    group by name \ninsert into TimeBatchOutputStream;\n\n@info(name = 'query3')\nfrom ProductionStream#window.length(3) \nselect name, sum(amount) as lastBatchTotal, avg(amount) as lastBatchAvg \n    group by name \ninsert into LengthSlidingOutputStream;\n\n@info(name = 'query4')\nfrom ProductionStream#window.lengthBatch(3) \nselect name, sum(amount) as lastBatchTotal, avg(amount) as lastBatchAvg \n    group by name \ninsert into LengthBatchOutputStream;\n</code></pre> </li> </ol> <p>The above Siddhi application has all four sample queries used as examples in this section. Those queries insert their output into four different output streams connected to log sinks to log the output of each query.</p> <ol> <li> <p>Simulate eight events for the <code>ProductionStream</code> input stream of the above Siddhi application as follows. For instructions to simulate events, see Testing Siddhi Applications.</p> name amount timestamp <code>doughnuts</code> <code>10</code> <code>1602486060000</code> <code>doughnuts</code> <code>10</code> <code>1602486120000</code> <code>doughnuts</code> <code>10</code> <code>1602486180000</code> <code>doughnuts</code> <code>10</code> <code>1602486240000</code> <code>doughnuts</code> <code>20</code> <code>1602486300000</code> <code>doughnuts</code> <code>20</code> <code>1602486360000</code> <code>doughnuts</code> <code>20</code> <code>1602486420000</code> <code>doughnuts</code> <code>30</code> <code>1602486480000</code> <p>The above simulation results in the following logs.</p> <p></p> </li> </ol>"},{"location":"guides/summarizing-data/#supported-methods-of-summarization","title":"Supported methods of summarization","text":"<p>WSO2 Streaming Integrator supports the following methods of summarization via Siddhi extensions. For more information about a summarization method, click on the relevant Siddhi link.</p> Method (Window type) Description Deduplicate Identifies duplicate events that arrive during a specified time interval and removes them. ever latest events based on a given unique keys. When a new event arrives with the same key it replaces the one that exist in the window. externalTimeBatch This is a batch (tumbling) time window that is determined based on an external time, i.e., time stamps that are specified via an attribute in the events. It holds the latest unique events that arrived during the last window time period. The unique events are determined based on the value for a specified unique key parameter. When a new event arrives within the time window with a value for the unique key parameter that is the same as that of an existing event in the window, the existing event expires and it is replaced by the new event. first This holds  only the first set of unique events according to the unique key parameter. When a new event arrives with a key that is already in the window, that event is not processed by the window. firstLengthBatch This holds a specific number of unique events (depending on which events arrive first). The unique events are selected based on a specific parameter that is considered as the unique key. When a new event arrives with a value for the unique key parameter that matches the same of an existing event in the window, that event is not processed by the window. firstTimeBatch This holds the unique events according to the unique key parameters that have arrived within the time period of that window and gets updated for each such time window. When a new event arrives with a key which is already in the window, that event is not processed by the window. length This holds the events of the latest window length with the unique key and gets updated for the expiry and arrival of each event. When a new event arrives with the key that is already there in the window, then the previous event expires and new event is kept within the window. lengthBatch This holds a specified number of latest unique events. The unique events are determined based on the value for a specified unique key parameter. The window is updated for every window length, i.e., for the last set of events of the specified number in a tumbling manner. When a new event arrives within the window length and with the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. time This holds the latest unique events that arrived during the previous time window. The unique events are determined based on the value for a specified unique key parameter. The window is updated with the arrival and expiry of each event. When a new event that arrives within a window time period has the same value for the unique key parameter as an existing event in the window, the previous event is replaced by the new event. timeBatch This holds latest events based on a unique key parameter. If a new event that arrives within the time period of a window has a value for the key parameter which matches that of an existing event, the existing event expires and it is replaced by the latest event. timeLengthBatch This holds latest events based on a unique key parameter. The window tumbles upon the elapse of the time window, or when a number of unique events have arrived. If a new event that arrives within the period of the window has a value for the key parameter which matches the value of an existing event, the existing event expires and it is replaced by the new event."},{"location":"guides/transforming-data/","title":"Transforming Data","text":"<p>The Streaming Integrator allows you to perform a wide range of transformations to the input data received. A main type of transformation supported is transforming a message from one format to another. In addition, you can perform a range of mathematical, regex, string, unit conversion, map, json, etc., transformations via Siddhi Extensions. You can also write a custom script to perform an required transformation of data.</p>"},{"location":"guides/transforming-data/#transforming-message-formats","title":"Transforming message formats","text":"<p>WSO2 Streaming Integrator can transform message formats when it consumes data as well as when it publishes data.</p>"},{"location":"guides/transforming-data/#transforming-the-message-format-when-consuming-data","title":"Transforming the message format when consuming data","text":"<p>Data publishers, databases, files, and messaging systems send data to WSO2 Streaming Integrator in different data formats. Therefore, WSO2 Streaming Integrator has the capability to consume data in a range of formats. Once it receives the data in a specific format, it can convert it to a different format for processing purposes.</p> <p>The Streaming Integrator can consume events in the default format in which they are sent/extracted or make changes to it to match the event schema as defined via the input stream.</p> <ul> <li> <p>Consuming messages in a default format</p> <p>Consuming messages via WSO2 Streaming Integrator by configuring Siddhi sources is explained in Extracting Data From Static Sources in Real Time and Receiving Data in Trasit.</p> <p>To receive data in a specific format, you need to annotate a mapper to the source configuration via the @map annotation. For more information, see Siddhi Query Guide - Source Mapper.</p> <p>To understand how to do this, consider factory where production bots publish the production amounts in a file. The the file contains rows in the CSV format. Therefore, WSO2 Streaming Integrator needs to consume the records in that format. Therefore, the source and the mapper can be configured as shown below.</p> <p><pre><code>@source(type='file', mode='LINE',\n    file.uri='file:/Users/foo/productions.csv',\n    tailing='true',\n    @map(type='csv'))\ndefine stream SweetProductionStream (name string, amount double);\n</code></pre> Here, in the <code>@map</code> annotation, <code>csv</code> is specified as the mapping type.</p> </li> <li> <p>Consuming messages in a custom format</p> <p>Custom mapping is performed when the schema of the event received needs to be updated to match the schema of the input stream.</p> <p>For this scenario, let's consider an example where the production bots are publishing messages via an HTTP sender in the JSON format, and as shown in the following example.</p> <pre><code>{\n\"sweet\": \"Jaffa Cake\",\n\"batch\": {\n\"batch id\": \"batch1\",\n\"count\": 10\n}\n}\n</code></pre> <p>The event stream is as follows:</p> <p><code>define stream SweetProductionStream (name string, amount double);</code></p> <p>In the given JSON event, the value for the <code>name</code> attribute is received with the <code>sweet</code> label, and the value for the <code>amount</code> attribute is published with the <code>count</code> label that is nested under <code>batch</code>. </p> <p>In this example, you can use custom mapping to derive the required data from the JSON event using JSON expressions.</p> Stream Attribute Name JSON Event Attribute Name JSON Path Expression <code>name</code> <code>sweet</code> <code>$.sweet</code> <code>amount</code> <code>count</code> <code>$.batch.count</code> <p>Now you can include an attribute configuration within the mapping configuration based on the JSON path expressions you identified as shown below.</p> <p><pre><code>@source(type='http', \n    receiver.url='http://localhost:5005/SweetProductionEP', \n    @map(type = 'json', \n        @attributes(name = '$.sweet', amount = '$.batch.count')))\ndefine stream SweetProductionStream (name string, amount double);\n</code></pre> The attributes configuration is annotated via the <code>@attributes</code> annotation within the <code>@map</code> annotation.</p> </li> </ul>"},{"location":"guides/transforming-data/#transforming-the-message-format-when-publishing-data","title":"Transforming the message format when publishing data","text":"<p>The different destinations to which you need to publish data via WSO2 Streaming Integrator accept data in different formats. Therefore, WSO2 Streaming Integrator supports publishing in a range of formats.</p> <p>WSO2 Streaming Integrator can publish messages in the default format or in a custom format. </p> <ul> <li> <p>Publishing messages in default format</p> <p>To understand this, consider a scenario where you receive production information in the JSON format, but you need to publish that information in a file in the XML format.</p> <p>To do this, you can define your output stream and connect a sink to it with a mapper of the <code>xml</code> type. as shown below.</p> <p><pre><code>@sink(type = 'file', \n    file.uri = \"file:/Users/foo/productions.csv\",\n    @map(type = 'xml'))\ndefine stream OutputStream (name string, amount double);\n</code></pre> Here, you have included the <code>@map</code> annotation within the <code>@sink</code> annotation and specified the mapping type as <code>xml</code> so that the WSO2 Streaming Integrator can publish the output in XML format.</p> </li> <li> <p>Publishing messages in custom format</p> <p>The schema of the event accepted by the destination to which you are sending your output can be different to the schema of your input event or the schema of the event at the time you were processing it. Therefore, WSO2 Streaming Integrator allows you to perform custom mapings when you publish the output. The custom mapping needs to be annotated to the mapping configuration via the <code>@payload</code> annotation.</p> <p>To understand this, consider the example of a sweet factory that needs to send it's production report to the factory manager. The output stream in which the output is generated after processing can be as follows:</p> <p><pre><code>define stream SweetProductionStream (name string, amount double);\n</code></pre> A sample JSON event that is published from the above schema without any custom mapping is as shown in the example below:</p> <pre><code>{\n\"event\":{\n\"name\":gingerbread,\n\"amount\":100\n}\n}\n</code></pre> <p>However, the head office system expects to receive the event in the following format.</p> <pre><code>{\"Product\":{\n\"ProductionData\":{\n\"Name\":gingerbread,\n\"Quantity\":100\n}\n}\n}\n</code></pre> <p>To achieve this, the sink connected to the output event stream needs to have a mapping configuration defined within it as shown below.</p> <pre><code>@sink(type='inMemory', \n    topic='{{ production }}', \n    @map(type='json', \n        enclosing.element='$.Product', \n        validate.json='true', \n        @payload( \"\"\"{\"ProductionData\":{\"Name\":\"{{ name }}\",\"Quantity\":{{ amount }}}}\"\"\")))\ndefine stream SweetProductionStream (name string, amount double);\n</code></pre> <p>In the above example, the mapping type the JSON. The <code>@payload</code> annotation encloses a JSON string that defines the custom mapping. The values for <code>name</code> and <code>amount</code> attributes in the stream are presented as <code>Name</code> and <code>Quantity</code> respectively. These are nested under <code>ProductionData</code> which is turn is enclosed in the <code>Product</code> enclosing element as per the required format.</p> </li> </ul>"},{"location":"guides/transforming-data/#transforming-with-inline-operators","title":"Transforming with inline operators","text":"<p>WSO2 Streaming Integrator is shipped with inline operators that allow you to do certain transformations to streaming data without downloading additional Siddhi extensions from the Siddhi store.</p> <p>For example, assume that instead of the amount produced during the specific production run, you need to present the total amount produced for the given product with each production run as well as the average. For this, you can write a query as follows:</p> <pre><code>define stream SweetProductionStream (name string, amount double);\n\n@info(name = 'Calculate Total and Average')\nfrom SweetProductionStream \nselect name, amount, sum(amount) as total, avg(amount) as average \ngroup by name\ninsert into ProductionTotalsStream;\n</code></pre> <p>In this example, the input event that reports only the name of the product and the amount produced is transformed into an output event that reports the product name, amount produced, the total produced for the given product, and the average produced per production run for the given product. The <code>group by</code> clause ensures that the calculations are done per product name. The <code>sum()</code> and <code>avg()</code> inline operators calculate the total and the average recpectively.</p>"},{"location":"guides/transforming-data/#transforming-with-supported-siddhi-extensions","title":"Transforming with supported Siddhi extensions","text":"<p>When you want to perform more advanced transformations that are not supported by the inline operators of the WSO2 Streaming Integrator, you can use one or more of the Siddhi extensions from the Siddhi Store.</p> <p>Some of these extensions are shipped with the WSO2 Streaming Integrator by default. If you want to use a Siddhi extension that is not shipped by default, you need to download and install it following the instructions in Downloading and Installing Siddhi Extensions.</p> <p>The following table describes the complete list of extensions that provide data transformation functionality.</p> Extension Description Siddhi-execution-math Transforms data by performing mathematical operations. Siddhi-execution-unitconversion Performs unit conversions ranging from length, weight, volume, etc. Siddhi-execution-string Performs string manipulations. Siddhi-execution-time Performs time-based transformations such as converting time zones. Siddhi-execution-map Converts events into maps and performs transformations such as concatenating and removing attributes. Siddhi-execution-reorder Rearranges the order of the incoming event flow. Siddhi-execution-json Performs manipulations to JSON strings. Siddhi-execution-list Performs manipulations to lists. Siddhi-gpl-pmml Processes the input stream attributes according to the defined PMML standard model and outputs the processed results together with the input stream attributes. Siddhi-execution-regex Finds the subsequence that matches the given regex pattern. Siddhi-execution-geo Provides geo data related functionality such as such as geocode, reverse geocode and finding places based on IP. Siddhi-execution-env Allows you to read environment properties inside Siddhi stream definitions and use it in queries. Siddhi-execution-streeamingml Provides streaming machine learning (clustering, classification and regression) for event streams. Siddhi-execution-tensorflow Siddhi-execution-r Processes events with R scripts."},{"location":"guides/transforming-data/#writing-a-custom-script-to-transform-data","title":"Writing a custom script to transform data","text":"<p>To write a custom script to transform data, you can use the siddhi-script-js Siddhi extension.</p> <p>For example, if the Sweet Factory wants to check whether the production amount reported in a production run is greater than the average production up to that production run, you can write a custom function shown in the following sample query.</p> <pre><code>from ProductionTotalsStream \nselect name, amount, total, average, js:eval(\"amount &gt; average\", 'bool') as exceedsAverage\ngroup by name \ninsert into MonitorProductionTrendStream;\n</code></pre> <p>Here, <code>js:eval(\"amount &gt; average\", 'bool') as exceedsAverage</code> is a custom function where an attribute named <code>exceedsAverage</code> returns the value <code>true</code> or <code>false</code> depending on whether the value for the <code>amount</code> attribute exceeds the value for the <code>average</code> attribute.</p>"},{"location":"guides/transforming-data/#try-it-out","title":"Try it out","text":"<p>To try out the transformations described above with some of the given examples, follow the steps below:</p> <ol> <li> <p>Start and Access Streaming Integrator Tooling.</p> </li> <li> <p>Open a new file. Then add and save the following Siddhi application.</p> <pre><code>@App:name('ProductionTotalsApp')\n@App:description('Transform Production Statistics')\n\n@source(type = 'http', receiver.url = \"http://localhost:5005/SweetProductionEP\",\n    @map(type = 'json',\n        @attributes(amount = \"$.batch.count\", name = \"$.sweet\")))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'file', file.uri = \"file:/Users/foo/productions.json\",\n    @map(type = 'json', validate.json = \"false\", enclosing.element = \"$.Product\",\n        @payload(\"\"\"{\"ProductionData\":{\"Name\":\"{{ name }}\",\"Quantity\":\"{{ amount }}\",\"Total\":{{ total }},\"Average\":{{ average }}}}\"\"\")))\ndefine stream ProductionTotalsStream (name string, amount double, total double, average double);\n\n@sink(type = 'log', prefix = \"Exceeds Average\",\n    @map(type = 'text'))\ndefine stream MonitorProductionTrendStream (name string, exceedsAverage bool);\n\n@info(name = 'Calculate Total and Average')\nfrom SweetProductionStream \nselect name, amount, sum(amount) as total, avg(amount) as average \n    group by name \ninsert into ProductionTotalsStream;\n\n@info(name = 'Compare with Average')\nfrom ProductionTotalsStream \nselect name, js:eval(\"('amount' &gt; 'average')\", 'bool') as exceedsAverage\ngroup by name \ninsert into MonitorProductionTrendStream;\n</code></pre> </li> </ol> <p>This Siddhi application does the following transformations:</p> <ul> <li> <p>Derives the values for <code>name</code> and <code>amount</code> attributes representing the name of the product and the amount produced. This is derived from input events sent is a custom format where the two required values are provided under the <code>Sweet</code> and <code>count</code> attributes, and the <code>count</code> attribute is nested under another attribute named <code>batch</code>.</p> </li> <li> <p>Publishes the production statistics in a custom format. <code>name</code> and <code>amount</code> attributes are presented as <code>Name</code> and <code>Quantity</code>, and nested under <code>ProductionData</code> in the <code>Product</code> enclosing element. These events are published in the <code>Users/foo/productions.json</code> file.</p> <pre><code>!!! tip\n    You can save the `productions.json` file mentioned above in a different location of your choice if required.</code></pre> </li> <li> <p>Calculates the total production amount and the average production amount per sweet, and presents them as values for the <code>total</code> and <code>average</code> attributes in the output event published in the <code>productions.json</code> file.</p> </li> <li> <p>Uses a custom script to check whether the amount produced of a sweet in the production run is greater than the average production for that sweets, and logs <code>true</code> or <code>false</code> in the terminal in the text format.</p> </li> <li> <p>To simulate events for this Siddhi application, issue the following six CURL commands.</p> <pre><code>curl -X POST \\\n  http://localhost:5005/SweetProductionEP \\\n  -H 'content-type: application/json' \\\n  -d '{\n  \"sweet\": \"Jaffa Cake\",\n  \"batch\": {\n    \"batch id\": \"batch1\",\n    \"count\": 10\n  }\n}'\n</code></pre> <pre><code>curl -X POST \\\nhttp://localhost:5005/SweetProductionEP \\\n-H 'content-type: application/json' \\\n-d '{\n\"sweet\": \"Gingerbread\",\n\"batch\": {\n \"batch id\": \"batch1\",\n \"count\": 65\n}\n}'\n</code></pre> <pre><code>curl -X POST \\\n http://localhost:5005/SweetProductionEP \\\n -H 'content-type: application/json' \\\n -d '{\n \"sweet\": \"Jaffa Cake\",\n \"batch\": {\n   \"batch id\": \"batch1\",\n   \"count\": 15\n }\n}'\n</code></pre> <pre><code>curl -X POST \\\nhttp://localhost:5005/SweetProductionEP \\\n-H 'content-type: application/json' \\\n-d '{\n\"sweet\": \"Gingerbread\",\n\"batch\": {\n \"batch id\": \"batch1\",\n \"count\": 55\n}\n}'\n</code></pre> <pre><code>curl -X POST \\\nhttp://localhost:5005/SweetProductionEP \\\n-H 'content-type: application/json' \\\n-d '{\n\"sweet\": \"Jaffa Cake\",\n\"batch\": {\n  \"batch id\": \"batch1\",\n  \"count\": 25\n}\n}'\n</code></pre> <pre><code>curl -X POST \\\nhttp://localhost:5005/SweetProductionEP \\\n-H 'content-type: application/json' \\\n-d '{\n\"sweet\": \"Gingerbread\",\n\"batch\": {\n  \"batch id\": \"batch1\",\n  \"count\": 45\n}\n}'\n</code></pre> </li> <li> <p>Open the <code>Users/foo/productions.json</code> file. The following content is available in it.</p> <pre><code>{\"Product\":{\"ProductionData\":{\"Name\":\"Jaffa Cake\",\"Quantity\":\"10.0\",\"Total\":10.0,\"Average\":10.0}\n{\"Product\":{\"ProductionData\":{\"Name\":\"Gingerbread\",\"Quantity\":\"65.0\",\"Total\":65.0,\"Average\":65.0}\n{\"Product\":{\"ProductionData\":{\"Name\":\"Jaffa Cake\",\"Quantity\":\"15.0\",\"Total\":25.0,\"Average\":12.5}\n{\"Product\":{\"ProductionData\":{\"Name\":\"Gingerbread\",\"Quantity\":\"55.0\",\"Total\":120.0,\"Average\":60.0}\n{\"Product\":{\"ProductionData\":{\"Name\":\"Jaffa Cake\",\"Quantity\":\"25.0\",\"Total\":50.0,\"Average\":16.666666666666668}\n{\"Product\":{\"ProductionData\":{\"Name\":\"Gingerbread\",\"Quantity\":\"45.0\",\"Total\":165.0,\"Average\":55.0}\n</code></pre> </li> <li> <p>Check the Streaming Integrator Tooling terminal. The following is logged in it.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Exceeds Average : name:\"Jaffa Cake\",\nexceedsAverage:false\n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Exceeds Average : name:\"Gingerbread\",\nexceedsAverage:false\n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Exceeds Average : name:\"Jaffa Cake\",\nexceedsAverage:true\n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Exceeds Average : name:\"Gingerbread\",\nexceedsAverage:false\n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Exceeds Average : name:\"Jaffa Cake\",\nexceedsAverage:true\n</code></pre> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Exceeds Average : name:\"Gingerbread\",\nexceedsAverage:false\n</code></pre> </li> </ul>"},{"location":"guides/triggering-integration-flows/","title":"Triggering Integration Flows","text":""},{"location":"guides/triggering-integration-flows/#introduction","title":"Introduction","text":"<p>Once the Streaming Integrator processes streaming data and generates an output, you are often required to take some action based on that output. The action required could be executing some code, calling an external service or triggering a complex integration flow. When it is required to trigger an integration flow, the Streaming Integrator can send a request to the Micro integrator to initiate such action.</p>"},{"location":"guides/triggering-integration-flows/#triggering-integration-via-streaming-integrator-as-fire-and-forget-manner","title":"Triggering integration via Streaming Integrator as fire and forget manner","text":"<p>In order to allow the Streaming Integrator to trigger an integration flow in the Micro Integrator, you need to do the following:</p> <ul> <li> <p>Design a Siddhi application with a <code>grpc-call</code> sink that allows an output event to be generated as a request that is sent to the Micro Integrator.</p> </li> <li> <p>Deploy the required artifacts in the Micro Integrator so that the Micro Integrator is triggered to take the required action when it receives the request from the Streaming Integrator.</p> </li> </ul>"},{"location":"guides/triggering-integration-flows/#designing-the-siddhi-application-in-the-streaming-integrator","title":"Designing the Siddhi application in the Streaming Integrator","text":"<p><code>gRPC</code> sink is a Siddhi extension via which you can send messages in a fire and forget manner from SI to MI and trigger a sequence.</p> <p>The following is a sample Siddhi application with a <code>gRPC</code> sink that triggers a sequence named <code>inSeq</code> in the micro integrator.</p> <pre><code>@App:name(\"grpc-call\")\n@App:description(\"This siddhi application will trigger inSeq in the MicroIntegrator\")\n\n@Source(type = 'http',\n        receiver.url='http://localhost:8006/productionStream',\n        basic.auth.enabled='false',\n        @map(type='json'))\ndefine stream InputStream(message string, headers string);\n\n@sink(\n    type='grpc',\n    publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/consume/inSeq',\n    headers='Content-Type:json',\n    metadata='Authorization:Basic YWRtaW46YWRtaW4=',\n    @map(type='json')\n)\ndefine stream FooStream (message string, headers string);\n\nfrom InputStream\nselect *\ninsert into FooStream;\n</code></pre> <p>Note the following about the <code>grpc-call</code> sink configuration:</p> <ul> <li> <p><code>consume</code> in the publisher URL path: This indicates that the gRPC request invokes the <code>consume</code> method of the Micro Integrator's gRPC inbound endpoint. This method does not send a response back to the client.</p> </li> <li> <p><code>headers</code> parameter: This is required to pass the content type so that the system can construct and read the message from the Micro Integrator.</p> </li> </ul> <p>After creating the Siddhi application:</p> <p>To deploy the above Siddhi application, save it as a <code>.siddhi</code> file in the <code>&lt;SI_HOME&gt;/WSO2/server/deployment/siddhi-files</code> directory.</p>"},{"location":"guides/triggering-integration-flows/#deploying-the-required-artifacts-in-the-micro-integrator","title":"Deploying the required artifacts in the Micro Integrator","text":"<p>The following artifacts need to be deployed in the Micro Integrator.</p> <ul> <li> <p>To start  gRPC server in the Micro Integrator son that it can receive the gRPC event sent by the Streaming Integrator, you need to deploy a gRPC inbound endpoint (similar to the sample configuration given below) by saving it as a <code>.xml</code> file in the <code>&lt;MI_HOME&gt;/repository/deployment/server/synapse-configs/default/inbound-endpoints</code> directory.</p> <p>Info</p> <p>Currently, WSO2 Integration Studio does not support GRPC Inbound Endpoint. This capability will be available in a future release.  For now, you need to create the inbound endpoint manually as an XML file.</p> </li> <li> <p>Both the inbound endpoint and the <code>grpc-call</code> sink in the Siddhi application refers to a sequence (<code>inSeq</code> in this example). A sequence with the same name and the required configuration should be added to the <code>&lt;MI_HOME&gt;/repository/deployment/server/synapse-configs/default/sequences</code> directory. The following is a sample configuration.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"&gt;\n&lt;log level=\"full\"/&gt;\n&lt;respond/&gt;\n&lt;/sequence&gt;\n</code></pre> </li> </ul>"},{"location":"guides/triggering-integration-flows/#triggering-integration-via-streaming-integrator-and-receiving-a-response-from-mi","title":"Triggering integration via Streaming Integrator and receiving a response from MI","text":"<p>The <code>gRPC-call</code> sink allows the Streaming Integrator to send messages to the Micro Integrator, trigger a sequence, and get a response back. In order to receive that response, the Streaming Integrator needs to use the <code>grpc-call-response</code> source.</p> <p>The following is a sample Siddhi application with a  <code>gRPC-call</code> sink that triggers a sequence named  <code>inSeq</code> in the Micro Integrator and then uses the <code>grpc-call response</code> source to process the response received from the Micro Integrator.</p> <pre><code>@App:name(\"grpc-call-response\")\n@App:description(\"Description of the plan\")\n\n@sink(\n    type='grpc-call', \n    publisher.url = 'grpc://localhost:8888/org.wso2.grpc.EventService/process/inSeq', \n    sink.id= '1', \n    headers='Content-Type:json', \n    @map(type='json')) \ndefine stream FooStream (message string, headers string);\n\n@source(type='grpc-call-response', sink.id= '1', @map(type='json'))\ndefine stream BarStream (message string);\n\n@Source(type = 'http',\n        receiver.url='http://localhost:8006/productionStream',\n        basic.auth.enabled='false',\n        @map(type='json'))\ndefine stream InputStream(message string, headers string);\n\nfrom InputStream\nselect *\ninsert into FooStream;\n\nfrom BarStream\nselect *\ninsert into TempStream;\n</code></pre> <p>Note the following about the <code>grpc-call</code> sink configuration:</p> <ul> <li> <p><code>process</code> in the publisher URL path: This indicates that the gRPC request invokes the <code>process</code> method of the gRPC server of the Micro Integrator's inbound endpoint that sends a response back to the client.</p> </li> <li> <p><code>sink.id</code> parameter: This is required when using the gRPC-call sink in order to map the request with its corresponding response.</p> </li> </ul> <p>After creating the Siddhi application:</p> <p>To deploy the above Siddhi application, save it as a <code>.siddhi</code> file in the <code>&lt;SI_HOME&gt;/WSO2/server/deployment/siddhi-files</code> directory.</p> <p>Once the Siddhib application is created and deployed, deploy the following artifacts in the Micro Integrator:</p> <ul> <li> <p>In order to start a gRPC server in the Micro Integrator to receive the gRPC event sent by the Streaming Integrator, deploy a GRPC inbound endpoint by adding the following sample configuration as a <code>.xml</code> file to the <code>&lt;MI_Home&gt;/repository/deployment/server/synapse-configs/default/inbound-endpoints</code> directory.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;inboundEndpoint xmlns=\"http://ws.apache.org/ns/synapse\"\nname=\"GrpcInboundEndpoint\"\nsequence=\"inSeq\"\nonError=\"fault\"\nprotocol=\"grpc\"\nsuspend=\"false\"&gt;\n&lt;parameters&gt;\n&lt;parameter name=\"inbound.grpc.port\"&gt;8888&lt;/parameter&gt;\n&lt;/parameters&gt;\n&lt;/inboundEndpoint&gt;\n</code></pre> </li> <li> <p>Add the following sample <code>inSeq</code> sequence to the <code>&lt;MI_HOME&gt;/repository/deployment/server/synapse-configs/default/sequences</code> directory.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;sequence xmlns=\"http://ws.apache.org/ns/synapse\" name=\"inSeq\"&gt;\n&lt;log level=\"full\"/&gt;\n&lt;respond/&gt;\n&lt;/sequence&gt;\n</code></pre> </li> </ul> <p>The respond mediator sends the response back to the Streaming Integrator.</p>"},{"location":"guides/use-cases/","title":"WSO2 Streaming Integrator Use Cases Overview","text":"<p>WSO2 Streaming Integrator receives input data, processes it and presents an output as shown in the diagram below.</p> <p></p> <p>As shown in the diagram above, WSO2 Streaming Integrator first extracts/receives input data, processes them and then presents the output by publishing/loading/writing it. You can perform supporting activities such as error handling, performing queries via API and analyzing metrics.</p>"},{"location":"guides/use-cases/#extractingreceiving-data","title":"Extracting/receiving data","text":"<p>WSO2 Streaming Integrator can extract static data from sources such as databases, files, and cloud storages, as well as receive data in transit from data publishers and messaging systems.</p> <p>For more information, see the following guides:</p> <ul> <li>Extracting Data from Static Sources in Real Time</li> <li>Receiving Data in Transit</li> </ul>"},{"location":"guides/use-cases/#stream-processing","title":"Stream Processing","text":"<p>The different types of stream processing activities that can be performed by WSO2 Streaming Integrator are as follows:</p> <ul> <li> <p>Cleansing data</p> </li> <li> <p>Transforming data</p> </li> <li> <p>Enriching data</p> </li> <li> <p>Aggregating data</p> </li> <li> <p>Correlating data</p> </li> </ul> <p>For more information about the above processing activities, see Processing Data</p>"},{"location":"guides/use-cases/#publishingloadingwriting-data","title":"Publishing/loading/writing data","text":"<p>Once the received data is processed, the output can be stored in a database, written in a file and/or saved in a cloud storage to be saved in a static manner. Alternatively, it can be published to a destination via a messaging system or a data publisher.</p> <p>For more information about how the output is published, see the following topics:</p> <ul> <li>Loading and Writing Data</li> <li>Publishing Data</li> </ul>"},{"location":"guides/use-cases/#supporting-tasks","title":"Supporting tasks","text":"<p>The following guides cover the supporting tasks that can be performed by WSO2 Streaming Integrator when processing Streaming Data.</p> <ul> <li> <p>Error Handling</p> </li> <li> <p>Metrics (Monitoring )</p> </li> <li> <p>Query API</p> </li> </ul>"},{"location":"overview/about_this_release/","title":"About this Release","text":"<p>WSO2 EI 7.0.0 is the successor of WSO2 EI 6.5.0.</p>"},{"location":"overview/architecture/","title":"Architecture","text":"<p>Stream integration refers to collecting, processing and, integrate or acting on data generated during business activities by various sources. This description is paramount when designing a solution to address a streaming integration use case. </p> <ul> <li> <p>Collecting: Receiving or capturing data from various data sources.</p> </li> <li> <p>Processing: Manipulation of data to identify interesting patterns and to extract information.</p> </li> <li> <p>Integrating: Making processed data available for consumers to consume in an expected format via a given protocol or a medium.</p> </li> <li> <p>Acting: Taking actions based on the results and findings done via processing the data. The action can be executing some random code, calling an external service, or triggering a complex integration. </p> </li> </ul> <p>The Streaming Integrator architecture reflects this natural flow in its design as illustrated below.</p> <p></p> <p>The Streaming Integrator contains Siddhi.io as its core to collect, and 60+ connectors to connect with various sources and destinations. The following are the major components and constructs of the Streaming Integrator.</p>"},{"location":"overview/architecture/#source","title":"Source","text":"<p>The source is the construct that is used in Siddhi to receive data from an external source. A stream must be attached to the source so that the data received at the source is passed into the stream to be accessed in subsequent queries. A source handles all the transport-related functionality when connecting to a data source.</p>"},{"location":"overview/architecture/#source-mapper","title":"Source Mapper","text":"<p>Data received by the source is passed into the source mappers and it maps incoming data to a format understandable by the Siddhi engine.</p>"},{"location":"overview/architecture/#siddhi","title":"Siddhi","text":"<p>Siddhi is a stream processing library written in Java. Users can write stream processing logic in the Siddhi Query Language (SiddhiQL). The Siddhi engine can run such queries against continuous streams of data.</p>"},{"location":"overview/architecture/#siddhi-app","title":"Siddhi App","text":"<p>A Siddhi application is a flat-file with siddhi extensions. It includes stream processing logic written in SiddhiQL. This is the deployable artifact in Siddhi which developers compile by writing stream processing logic.</p>"},{"location":"overview/architecture/#siddhiql","title":"SiddhiQL","text":"<p>An SQL-like language that lets users write stream processing logic that can be read by the Siddhi engine.</p>"},{"location":"overview/architecture/#store-api","title":"Store API","text":"<p>A REST API hosted in the SI server to let users fetch data stored in a persistent store or in-memory on-demand using ad-hoc siddhi queries.</p>"},{"location":"overview/architecture/#sink","title":"Sink","text":"<p>The sink is a construct in Siddhi that publishes data to an external destination. You can attach a stream to a sink so that all the events flowing through this stream are published via the sink. The sink handles all transport-related functionality.</p>"},{"location":"overview/architecture/#sink-mapper","title":"Sink Mapper","text":"<p>Event streams to be published in an external destination have to be mapped to a data format (e.g., JSON, XML) required by the destination. The sink mapper does this mapping.</p>"},{"location":"overview/overview/","title":"Introduction","text":"<p>WSO2 Streaming Integrator(SI) is a streaming data processing server that integrates streaming data and takes action based on streaming data. The streaming integration capabilities of EI are delivered via this runtime</p> <p>WSO2 SI can be effectively used for:</p> <ul> <li>Realtime ETL: CDC for DBs, tailing files, scraping HTTP Endpoints, etc.</li> <li>Work with streaming messaging systems: It is fully compatible with Kafka and NATS, and provides advanced stream processing capabilities required to utilize the full potential of streaming data.</li> <li>Streaming data Integration: Allows you to treat all data sources as streams and connect them with any destination.</li> <li>Execute complex integrations based on streaming data: SI has native support to work hand-in-hand with WSO2 Micro integrator to trigger complex integration flows based on decisions derived via stateful stream processing logic.</li> </ul> <p>Try it out!</p> <p>To try out each of the above use cases with SI, see Tutorials</p>"},{"location":"overview/overview/#key-features","title":"Key Features","text":"<p>WSO2 SI is powered by Siddhi.io, a well known cloud native open source stream processing engine. Siddhi allows you to write complex stream processing logic using an intuitive SQL-like language known as SiddhiQL. You can perform the following actions on the fly using Siddhi queries and constructs.</p> <ul> <li>Extracting data from static sources in real time.</li> <li>Loading and writing data to databases, files and cloud based storages.</li> <li>Receiving data in transit from message brokers and data publishers.</li> <li>Publishing Data to destinations.</li> <li>Stream processing activities including cleansing, transforming, enriching correlating, and summarizing data.</li> <li>Performing real time ETL.</li> </ul> <p></p> <p>With 60+ prebuilt and well tested collection of connectors, WSO2 SI allows you to connect any data source with any destination regardless of the different protocols and data formats used by the different endpoints.</p> <p>The SI Store API exposes aggregated and collected data streams to in-memory and persistence storages via a REST API, allowing you to execute queries and generate summarized information on demand.</p> <p>Synapse integration flows deployed in WSO2 Micro Integration(MI) can be executed directly by SI. This allows you to build robust data processing and integration pipelines by combining powerful stream processing and integration capabilities.</p>"},{"location":"overview/overview/#tooling","title":"Tooling","text":"<p>WSO2 SI is coupled with the Streaming Integrator Tooling; a comprehensive streaming integration flow designer for developing) Siddhi applications  by writing queries or via the drag-and-drop functionality, testing them thoroughly before using them in production, and then deploying them in the SI server or exporting them to be deployed as a K8 or a Docker image.</p>"},{"location":"overview/overview/#centralized-and-decentralized-deployment","title":"Centralized and Decentralized Deployment","text":"<p>Being container-friendly by design with a small image size, low resource footprint, a startup time less than two seconds, etc., WSO2 SI can be easily deployed in VMs, Docker or K8s.</p> <p>Its native support for Kubernetes with a K8s Operator provides a convenient way to deploy SI on a K8s cluster with a single command, thereby eliminating the need for manual configurations.</p> <p>Deploying SI as a highly available minimum HA cluster allows you to achieve zero data loss with just two SI nodes.</p> <p>What's Next</p> <ul> <li>Quick Start Guide</li> <li>Create Your First Siddhi Application</li> </ul>"},{"location":"quick-start-guide/hello-world-with-docker/","title":"Getting the Streaming Integrator Running with Docker in 5 Minutes","text":""},{"location":"quick-start-guide/hello-world-with-docker/#introduction","title":"Introduction","text":"<p>This guide shows you how to run Streaming Integrator in Docker. This involves installing Docker, running the Streaming Integrator in Docker and then deploying and running a Siddhi application in the Docker environment.</p> <p>Before you begin:</p> <ul> <li> <p>The system requirements are as follows:</p> <ul> <li>3 GHz Dual-core Xeon/Opteron (or latest)</li> <li>8 GB RAM</li> <li>10 GB free disk space</li> </ul> </li> <li> <p>Install Docker by following the instructions provided in here.</p> </li> </ul>"},{"location":"quick-start-guide/hello-world-with-docker/#downloading-and-installing-the-streaming-integrator","title":"Downloading and installing the Streaming Integrator","text":"<p>In this scenario, you are downloading and installing the Streaming Integrator via Docker.</p> <p>WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2.</p> <p>To run the Streaming Integrator in the  open source image that is available for it</p> <ol> <li> <p>To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command.</p> <p><code>docker run -it wso2/streaming-integrator</code></p> </li> <li> <p>Expose the required ports via docker when running the docker container. In this scenario, you need to expose the following ports:</p> <ul> <li>The 9443 port where the Streaming Integrator server is run.</li> <li>The 8006 HTTP port from which Siddhi application you are deploying in this scenario receives messages.</li> </ul> <p>To expose these ports, issue the following command.</p> <p><code>docker run -p 9443:9443 -p 8006:8006 wso2/streaming-integrator</code></p> </li> </ol>"},{"location":"quick-start-guide/hello-world-with-docker/#creating-and-deploying-the-siddhi-application","title":"Creating and deploying the Siddhi application","text":"<p>Let's create a simple Siddhi application that receives an HTTP message, does a simple transformation to the message, and then logs it in the SI console.</p> <ol> <li> <p>Start the Streaming Integrator Tooling via one of the following methods depending on your operating system:</p> <ul> <li> <p>On MacOS/Linux/CentOS, open a terminal and issue the following command:</p> <p><code>sudo wso2si-tooling-&lt;VERSION&gt;</code></p> </li> <li> <p>On windows, go to Start Menu -&gt; Programs -&gt; WSO2 -&gt; Streaming Integrator Tooling. A terminal opens.</p> </li> </ul> <p>Then access the Streaming Integration Tooling via the <code>http://&lt;HOST_NAME&gt;:&lt;TOOLING_PORT&gt;/editor</code> URL.</p> <p>Info<p>The default URL is <code>http://&lt;localhost:9390/editor</code>.</p> </p> <p>The Streaming Integration Tooling opens as shown below.    </p> </li> <li> <p>Click New and copy-paste the following Siddhi application to the new file you opened.</p> <pre><code>@App:name('MySimpleApp')\n\n@App:description('Receive events via HTTP transport and view the output on the console')\n\n@Source(type = 'http', receiver.url='http://0.0.0.0:8006/productionStream', basic.auth.enabled='false',\n   @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream TransformedProductionStream (nameInUpperCase string, amount double);\n\n-- Simple Siddhi query to transform the name to upper case.\nfrom SweetProductionStream\nselect str:upper(name) as nameInUpperCase, amount\ninsert into TransformedProductionStream;\n</code></pre> <p>Note</p> <p>Note the following about this Siddhi application. - The Siddhi application operates in Docker. Therefore, the HTTP source configured in it uses a receiver URL where the host number is <code>0.0.0.0</code>. - The <code>8006</code> port of the receiver URL is the same HTTP port that you previously exposed via Docker.</p> </li> <li> <p>Save the Siddhi application by clicking File =&gt; Save.</p> </li> <li> <p>To deploy the Siddhi application, click the Deploy menu option and then click Deploy to Server. The Deploy Siddhi Apps to Server dialog box opens as shown in the example below.</p> <p></p> <ol> <li> <p>In the Add New Server section, enter information as follows:</p> Field Value Host 0.0.0.0 Port <code>9443</code> User Name <code>admin</code> Password <code>admin</code> <p></p> <p>Then click Add.</p> </li> <li> <p>Select the check boxes for the MySimpleApp Siddhi application and the server you added as shown below.</p> <p></p> </li> <li> <p>Click Deploy.</p> <p>When the Siddhi application is successfully deployed, the following message appears in the Deploy Siddhi Apps to Server dialog box.</p> <p></p> <p>The following is logged in the console in which you started the Streaming Integrator in Docker.</p> <p></p> </li> </ol> </li> </ol>"},{"location":"quick-start-guide/hello-world-with-docker/#trying-out-the-siddhi-application","title":"Trying-out the Siddhi application","text":"<p>To try out the <code>MySimpleApp</code> Siddhi application you deployed in Docker, issue the following CURL command.</p> <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\"  http://0.0.0.0:8006/productionStream --header \"Content-Type:application/json\"\n</code></pre> <p>The following output appears in the console in which you started the Streaming Integrator in Docker.</p> <p></p>"},{"location":"quick-start-guide/hello-world-with-kubernetes/","title":"Getting the Streaming Integrator Running in Kubernetes in 5 Minutes","text":""},{"location":"quick-start-guide/hello-world-with-kubernetes/#introduction","title":"Introduction","text":"<p>This quick start guide gets you to start and run the Streaming Integrator in a Kubernetes cluster in 5 minutes.</p> <p>Before you begin:</p> <ul> <li> <p>Create a Kubernetes cluster. In this quick start guide, you can do this via Minikube as follows.</p> <ol> <li> <p>Install Minikube  and start a cluster by following the Minikube Documentation.</p> </li> <li> <p>Enable ingress on Minikube by issuing the following command.</p> <p><code>minikube addons enable ingress</code></p> </li> </ol> </li> <li> <p>Make sure that you have admin privileges to install the Siddhi operator.</p> </li> </ul>"},{"location":"quick-start-guide/hello-world-with-kubernetes/#installing-the-siddhi-operator-for-the-streaming-integrator","title":"Installing the Siddhi Operator for the Streaming Integrator","text":"<p>To install the Siddhi Operator, follow the procedure below:</p> <ol> <li> <p>To install the Siddhi Kubernetes operator for streaming integrator issue the following commands:</p> <p><code>kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/00-prereqs.yaml</code></p> <p><code>kubectl apply -f https://github.com/wso2/streaming-integrator/releases/download/v1.0.0-m1/01-siddhi-operator.yaml</code></p> </li> <li> <p>To verify whether the Siddhi operator is successfully installed, issue the following command.</p> <p><code>kubectl get deployment</code></p> <p>If the installation is successful, the following deployments should be running in the Kubernetes cluster.</p> <p></p> </li> </ol>"},{"location":"quick-start-guide/hello-world-with-kubernetes/#deploying-siddhi-applications-in-kubernetes","title":"Deploying Siddhi applications in Kubernetes","text":"<p>You can deploy multiple Siddhi applications in one or more selected containers via Kubernetes. In this example, let's deploy just one Siddhi application in one container for the ease of understanding how to run the Streaming Integrator in a Kubernetes cluster.</p> <ol> <li> <p>First, let's design a simple Siddhi application that consumes events via HTTP to detect power surges. It filters events for a specific device type (i.e., dryers) and that also report a value greater than 600 for <code>power</code>.</p> <pre><code>    @App:name(\"PowerSurgeDetection\")\n@App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")\n/*\n        Input: deviceType string and powerConsuption int(Watt)\n        Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.\n    */\n\n\n@source(\ntype='http',\nreceiver.url='${RECEIVER_URL}',\nbasic.auth.enabled='false',\n@map(type='json')\n)\n\ndefine stream DevicePowerStream(deviceType string, power int);\n\n@sink(type='log', prefix='LOGGER')\ndefine stream PowerSurgeAlertStream(deviceType string, power int);\n\n@info(name='surge-detector')\nfrom DevicePowerStream[deviceType == 'dryer' and power &gt;= 600]\nselect deviceType, power\ninsert into PowerSurgeAlertStream;\n</code></pre> </li> <li> <p>The above Siddhi application needs to be deployed via a YAML file. Therefore, enter basic information for the YAML file and include the Siddhi application in a section named <code>spec</code> as shown below.</p> <pre><code>apiVersion: siddhi.io/v1alpha2\nkind: SiddhiProcess\nmetadata:\n  name: streaming-integrator\nspec:\n  apps:\n    - script: |\n        @App:name(\"PowerSurgeDetection\")\n        @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")\n        /*\n            Input: deviceType string and powerConsuption int(Watt)\n            Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.\n        */\n\n        @source(\n          type='http',\n          receiver.url='${RECEIVER_URL}',\n          basic.auth.enabled='false',\n          @map(type='json')\n        )\n        define stream DevicePowerStream(deviceType string, power int);\n        @sink(type='log', prefix='LOGGER')\n        define stream PowerSurgeAlertStream(deviceType string, power int);\n        @info(name='surge-detector')\n        from DevicePowerStream[deviceType == 'dryer' and power &gt;= 600]\n        select deviceType, power\n        insert into PowerSurgeAlertStream;\n</code></pre> </li> <li> <p>Add a section named `container' and and parameters with values to configure the container in which the Siddhi application is to be deployed.</p> <pre><code>container:\nenv:\n  -\n    name: RECEIVER_URL\n    value: \"http://0.0.0.0:8080/checkPower\"\n  -\n    name: BASIC_AUTH_ENABLED\n    value: \"false\"\n</code></pre> <p>Here, you are specifying that Siddhi applications running within the container should receive events to the <code>http://0.0.0.0:8080/checkPower</code> URL and basic authentication is not enabled for them.</p> </li> <li> <p>Add a <code>runner</code> section and add configurations related to authorization such as users and roles. For this example, you can configure this section as follows.</p> <pre><code>runner: |\nauth.configs:\n  type: 'local'        # Type of the IdP client used\n  userManager:\n    adminRole: admin   # Admin role which is granted all permissions\n    userStore:         # User store\n      users:\n      -\n        user:\n          username: root\n          password: YWRtaW4=\n          roles: 1\n      roles:\n      -\n        role:\n          id: 1\n          displayName: root\n  restAPIAuthConfigs:\n    exclude:\n      - /simulation/*\n      - /stores/*\n</code></pre> To view the complete file, click here.<p>apiVersion: siddhi.io/v1alpha2 kind: SiddhiProcess metadata:   name: streaming-integrator-app spec:   apps:     - script: |         @App:name(\"PowerSurgeDetection\")         @App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")         /*             Input: deviceType string and powerConsuption int(Watt)             Output: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.         */</p> <pre><code>    @source(\n      type='http',\n      receiver.url='${RECEIVER_URL}',\n      basic.auth.enabled='false',\n      @map(type='json')\n    )\n    define stream DevicePowerStream(deviceType string, power int);\n    @sink(type='log', prefix='LOGGER')\n    define stream PowerSurgeAlertStream(deviceType string, power int);\n    @info(name='surge-detector')\n    from DevicePowerStream[deviceType == 'dryer' and power &gt;= 600]\n    select deviceType, power\n    insert into PowerSurgeAlertStream;</code></pre> <p>container:     env:       -         name: RECEIVER_URL         value: \"http://0.0.0.0:8080/checkPower\"       -         name: BASIC_AUTH_ENABLED         value: \"false\"</p> <p>runner: |     auth.configs:       type: 'local'        # Type of the IdP client used       userManager:         adminRole: admin   # Admin role which is granted all permissions         userStore:         # User store           users:           -             user:               username: root               password: YWRtaW4=               roles: 1           roles:           -             role:               id: 1               displayName: root       restAPIAuthConfigs:         exclude:           - /simulation/*           - /stores/*</p> </li> <li> <p>Save the file as <code>siddhi-process.yaml</code> in a preferred location</p> </li> <li> <p>To apply the configurations in this YAML file to the Kubernetes cluster, issue the following command.</p> <p><code>kubectl apply -f &lt;PATH_to_siddhi-process.yaml&gt;</code></p> <p>Info</p> <p>This file overrules the configurations in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> </li> </ol>"},{"location":"quick-start-guide/hello-world-with-kubernetes/#invoking-the-siddhi-application","title":"Invoking the Siddhi application","text":"<p>To invoke the <code>PowerSurgeDetection</code> Siddhi application that you deployed in the Kubernetes cluster, follow the steps below.</p> <ol> <li> <p>First, get the external IP of minikube by issuing the following command.</p> <p><code>minikube ip</code></p> <p>Add the IP it returns to the <code>/etc/hosts</code> file in your machine.</p> </li> <li> <p>Issue the following CURL command to invoke the <code>PowerSurgeDetection</code> Siddhi application.</p> <pre><code>curl -X POST \\\n  http://siddhi/streaming-integrator-0/8080/checkPower \\\n    -H 'Accept: */*' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Host: siddhi' \\\n    -d '{\n        \"deviceType\": \"dryer\",\n        \"power\": 600\n        }'\n</code></pre> </li> <li> <p>To monitor the associated logs for the above siddhi application, get a list of the available pods by issuing the following command.</p> <p>`kubectl get pods'</p> <p>This returns the list of pods as shown in the example below.</p> <pre><code>NAME                                        READY    STATUS    RESTARTS    AGE\nstreaming-integrator-app-0-b4dcf85-npgj7     1/1     Running      0        165m\nstreaming-integrator-5f9fcb7679-n4zpj        1/1     Running      0        173m\n</code></pre> </li> <li> <p>To monitor the logs for the required pod, issue a command similar to the following. In this example, the pod to be monitored is <code>streaming-integrator-app-0-b4dcf85-npgj7</code>.</p> <p><code>streaming-integrator-app-0-b4dcf85-npgj7</code></p> </li> </ol>"},{"location":"quick-start-guide/quick-start-guide/","title":"Streaming Integrator Quick Start Guide","text":""},{"location":"quick-start-guide/quick-start-guide/#introduction","title":"Introduction","text":"<p>This quick start guide gets you started with the Streaming Integrator (SI), in just 5 minutes.</p> <p>In this guide, you will download the SI distribution as well as Kafka, and then try out a simple Siddhi application. This Siddhi application consumes messages from a file processes the data, generates an output, and then publishes that output to a Kafka topic.</p> <p>The following is the outline of this quick start guide.</p> <p></p>"},{"location":"quick-start-guide/quick-start-guide/#step-1-download-the-streaming-integrator","title":"Step 1: Download the Streaming Integrator","text":"<p>Download the Streaming Integrator distribution from WSO2 Streaming Integrator site and extract it to a location of your choice. Hereafter, the extracted location is referred to as <code>&lt;SI_HOME&gt;</code>.</p>"},{"location":"quick-start-guide/quick-start-guide/#step-2-start-the-streaming-integrator","title":"Step 2: Start the Streaming Integrator","text":"<p>To start WSO2 Streaming Integrator, navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory from the CLI, and issue the appropriate command based on your operating system:</p> <ul> <li>For Linux: <code>./server.sh</code></li> <li>For Windows: <code>server.bat --run</code></li> </ul>"},{"location":"quick-start-guide/quick-start-guide/#step-3-download-kafka","title":"Step 3: Download Kafka","text":"<p>Download the Kafka broker from the Apache site and extract it. This directory is referred to as <code>&lt;KAFKA_HOME&gt;</code> from here on.</p>"},{"location":"quick-start-guide/quick-start-guide/#step-4-create-and-deploy-a-simple-siddhi-application","title":"Step 4: Create and deploy a simple Siddhi application","text":"<p>Let's create a simple Siddhi application that reads data from a CSV file, does a simple transformation to the data, and then publishes the results to a Kafka topic so that multiple subscriber applications can have access to that data.</p> <p></p> <ol> <li> <p>Download <code>productions.csv</code> file from here and save it in a location of your choice.</p> <p>Info</p> <p>In this example, the file is located in the <code>Users/foo</code>directory.</p> </li> <li> <p>Open a text file and copy-paste following Siddhi application into it.</p> <p>Tip</p> <p>Here, you are instructed to use a text editor to deploy a Siddhi Application that is already tested in order to minimize the time you spend to follow this guide. It is recommended to design Siddhi application via Streaming Integration Tooling that offers features such as syntax checking, event simulation for testing purposes, reformatting code, the option to design applications in a graphical interface or by writing code, and many more. For more information, see Streaming Integrator Tooling Overview.</p> <pre><code>@App:name('ManageProductionStats')\n@App:description('Receive events via file and publish to Kafka topic')\n\n@source(type = 'file', mode = \"LINE\", file.uri = \"file:/Users/foo/productions.csv\", tailing = \"true\",\n    @map(type = 'csv'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type = 'kafka', bootstrap.servers = \"localhost:9092\", topic = \"total_production\", is.binary.message = \"false\", partition.no = \"0\",\n    @map(type = 'json'))\ndefine stream TotalProductionStream (name string, amount double);\n\n-- Simple Siddhi query to calculate production totals.\n@info(name = 'query1')\nfrom SweetProductionStream \nselect name, sum(amount) as amount \ngroup by name\ninsert into TotalProductionStream;\n</code></pre> <p>The above Siddhi application reads input data from a file named <code>production.csv</code> in the CSV format, processes it and publishes the resulting output in a Kafka topic named <code>total_production</code>. As a result, any application that cannot read streaming data, but is capable of subscribing to a Kafka topic can access the output. The each input event reports the amount of a specific sweet produced in a production run. The Streaming Integrator calculates the total produced of each sweet with each event. Therefore, each output event reports the total amount produced for a sweet from the time you started running the Siddhi application. </p> </li> <li> <p>Save this file as <code>ManageProductionStats.siddhi</code> in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> <p>This deploys the <code>ManageProductionStats</code> in the Streaming Integrator. The following message appears to indicate that the Siddhi application is successfully installed.</p> <p><code>INFO {org.wso2.carbon.siddhi.editor.core.internal.WorkspaceDeployer} - Siddhi App ManageProductionStats.siddhi successfully deployed.</code></p> </li> </ol>"},{"location":"quick-start-guide/quick-start-guide/#step-5-install-the-required-extensions","title":"Step 5: Install the required extensions","text":"<p>The <code>ManageProductionStats</code> Siddhi application uses a Kafka sink. However, the Siddhi extension for Kafka is not installed by default. To install it so that the Siddi application can integrate with Kafka as expected, follow the steps below:</p> <ol> <li> <p>Navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory and issue the appropriate command based on your operating system:</p> <ul> <li>For Linux: <code>./extension-installer.sh install</code></li> <li>For Windows: <code>extension-installer.bat install</code></li> </ul> <p>As a result, a message appears in the terminal with a list of extensions used in your Siddhi application that are not completely installed, and requests you to confirm whether the system should proceed to install them</p> </li> <li> <p>Enter <code>Y</code> in the terminal to confirm that you want to proceed to install the required extensions, and then press the return key. Then the following message is displayed to indicate that the extension isinstalled.</p> <p></p> </li> <li> <p>Restart the WSO2 Streaming Integrator server as instructed.</p> </li> </ol>"},{"location":"quick-start-guide/quick-start-guide/#step-6-start-kafka-and-create-a-topic","title":"Step 6: Start Kafka and create a topic","text":"<p>Let's start the Kafka server and create a Kafka topic so that the <code>ManageProductionStats.siddhi</code> application you created can publish its output to it.</p> <p>To start Kafka:</p> <ol> <li> <p>Navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and start a zookeeper node by issuing the following command.</p> <p><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties</code></p> </li> <li> <p>Navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and start Kafka server node by issuing the following command.</p> <p><code>sh bin/kafka-server-start.sh config/server.properties</code></p> </li> </ol> <p>To create a Kafka topic named <code>total_production</code>:</p> <ol> <li> <p>Navigate to <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command:</p> <p><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic total_production</code></p> </li> </ol>"},{"location":"quick-start-guide/quick-start-guide/#step-7-test-your-siddhi-application","title":"Step 7: Test your Siddhi application","text":"<p>To test the <code>ManageProductionStats</code> Siddhi application you created, follow the steps below.</p> <ol> <li> <p>Open the <code>/Users/foo/productions.csv</code> file and add five rows in it as follows:</p> <p><code>Toffee,40.0</code> <code>Almond cookie, 70.0</code> <code>Baked alaska, 30.0</code> <code>Toffee, 60.0</code> <code>Baked alaska, 20.0</code></p> <p>Save your changes.</p> </li> <li> <p>To observe the messages in the <code>total_production</code> topic, navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command:</p> <p><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic total_production --from-beginning</code></p> </li> </ol> <p>You can see the following message in the Kafka Consumer log. </p> <pre><code>{\"event\":{\"name\":\"Almond cookie\",\"amount\":100.0}}\n{\"event\":{\"name\":\"Baked alaska\",\"amount\":120.0}}\n{\"event\":{\"name\":\"Toffee\",\"amount\":160.0}}\n{\"event\":{\"name\":\"Almond cookie\",\"amount\":230.0}}\n{\"event\":{\"name\":\"Baked alaska\",\"amount\":260.0}}\n{\"event\":{\"name\":\"Toffee\",\"amount\":320.0}}\n</code></pre>"},{"location":"quick-start-guide/quick-start-guide/#whats-next","title":"What's next?","text":"<p>Once you try out this quick start guide, you can proceed to one of the following sections.</p> <ul> <li> <p>Learn the basic functionality of the Streaming Integrator in less than 30 minutes by Creating Your First Siddhi Application</p> </li> <li> <p>Try out Streaming Integrator tutorials.</p> </li> <li> <p>Learn how to run WSO2 Streaming Integrator in containerized environments, try Running SI with Docker and Kubernetes</p> </li> </ul>"},{"location":"quick-start-guide/getting-started/create-the-siddhi-application/","title":"Step 2: Create the Siddhi Application","text":"<p>Let's create your first Siddhi application.</p> <p>For this purpose, you can consider an example where production information is published in a database table. This information needs to be captured as and when it is published in the database, and published in a file after changing the case of the product name.</p> <p>The following image depicts the procedure to be followed by the Siddhi application you create.</p> <p></p> <ol> <li> <p>Extract the Streaming Integrator Tooling pack to a preferred location. Hereafter, the extracted location is referred to as <code>&lt;SI_TOOLING_HOME&gt;</code>.</p> </li> <li> <p>Navigate to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issue the appropriate command depending on your operating system to start the Streaming Integration tooling.</p> <ul> <li> <p>For Windows: <code>tooling.bat</code></p> </li> <li> <p>For Linux/MacOS:\u00a0<code>./tooling.sh</code></p> </li> </ul> </li> <li> <p>Access the Streaming Integration Tooling via the <code>http://&lt;HOST_NAME&gt;:&lt;TOOLING_PORT&gt;/editor</code> URL.</p> <p>Info</p> <p>The default URL is <code>http://&lt;localhost:9390/editor</code>.</p> </li> </ol> <p>The Streaming Integration Tooling opens as shown below.</p> <p></p> <ol> <li> <p>Open a new Siddhi file by clicking New.</p> <p>The new file opens as follows.</p> <p></p> </li> <li> <p>Specify a name for the new Siddhi application via the <code>@App:name</code> annotation.</p> <pre><code>@App:name(\"SweetFactoryApp\")\n</code></pre> </li> <li> <p>First, let's define the stream that receives the input data. </p> <p><code>define stream InsertSweetProductionStream (name string,amount double);</code></p> </li> <li> <p>To allow the <code>InsertSweetProductionStream</code> stream to capture inserts from the <code>productions</code> database, connect a source of the <code>cdc</code> type to it as shown below.</p> <p><pre><code>@source(type='cdc',url = \"jdbc:mysql://localhost:3306/production\",username = \"wso2si\",password = \"wso2\",table.name = \"SweetProductionTable\",operation = \"insert\",\n    @map(type='keyvalue'))\ndefine stream InsertSweetProductionStream (name string,amount double);\n</code></pre> 8. To publish the captured data into a file, define an output stream as follows.</p> <p><code>define stream ProductionUpdatesStream (name string,amount double);</code></p> </li> <li> <p>To publish the output events to a file, connect a file source to the stream as shown below:</p> <pre><code>@sink(type='file',file.uri = \"/Users/foo/productioninserts.csv\",\n@map(type='csv'))\ndefine stream ProductionUpdatesStream (name string,amount double);\n</code></pre> </li> </ol> <p>Here, you are publishing the output in the text format to a file named <code>productioninserts.csv</code> in the <code>/Users/foo</code> directory.</p> <ol> <li> <p>To convert the case of the product name from lower case to title case, and then publish the converted events to the file, write a query as follows.</p> <pre><code>@info(name='query1')\nfrom InsertSweetProductionStream \nselect str:upper(name) as name, amount \ngroup by name \ninsert  into ProductionUpdatesStream;\n</code></pre> <p>This query gets the information inserted into the <code>productions</code> database table from the <code>InsertSweetProductionStream</code>stream. The <code>str:upper()</code> function included in the <code>select</code> clause converts the product name from lower case to title case. Once this conversion is done, the converted events are directed to the <code>ProductionUpdatesStream</code> stream. These events are written into the <code>/Users/foo/productioninserts.csv</code> file because it is configured via the <code>file</code> source you previously annotated to the <code>ProductionUpdatesStream</code> stream.</p> </li> <li> <p>Save the Siddhi application.</p> </li> </ol> <p>The completed Siddhi application looks as follows:</p> <pre><code>@App:name('SweetFactoryApp')\n\n@source(type='cdc',url = \"jdbc:mysql://localhost:3306/production\",username = \"wso2si\",password = \"wso2\",table.name = \"SweetProductionTable\",operation = \"insert\",\n    @map(type='keyvalue'))\ndefine stream InsertSweetProductionStream (name string,amount double);\n\n@sink(type='file',file.uri = \"/Users/foo/productioninserts.csv\",\n    @map(type='csv'))\ndefine stream ProductionUpdatesStream (name string,amount double);\n\n@info(name='query1')\nfrom InsertSweetProductionStream \nselect str:upper(name) as name, amount \ngroup by name \ninsert  into ProductionUpdatesStream;\n</code></pre>"},{"location":"quick-start-guide/getting-started/create-the-siddhi-application/#installing-the-required-extensions","title":"Installing the required extensions","text":"<p>The Streaming Integrator is by default shipped with most of the available Siddhi extensions by default. If a Siddhi extension you require is not shipped by default, you can download and install it via the Extension Installer tool. The <code>SweetFactoryApp</code> Siddhi application you created uses a source of the <code>cdc</code> type. The <code>cdc-mysql</code> extension that is required for this source is not shipped with WSO2 Streaming Integrator by default. Therefore, let's install it as follows.</p> <ol> <li> <p>In Streaming Integrator Tooling, click Tools, and then click Extension Installer. The Extension Installer dialog box opens.</p> <p></p> <p>Click Install for Change Data Capture - MySQL.</p> </li> <li> <p>Restart Streaming Integrator Tooling.</p> </li> </ol>"},{"location":"quick-start-guide/getting-started/create-the-siddhi-application/#testing-the-siddhi-application","title":"Testing the Siddhi application","text":"<p>Before deploying the <code>SweetFactoryApp</code> Siddhi application to the Streaming Integrator Server, you need to test it to check whether the Siddhi queries you wrote work as expected. For this purpose, you can simulate events via the Event Simulator in Streaming Integrator Tooling as follows:</p> <p>Tip</p> <p>Although you are using the Event Simulateor instead of performing an insert operation in the MySQL database table you created, you need to start the MySQL server before following the steps below.</p> <ol> <li> <p>In Streaming Integrator Tooling, click the Event Simulator icon in the left pane of the editor to open the Single Simulation panel.</p> <p></p> <p>It opens the left panel for event simulation as follows.</p> <p></p> </li> <li> <p>Enter Information in the Single Simulation panel as described below.</p> <p></p> <ol> <li> <p>In the Siddhi App Name field, select SweetFactoryApp.</p> </li> <li> <p>In the Stream Name field, select InsertSweetProductionStream.</p> </li> <li> <p>Under Attributes, enter values as follows:</p> Attribute Value name (STRING) <code>gingerbread</code> amount (DOUBLE) <code>100</code> </li> </ol> </li> <li> <p>Click Start and Send. </p> </li> <li> <p>Open the <code>/Users/foo/productioninserts.csv</code>. It should contain the following row.</p> <p><code>GINGERBREAD,100.0</code></p> </li> </ol> <p>What's Next?</p> <p>Now you can deploy the <code>SweetFactoryApp</code> Siddhi application you created. To do this, proceed to Step 3: Deploy the Siddhi Application.</p>"},{"location":"quick-start-guide/getting-started/deploy-siddhi-application/","title":"Step 3: Deploy the Siddhi Application","text":"<p>The <code>SweetFactoryApp</code> that you created in Step 2: Create the Siddhi Application is now ready to be deployed in the Streaming Integrator server, exported as a Docker image, or deployed in Kubernetes.</p>"},{"location":"quick-start-guide/getting-started/deploy-siddhi-application/#deploying-in-streaming-integrator-server","title":"Deploying in Streaming Integrator server","text":"<p>To deploy your Siddhi application in the Streaming Integrator server, follow the procedure below:</p> <p>Info</p> <p>To deploy the Siddhi application, you need to run both the Streaming Integrator server and Streaming Integrator Tooling. The home directories of the Streaming Integrator server is referred to as <code>&lt;SI_HOME&gt;</code> and the home directory of Streaming Integrator Tooling is referred to as <code>&lt;SI_TOOLING_HOME&gt;</code>.</p> <ol> <li>Start the Streaming Integrator server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory from the CLI, and issuing the appropriate command based on your operating system:</li> <li>For Windows: <code>server.bat --run</code></li> <li> <p>For Linux/Mac OS: \u00a0<code>./server.sh</code></p> </li> <li> <p>In the Streaming Integrator Tooling, click Deploy and then click Deploy to Server.</p> <p></p> <p>The Deploy Siddhi Apps to Server dialog box opens as follows.</p> <p></p> </li> <li> <p>In the Add New Server section, enter information as follows:</p> Field Value Host Your host Port <code>9443</code> User Name <code>admin</code> Password <code>admin</code> <p></p> <p>Then click Add.</p> </li> <li> <p>Select the check boxes for the SweetFactoryApp.siddhi Siddhi application and the server you added as shown below.</p> <p></p> </li> <li> <p>Click Deploy.</p> <p>As a result, the <code>SweetFactoryApp</code> Siddhi application is saved in the <code>&lt;SI_HOME&gt;/deployment/siddhi-files</code> directory, and the following is message displayed in the dialog box.</p> <p></p> </li> </ol>"},{"location":"quick-start-guide/getting-started/deploy-siddhi-application/#deploying-in-docker","title":"Deploying in Docker","text":"<p>To export the <code>SweetFactoryApp</code> Siddhi application as a Docker artifact, follow the procedure below:</p> <ol> <li> <p>Open the Streaming Integrator Tooling.</p> </li> <li> <p>Click Export in the top menu, and then click For Docker.</p> <p></p> <p>As a result, Step 1 of the Export Siddhi Apps for Docker image wizard opens as follows.</p> <p></p> </li> <li> <p>Select the SweetFactoryApp.siddhi check box and click Next.</p> </li> <li> <p>In Step 2, you can template values of the Siddhi Application.</p> <p></p> <p>Click Next without templating any value of the Siddhi application.</p> <p>Info</p> <p>For detailed information about templating the values of a Siddhi Application, see Exporting Siddhi Apps for Docker Image.</p> </li> <li> <p>In Step 3, you can update configurations of the Streaming Integrator.</p> <p></p> <p>Leave the default configurations, and click Next.</p> </li> <li> <p>In Step 4, you can provide arguments for the values that were templated in Step 2.</p> <p></p> <p>There are no values to be configured because you did not template any values in Step 2. Therefore click Next.</p> </li> <li> <p>In Step 5, you can choose additional dependencies to be bundled. This is applicable when Sources, Sinks and etc. with additional dependencies are used in the Siddhi Application (e.g., a Kafka Source/Sink, or a MongoDB Store).     In this scenario, there are no such dependencies. Therefore nothing is shown as additional JARs.</p> <p></p> <p>Click Export. The Siddhi application is exported as a Docker artifact in a zip file to the default location in your machine, based on your operating system and browser settings.</p> </li> </ol> <p>What's Next?</p> <p>Now you can run the <code>SweetFactoryApp</code> in the Streaming Integrator server. To do this, proceed to Step 4: Running the Siddhi Application</p>"},{"location":"quick-start-guide/getting-started/download-install-and-start-si/","title":"Step 1: Download Streaming Integrator and Dependencies","text":"<p>First, you are required to download the Streaming Integrator and the other software needed for the scenario you are trying out. To do this, follow the topics below.</p> <p>Before you begin:</p> <ul> <li>Install Oracle Java SE Development Kit (JDK) version 1.8.</li> <li>Set the Java home environment variable.</li> </ul>"},{"location":"quick-start-guide/getting-started/download-install-and-start-si/#downloading-the-streaming-integrator-runtime-and-tooling","title":"Downloading the Streaming Integrator runtime and tooling","text":"<ul> <li> <p>To download the Streaming Integrator runtime, visit the Streaming Integrator Product Page. Enter you email address and agree to the license. Then click Zip Archive download the Streaming Integrator as a zip file.</p> </li> <li> <p>To download Streaming Integrator Tooling, click Tooling in the Streaming Integrator Product Page. Enter you email address and agree to the license. Then click MacOS Installer pkg download the Streaming Integrator as a zip file.</p> </li> </ul>"},{"location":"quick-start-guide/getting-started/download-install-and-start-si/#downloading-the-other-dependencies-for-your-scenario","title":"Downloading the other dependencies for your scenario","text":"<p>This section shows how to prepare your production environment for the scenario described in the Streaming Integration Overview section.</p>"},{"location":"quick-start-guide/getting-started/download-install-and-start-si/#setting-up-a-mysql-database-table","title":"Setting up a MySQL database table","text":"<p>In this scenario, the Streaming Integrator reads input data from a MySQL database table. Therefore, let's download and install MySQL and define the database and the database table as follows:</p> <ol> <li> <p>Download MySQL 5.1.49 from MySQL Community Downloads.</p> </li> <li> <p>Enable binary logging in the MySQL server. For detailed instructions, see Debezium documentation - Enabling the binlog.  </p> <p>Note</p> <p>If you are using MySQL 8.0, use the following query to check the binlog status: <pre><code>SELECT variable_value as \"BINARY LOGGING STATUS (log-bin) ::\"\nFROM performance_schema.global_variables WHERE variable_name='log_bin';\n</code></pre></p> </li> <li> <p>Once you install MySQL and start the MySQL server, create the database and the database table you require as follows:</p> <ol> <li> <p>Let's create a new database in the MySQL server which you are to use throughout this tutorial. To do this, execute the following query.     <pre><code>CREATE SCHEMA production;\n</code></pre></p> </li> <li> <p>Create a new user by executing the following SQL query. <code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2si' IDENTIFIED BY 'wso2';</code></p> </li> <li>Switch to the <code>production</code> database and create a new table, by executing the following queries: <code>use production;</code> <code>CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));</code> </li> </ol> </li> </ol>"},{"location":"quick-start-guide/getting-started/download-install-and-start-si/#download-kafka-and-create-topics","title":"Download Kafka and create topics","text":"<p>This scenario involves publishing some filtered production data to a Kafka topic named <code>eclair_production</code>. </p> <ol> <li> <p>Download the Kafka broker from the Apache site and extract it.    This directory is referred to as <code>&lt;KAFKA_HOME&gt;</code> from here on.</p> </li> <li> <p>Start Kafka as follows:</p> <ol> <li> <p>First, start a zoo keeper node. To do this, navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command.</p> <p><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties</code></p> </li> <li> <p>Next, start a Kafka server node. To do this, issue the following command from the same directory.</p> <p><code>sh bin/kafka-server-start.sh config/server.properties</code></p> </li> <li> <p>To create a Kafka topic named <code>eclair-production</code>, issue the following command from the same directory.</p> <p><code>bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic eclair-production</code></p> </li> </ol> </li> </ol>"},{"location":"quick-start-guide/getting-started/download-install-and-start-si/#starting-the-wso2-streaming-integrator-server","title":"Starting the WSO2 Streaming Integrator Server","text":"<p>To start WSO2 Streaming Integrator, navigate to the <code>&lt;SI_HOME&gt;/bin</code> directory from the CLI, and issue the appropriate command based on your operating system:</p> <ul> <li>For Linux: <code>./server.sh</code></li> <li>For Windows: <code>server.bat --run</code> </li> </ul> <p>Now you have completed a WSO2 Streaming Integrator setup that is capable of the following:</p> <ul> <li> <p>Design, test and deploy Siddhi applications via Streaming Integrator Tooling.</p> </li> <li> <p>Consume data from as well as publish data to MySQL databases.</p> </li> <li> <p>Consume data from as well as publish data to Kafka topics.</p> </li> </ul> <p>What's Next?</p> <p>To design a Siddhi application, proceed to Step 2: Create the Siddhi Application.</p>"},{"location":"quick-start-guide/getting-started/getting-started-guide-overview/","title":"Streaming Integration Overview","text":"<p>This section helps you to understand streaming integration and how to perform it with WSO2 Streaming Integrator in less than 30 minutes</p> <p>First, let's understand the concept of streaming integration. For this, let's consider an example of a sweet factory where the system publishes the production details after each production run. The details published include the name of the sweet and the amount produced.</p> <p></p> <p>Each production run that reports the name of the sweet and the amount is an event.</p> <p>Many such events form a stream.</p> <p>In Streaming typically involves handling data received/published via streams in high speed and high volume. In this example, there can be multiple factories in several locations that results in a few production runs being reported in a given second.</p> <p>WSO2 Streaming Integrator allows data to be received from multiple sources such as files, databases, cloud-based applications, and streaming messaging systems (such as Kafka) in a streaming manner. It uses the Siddhi Query Language, to process this data, and then publishes the results in a streaming manner to multiples destinations such as files, databases, cloud-based applications, and streaming messaging systems in a streaming manner.</p> <p>Streaming Integration involves processing streaming data and incorporating them into integration flows so that action can be taken based on the output derived. In this example, you may want to filter production details relating to a specific sweet.</p> <p>To understand how Streaming Integration is performed via the WSO2 Streaming Integrator, follow the sections below.</p> <p>Step 1: Download Streaming Integrator and Dependencies Step 2: Create the Siddhi Application Step 3: Deploy the Siddhi Application Step 4: Run the Siddhi Application Step 5: Update the Siddhi Application Step 6: Handle Errors Step 7: Monitor Statistics</p>"},{"location":"quick-start-guide/getting-started/handle-errors/","title":"Step 6: Handle Errors","text":"<p>The events handled by Siddhi applications can result in errors due to multiple reasons such as errors in the transport, mapping errors, etc. WSO2 Streeaming Integrator allows you to specify how you want such errors to be managed if they occur It provides an error store in which you can store events with errors so that you can later view them, correct the error (i.e., if they are mapping errors) and replay them. For the different types of actions you can take to manage errors, see the Error Handling Guide.</p> <p>Let's assume that the foreman of the Sweet Factory in this scenario requires errors to be stored in an error store so that they can be checked and replayed after making a correction.</p> <p>To implement the above, follow the topics below.</p>"},{"location":"quick-start-guide/getting-started/handle-errors/#configuring-the-error-store","title":"Configuring the error store","text":"<p>To configure a new error store in which you can store the events with errors, follow the steps below:</p> <ol> <li> <p>Start the MySQL server if it is not already started.</p> </li> <li> <p>Create a new database named <code>siddhierrorstoredb</code>; by issuing the following command in the MySQL console.</p> <p><code>mysql&gt; create database siddhierrorstoredb;</code></p> </li> <li> <p>To switch to the new database, issue the following command.</p> <p><code>mysql&gt; use siddhierrorstoredb;</code></p> </li> <li> <p>To enable the error store, open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file and add a configuration as follows:</p> <p><pre><code>error.store:\n  enabled: true\n  bufferSize: 1024\n  dropWhenBufferFull: true\n  errorStore: org.wso2.carbon.streaming.integrator.core.siddhi.error.handler.DBErrorStore\n  config:\n    datasource: SIDDHI_ERROR_STORE_DB\n    table: SIDDHI_ERROR_STORE_TABLE\n</code></pre> 5. The above configuration refers to a data source named <code>SIDDHI_ERROR_STORE_DB</code>. Define this data source as follows under <code>Data sources</code> in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>- name: SIDDHI_ERROR_STORE_DB\n  description: The datasource used for Siddhi error handling feature\n  jndiConfig:\n    name: jdbc/SiddhiErrorStoreDB\n  definition:\n    type: RDBMS\n    configuration:\n      jdbcUrl: 'jdbc:mysql://localhost:3306/siddhierrorstoredb?useSSL=false'\n      username: root\n      password: root\n      driverClassName: com.mysql.jdbc.Driver\n      minIdle: 5\n      maxPoolSize: 50\n      idleTimeout: 60000\n      connectionTestQuery: SELECT 1\n      validationTimeout: 30000\n      isAutoCommit: false\n</code></pre> </li> </ol>"},{"location":"quick-start-guide/getting-started/handle-errors/#configuring-the-siddhi-application-to-store-events-with-errors","title":"Configuring the Siddhi application to store events with errors","text":"<p>In this section, let's update the <code>SweetFactoryApp</code> Siddhi application to store mapping errors that may occur when it reads events  from the <code>production.csv</code> file.</p> <p>The stream that reads the events from the file is <code>FilterStream</code>. Therefore, add the <code>@OnError</code> annotation to it as shown below.</p> <pre><code>@source(type='file', mode='LINE',\n   file.uri='file:/Users/foo/productioninserts.csv',\n   tailing='true',\n   @map(type='csv'))\n@OnError(action='STORE')\ndefine stream FilterStream (name string,amount double);\n</code></pre> <p>The completed Siddhi application looks as follows</p> <pre><code>@App:name('SweetFactoryApp')\n\n@App:statistics(reporter = 'prometheus')\n\n\n@source(type='cdc',url = \"jdbc:mysql://localhost:3306/production\",username = \"wso2si\",password = \"wso2\",table.name = \"SweetProductionTable\",operation = \"insert\",\n    @map(type='keyvalue'))\ndefine stream InsertSweetProductionStream (name string,amount double);\n\n@source(type='file', mode='LINE',\n   file.uri='file:/Users/foo/productioninserts.csv',\n   tailing='true',\n   @map(type='csv'))\n@OnError(action='STORE')\ndefine stream FilterStream (name string,amount double);\n\n@sink(type='file',file.uri = \"/Users/foo/productioninserts.csv\",\n    @map(type='text'))\ndefine stream ProductionUpdatesStream (name string,amount double);\n\n@sink(type = 'kafka', bootstrap.servers = \"localhost:9092\", topic = \"eclair_production\", is.binary.message = \"false\", partition.no = \"0\",\n         @map(type = 'json'))\ndefine stream PublishFilteredDataStream (name string,amount double);\n\n@info(name='query1')\nfrom InsertSweetProductionStream \nselect str:upper(name) as name, amount \ngroup by name \ninsert  into ProductionUpdatesStream;\n\nfrom FilterStream [name=='Eclairs']\nselect * \ngroup by name \ninsert  into PublishFilteredDataStream;\n</code></pre>"},{"location":"quick-start-guide/getting-started/handle-errors/#testing-the-siddhi-aplication","title":"Testing the Siddhi aplication","text":"<p>For testing purposes, let's generate an error with a mapping error as follows:</p> <ol> <li> <p>Open the <code>/Users/foo/productioninserts.csv</code>, and manually enter an erroneous row in it as follows:</p> <p><code>ECLAIRS,TOFFEE,40.0</code></p> <p>Here, you are creating a file event with values for three attributes whereas the event schema (as defined via the <code>FilterStream</code> stream) has only two attributes. This results in a mapping error which is logged as follows in the terminal window in which you are running the Streaming Integrator server.</p> </li> <li> <p>If you have not already started the Streaming Integrator Tooling server, start it by navigating to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issuing one of the following commands as appropriate, based on your operating system:</p> <ul> <li> <p>For Windows: <code>streaming-integrator-tooling.bat</code></p> </li> <li> <p>For Linux: <code>./streaming-integrator-tooling.sh</code></p> </li> </ul> <p>Then Access the Streaming Integrator Tooling via the URL that appears in the start up log with the text <code>Editor Started on:</code>.</p> </li> <li> <p>To open the Error Store Explorer, click Tools and then click Error Store Explorer.</p> <p>The Error Store Explorer opens as shown below. </p> <p></p> </li> <li> <p>Click Connect to Server. Then enter information as follows:</p> <p>To check the port of the Streaming Integrator Server, Open /conf/server/deployment.yaml file. Under Listener Configurations of wso2.transport.http, locate the listener configuration with msf4j-https as the ID and specify its port as shown in the extract below. <p></p> Parameter Value Host <code>localhost</code> Port <code>9443</code> Username <code>admin</code> Password <code>admin</code> <p>Then click Connect.</p> <p>As a result, one error is displayed as shown below.</p> <p></p> <p>The single error displayed is the mapping error that you previously generated by adding a row with an additional value in the <code>production.csv</code> file.</p> <li> <p>To view details of the error, click Detailed Info. The following is displayed.</p> <p></p> </li> <li> <p>You can correct the mapping and replay the event. To do this, remove one of the additional values (i.e., one of the sweet names) as shown below, and click Replay.</p> <p></p> <p>As a result, the Error Entry dialog box closes, and the Error Store Explorer dialog box is displayed with no errors.</p> </li> <p>What's Next?</p> <p>To view the statistics generated by the <code>SweetFactoryApp</code> Siddhi application, proceed to Step 8: Monitor Statistics.</p>"},{"location":"quick-start-guide/getting-started/monitor-statistics/","title":"Step 7: Monitor Statistics","text":"<p>This step shows how you can monitor the CDC and file statistics of the WSO2 Streaming Integrator deployment you started and the <code>SweetFactoryApp</code> Siddhi application you created and deployed in the previous steps. For this purpose, you are using the some of the pre-configured dashboards provided by WSO2 Streaming Integrator. You can host these dashboards in Grafana and view statistices related to ETL activities carried out by the Streaming Integrator.For more information about these dashboards, see Monitoring ETL Statistics with Grafana</p>"},{"location":"quick-start-guide/getting-started/monitor-statistics/#configuring-wso2-si-to-visualize-statistics","title":"Configuring WSO2 SI to visualize statistics","text":"<p>To be able to see visualizations of statistics generated by WSO2 Streaming Integrator, you are required to download and install Prometheus and Grafana. You need to download the required pre-configured dashboards and import them to Grafana.</p>"},{"location":"quick-start-guide/getting-started/monitor-statistics/#downloading-the-required-dashboards","title":"Downloading the required dashboards","text":"<p>WSO2 Streaming Integrator provides you with pre-configured dashboards in JSON format. You can import these dashboards bto Grafana to view statistics of your Streaming Integrator deployment.</p> <p>For this scenario, download the following dashboards:</p> <ul> <li> <p>WSO2 Streaming Integrator - Overall Statistics.json</p> </li> <li> <p>WSO2 Streaming Integrator - App Statistics.json</p> </li> <li> <p>WSO2 Streaming Integrator - CDC Statistics.json</p> </li> <li> <p>WSO2 Streaming Integrator - CDC Streaming Statistics.json</p> </li> <li> <p>WSO2 Streaming Integrator - File Sink Statistics.json</p> </li> <li> <p>WSO2 Streaming Integrator - File Source Statistics.json</p> </li> <li> <p>WSO2 Streaming Integrator - File Statistics.json</p> </li> </ul>"},{"location":"quick-start-guide/getting-started/monitor-statistics/#downloading-and-setting-up-prometheus","title":"Downloading and setting up Prometheus","text":"<p>WSO2 Streaming Integrator uses Perometheus to expose its statistics to Grafana. Therefore, to download and configure Prometheus, follow the steps below:</p> <ol> <li> <p>Download Prometheus from the Prometheus site. For instructions, see the Prometheus Getting Started Guide.</p> </li> <li> <p>Extract the downloaded file. The directory that opens as a result is referred to as the <code>&lt;PROMETHEUS_HOME&gt;</code> from here on.</p> </li> <li> <p>To enable statistics for the Prometheus reporter, open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file and set the <code>enabled</code> parameter in the <code>wso2.metrics</code> section to <code>true</code>, and update the other parameters in the section as shown below. You also need to add the <code>metrics.prometheus:</code> as shown.</p> <pre><code> wso2.metrics:\n   # Enable Metrics\n   enabled: true\n   reporting:\n     console:\n       - # The name for the Console Reporter\n         name: Console\n\n         # Enable Console Reporter\n         enabled: false\n\n         # Polling Period in seconds.\n         # This is the period for polling metrics from the metric registry and printing in the console\n         pollingPeriod: 2\n\n metrics.prometheus:\n  reporting:\n    prometheus:\n      - name: prometheus\n        enabled: true\n        serverURL: \"http://localhost:9005\"\n</code></pre> </li> <li> <p>Open the <code>&lt;PROMETHEUS_HOME&gt;/prometheus.yml</code> file and add the following configuration in the <code>scrape_configs:</code> section.</p> <pre><code> scrape_configs:\n   - job_name: 'prometheus'\n     static_configs:\n     - targets: ['localhost:9005']\n</code></pre> </li> <li> <p>In the terminal, navigate to the <code>&lt;PROMETHEUS_HOME</code> and issue the following command to start the Prometheus server.</p> <p><code>./prometheus</code></p> </li> </ol> <p>Info</p> <p>The above steps to configure and start Prometheus need to be performed before you start the Grafana server.</p> <p>Control the growth of the WSO2_METRICS_DB</p> <p>After enabling <code>metrics.prometheus</code> you might notice the growth of the WSO2_METRICS_DB. Below are the 2 options that can be followed to overcome this issue.</p> <p>Option 1</p> <ol> <li>Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</li> <li>Set the <code>Enable JDBC Reporter</code> parameter to false. Note that after you disable this, only the real-time metrics are displayed and information relating to metrics history is not displayed. <pre><code># Enable JDBC Reporter\nenabled: false\n</code></pre></li> <li>Restart the SI component to apply the changes.</li> </ol> <p>Option 2</p> <ol> <li>Open the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</li> <li>Enable scheduled cleanup of the database under the <code>wso2.metrics.jdbc</code> parameter. <pre><code>wso2.metrics.jdbc:\n# Data Source Configurations for JDBC Reporters\ndataSource:\n    - &amp;JDBC01\n    dataSourceName: java:comp/env/jdbc/WSO2MetricsDB\n    # Schedule regular deletion of metrics data older than a set number of days.\n    # It is recommended that you enable this job to ensure your metrics tables do not get extremely large.\n    # Deleting data older than seven days should be sufficient.\n    scheduledCleanup:\n        # Enable scheduled cleanup to delete Metrics data in the database.\n        enabled: true\n\n        # The scheduled job will cleanup all data older than the specified days\n        daysToKeep: 3\n\n        # This is the period for each cleanup operation in seconds.\n        scheduledCleanupPeriod: 86400\n</code></pre></li> </ol>"},{"location":"quick-start-guide/getting-started/monitor-statistics/#downloading-and-setting-up-grafana","title":"Downloading and setting up Grafana","text":"<p>The pre-configured dashboards provided by WSO2 Streaming Integrator which you previously downloaded are rendered in Grafana to visualize statistics. To download and set up Grafana, follow the steps below:</p> <p>Before you begin:</p> <p>Start the Prometheus server as instructed under Downloading and setting up Prometheus.</p> <ol> <li> <p>Download Grafana from the Grafana Labs - Download Grafana.</p> </li> <li> <p>Start Grafana.</p> <p>Info</p> <p>The procedure to start Grafana depends on your operating system and the installation process. e.g., If your operating system is Mac OS and you have installed Grafana via Homebrew, you start Grafana by issuing the <code>brew services start grafana</code> command.</p> </li> <li> <p>In the Data Sources section, click Add your first data source. In the Add data source page that appears, click Select for Prometheus.</p> </li> <li> <p>In the Add data source page -&gt; Settings tab, update the configurations for Prometheus as follows. </p> </li> <li> <p>Click Default to make Prometheus the default data source.</p> </li> <li> <p>Under HTTP, enter <code>http://localhost:9090</code> as the URL.</p> </li> <li> <p>Click Save &amp; Test. If the data source is successfully configured, it is indicated via a message.        </p> </li> <li> <p>To import the dashboards that you previously downloaded as JSON files, follow the procedure below:</p> <ol> <li> <p>Start Grafana and access it via http://localhost:3000/.</p> </li> <li> <p>To load a new dashboard, click the plus icon (+) in the side panel. Then click Import.</p> </li> <li> <p>In the Import page, click Upload .json file. Then browse and select the .json file of the preconfigured dashboard that you downloaded (i.e., in step 5, substep 1).</p> </li> <li> <p>If required, change the unique identifier displayed in the Unique Identifier (uid).</p> </li> <li> <p>Click Import.</p> </li> </ol> </li> </ol>"},{"location":"quick-start-guide/getting-started/monitor-statistics/#enable-the-siddhi-application-to-publish-statistics","title":"Enable the Siddhi application to publish statistics","text":"<p>To enable the <code>SweetFactoryApp</code> Siddhi application to publish statistics to Prometheus, add the <code>@App:statistics(reporter = 'prometheus')</code> annotation to it below the <code>@App:name</code> annotation as shown below:</p> <p>Tip</p> <p>You can update the Siddhi application in Streaming Integrator Tooling and deploy it again in the Streaming Integrator server as you did in Step 5: Update the Siddhi Application.</p>"},{"location":"quick-start-guide/getting-started/monitor-statistics/#viewing-statistics","title":"Viewing statistics","text":"<p>To generate some statistics and view them, follow the procedure below.</p> <ol> <li> <p>Start WSO2 Streaming Integrator.</p> </li> <li> <p>To generate statistics, insert as many events as you want into the <code>SweetProductionTable</code> MySQL table that you created for this scenario in Step 1: Download Streaming Integrator and Dependencies. Also, manually add as many rows as you want in the <code>/Users/foo/productioninserts.csv</code> file.</p> </li> <li> <p>Access Grafana via the <code>localhost:3000</code> URL.</p> </li> <li> <p>In the side panel, click the Dashboards icon and click Dashboards.</p> <p></p> <p>Then click on the WSO2 Streaming Integrator - Overall Statistics dashboard. It opens as follows.</p> <p>Info</p> <p>The statistics displayed will be different based on the number of records you inserted to the <code>SweetProductionTable</code> MySQL table and the number of rows you added in the <code>/Users/foo/productioninserts.csv</code> file during the last 30 minutes. You can also change the time interval for which statistics are displayed via the field for selecting the time interval in the top panel.</p> <p></p> </li> <li> <p>Under Overview Statistics, click SweetFactoryApp. The overview-statistics / WSO2 Streaming Integrator App Statistics dashboard opens.</p> <p></p> </li> <li> <p>Scroll down to the Sources section. The following is displayed.</p> <p></p> <p>The two entries displayed above represent the <code>file</code> source and the <code>cdc</code> source used in the <code>SweetFactoryApp</code> Siddhi application.</p> </li> <li> <p>Scroll down further to the Destinations section. The <code>file</code> sink in the <code>SweetFactoryApp</code> Siddhi application is displayed as shown below.</p> <p></p> </li> <li> <p>Under Sources, click on the link to the <code>productioninserts.csv</code> file. The WSO2 Streaming Integrator - File Statistics dashboard opens. The contents of the <code>productioninserts.csv</code> file is the output of one query and the input of another. Therefore, it is a source as well as a destination, statistics are displayed for it under Source and Sink as shown below.</p> <p>Source Statistics</p> <p></p> <p>Sink Statistics</p> <p></p> </li> <li> <p>Under WSO2 Streaming Integrator - File Statistics dashboard -&gt; Sources, click on the file link. The file-statistics / WSO2 Streaming Integrator / File Source Statistics  dashboard opens displaying detailed statistics for the file when it is functioning as a source.</p> <p></p> </li> <li> <p>Under WSO2 Streaming Integrator - File Statistics dashboard -&gt; Sources, click on the file link. The file-statistics / WSO2 Streaming Integrator / File Sink Statistics  dashboard opens displaying detailed statistics for the file when it is functioning as a source.</p> <p></p> </li> <li> <p>In the overview-statistics / WSO2 Streaming Integrator App Statistics dashboard -&gt; CDC section, click on the SweetProductionTable link. The cdc-statistics / WSO2 Streaming Integrator / CDC Statistics  dashboard opens with statistics generated for the <code>cdc</code> source in the <code>SweetFactoryApp</code> Siddhi application.</p> <p></p> <p>Under Streaming, click on the SweetProductionTable link. The cdc-statistics / WSO2 Streaming Integrator / CDC Streaming Statistics dashboard opens as follows.</p> <p></p> </li> </ol> <p>What's Next?</p> <ul> <li>To learn more about the key concepts of WSO2 Streaming Integrator, see Key Concepts. </li> <li>For more hands-on experience with WSO2 Streaming Integrator, try the Tutorials. </li> <li>For more guidance as you use WSO2 Streaming Integrator for your Streaming Integration use cases, see Use Cases.</li> <li>Learn how to run WSO2 Streaming Integrator in containerized environments, try Running SI with Docker and Kubernetes</li> </ul>"},{"location":"quick-start-guide/getting-started/test-siddhi-application/","title":"Step 4: Run the Siddhi Application","text":"<p>In this step, let's run the <code>SweetFactoryApp</code> Siddhi application that you created, tested and deployed.</p>"},{"location":"quick-start-guide/getting-started/test-siddhi-application/#installing-the-required-extensions","title":"Installing the required extensions","text":"<p>In Step 2: Create the Siddhi Application, you installed the <code>cdc-mysql</code> Siddhi extension in Streaming Integrator Tooling to test the <code>SweetFacoryApp</code> Siddhi application. Now let's install it in the Streaming Integrator server so that you can run the same Siddhi application there.</p> <ol> <li> <p>Start the Streaming Integrator server by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory from the CLI, and issuing the appropriate command based on your operating system:</p> </li> <li> <p>For Windows: <code>server.bat --run</code></p> </li> <li> <p>For Linux/Mac OS: \u00a0<code>./server.sh</code></p> </li> <li> <p>To install the <code>cdc-mysql</code> extension, issue the following command from the <code>&lt;SI_HOME&gt;/bin</code> directory. </p> <ul> <li>For Windows: <code>extension-installer.bat install cdc-mysql</code></li> <li>For Linux/Mac OS: \u00a0<code>./extension-installer.sh install install cdc-mysql</code></li> </ul> <p>Once the installation is complete, a message is logged to inform you that the extension is successfully installed.</p> </li> <li> <p>Restart the Streaming Integrator server.</p> </li> </ol>"},{"location":"quick-start-guide/getting-started/test-siddhi-application/#generating-an-input-event","title":"Generating an input event","text":"<p>To generate an input event, insert a record in the <code>production</code> database table by issuing the following command in the MySQL console.</p> <p><code>insert into SweetProductionTable values('chocolate',100.0);</code></p> <p>Then open the <code>/Users/foo/productioninserts.csv</code> file. The following record should be displayed.</p> <p></p> <p>What's Next?</p> <p>Now you can try extending the <code>SweetFactoryApp</code> Siddhi application to perform more streaming integration activities. To try this, proceed to Step 5: Update the Siddhi Application.</p>"},{"location":"quick-start-guide/getting-started/update-the-siddhi-application/","title":"Step 5: Update the Siddhi Application","text":"<p>A Siddhi application can be easily extended to consume messages from more sources, to carry out more processing activities for data or to publish data to more destinations. For this example, consider a scenario where you also need to filter out the production data of eclairs and publish it to a Kafka topic so that applications that cannot read streaming data can have access to it. This involves extending the <code>SweetFactoryApp</code> Siddhi application to include Kafka in the streaming flow so that it functions as shown in the diagram below.</p> <p> </p> <p>To update the <code>SweetFactoryApp</code> Siddhi application so that it functions as described, follow the steps below:</p> <ol> <li> <p>Start and access Streaming Integrator Tooling. Click the File Explorer icon in the side panel and then click SweetFactoryApp.siddhi to open the Siddhi application that you already created and saved.</p> </li> <li> <p>Define another stream to which you can direct the filtered events you need to publish in a the Kafka topic.</p> <p><code>define stream FilterStream (name string,amount double);</code></p> </li> <li> <p>To publish the events filtered into the <code>PublishFilteredDataStream</code> stream, connect a source of the <code>kafa</code> type to it as shown below.</p> <pre><code>@sink(type = 'kafka', bootstrap.servers = \"localhost:9092\", topic = \"eclair-production\", is.binary.message = \"false\", partition.no = \"0\",\n         @map(type = 'json'))\ndefine stream PublishFilteredDataStream (name string,amount double);\n</code></pre> </li> </ol> <p>The above sink annotation publishes all the events received into the <code>PublishFilteredDataStream</code> stream into a topic named <code>eclair-production</code> in <code>json</code> format.</p> <ol> <li> <p>Let's create another stream to read from the <code>/Users/foo/productioninserts.csv</code> file to which you have been publishing data.</p> <p>Tip</p> <p>Alternatively, you can write the query to read from one of the existing streams. However, in this example, let's create a new stream to understand how WSO2 Streaming Integrator reads data from files.</p> <pre><code>@source(type='file', mode='LINE',\n   file.uri='file:/Users/foo/productioninserts.csv',\n   tailing='true',\n   @map(type='csv'))\ndefine stream FilterStream (name string,amount double);\n</code></pre> <p>Here, you are configuring the file to be read line by line in the tailing mode. Therefore, any new row added to the file is captured as an event in the <code>FilterStream</code> stream as and when it is added.</p> </li> <li> <p>Now let's add the query to filter the required information and publish it.</p> <pre><code>from FilterStream [name=='ECLAIRS']\nselect * \ngroup by name \ninsert  into PublishFilteredDataStream;\n</code></pre> <p>In the <code>from</code> clause, <code>[name=='ECLAIRS']</code> filters all production runs where the name of the sweet produced is <code>Eclairs</code>. Then all the filtered events are inserted into the <code>PublishFilteredDataStream</code> stream so that they can be published in the <code>eclair_production</code> Kafka topic.</p> </li> <li> <p>Save your changes.</p> <p>The completed Siddhi application looks as follows:</p> <pre><code>@App:name('SweetFactoryApp')\n\n@source(type='cdc',url = \"jdbc:mysql://localhost:3306/production\",username = \"wso2si\",password = \"wso2\",table.name = \"SweetProductionTable\",operation = \"insert\",\n    @map(type='keyvalue'))\ndefine stream InsertSweetProductionStream (name string,amount double);\n\n@source(type='file', mode='LINE',\n   file.uri='file:/Users/foo/productioninserts.csv',\n   tailing='true',\n   @map(type='csv'))\ndefine stream FilterStream (name string,amount double);\n\n@sink(type='file',file.uri = \"/Users/foo/productioninserts.csv\",\n    @map(type='csv'))\ndefine stream ProductionUpdatesStream (name string,amount double);\n\n@sink(type = 'kafka', bootstrap.servers = \"localhost:9092\", topic = \"eclair_production\", is.binary.message = \"false\", partition.no = \"0\",\n         @map(type = 'json'))\ndefine stream PublishFilteredDataStream (name string,amount double);\n\n@info(name='query1')\nfrom InsertSweetProductionStream \nselect str:upper(name) as name, amount \ngroup by name \ninsert  into ProductionUpdatesStream;\n\nfrom FilterStream [name=='ECLAIRS']\nselect * \ngroup by name \ninsert  into PublishFilteredDataStream;\n</code></pre> </li> <li> <p>Deploy the updated <code>SweetFactoryApp</code> Siddhi application as you previously did in Step 3: Deploy the Siddhi Application.</p> </li> <li> <p>The <code>kafka</code> extension is not shipped with the Streaming Integrator Server by default. Therefore, install it via the Extension Installer Tool. You can do this by starting the Streaming Integrator server and then issuing the appropriate command (based on your operating system) from the <code>&lt;SI_HOME&gt;/bin</code> directory.</p> <ul> <li>For Linux: <code>./extension-installer.sh install kafka</code></li> <li>For Windows: <code>extension-installer.bat install kafka</code></li> </ul> </li> <li> <p>To test the Siddhi application after the update, insert records into the <code>production</code> database as follows.</p> <p><code>insert into SweetProductionTable values('eclairs',100.0);</code></p> <p><code>insert into SweetProductionTable values('eclairs',60.0);</code></p> <p><code>insert into SweetProductionTable values('toffee',40.0);</code></p> </li> <li> <p>To check the messages in the <code>eclair_production</code> topic, navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command:</p> <p><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic eclair-production --from-beginning</code></p> <p>You can see the following messages in the Kafka Consumer log.</p> <pre><code>{\"event\":{\"name\":\"ECLAIRS\",\"amount\":100.0}}\n{\"event\":{\"name\":\"ECLAIRS\",\"amount\":60.0}}\n</code></pre> </li> </ol> <p>Note that the third record you inserted does not appear in the Kafka consumer log because the value for the <code>name</code> field is not <code>ECLAIRS</code> and therefore, it is filtered out.</p> <p>What's Next?</p> <p>Next, you can configure WSO2 Streaming Integrator to handle errors that can occur in the Streaming Integration flow of the <code>SweetFactoryApp</code> Siddhi application. To do this, proceed to Step 7: Handle Errors.</p>"},{"location":"ref/authentication-APIs/","title":"authentication APIs","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/authentication-APIs/#authentication-apis","title":"Authentication APIs","text":"<ul> <li>Log in to a dashboard     application</li> <li>Log out of the dashboard     application</li> <li>Redirect URL for login using authorization grant     type</li> </ul>"},{"location":"ref/authentication-APIs/#log-in-to-a-dashboard-application","title":"Log in to a dashboard application","text":""},{"location":"ref/authentication-APIs/#overview","title":"Overview","text":"Overview Logs in to the apps in dashboard runtime such as portal, monitoring or business-rules app. API Context <code>             /login/{appName}            </code> HTTP Method <code>             POST            </code> Request/Response Format <pre><code>application/x-www-form-urlencoded</code></pre> Runtime Dashboard"},{"location":"ref/authentication-APIs/#parameter-description","title":"Parameter description","text":"Parameter Type Description Possible Values <code>             appName            </code> Path param The application to which you need to log in. portal/monitoring/business-rules username Body param Username for the login password Body param Password for the login grantType Body param Grant type used for the login password/ <pre><code>          refresh_token\n        </code></pre> <pre><code>\n\n        </code></pre> <pre><code>          authorization_code\n        </code></pre> <pre><code>rememberMe</code></pre> Body param Whether remember me function enabled false/true"},{"location":"ref/authentication-APIs/#curl-command-syntax","title":"curl command syntax","text":"<pre><code>    curl -X POST \"https://analytics.wso2.com/login/{appName}\" -H \"accept: application/json\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username={username}&amp;password={password}&amp;grantType={grantTypr}&amp;rememberMe={rememberMe}\"\n</code></pre>"},{"location":"ref/authentication-APIs/#sample-curl-command","title":"Sample curl command","text":"<pre><code>    curl -X POST \"https://localhost:9643/login/portal\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=admin&amp;password=admin&amp;grantType=password\"\n</code></pre>"},{"location":"ref/authentication-APIs/#sample-output","title":"Sample output","text":"<pre><code>    {\"authUser\":\"admin\",\"pID\":\"71368eff-cc71-44ef\",\"lID\":\"a60c1098-3de0-42fb\",\"validityPeriod\":3600}\n</code></pre>"},{"location":"ref/authentication-APIs/#response","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/authentication-APIs/#log-out-of-the-dashboard-application","title":"Log out of the dashboard application","text":""},{"location":"ref/authentication-APIs/#overview_1","title":"Overview","text":"Overview Logs out of the dashboard application. API Context <code>/logout/{appName}</code> HTTP Method <code>POST</code> Request/Response Format <code>application/json</code> Runtime Dashboard"},{"location":"ref/authentication-APIs/#curl-command-syntax_1","title":"curl command syntax","text":"<pre><code>    curl -X POST \"https://analytics.wso2.com/logout/{appName}\" -H \"accept: application/json\" -H \"Authorzation: Bearer {access token}\"\n</code></pre>"},{"location":"ref/authentication-APIs/#sample-curl-command_1","title":"Sample curl command","text":"<pre><code>    curl -X POST \"https://analytics.wso2.com/logout/portal\" -H \"accept: application/json\" -H \"Authorzation: Bearer 123456\"\n</code></pre>"},{"location":"ref/authentication-APIs/#sample-output_1","title":"Sample output","text":"<pre><code>    N/A\n</code></pre>"},{"location":"ref/authentication-APIs/#response_1","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/authentication-APIs/#redirect-url-for-login-using-authorization-grant-type","title":"Redirect URL for login using authorization grant type","text":""},{"location":"ref/authentication-APIs/#overview_2","title":"Overview","text":"Overview Redirects URL by the IS in authorization grant type - OAuth2. API Context <code>/login/callback/{appName}</code> HTTP Method <code>GET</code> Request/Response Format JSON Runtime Dashbaord"},{"location":"ref/authentication-APIs/#parameter-description_1","title":"Parameter description","text":"Parameter Description <code>{appName}</code> The application of which the URL needs to be redirected."},{"location":"ref/authentication-APIs/#curl-command-syntax_2","title":"curl command syntax","text":""},{"location":"ref/authentication-APIs/#sample-curl-command_2","title":"Sample curl command","text":"<pre><code>    curl -X GET \"https://localhost:9643/login/callback/portal\"\n</code></pre>"},{"location":"ref/authentication-APIs/#sample-output_2","title":"Sample output","text":""},{"location":"ref/authentication-APIs/#response_2","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/authorization-Permission-Model/","title":"Authorization &amp; Permission Model","text":"<p>This page will provide information about the permission model of REST APIs available in each runtime of WSO2 Stream Processor. If you want the complete set of REST APIs which available in WSO2 SP, please find it here . You can find the REST APIs for each runtime and its permission model in the below pages.</p> <ul> <li>Worker Runtime - REST APIs Permission     Model</li> <li>Manager Runtime - REST APIs Permission     Model</li> </ul>"},{"location":"ref/business-Rules-APIs/","title":"Business Rules APIs","text":""},{"location":"ref/business-Rules-APIs/#listing-the-available-business-rule-instances","title":"Listing the available business rule instances","text":""},{"location":"ref/business-Rules-APIs/#overview","title":"Overview","text":"Description Returns the list of business rule instances that are currently available. API Context <code>/business-rules/instances</code> HTTP Method <code>GET</code> Request/Response Format application/json Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime tooling"},{"location":"ref/business-Rules-APIs/#curl-command-syntax","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/business-rules/instances\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-curl-command","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9743/business-rules/instances\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output","title":"Sample output","text":"<pre><code>[ \"Found Business Rules\", \"Loaded available business rules\", [ [ { \"ruleTemplateUUID\": \"identifying-continuous-production-decrease\", \"properties\": { \"timeInterval\": \"6\", \"timeRangeInput\": \"5\", \"email\": \"example@email.com\", \"validateTimeRange\": \"function validateTimeRange(number) {\\n\\tif (!isNaN(number) &amp;&amp; (number &gt; 0)) {\\n\\t\\treturn number;\\n\\t} else {\\n\\t\\tthrow 'A positive number expected for time range';\\n\\t}\\n}\", \"getUsername\": \"function getUsername(email) {\\n\\tif (email.match(/\\\\S+@\\\\S+/g)) {\\n\\t\\tif (email.match(/\\\\S+@\\\\S+/g)[0] === email) {\\n\\t\\t\\treturn email.split('@')[0];\\n\\t\\t}\\n\\t\\tthrow 'Invalid email address provided';\\n\\t}\\n\\tthrow 'Invalid email address provided';\\n}\", \"timeRange\": \"5\", \"username\": \"example\" }, \"uuid\": \"samplesiddhiapp\", \"name\": \"SampleSiddhiApp\", \"templateGroupUUID\": \"3432442\", \"type\": \"template\" }, 1 ] ], 0 ]\n</code></pre>"},{"location":"ref/business-Rules-APIs/#response","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes.</p>"},{"location":"ref/business-Rules-APIs/#delete-business-rule-with-given-uuid","title":"Delete business rule with given UUID","text":""},{"location":"ref/business-Rules-APIs/#overview_1","title":"Overview","text":"Description Deletes the business rule with the given UUID.  API Context <code>/business-rules/instances/{businessRuleInstanceID}?force-delete=false</code> HTTP Method <code>DELETE</code> Request/Response Format application/json Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime tooling"},{"location":"ref/business-Rules-APIs/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>{businessRuleInstanceID}</code> The UUID (Uniquely Identifiable ID) of the business rules instance to be deleted."},{"location":"ref/business-Rules-APIs/#curl-command-syntax_1","title":"curl command syntax","text":"<pre><code>curl -X DELETE \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:admin\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-curl-command_1","title":"Sample curl command","text":"<pre><code>curl -X DELETE \"https://localhost:9743/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:admin\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output_1","title":"Sample output","text":"<pre><code>[ \"Deletion Successful\", \"Successfully deleted the business rule\", 6 ]\n</code></pre>"},{"location":"ref/business-Rules-APIs/#response_1","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/business-Rules-APIs/#fetch-template-group-with-the-given-uuid","title":"Fetch template group with the given UUID","text":""},{"location":"ref/business-Rules-APIs/#overview_2","title":"Overview","text":"Description Returns the template group that has the given UUID. API Context <code>/business-rules/template-groups/{templateGroupID}</code> HTTP Method <code>GET</code> Request/Response Format application/json  Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime tooling"},{"location":"ref/business-Rules-APIs/#parameter-description_1","title":"Parameter description","text":"Parameter Description <code>{templateGroupID}</code> The UUID of the template group to be fetched."},{"location":"ref/business-Rules-APIs/#curl-command-syntax_2","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/business-rules/template-groups/{templateGroupID}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-curl-command_2","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9743/business-rules/template-groups/sweet-factory\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output_2","title":"Sample output","text":"<pre><code>[ \"Found Template Group\", \"Loaded template group with uuid '3432442'\", { \"uuid\": \"3432442\", \"name\": \"Sweet Factory\", \"description\": \"Configure Sweet Factory Rules\", \"ruleTemplates\": [ { \"uuid\": \"identifying-continuous-production-decrease\", \"name\": \"Identify Continuous Production Decrease\", \"type\": \"template\", \"instanceCount\": \"many\", \"script\": \"var timeRange = validateTimeRange(${timeRangeInput});\\nvar username = getUsername('${email}');\\n\\n// Validates the input provided for time range\\nfunction validateTimeRange(number) {\\n\\tif (!isNaN(number) &amp;&amp; (number &gt; 0)) {\\n\\t\\treturn number;\\n\\t} else {\\n\\t\\tthrow 'A positive number expected for time range';\\n\\t}\\n}\\n\\n// Gets the username from provided email\\nfunction getUsername(email) {\\n\\tif (email.match(/\\\\S+@\\\\S+/g)) {\\n\\t\\tif (email.match(/\\\\S+@\\\\S+/g)[0] === email) {\\n\\t\\t\\treturn email.split('@')[0];\\n\\t\\t}\\n\\t\\tthrow 'Invalid email address provided';\\n\\t}\\n\\tthrow 'Invalid email address provided';\\n}\", \"description\": \"Alert factory managers if rate of production continuously decreases for `X` time period\", \"templates\": [ { \"type\": \"siddhiApp\", \"content\": \"@App:name('SweetFactory-TrendAnalysis')\\n\\n@source(type='http', @map(type='json'))\\ndefine stream SweetProductionStream (name string, amount double, factoryId int);\\n\\n@sink(type='log', @map(type='text', @payload(\\\"\\\"\\\"\\nHi ${username},\\nProduction at Factory {{ factoryId }} has gone\\nfrom {{ initalamout }} to {{ finalAmount }} in ${timeInterval} seconds!\\\"\\\"\\\")))\\ndefine stream ContinousProdReductionStream (factoryId int, initaltime long, finalTime long, initalamout double, finalAmount double);\\n\\nfrom \n</code></pre>"},{"location":"ref/business-Rules-APIs/#response_2","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/business-Rules-APIs/#fetch-rule-templates-of-the-template-group-with-given-uuid","title":"Fetch rule templates of the template group with given UUID","text":""},{"location":"ref/business-Rules-APIs/#overview_3","title":"Overview","text":"Description Returns the rule templates of the template group with the given UUID. API Context <code>/business-rules/template-groups/{templateGroupID}/templates</code> HTTP Method <code>GET</code> Request/Response Format application/json Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime Tooling"},{"location":"ref/business-Rules-APIs/#parameter-description_2","title":"Parameter description","text":"Parameter Description <code>{templateGroupID}</code> The UUID of the template group of which the rule templates need to be fetched."},{"location":"ref/business-Rules-APIs/#curl-command-syntax_3","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://localhost:9643/business-rules/template-groups/{templateGroupID}/templates\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-curl-command_3","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9743/business-rules/template-groups/sweet-factory/templates\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output_3","title":"Sample output","text":"<pre><code>[ \"Found Rule Templates\", \"Loaded available rule templates for template group with uuid '3432442'\", [ { \"uuid\": \"identifying-continuous-production-decrease\", \"name\": \"Identify Continuous Production Decrease\", \"type\": \"template\", \"instanceCount\": \"many\", \"script\": \"var timeRange = validateTimeRange(${timeRangeInput});\\nvar username = getUsername('${email}');\\n\\n// Validates the input provided for time range\\nfunction validateTimeRange(number) {\\n\\tif (!isNaN(number) &amp;&amp; (number &gt; 0)) {\\n\\t\\treturn number;\\n\\t} else {\\n\\t\\tthrow 'A positive number expected for time range';\\n\\t}\\n}\\n\\n// Gets the username from provided email\\nfunction getUsername(email) {\\n\\tif (email.match(/\\\\S+@\\\\S+/g)) {\\n\\t\\tif (email.match(/\\\\S+@\\\\S+/g)[0] === email) {\\n\\t\\t\\treturn email.split('@')[0];\\n\\t\\t}\\n\\t\\tthrow 'Invalid email address provided';\\n\\t}\\n\\tthrow 'Invalid email address provided';\\n}\", \"description\": \"Alert factory managers if rate of production continuously decreases for `X` time period\", \"templates\": [ { \"type\": \"siddhiApp\", \"content\": \"@App:name('SweetFactory-TrendAnalysis')\\n\\n@source(type='http', @map(type='json'))\\ndefine stream SweetProductionStream (name string, amount double, factoryId int);\\n\\n@sink(type='log', @map(type='text', @payload(\\\"\\\"\\\"\\nHi ${username},\\nProduction at Factory {{ factoryId }} has gone\\nfrom {{ initalamout }} to {{ finalA }};\"}]}]]\n</code></pre>"},{"location":"ref/business-Rules-APIs/#response_3","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/business-Rules-APIs/#fetch-rule-template-of-specific-uuid-available-under-a-template-group-with-specific-uuid","title":"Fetch rule template of specific UUID available under a template group with specific UUID","text":""},{"location":"ref/business-Rules-APIs/#overview_4","title":"Overview","text":"Description Returns the rule template with the specified UUID that is defined under the template group with the specified UUID. API Context <code>/business-rules/template-groups/{templateGroupID}/templates/{ruleTemplateID}</code> HTTP Method <code>GET</code> Request/Response Format application/json Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime Tooling"},{"location":"ref/business-Rules-APIs/#parameter-description_3","title":"Parameter description","text":"Parameter Description <code>{templateGroupID}</code> The UUID of the template group from which the specified rule template needs to be retrieved. <code>{ruleTemplateID}</code> The UUID of the rule template that needs to be retrieved from the specified template group."},{"location":"ref/business-Rules-APIs/#curl-command-syntax_4","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://localhost:9743/business-rules/template-groups/{templateGroupID}/templates/{ruleTemplateID}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-curl-command_4","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates/identifying-continuous-production-decrease\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output_4","title":"Sample output","text":"<pre><code>[ \"Found Rule Template\", \"Loaded rule template with uuid 'identifying-continuous-production-decrease'\", { \"uuid\": \"identifying-continuous-production-decrease\", \"name\": \"Identify Continuous Production Decrease\", \"type\": \"template\", \"instanceCount\": \"many\", \"script\": \"var timeRange = validateTimeRange(${timeRangeInput});\\nvar username = getUsername('${email}');\\n\\n// Validates the input provided for time range\\nfunction validateTimeRange(number) {\\n\\tif (!isNaN(number) &amp;&amp; (number &gt; 0)) {\\n\\t\\treturn number;\\n\\t} else {\\n\\t\\tthrow 'A positive number expected for time range';\\n\\t}\\n}\\n\\n// Gets the username from provided email\\nfunction getUsername(email) {\\n\\tif (email.match(/\\\\S+@\\\\S+/g)) {\\n\\t\\tif (email.match(/\\\\S+@\\\\S+/g)[0] === email) {\\n\\t\\t\\treturn email.split('@')[0];\\n\\t\\t}\\n\\t\\tthrow 'Invalid email address provided';\\n\\t}\\n\\tthrow 'Invalid email address provided';\\n}\", \"description\": \"Alert factory managers if rate of production continuously decreases for `X` time period\", \"templates\": [ { \"type\": \"siddhiApp\", \"content\": \"@App:name('SweetFactory-TrendAnalysis')\\n\\n@source(type='http', @map(type='json'))\\ndefine stream SweetProductionStream (name string, amount double, factoryId int);\\n\\n@sink(type='log', @map(type='text', @payload(\\\"\\\"\\\"\\nHi ${username},\\nProduction at Factory {{ factoryId }} has gone\\nfrom {{ initalamout }} to {{ finalAmount }} in ${timeInterval} seconds!\\\"\\\"\\\")))\\ndefine stream ContinousProdReductionStream (factoryId int, initaltime long, finalTime long, initalamout do\n</code></pre>"},{"location":"ref/business-Rules-APIs/#response_4","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/business-Rules-APIs/#fetch-available-template-groups","title":"Fetch available template groups","text":""},{"location":"ref/business-Rules-APIs/#overview_5","title":"Overview","text":"Description Returns all the template groups that are currently available in the SI setup. API Context <code>/business-rules/template-groups</code> HTTP Method <code>GET</code> Request/Response Format application/json Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime Tooling"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_5","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/business-rules/template-groups\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-curl-command_5","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9743/business-rules/template-groups\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output_5","title":"Sample output","text":"<pre><code>[ \"Found Template Groups\", \"Loaded available template groups\", [ { \"uuid\": \"stock-exchange\", \"name\": \"Stock Exchange\", \"description\": \"Domain for stock exchange analytics\", \"ruleTemplates\": [ { \"uuid\": \"stock-exchange-input\", \"name\": \"Stock Exchange Input\", \"type\": \"input\", \"instanceCount\": \"many\", \"script\": \"\", \"description\": \"configured http source to receive stock exchange updates\", \"templates\": [ { \"type\": \"siddhiApp\", \"content\": \"@App:name('appName1')\\n@Source(type = 'http', receiver.url='${receiverUrl}',  basic.auth.enabled='false',\\n@map(type='text'))\\ndefine stream StockInputStream(symbol string, price float, volume long, name string);\", \"exposedStreamDefinition\": \"define stream StockInputStream(symbol string, price float, volume long, name string);\" } ], \"properties\": { \"receiverUrl\": { \"fieldName\": \"Receiver URL\", \"description\": \"Enter the URL of the http receiver for text messages. One URL can only be used once\", \"defaultValue\": \"https://localhost:8005/stockInputStream\" } } }, { \"uuid\": \"stock-exchange-output\", \"name\": \"Stock Exchange Output\", \"type\": \"output\", \"instanceCount\": \"many\", \"script\": \"\", \"description\": \"configured output to log the filtered stock exchange data\", \"templates\": [ { \"type\": \"siddhiApp\", \"content\": \"@App:name('appName2')\\n\\ndefine stream StockOutputStream(companyName string, companySymbol string, sellingPrice float);\\\n</code></pre>"},{"location":"ref/business-Rules-APIs/#response_5","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes.</p>"},{"location":"ref/business-Rules-APIs/#fetch-business-rule-instance-with-given-uuid","title":"Fetch business rule instance with given UUID","text":""},{"location":"ref/business-Rules-APIs/#overview_6","title":"Overview","text":"Description Returns the business rule instance with the given UUID. API Context <code>/business-rules/instances/{businessRuleInstanceID}</code> HTTP Method <code>GET</code> Request/Response Format <code>application/json</code> Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime Tooling"},{"location":"ref/business-Rules-APIs/#parameter-description_4","title":"Parameter description","text":"Parameter Description <code>{businessRuleInstanceID}</code> The UUID of the business rules instance to be fetched."},{"location":"ref/business-Rules-APIs/#curl-command-syntax_6","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/business-rules/instances/{businessRuleInstanceID}\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-curl-command_6","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9743/business-rules/instances/business-rule-1\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output_6","title":"Sample output","text":"<pre><code>[ \"Found Business Rule\", \"Loaded business rule with uuid 'sample'\", { \"ruleTemplateUUID\": \"identifying-continuous-production-decrease\", \"properties\": { \"timeInterval\": \"6\", \"timeRangeInput\": \"5\", \"email\": \"example@email.com\", \"validateTimeRange\": \"function validateTimeRange(number) {\\n\\tif (!isNaN(number) &amp;&amp; (number &gt; 0)) {\\n\\t\\treturn number;\\n\\t} else {\\n\\t\\tthrow 'A positive number expected for time range';\\n\\t}\\n}\", \"getUsername\": \"function getUsername(email) {\\n\\tif (email.match(/\\\\S+@\\\\S+/g)) {\\n\\t\\tif (email.match(/\\\\S+@\\\\S+/g)[0] === email) {\\n\\t\\t\\treturn email.split('@')[0];\\n\\t\\t}\\n\\t\\tthrow 'Invalid email address provided';\\n\\t}\\n\\tthrow 'Invalid email address provided';\\n}\", \"timeRange\": \"5\", \"username\": \"example\" }, \"uuid\": \"sample\", \"name\": \"Sample\", \"templateGroupUUID\": \"3432442\", \"type\": \"template\" } ]\n</code></pre>"},{"location":"ref/business-Rules-APIs/#response_6","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes.</p>"},{"location":"ref/business-Rules-APIs/#create-and-save-a-business-rule","title":"Create and save a business rule","text":""},{"location":"ref/business-Rules-APIs/#overview_7","title":"Overview","text":"Description Creates and saves a business rule. API Context <code>/business-rules/instances?deploy={deploymentStatus}</code> HTTP Method <code>POST</code> Request/Response Format <code>application/json</code> Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime Tooling"},{"location":"ref/business-Rules-APIs/#parameter-description_5","title":"Parameter description","text":"Parameter Description <code>{deploymentStatus}</code>"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_7","title":"curl command syntax","text":""},{"location":"ref/business-Rules-APIs/#sample-curl-command_7","title":"Sample curl command","text":"<pre><code>curl -X POST \"https://localhost:9743/business-rules/instances?deploy=true\" -H \"accept: application/json\" -H \"content-type: multipart/form-data\" -F 'businessRule={\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"6\",\"timeRangeInput\":\"5\",\"email\":\"example@email.com\"}}' -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#sample-output_7","title":"Sample output","text":""},{"location":"ref/business-Rules-APIs/#response_7","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/business-Rules-APIs/#update-business-rules-instance-with-given-uuid","title":"Update business rules instance with given UUID","text":""},{"location":"ref/business-Rules-APIs/#overview_8","title":"Overview","text":"Description Updates the business rules instance with the given UUID.  API Context <code>/business-rules/instances/{businessRuleInstanceID}?deploy={deploymentStatus}</code> HTTP Method <code>PUT</code> Request/Response Format application/json Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime Tooling"},{"location":"ref/business-Rules-APIs/#parameter-description_6","title":"Parameter description","text":"Parameter Description <code>{businessRuleInstanceID}</code> The UUID of the business rules instance to be updated. <code>{deploymentStatus}</code>"},{"location":"ref/business-Rules-APIs/#sample-curl-command_8","title":"Sample curl command","text":"<pre><code>curl -X PUT \"https://localhost:9743/business-rules/instances/business-rule-5?deploy=true\" -H \"accept: application/json\" -H \"content-type: application/json\" -d '{\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"9\",\"timeRangeInput\":\"8\",\"email\":\"newexample@email.com\"}}' -u admin:admin -k\n</code></pre>"},{"location":"ref/business-Rules-APIs/#response_8","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes.</p>"},{"location":"ref/configuring-default-ports/","title":"Configuring Default Ports","text":"<p>This page describes the default ports that are used for each runtime when the\u00a0port offset\u00a0is 0 .</p>"},{"location":"ref/configuring-default-ports/#common-ports","title":"Common Ports","text":"<p>The following ports are common to all runtimes.</p> 7611 Thrift TCP port to receive events from clients. 7711 Thrift SSL port for secure transport where the client is authenticated. 9611 Binary TCP port to receive events from clients. 9711 Binary SSL port for secure transport where the client is authenticated. <p>You can offset binary and thrift by configuring the offset parameter in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file. The following is a sample configuration.</p> <pre><code>  # Carbon Configuration Parameters\nwso2.carbon:\n    # value to uniquely identify a server\n  id: wso2-si\n    # server name\n  name: WSO2 Streaming Integrator\n    # server type\n  type: wso2-si\n    # ports used by this server\n  ports:\n      # port offset\n    offset: 1\n</code></pre>"},{"location":"ref/configuring-default-ports/#server-runtime","title":"Server runtime","text":"9090 HTTP netty transport 9443 HTTPS netty transport"},{"location":"ref/configuring-default-ports/#streaming-integrator-tooling-runtime","title":"Streaming Integrator Tooling runtime","text":"9390 HTTP netty transport 9743 HTTPS netty transport"},{"location":"ref/configuring-default-ports/#dashboard-runtime","title":"Dashboard runtime","text":"9290 HTTP netty transport 9643 HTTPS netty transport <p>Tip</p> <p>The following example shows how to overide the default netty port for the Streaming Integrator Tooling by updating the required parameters in the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>    wso2.transport.http:\n     transportProperties:\n      listenerConfigurations:\n     -\n          id: \"default\"\n     port: 9390\n        -\n          id: \"msf4j-https\"\n     port: 9743\n</code></pre>"},{"location":"ref/configuring-default-ports/#clustering-ports","title":"Clustering Ports","text":"<p>Ports that are required for clustering deployment:</p>"},{"location":"ref/configuring-default-ports/#minimum-high-availability-ha-deployment","title":"Minimum High Availability (HA) Deployment:","text":""},{"location":"ref/configuring-default-ports/#server-node","title":"Server node:","text":"9090 HTTP netty transport. 9090 Specify the port of the node for the\u00a0<code>advertisedPort</code>\u00a0parameter in the <code>liveSync</code> section. The HTTP netty transport port is considered the default port. 9443 HTTPS netty transport."},{"location":"ref/configuring-default-ports/#multi-datacenter-high-availability-deployment","title":"Multi Datacenter High Availability Deployment","text":"<p>Other than the ports used in clustering setups (i.e., a Minimum HA Deployment or a scalable cluster), the following is required:</p> 9092 Ports of the two separate instances of the broker deployed in each data center (e.g., <code>bootstrap.servers= 'host1:9092, host2:9092'. The default is</code>9092` where the external kafka servers start.)"},{"location":"ref/environment-compatibility/","title":"Environment Compatibility","text":"<p>This section describes the environment compatibilities of WSO2 Streaming Integrator.</p>"},{"location":"ref/environment-compatibility/#tested-operating-systems","title":"Tested Operating Systems","text":"<p>WSO2 SI is tested with the following operating systems:</p> Operating System Versions Windows 710 Ubuntu 16.0417.0418.04 MacOS High Sierra"},{"location":"ref/environment-compatibility/#tested-jdks","title":"Tested JDKs","text":"<p>WSO2 SI is tested with the following JDKs</p> JDK Version Oracle JDK 8 OpenJDK 8"},{"location":"ref/environment-compatibility/#tested-dbmss-database-management-systems","title":"Tested DBMSs (Database Management Systems)","text":"<p>WSO2 SI is tested with the following DBMSs</p> DBMS Version H2 1.4.187 MySQL 5.7 Microsoft SQL Server 2017 Oracle 11.2.0.2-x PostgreSQL 9.6"},{"location":"ref/environment-compatibility/#tested-web-browsers","title":"Tested Web Browsers","text":"<p>WSO2 SI is tested with the following web browsers.</p> <ul> <li>Firefox (56.0 and later)</li> <li>Google Chrome</li> <li>Safari</li> </ul>"},{"location":"ref/environment-compatibility/#known-incompatibilities","title":"Known Incompatibilities","text":"<p>Given below are the OS and JDK incompatibilities we have come across while testing WSO2 Streaming Integrator</p> Operating System Operating System Version JDK Version macOS High Sierra 10.13.1 (17B1003) JDK1.8.0_20JDK1.8.0_144JDK1.8.0_152JDK1.8.0_162 macOS High Sierra 10.13.2 JDK1.8.0_144JDK1.8.0_152 Ubunto 14.04 JDK1.8.0_151JDK1.8.0_152JDK1.8.0_161 Ubunto 16.04 JDK1.8.0_151JDK1.8.0_152JDK1.8.0_161JDK1.8.0_162"},{"location":"ref/event-simulation-apis/","title":"Event simulation apis","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/event-simulation-apis/#event-simulation-apis","title":"Event Simulation APIs","text":""},{"location":"ref/event-simulation-apis/#sending-a-single-event-for-simulation","title":"Sending a single event for simulation","text":""},{"location":"ref/event-simulation-apis/#overview","title":"Overview","text":"Description Sends a single event for simulation. API Context <code>/simulation/single</code> HTTP Method POST Request/Response format Request: <code>text/plain</code> Response: <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/event-simulation-apis/#curl-command-syntax","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9390/simulation/single\" -H \"accept: application/json\" -H \"content-type: text/plain\" -d \"{ \\\"streamName\\\": \\\"FooStream\\\", \\\"siddhiAppName\\\": \\\"TestSiddhiApp\\\", \\\"timestamp\\\": \\\"1500319950004\\\", \\\"data\\\": [ \\\"foo\\\", \\\"bar\\\", \\\"12345\\\" ]}\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#uploading-a-feed-simulation-configuration","title":"Uploading a feed simulation configuration","text":""},{"location":"ref/event-simulation-apis/#overview_1","title":"Overview","text":"Description Uploads a feed simulation configuration. API Context <code>/simulation/feed</code> HTTP Method POST Request/Response format Request: <code>text/plain</code> Response: <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/event-simulation-apis/#curl-command-syntax_1","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_1","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9390/simulation/feed\" -H \"accept: application/json\" -H \"content-type: text/plain\" -d \"{\\\"properties\\\":{\\\"simulationName\\\":\\\"TestFeedSimulation\\\",\\\"startTimestamp\\\":\\\"1500319950003\\\",\\\"endTimestamp\\\":\\\"1500319950009\\\",\\\"noOfEvents\\\":\\\"100\\\",\\\"description\\\":\\\"Test feed simulator\\\",\\\"timeInterval\\\":\\\"1000\\\"},\\\"sources\\\":[{\\\"siddhiAppName\\\":\\\"TestSiddhiApp\\\",\\\"streamName\\\":\\\"FooStream\\\",\\\"timestampInterval\\\":\\\"1000\\\",\\\"simulationType\\\":\\\"CSV_SIMULATION\\\",\\\"fileName\\\":\\\"foostream.csv\\\",\\\"delimiter\\\":\\\",\\\",\\\"isOrdered\\\":true,\\\"indices\\\":\\\"0,1,2\\\"}]}\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_1","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_1","title":"Response","text":"HTTP Status Code <p>Possible codes are 200, 403, 404, and 409.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#retrieving-all-feed-simulation-configurations","title":"Retrieving all feed simulation configurations","text":""},{"location":"ref/event-simulation-apis/#overview_2","title":"Overview","text":"Description Retrieves all feed simulation configurations. API Context <code>/simulation/feed</code> HTTP Method GET Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/event-simulation-apis/#curl-command-syntax_2","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_2","title":"Sample curl command","text":"<pre><code>curl -X GET \"http://localhost:9390/simulation/feed\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_2","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_2","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#updating-a-feed-simulation-configuration","title":"Updating a feed simulation configuration","text":""},{"location":"ref/event-simulation-apis/#overview_3","title":"Overview","text":"Description Updates a feed simulation configuration. API Context <code>/simulation/feed/{simulationName}</code> HTTP Method PUT Request/Response format Request: <code>text/plain</code> Response: <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter description</p> Parameter Description <code>{simulationName}</code> The name of the simulation configuration that needs to be updated."},{"location":"ref/event-simulation-apis/#curl-command-syntax_3","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_3","title":"Sample curl command","text":"<pre><code>curl -X PUT \"http://localhost:9390/simulation/feed/TestFeedSimulation\" -H \"accept: application/json\" -H \"content-type: text/plain\" -d \"{\\\"properties\\\":{\\\"simulationName\\\":\\\"TestFeedSimulation\\\",\\\"startTimestamp\\\":\\\"\\\",\\\"endTimestamp\\\":\\\"\\\",\\\"noOfEvents\\\":\\\"100\\\",\\\"description\\\":\\\"Test feed simulator\\\",\\\"timeInterval\\\":\\\"1000\\\"},\\\"sources\\\":[{\\\"siddhiAppName\\\":\\\"TestSiddhiApp\\\",\\\"streamName\\\":\\\"BarStream\\\",\\\"timestampInterval\\\":\\\"1000\\\",\\\"simulationType\\\":\\\"CSV_SIMULATION\\\",\\\"fileName\\\":\\\"foostream.csv\\\",\\\"delimiter\\\":\\\",\\\",\\\"isOrdered\\\":true,\\\"indices\\\":\\\"0,1,2\\\"}]}\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_3","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_3","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#retrieving-a-specific-feed-simulation-configuration","title":"Retrieving a specific feed simulation configuration","text":""},{"location":"ref/event-simulation-apis/#overview_4","title":"Overview","text":"Description Retrieves a specific feed simulation configuration. API Context <code>/simulation/feed/{simulationName}</code> HTTP Method GET Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{simulationName}</code> The name of the feed simulation configuration that needs to be retrieved."},{"location":"ref/event-simulation-apis/#curl-command-syntax_4","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_4","title":"Sample curl command","text":"<pre><code>curl -X GET \"http://localhost:9390/simulation/feed/TestFeedSimulation\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_4","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_4","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#deleting-a-feed-simulation-configuration","title":"Deleting a feed simulation configuration","text":""},{"location":"ref/event-simulation-apis/#overview_5","title":"Overview","text":"Description Deletes a feed simulation configuration. API Context <code>/simulation/feed/{simulationName}</code> HTTP Method DELETE Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{simulationName}</code> The name of the simulation configuration that needs to be deleted."},{"location":"ref/event-simulation-apis/#curl-command-syntax_5","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_5","title":"Sample curl command","text":"<pre><code>curl -X DELETE \"http://localhost:9390/simulation/feed/TestFeedSimulation\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_5","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_5","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#running-a-feed-simulation-configuration","title":"Running a feed simulation configuration","text":""},{"location":"ref/event-simulation-apis/#overview_6","title":"Overview","text":"Description Runs a feed simulation configuration. API Context <code>/simulation/feed/{simulationName}?action=run</code> HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{simulationName}</code> The name of the simulation configuration that needs to be run."},{"location":"ref/event-simulation-apis/#curl-command-syntax_6","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_6","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9390/simulation/feed/TestFeedSimulation/?action=run\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_6","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_6","title":"Response","text":"HTTP Status Code <p>Possible codes are 200, 403, 404, and 409.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#pausing-a-feed-simulation","title":"Pausing a feed simulation","text":""},{"location":"ref/event-simulation-apis/#overview_7","title":"Overview","text":"Description Pauses a currently active feed simulation. API Context <code>/simulation/feed/{simulationName}?action=pause</code> HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{simulationName}</code> The name of the simulation configuration that needs to be paused."},{"location":"ref/event-simulation-apis/#curl-command-syntax_7","title":"curl command syntax","text":"<pre><code>curl -X POST \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/simulation/feed/&lt;FEED_NAME&gt;/?action=pause\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-curl-command_7","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9390/simulation/feed/TestFeedSimulation/?action=pause\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_7","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_7","title":"Response","text":"HTTP Status Code <p>Possible codes are 200, 403, 404, and 409.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#resuming-a-feed-simulation","title":"Resuming a feed simulation","text":""},{"location":"ref/event-simulation-apis/#overview_8","title":"Overview","text":"Description Resumes a paused feed simulation. API Context <code>/simulation/feed/{simulationName}?action=resume</code> HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{simulationName}</code> The name of the simulation configuration that needs to be resumed."},{"location":"ref/event-simulation-apis/#curl-command-syntax_8","title":"curl command syntax","text":"<pre><code>curl -X POST \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/simulation/feed/&lt;SIMULATION_NAME&gt;/?action=resume\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-curl-command_8","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9390/simulation/feed/TestFeedSimulation/?action=resume\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_8","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_8","title":"Response","text":"HTTP Status Code <p>Possible codes are 200, 403, 404, and 409.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#stopping-a-feed-simulation","title":"Stopping a feed simulation","text":""},{"location":"ref/event-simulation-apis/#overview_9","title":"Overview","text":"Description Stops a currently active feed simulation. API Context <code>/simulation/feed/{simulationName}?action=stop</code> HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{simulationName}</code> The name of the simulation configuration that needs to be stopped."},{"location":"ref/event-simulation-apis/#curl-command-syntax_9","title":"curl command syntax","text":"<pre><code>curl -X POST \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/simulation/feed/&lt;SIMULATION_NAME&gt;/?action=stop\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-curl-command_9","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9390/simulation/feed/TestFeedSimulation/?action=stop\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_9","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_9","title":"Response","text":"HTTP Status Code <p>Possible codes are 200, 404, and 409.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#retrieving-a-simulation-configuration-status-by-name","title":"Retrieving a simulation configuration status by name","text":""},{"location":"ref/event-simulation-apis/#overview_10","title":"Overview","text":"Description Retrieves the status of a given simulation configuration. API Context <code>/simulation/feed/{simulationName}/status</code> HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{simulationName}</code> The name of the simulation configuration of which the status needs to be checked."},{"location":"ref/event-simulation-apis/#curl-command-syntax_10","title":"curl command syntax","text":"<pre><code>curl -X GET \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/simulation/feed/&lt;SIMULATION_NAME&gt;/status\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-curl-command_10","title":"Sample curl command","text":"<pre><code>curl -X GET \"http://localhost:9390/simulation/feed/TestFeedSimulation/status\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_10","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_10","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#uploading-a-csv-file","title":"Uploading a CSV file","text":""},{"location":"ref/event-simulation-apis/#overview_11","title":"Overview","text":"Description Uploads a CSV file for feed simulation. API Context <code>/simulation/files</code> HTTP Method GET Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/event-simulation-apis/#curl-command-syntax_11","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_11","title":"Sample curl command","text":""},{"location":"ref/event-simulation-apis/#sample-output_11","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_11","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#fetching-csv-file-names","title":"Fetching CSV file names","text":""},{"location":"ref/event-simulation-apis/#overview_12","title":"Overview","text":"Description Fetches the names of CSV files that are currently uploaded in the system. API Context <code>/simulation/files</code> HTTP Method GET Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/event-simulation-apis/#curl-command-syntax_12","title":"curl command syntax","text":"<pre><code>curl -X GET \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/simulation/files\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-curl-command_12","title":"Sample curl command","text":"<pre><code>curl -X GET \"http://localhost:9390/simulation/files\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_12","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_12","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#updating-a-csv-file","title":"Updating a CSV file","text":""},{"location":"ref/event-simulation-apis/#overview_13","title":"Overview","text":"Description Updates a CSV file that is already uploaded in the system. API Context <code>/simulation/files/{fileName}</code> HTTP Method PUT Request/Response format <code></code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{fileName}</code> The name of the CSV file that needs to be updated."},{"location":"ref/event-simulation-apis/#curl-command-syntax_13","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_13","title":"Sample curl command","text":"<pre><code>curl -X PUT -F 'file=@foostream.csv' http://localhost:9390/simulation/files/foostream.csv?fileName=foostream.csv\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_13","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_13","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#deleting-a-csv-file","title":"Deleting a CSV file","text":""},{"location":"ref/event-simulation-apis/#overview_14","title":"Overview","text":"Description Deletes the specified CSV file. API Context <code>/simulation/files/{fileName}</code> HTTP Method DELETE Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{fileName}</code> The name of the CSV file that needs to be deleted."},{"location":"ref/event-simulation-apis/#curl-command-syntax_14","title":"curl command syntax","text":"<pre><code>curl -X DELETE \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/simulation/files/&lt;FILE_NAME&gt;.csv\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-curl-command_14","title":"Sample curl command","text":"<pre><code>curl -X DELETE \"http://localhost:9390/simulation/files/CSVTestFile.csv\" -H \"accept: application/json\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_14","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_14","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#testing-a-database-connection","title":"Testing a database connection","text":""},{"location":"ref/event-simulation-apis/#overview_15","title":"Overview","text":"Description Tests a database connection. API Context /simulation/connectToDatabase HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/event-simulation-apis/#curl-command-syntax_15","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_15","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9090/simulation/connectToDatabase\" -H \"accept: application/json\" -H \"content-type: application/json\" -d \"{ \\\"dataSourceLocation\\\": \\\"jdbc:mysql://localhost:3306/DatabaseFeedSimulation\\\", \\\"driver\\\": \\\"com.mysql.jdbc.Driver\\\", \\\"username\\\": \\\"root\\\", \\\"password\\\": \\\"password\\\"}\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_15","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_15","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#retrieving-database-tables","title":"Retrieving database tables","text":""},{"location":"ref/event-simulation-apis/#overview_16","title":"Overview","text":"Description Retrieves database tables. API Context /simulation/connectToDatabase/retrieveTableNames HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/event-simulation-apis/#curl-command-syntax_16","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_16","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9090/simulation/connectToDatabase/retrieveTableNames\" -H \"accept: application/json\" -H \"content-type: application/json\" -d \"{ \\\"dataSourceLocation\\\": \\\"jdbc:mysql://localhost:3306/DatabaseFeedSimulation\\\", \\\"driver\\\": \\\"com.mysql.jdbc.Driver\\\", \\\"username\\\": \\\"root\\\", \\\"password\\\": \\\"password\\\"}\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_16","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_16","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/event-simulation-apis/#retrieving-database-table-columns","title":"Retrieving database table columns","text":""},{"location":"ref/event-simulation-apis/#overview_17","title":"Overview","text":"Description Retrieves database table columns. API Context /simulation/connectToDatabase/{tableName}/retrieveColumnNames HTTP Method POST Request/Response format <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling <p>Parameter Description</p> Parameter Description <code>{tableName}</code> The name of the database table of which the columns need to be retrieved."},{"location":"ref/event-simulation-apis/#curl-command-syntax_17","title":"curl command syntax","text":""},{"location":"ref/event-simulation-apis/#sample-curl-command_17","title":"Sample curl command","text":"<pre><code>curl -X POST \"http://localhost:9090/simulation/connectToDatabase/DataTable/retrieveColumnNames\" -H \"accept: application/json\" -H \"content-type: application/json\" -d \"{ \\\"dataSourceLocation\\\": \\\"jdbc:mysql://localhost:3306/DatabaseFeedSimulation\\\", \\\"driver\\\": \\\"com.mysql.jdbc.Driver\\\", \\\"username\\\": \\\"root\\\", \\\"password\\\": \\\"password\\\"}\"\n</code></pre>"},{"location":"ref/event-simulation-apis/#sample-output_17","title":"Sample output","text":""},{"location":"ref/event-simulation-apis/#response_17","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/hTTP-Status-Codes/","title":"HTTP Status Codes","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/hTTP-Status-Codes/#http-status-codes","title":"HTTP Status Codes","text":"<p>When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method (<code>POST, GET, PUT, DELETE</code>) executed. The following are the definitions of the various HTTP status codes that are returned.</p>"},{"location":"ref/hTTP-Status-Codes/#http-status-codes-indicating-successful-delivery","title":"HTTP status codes indicating successful delivery","text":"Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful <code>GET</code> and <code>PUT</code> REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful <code>POST</code> REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful <code>DELETE</code> REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed.\u00a0This generally occurs when your successful in trying to undeploy an application."},{"location":"ref/hTTP-Status-Codes/#error-http-status-codes","title":"Error HTTP status codes","text":"Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful <code>GET</code> and <code>PUT</code> REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred."},{"location":"ref/healthcheck-APIs/","title":"Healthcheck APIs","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/healthcheck-APIs/#healthcheck-api","title":"Healthcheck API","text":""},{"location":"ref/healthcheck-APIs/#overview","title":"Overview","text":"Description API Context HTTP Method <code>GET</code> Request/Response Format Authentication Basic Username <code>admin</code> Password <code>admin</code> Runtime"},{"location":"ref/healthcheck-APIs/#curl-command-syntax","title":"curl command syntax","text":"<pre><code>curl -k -X GET http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/health\n</code></pre>"},{"location":"ref/healthcheck-APIs/#sample-curl-command","title":"Sample curl command","text":"<pre><code>curl -k -X GET http://localhost:9090/health \n</code></pre>"},{"location":"ref/healthcheck-APIs/#sample-output","title":"Sample output","text":"<pre><code>{\"status\":\"healthy\"}    </code></pre>"},{"location":"ref/healthcheck-APIs/#response","title":"Response","text":"HTTP Status Code <p>200</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/performance-analysis-results/","title":"Performance Analysis Results","text":"<p>Notes</p> <ul> <li>These performance statistics were taken when the load average was below 3.8 in the 4 core instance.</li> <li>Most of the examples given below use log sinks to log statistics for performance monitoring purposes. However, note that log sinks incur a high system overhead and can lower performance by even more than 50%.</li> </ul>"},{"location":"ref/performance-analysis-results/#performance-analysis-results-summary","title":"Performance analysis results summary","text":"<p>The recommended CPU and memory specifications for Docker containers are as follows:</p> <ul> <li> <p>CPU: 4 cores</p> </li> <li> <p>Memory: 8GB</p> </li> </ul> <p>The recommended memory specifications SI server as follows. These are configured in the <code>&lt;SI_Home&gt;/wso2/server/bin/carbon.sh</code> file.</p> <ul> <li> <p>Xms: 2GB</p> </li> <li> <p>Xmx: 4GB</p> </li> </ul> <p>The exact specifications used in the use cases listed in this section are summarised in the table below:</p> Scenario CPU Memory SI Memory Allocation Input TPS Input Message Size Output TPS Consuming events using Kafka source 4 cores 8GB - Xms: 2g - Xmx: 4g 180K 60 bytes Not Available Consuming messages from an HTTP Source 4 cores 8GB - Xms: 2g - Xmx: 4g 30K 60 bytes Not Available Sending HTTP requests and consuming the responses 4 cores 8GB - Xms: 2g - Xmx: 4g 29K Sent: 60 bytesReceived: 60 bytes - To HTTP Source: 29K - To HTTP Request Sink: 29K Performing ETL tasks 4 cores 16GB - Xms: 2g - Xmx: 4g 29K Read: 100 bytesStored: 200 bytes To Oracle Store: 72K Consuming messages from a Kafka source and publish to an HTTP endpoint 2 cores Docker Memory: 3GB - Xms: 256m - Xmx: 1g 10K Consumed: 400 bytesPublished: 600 bytes 10K Consuming messages from a CSV file and publish to a MySQL table 4 cores Docker Memory: 8GB - Xms: 2g  - Xmx: 4g 9K Read: 300 bytesPublished: 300 bytes 9K Monitoring a database table in MySQL and publishing data to a Kafka topic 4 cores Docker Memory: 8GB - Xms: 2g  - Xmx: 4g 13K Read: 300 bytesPublished: 300 bytes 13K Read XML file and mapping to a stream 4 cores Docker Memory: 8GB - Xms: 2g  - Xmx: 4g 40K read: 350 bytes 40K Reading an XML file and publishing to a Kafka topic 4 cores Docker Memory: 8GB - Xms: 2g  - Xmx: 4g 38K read: 350 bytesPublished: 350 bytes 38K"},{"location":"ref/performance-analysis-results/#consuming-events-using-kafka-source","title":"Consuming events using Kafka source","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances","title":"Specifications of EC2 Instances","text":"<ul> <li>Stream Processor : c5.xLarge</li> <li>Kafka server     : c5.xLarge</li> <li>Kafka publisher  : c5.xLarge</li> </ul>"},{"location":"ref/performance-analysis-results/#siddhi-application","title":"Siddhi Application","text":"<pre><code>@App:name(\"HelloKafka\")\n\n@App:description('Consume events from a Kafka Topic and publish to a different Kafka Topic')\n\n@source(type='kafka',\ntopic.list='kafka_topic',\npartition.no.list='0',\nthreading.option='single.thread',\ngroup.id=\"group\",\nbootstrap.servers='172.31.0.135:9092',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream KafkaSourceThroughputStream(count long);\n\nfrom SweetProductionStream#window.timeBatch(5 sec)\nselect count(*)/5 as count\ninsert into KafkaSourceThroughputStream;\n</code></pre>"},{"location":"ref/performance-analysis-results/#results","title":"Results","text":"<p>Average Consuming TPS from Kafka: 180K</p>"},{"location":"ref/performance-analysis-results/#consuming-messages-from-an-http-source","title":"Consuming messages from an HTTP Source","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_1","title":"Specifications of EC2 Instances","text":"<ul> <li>Stream Processor : c5.xLarge</li> <li>JMeter           : c5.xLarge</li> </ul>"},{"location":"ref/performance-analysis-results/#siddhi-application_1","title":"Siddhi Application","text":"<pre><code>@App:name(\"HttpSource\")\n\n@App:description('Consume events from http clients')\n\n@source(type='http', worker.count='20', receiver.url='http://172.31.2.99:8081/service',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream HttpSourceThroughputStream(count long);\n\nfrom SweetProductionStream#window.timeBatch(5 sec)\nselect count(*)/5 as count\ninsert into HttpSourceThroughputStream;\n</code></pre>"},{"location":"ref/performance-analysis-results/#results_1","title":"Results","text":"<p>Average Consuming TPS from Http Source: 30K</p>"},{"location":"ref/performance-analysis-results/#sending-http-requests-and-consuming-the-responses","title":"Sending HTTP requests and consuming the responses","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_2","title":"Specifications of EC2 Instances","text":"<ul> <li>Stream Processor : c5.xLarge</li> <li>JMeter           : c5.xLarge</li> <li>Web server       : c5.xLarge</li> </ul>"},{"location":"ref/performance-analysis-results/#siddhi-application_2","title":"Siddhi Application","text":"<pre><code>@App:name(\"HttpRequestResponse\")\n\n@App:description('Consume events from an HTTP source, ')\n\n@source(type='http', worker.count='20', receiver.url='http://172.31.2.99:8081/service',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http-request', l, sink.id='production-request', publisher.url='http://172.17.0.1:8688//netty_echo_server', @map(type='json'))\ndefine stream HttpRequestStream (batchNumber double, lowTotal double);\n\n@source(type='http-response' , sink.id='production-request', http.status.code='200',\n@map(type='json'))\ndefine stream HttpResponseStream(batchNumber double, lowTotal double);\n\n@sink(type='log')\ndefine stream FinalThroughputStream(count long);\n\n@sink(type='log')\ndefine stream InputThroughputStream(count long);\n\nfrom SweetProductionStream\nselect 1D as batchNumber, 1200D as lowTotal\ninsert into HttpRequestStream;\n\nfrom SweetProductionStream#window.timeBatch(5 sec)\nselect count(*)/5 as count\ninsert into InputThroughputStream;\n\nfrom HttpResponseStream#window.timeBatch(5 sec)\nselect count(*)/5 as count\ninsert into FinalThroughputStream;\n</code></pre>"},{"location":"ref/performance-analysis-results/#results_2","title":"Results","text":"<ul> <li>Average Consuming TPS to HTTP Source          : 29K</li> <li>Average Publishing TPS from HTTP request sink  : 29K</li> <li>Average Consuming TPS from HTTP response source: 29K</li> </ul>"},{"location":"ref/performance-analysis-results/#performing-etl-tasks","title":"Performing ETL tasks","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_3","title":"Specifications of EC2 Instances","text":"<ul> <li>Stream Processor : m4.xlarge</li> <li>JMeter           : m4.xlarge</li> <li>Web server       : m4.xlarge</li> </ul>"},{"location":"ref/performance-analysis-results/#siddhi-application_3","title":"Siddhi Application","text":"<p>This scenario was tested using two Siddhi applications that execute the process explained below.</p> <p></p> <p>The two Siddhi applications are as follows:</p> <p>ETLFIleRecordsCopier.siddhi</p> <pre><code>@App:name('ETLFileRecordsCopier')\n@App:description('This sample demonstrates on integrating a File in a particular location with a Database.')\n\n@source(type='file', mode='LINE',\n    dir.uri='file:/Users/wso2/demo/accurate-files',\n    action.after.process='MOVE',\n    move.after.process='file:/Users/wso2/demo/moved',\n    tailing='false',\n    header.present='true',\n    @map(\n        type='csv',\n        delimiter='|',\n        @attributes(code = '0', serialNo = '1', amount = '2', fileName = 'trp:file.path', eof = 'trp:eof')))\ndefine stream FileReaderStream (code string, serialNo string, amount double, fileName string, eof string); -- Reads from file\n\n@Store(type=\"rdbms\",\n      jdbc.url=\"jdbc:mysql://localhost:3306/batchInformation?useSSL=false\",\n      username=\"root\",\n      password=\"root\" ,\n      jdbc.driver.name=\"com.mysql.jdbc.Driver\",\n      isAutoCommit = 'true')\ndefine table AccurateBatchTable(serialNo string, amount double, fileName string, status string, timestamp long);\n\n@sink(type='log', prefix='File to DB copying has Started: ')\ndefine stream FileReadingStartStream(fileName string);\n\n@sink(type='log', prefix='File to DB copying has Finished: ')\ndefine stream FileReadingEndStream(fileName string);\n\n\nfrom FileReaderStream\nselect serialNo, amount, fileName, \"test\" as status, eventTimestamp() as timestamp, count() as rowNumber, eof\ninsert into DataStream;\n\nfrom DataStream\nselect *\ninsert into DataStreamPassthrough;\n\n-- Write to DB Passthrough\nfrom DataStreamPassthrough#window.externalTimeBatch(timestamp, 5 sec, timestamp, 10 sec)\nselect serialNo, amount, fileName, status, timestamp, rowNumber, eof\ninsert into TemporaryTablePassthroughStream;\n\n-- Log First Record\nfrom TemporaryTablePassthroughStream[rowNumber == 1]\nselect fileName\ninsert into FileReadingStartStream;\n\n-- Log Every 100000th Record\nfrom TemporaryTablePassthroughStream\nselect fileName, rowNumber as rows\ninsert into FileReadingInProgressStream;\n\n-- Log Last Record\nfrom TemporaryTablePassthroughStream[eof == 'true']\nselect fileName\ninsert into FileReadingEndStream;\n\n-- Write to DB\nfrom TemporaryTablePassthroughStream#window.batch()\nselect serialNo, amount, fileName, status, timestamp\ninsert into AccurateBatchTable;\n</code></pre> <p>ETLFileAnalyzer.siddhi</p> <p><pre><code>@App:name('ETLFileAnalyzer')\n@App:description('This sample demonstrates on moving files to a specific location comparing its content with the header values.')\n\n@source(type='file', mode='REGEX',\n    dir.uri='file:/Users/wso2/demo/new',\n    action.after.process='MOVE',\n    move.after.process='file:/Users/wso2/demo/header-processed',\n    tailing='false',\n    @map(\n        type='text',\n        fail.on.missing.attribute = 'false',\n        regex.A='HDprod-[a-zA-z]*-[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]-([0-9]+)',\n        @attributes(\n            expectedRowCount = 'A[1]',\n            fileName = 'trp:file.path')))\ndefine stream HeaderReaderStream (fileName string, expectedRowCount long);\n\n@source(type='file', mode='LINE',\n    dir.uri='file:/Users/wso2/demo/header-processed',\n    tailing='false',\n    header.present='true',\n    @map(\n        type='csv',\n        delimiter='|',\n        @attributes(code = '0', serialNo = '1', amount = '2', fileName = 'trp:file.path', eof = 'trp:eof')))\ndefine stream FileReaderStream (code string, serialNo string, amount double, fileName string, eof string);\n\n@sink(type='log', prefix='Accurate Batch: ')\ndefine stream AccurateFileNotificationStream (fromPath string);\n\n@sink(type='log', prefix='Inaccurate Batch: ')\ndefine stream InaccurateFileNotificationStream (fromPath string);\n\n@sink(type='log', prefix='Batch checking started: ')\ndefine stream ExpectedRowCountsStream (fileName string, expectedRowCount long);\n\ndefine stream AnalyzingLogStream (fileName string, rowCount long);\n\ndefine table ExpectedRowCountsTable (fileName string, expectedRowCount long, existingRowCount long);\n\n@sink(type='log', prefix='Batch checking finished: ')\ndefine stream ExistingRowCountsStream (fileName string, existingRowCount long);\n\n-- Expected Row Count reader. Moves file from 'new' to 'header-processed'\nfrom HeaderReaderStream[NOT(expectedRowCount is null) and NOT(fileName is null)]\nselect *\ninsert into ExpectedRowCountsStream;\n\nfrom ExpectedRowCountsStream\nselect fileName, expectedRowCount, -1L as existingRowCount\ninsert into ExpectedRowCountsTable;\n\n-- Existing Row Count calculator. Moves file from 'header-processed' to 'rows-counted'\nfrom FileReaderStream\nselect *\ninsert into FileDataStream;\n\npartition with (fileName of FileDataStream)\nbegin\n    from FileDataStream\n    select fileName, count() as rowCount, eof\n    insert into #ThisFileRowCounts;\n\n    from #ThisFileRowCounts\n    select fileName, rowCount\n    insert into AnalyzingLogStream;\n\n    from #ThisFileRowCounts[eof == 'true']\n    select fileName, rowCount as existingRowCount\n    insert into ExistingRowCountsStream;\nend;\n\n-- Existing vs. Expected Row Counts comparer\nfrom ExistingRowCountsStream as S inner join ExpectedRowCountsTable as T on str:replaceFirst(S.fileName, 'header-processed', 'new') == T.fileName\nselect S.fileName as fromPath, T.expectedRowCount as expectedRowCount, S.existingRowCount as existingRowCount\ninsert into FileInfoMatcherStream;\n\nfrom FileInfoMatcherStream\nselect fromPath, existingRowCount\nupdate ExpectedRowCountsTable\n    set ExpectedRowCountsTable.existingRowCount = existingRowCount\n    on ExpectedRowCountsTable.fileName == fromPath;\n\n-- Accurate file mover\nfrom FileInfoMatcherStream[expectedRowCount == existingRowCount]\nselect fromPath\ninsert into AccurateFileStream;\n\nfrom AccurateFileStream#file:move(fromPath, '/Users/wso2/demo/accurate-files/')\nselect fromPath\ninsert into AccurateFileNotificationStream;\n\n-- Inaccurate batch file mover\nfrom FileInfoMatcherStream[expectedRowCount != existingRowCount]\nselect fromPath\ninsert into InaccurateFileStream;\n\nfrom InaccurateFileStream#file:move(fromPath, '/Users/wso2/demo/inaccurate-files/')\nselect fromPath\ninsert into InaccurateFileNotificationStream;\n</code></pre> For a detailed description of this scenario, see the Streaming ETL with WSO2 Streaming Integrator article</p>"},{"location":"ref/performance-analysis-results/#results_3","title":"Results","text":"<p>The performance statistics of this scenario are as follows:</p> <ul> <li>Lines     : 6,140,031</li> <li>Size      : 124MB</li> <li>Database  : AWS RDS instance with oracle-ee 12.1.0.2.v15</li> <li>Duration  : 1.422 minutes (85373ms)</li> </ul>"},{"location":"ref/performance-analysis-results/#consuming-messages-from-a-kafka-source-and-publish-to-an-http-endpoint","title":"Consuming messages from a Kafka source and publish to an HTTP endpoint","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_4","title":"Specifications of EC2 Instances","text":""},{"location":"ref/performance-analysis-results/#docker-resource-allocation","title":"Docker resource allocation","text":"Memory  3GB  CPU  2 Cores"},{"location":"ref/performance-analysis-results/#server-memory-allocation","title":"Server memory allocation","text":"Xms  256m  Xmx  1g"},{"location":"ref/performance-analysis-results/#siddhi-applications","title":"Siddhi applications","text":"<p>The following Siddhi applications were used in this scenario:</p> <ul> <li> <p>To read messages from a Kafka topic, do a transformation and insert into an in-memory topic:</p> <pre><code>@App:name('kafka-consumer')\n\n@App:description('Reads messages from kafka topics and puts into in-memory-input topic')\n\n@sink(type = 'inMemory', topic = \"in-memory-input\",\n  @map(type = 'passThrough'))\ndefine stream ToInMemoryInput (kafkaConsumerInTS long, kafkaConsumerOutTS long, locations string, material string, createdDate string, sid string, headline string, body string, publishTS long, id string);\n\n@source(type = 'kafka', topic.list = \"test3\", threading.option = \"single.thread\", group.id = \"group1\", \n  bootstrap.servers = \"172.31.39.91:9092\", optional.configuration = \"auto.offset.reset:latest\",\n  @map(type = 'json', fail.on.missing.attribute = \"false\", enclosing.element = \"$\"))\ndefine stream FromKafkaMessage (locations string, material string, createdDate string, sid string, headline string, body string, publishTS string, id string, updatedDate string);\n\n@sink(type = 'log', prefix = '----------------------Kafka Consumer Throughput per second: ',\n  @map(type = 'json'))\ndefine stream LogSink (totalEventsPerSec long);\n\n@info(name = 'Kafka Consumer Event Timestamp')\nfrom FromKafkaMessage\nselect eventTimestamp() as kafkaConsumerInTS, time:timestampInMilliseconds() as kafkaConsumerOutTS, locations, material, \nstr:replaceFirst(createdDate, 'Z', 'GMT') as createdDate, \nsid, headline, body, time:timestampInMilliseconds(str:replaceFirst(ifThenElse(publishTS is null, updatedDate, publishTS), 'Z', 'GMT'), \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\") as publishTS, ifThenElse( id is null, 'null', id) as id\ninsert into ToInMemoryInput;\n\nfrom FromKafkaMessage#window.timeBatch(1 sec)\nselect count() as totalEventsPerSec\ninsert into LogSink;\n</code></pre> </li> <li> <p>To filter dynamic headers from incoming data stream</p> <p><pre><code>@App:name('Intermediate-process')\n@App:description('Filter dynamic headers from incoming data stream')\n\n@sink(type = \"inMemory\", topic = \"in-memory-output\", @map(type = \"passThrough\"))\ndefine stream ToInMemoryOutput (sid string, connectionId string, headers string, data string);\n\n@source(type = 'inMemory', topic = \"in-memory-input\", @map(type = 'passThrough'))\ndefine stream FromInMemoryInput (kafkaConsumerInTS long, kafkaConsumerOutTS long, locations object, material \nobject, createdDate string, sid string, headline string, body string, publishTS long, id string);\n\n\n@info(name = 'Filter Heards Messages')\nfrom FromInMemoryInput\nselect  sid, \"test_connectionId\" as connectionId, \"'connectionId:test_connectionId','appKey:workManWork','Content-type:application/json'\" as headers,\n str:fillTemplate(\"\"\"\n    {\n        \"type\": \"heards_sub_resp\",\n        \"publishTS\": {{ publishTS }},\n        \"dynamicAppInTS\": {{ dynamicAppInTS }},\n        \"dynamicAppOutTS\": {{ dynamicAppOutTS }},\n        \"kafkaConsumerInTS\": {{ kafkaConsumerInTS }},\n        \"kafkaConsumerOutTS\": {{ kafkaConsumerOutTS }},\n        \"headline\":\"{{ headline }}\",\n        \"body\":\"{{ body }}\",\n        \"id\": \"{{ id }}\",\n        \"material\": {{ material }},\n        \"locations\": {{ locations }},\n        \"createdDate\": \"{{ createdDate }}\",\n        \"sid\":\"{{ sid }}\",\n        \"correlationId\":\"{{ correlationId }}\" \n    }\"\"\", \n    map:create(\n    'headline', headline, \n    'body', body, \n    'id', id, \n    'material', json:getString(material, '$'), \n    'locations', json:getString(locations, '$'), \n    'createdDate', str:replaceFirst(createdDate, 'GMT', 'Z'), \n    'sid', sid, \n    'publishTS', publishTS, \n    'dynamicAppOutTS', time:timestampInMilliseconds(), \n    'dynamicAppInTS', eventTimestamp(), \n    'kafkaConsumerInTS', kafkaConsumerInTS, \n    'kafkaConsumerOutTS', kafkaConsumerOutTS, \n    'correlationId', 'Test123')) as data\ninsert into ToInMemoryOutput;\n</code></pre> - To read output messages from the in-memory-output topic and publish them to the HTTP client</p> <pre><code>@App:name('ws-publisher')\n@App:description('Reads from in-memory-output topic and publishes messages to client')\n\n@source(type = 'inMemory', topic = \"in-memory-output\", @map(type = 'passThrough'))\ndefine stream fromInMemoryOutput (sid string, connectionId string, headers string, data string);\n\n@sink(type = 'http',\n  method = \"POST\",\n  publisher.url = \"http://172.31.39.177:8280/services/TestProxy\",\n  headers = \"{{ headers }}\",\n  on.error = \"LOG\",\n  max.pool.active.connections=\"1000\",\n  ssl.verification.disabled = \"true\",\n  @map(type = 'json',\n    @payload(\"\"\"{\"data\":{{ data }} }\"\"\")))\n\ndefine stream ToWsClient (data string, wsPublisherOutTS long, headers string, connectionId string, sid string);\n\n@sink(type = 'log', prefix = '----------------------WS Publisher Throughput per second: ',\n  @map(type = 'json'))\ndefine stream LogSink (totalEventsPerSec long);\n\n@info(name = 'Add Filtered Message Timestamp')\nfrom fromInMemoryOutput\nselect json:toString(json:setElement(json:setElement(json:toObject(data), '$', eventTimestamp(), 'wsPublisherInTS'), '$', time:timestampInMilliseconds(), 'wsPublisherOutTS')) as data, time:timestampInMilliseconds() as wsPublisherOutTS, headers, connectionId, sid\ninsert into ToWsClient;\n\nfrom ToWsClient#window.timeBatch(1 sec)\nselect count() as totalEventsPerSec\ninsert into LogSink;\n</code></pre> </li> </ul>"},{"location":"ref/performance-analysis-results/#results_4","title":"Results","text":"<ul> <li> <p>Memory consumed: 1g</p> </li> <li> <p>TPS: 10,000</p> </li> </ul>"},{"location":"ref/performance-analysis-results/#consuming-messages-from-a-csv-file-and-publish-to-a-mysql-table","title":"Consuming messages from a CSV file and publish to a MySQL table","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_5","title":"Specifications of EC2 Instances","text":""},{"location":"ref/performance-analysis-results/#docker-resource-allocation_1","title":"Docker resource allocation","text":"Memory  8GB  CPU  4 Cores"},{"location":"ref/performance-analysis-results/#server-memory-allocation_1","title":"Server memory allocation","text":"Xms  2g  Xmx  4g"},{"location":"ref/performance-analysis-results/#siddhi-application_4","title":"Siddhi application","text":"<pre><code>@App:name(\"FileToRdbms\")\n\n@App:description(\"Description of the plan\")\n\n@store(type='rdbms' , jdbc.url='jdbc:mysql://172.31.18.173:3306/purchesOrder?useSSL=false',username='root',password='root',jdbc.driver.name='com.mysql.jdbc.Driver') \ndefine table  PurchesOrderTable (orderID string, numberOfItems int, totalValue double, paymentStatus string, deliveryAddress string );\n\n\n@source(type='file', mode='line',\nfile.uri='file:/home/ubuntu/csv/productTable.csv',\ntailing='false',\naction.after.process='MOVE',\nmove.after.process='file:/home/ubuntu/csv/moved',\n@map(type='csv', delimiter=','))\ndefine stream InventoryUpdate (orderID string, numberOfItems int, totalValue double, paymentStatus string, deliveryAddress string);\n\n@async(buffer.size='4096', workers='2', batch.size.max='5000') \ndefine stream IntrimEventStream(orderID string, numberOfItems int, totalValue double, paymentStatus string, deliveryAddress string);\n\n\nfrom InventoryUpdate\nselect *\ninsert into IntrimEventStream;\n\n\nfrom IntrimEventStream\nselect *\ninsert into PurchesOrderTable;\n\n\nfrom InventoryUpdate#window.timeBatch(1 sec)\nselect count() as throughput\ninsert into OutputStream;\n\nfrom OutputStream#log('TPS: ')\ninsert into TempStream;\n</code></pre>"},{"location":"ref/performance-analysis-results/#results_5","title":"Results","text":"<ul> <li> <p>Memory consumed: 2.56g</p> </li> <li> <p>TPS: 9,000</p> </li> </ul>"},{"location":"ref/performance-analysis-results/#monitoring-a-database-table-in-mysql-and-publishing-data-to-a-kafka-topic","title":"Monitoring a database table in MySQL and publishing data to a Kafka topic","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_6","title":"Specifications of EC2 Instances","text":""},{"location":"ref/performance-analysis-results/#docker-resource-allocation_2","title":"Docker resource allocation","text":"Memory  8GB  CPU  4 Cores"},{"location":"ref/performance-analysis-results/#server-memory-allocation_2","title":"Server memory allocation","text":"Xms  2g  Xmx  4g"},{"location":"ref/performance-analysis-results/#siddhi-applications_1","title":"Siddhi applications","text":"<pre><code>@App:name(\"PurchaseOrderSiddhiApp\")\n\n@App:description(\"Description of the plan\")\n\n--@sink(type='log')\n@source(type = 'cdc', url = \"jdbc:mysql://172.31.18.173:3306/order?useSSL=false\", username = \"root\", password = \"root\", table.name = \"PurchesOrders\", operation = \"insert\", \n    @map(type = 'keyvalue', fail.on.missing.attribute = \"false\"))\ndefine stream PurchesOrderStream (orderID string, numberOfItems int, totalValue double, paymentStatus string, deliveryAddress string );\n\n@sink(type='kafka',\n      topic='delivery_items_topic',\n      bootstrap.servers='172.31.3.169:9092',\n      partition.no='0',\n      @map(type='xml'))\ndefine stream kafkaPublisherStream(orderID string, numberOfItems int, totalPayable double, deliveryAddress string);\n\n@sink(type='log') \ndefine stream kafkPubTps(pubCount long);\n\n@sink(type='log') \ndefine stream publishTps(recCount long);\n\n\nfrom PurchesOrderStream[paymentStatus =='cod' or paymentStatus=='paid']\nselect orderID, numberOfItems, ifThenElse(paymentStatus=='cod', totalValue, 0.0) as totalPayable, deliveryAddress\ninsert into kafkaPublisherStream;\n\n\nfrom PurchesOrderStream#window.timeBatch(1 sec)\nselect count() as recCount\ninsert into publishTps;\n\n\nfrom kafkaPublisherStream#window.timeBatch(1 sec)\nselect count() as pubCount\ninsert into kafkPubTps;\n</code></pre>"},{"location":"ref/performance-analysis-results/#results_6","title":"Results","text":"<ul> <li> <p>Memory Consumption: 1.5g</p> </li> <li> <p>Time taken: 46 minutes</p> </li> <li> <p>Data set size: 34,330,327</p> </li> </ul>"},{"location":"ref/performance-analysis-results/#read-xml-file-and-mapping-to-a-stream","title":"Read XML file and mapping to a stream","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_7","title":"Specifications of EC2 Instances","text":""},{"location":"ref/performance-analysis-results/#docker-resource-allocation_3","title":"Docker resource allocation","text":"Memory  8GB  CPU  4 Cores"},{"location":"ref/performance-analysis-results/#server-memory-allocation_3","title":"Server memory allocation","text":"Xms  2g  Xmx  4g"},{"location":"ref/performance-analysis-results/#siddhi-applications_2","title":"Siddhi applications","text":"<pre><code>@App:name(\"NodesConvertor\")\n\n\n@App:description(\"Description of the plan\")\n\n@source(\n    type = 'file', \n    file.uri = \"file:/home/ubuntu/csv/input.xml\", \n    mode = \"line\",\n    tailing = \"false\", \n    action.after.process='keep',\n    @map(type='xml', \n        enclosing.element=\"/osm/node\",\n        enclosingElementAsEvent=\"true\",\n        enable.streaming.xml.content=\"true\",\n        fail.on.missing.attribute=\"false\",\n        @attributes(id = \"/node/@id\", lat = \"/node/@lat\", lon = \"/node/@lon\", version = \"/node/@version\", timestamp = \"/node/@timestamp\", changeset = \"/node/@changeset\")))\ndefine stream FooStream (id string, lat string, lon string, version string, timestamp string, changeset string);\n\n\n@info(name = 'totalQuery')\nfrom FooStream#window.timeBatch(1 sec)\nselect count() as throughput\ninsert into OutputStream;\n\nfrom OutputStream#log('TPS: ')\ninsert into TempStream;\n</code></pre>"},{"location":"ref/performance-analysis-results/#results_7","title":"Results","text":"<ul> <li> <p>Memory consumption: 1.2g</p> </li> <li> <p>TPS: 40,000</p> </li> </ul>"},{"location":"ref/performance-analysis-results/#reading-an-xml-file-and-publishing-to-a-kafka-topic","title":"Reading an XML file and publishing to a Kafka topic","text":""},{"location":"ref/performance-analysis-results/#specifications-of-ec2-instances_8","title":"Specifications of EC2 Instances","text":""},{"location":"ref/performance-analysis-results/#docker-resource-allocation_4","title":"Docker resource allocation","text":"Memory  8GB  CPU  4 Cores"},{"location":"ref/performance-analysis-results/#server-memory-allocation_4","title":"Server memory allocation","text":"Xms  2g  Xmx  4g"},{"location":"ref/performance-analysis-results/#siddhi-applications_3","title":"Siddhi applications","text":"<pre><code>@App:name(\"NodesConvertor\")\n@App:description(\"Description of the plan\")\n\n@source(\n    type = 'file', \n    file.uri = \"file:/home/ubuntu/csv/input.xml\", \n    mode = \"line\",\n    tailing = \"false\", \n    action.after.process='keep',\n    @map(type='xml', \n        enclosing.element=\"/osm/node\",\n        enclosingElementAsEvent=\"true\",\n        enable.streaming.xml.content=\"true\",\n        fail.on.missing.attribute=\"false\",\n        @attributes(id = \"/node/@id\", lat = \"/node/@lat\", lon = \"/node/@lon\", version = \"/node/@version\", timestamp = \"/node/@timestamp\", changeset = \"/node/@changeset\")))\ndefine stream FooStream (id string, lat string, lon string, version string, timestamp string, changeset string);\n\n@sink(type='kafka',\n      topic='kafka_result_topic',\n      bootstrap.servers='172.31.3.169:9092',\n      partition.no='0',\n      @map(type='xml'))\ndefine stream kafkaStream(id string, lat string, lon string, version string, timestamp string, changeset string);\n\n\n@info(name = 'totalQuery')\nfrom FooStream#window.timeBatch(1 sec)\nselect count() as throughput\ninsert into OutputStream;\n\nfrom OutputStream#log('TPS: ')\ninsert into TempStream;\n\nfrom FooStream \nselect * \ninsert into kafkaStream;\n</code></pre>"},{"location":"ref/performance-analysis-results/#results_8","title":"Results","text":"<ul> <li> <p>Memory consumption: 1.7g</p> </li> <li> <p>TPS: 38,000</p> </li> </ul>"},{"location":"ref/permission-apis/","title":"Permission APIs","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/permission-apis/#permission-apis","title":"Permission APIs","text":""},{"location":"ref/permission-apis/#adding-a-permission-string","title":"Adding a permission string","text":""},{"location":"ref/permission-apis/#overview","title":"Overview","text":"Description Adds a new permission string. API Context <code>/permissions</code> HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/permission-apis/#curl-command-syntax","title":"curl command syntax","text":"<pre><code>curl -X POST https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/permissions/ -H 'content-type: application/json' -d ' { \"appName\":\"&lt;SIDDHI_APPLICATION_NAME&gt;\", \"permissionString\":\"&lt;PERMISSION_STRING&gt;\"}' -k\n</code></pre>"},{"location":"ref/permission-apis/#sample-curl-command","title":"Sample curl command","text":"<pre><code>curl -X POST https://localhost:9443/permissions/ -H 'content-type: application/json' -d ' { \"appName\":\"MON\", \"permissionString\":\"MON.manager\"}' -k\n</code></pre>"},{"location":"ref/permission-apis/#sample-output","title":"Sample output","text":"<p>Returns the permission ID for the particular permission string</p>"},{"location":"ref/permission-apis/#response","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/permission-apis/#getting-a-permission-id-for-a-permission-string","title":"Getting a permission ID for a permission string","text":""},{"location":"ref/permission-apis/#overview_1","title":"Overview","text":"Description Returns the permission ID for a given permission string. API Context <code>/permissions/app/{appName}</code> HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime server/tooling Parameter Description <code>{appName}</code>"},{"location":"ref/permission-apis/#curl-command-syntax_1","title":"curl command syntax","text":"<pre><code>curl -X GET https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/permissions/app/{appName}\n</code></pre>"},{"location":"ref/permission-apis/#sample-curl-command_1","title":"Sample curl command","text":"<pre><code>curl -X GET https://localhost:9443/permissions/app/MON\n</code></pre>"},{"location":"ref/permission-apis/#sample-output_1","title":"Sample output","text":"<pre><code>[{\"permissionID\": \"f0c74633-2f07-3896-841a-154afb0c29da\",\"permissionString\": \"MON.consumer\"}]\n</code></pre>"},{"location":"ref/permission-apis/#response_1","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/permission-apis/#checking-whether-a-specific-user-role-is-granted-a-specific-permission","title":"Checking whether a specific user role is granted a specific permission","text":""},{"location":"ref/permission-apis/#overview_2","title":"Overview","text":"Description Checks whether the specified user role is granted a specific permission. API Context <code>/permissions/auth/{permissionID}/{roleName}</code> HTTP Method GET Request/Response format Authentication Username Password Runtime server/tooling Parameter Description <code>{permissionID}</code> The ID of a specific permission. The API checks whether this permission is granted to the specified user role. <code>{roleName}</code> The ID of a specific user role. The API checks whether this user role is granted the specified permission ID."},{"location":"ref/permission-apis/#curl-command-syntax_2","title":"curl command syntax","text":"<pre><code>curl --location --request GET 'https://&lt;host&gt;:port/permissions/auth/&lt;permission-string-id&gt;/&lt;user&gt;'\n</code></pre>"},{"location":"ref/permission-apis/#sample-curl-command_2","title":"Sample curl command","text":"<pre><code>curl --location --request GET 'https://localhost:9443/permissions/auth/f0c74633-2f07-3896-841a-154afb0c29da/admin'\n</code></pre>"},{"location":"ref/permission-apis/#sample-output_2","title":"Sample output","text":"<pre><code>{ \"code\": 4, \"type\": \"ok\", \"message\": \"Checking permission for app:f0c74633-2f07-3896-841a-154afb0c29da role: admin successful\" }\n</code></pre>"},{"location":"ref/permission-apis/#response_2","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/permission-apis/#deleting-a-permission-string","title":"Deleting a permission string","text":""},{"location":"ref/permission-apis/#overview_3","title":"Overview","text":"Description Deletes the specified permission string. API Context <code>/permissions/{permissionID}</code> HTTP Method DELETE Request/Response format Authentication Username Password Runtime server/tooling Parameter Description <code>{permissionID}</code> The ID of the permission string to be deleted."},{"location":"ref/permission-apis/#curl-command-syntax_3","title":"curl command syntax","text":"<pre><code>curl -X DELETE https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/permissions/{permissionID}\n</code></pre>"},{"location":"ref/permission-apis/#sample-curl-command_3","title":"Sample curl command","text":"<pre><code>curl -X DELETE https://localhost:9443/permissions/e9687c6f-b5b2-3216-b3bd-82e7a8e14367\n</code></pre>"},{"location":"ref/permission-apis/#sample-output_3","title":"Sample output","text":"<pre><code>{ \n  \"code\": 4, \n  \"type\": \"ok\", \n  \"message\": \"Deleted permission with ID: f0c74633-2f07-3896-841a-154afb0c29da\" \n}\n</code></pre>"},{"location":"ref/permission-apis/#response_3","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/permission-apis/#listing-roles-with-a-specific-permission","title":"Listing roles with a specific permission","text":""},{"location":"ref/permission-apis/#overview_4","title":"Overview","text":"Description Lists the user roles that are currently granted the specified user role. API Context <code>/permissions/{permissionsID}/roles</code> HTTP Method GET Request/Response format Authentication Username Password Runtime server/tooling Parameter Description <code>{permissionID}</code> The ID of the permission for which the user roles need to be listed."},{"location":"ref/permission-apis/#curl-command-syntax_4","title":"curl command syntax","text":"<pre><code>curl -X GET https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/permissions/{permissionID}/roles\n</code></pre>"},{"location":"ref/permission-apis/#sample-curl-command_4","title":"Sample curl command","text":"<pre><code>curl -X GET https://localhost:9443/permissions/8dc31fec-8364-3082-9f88-c7ca7d979873/roles\n</code></pre>"},{"location":"ref/permission-apis/#sample-output_4","title":"Sample output","text":""},{"location":"ref/permission-apis/#response_4","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/permission-apis/#revoking-a-specific-permission-for-all-roles","title":"Revoking a specific permission for all roles","text":""},{"location":"ref/permission-apis/#overview_5","title":"Overview","text":"Description Revokes the specified permission for all the user roles. API Context <code>/permissions/revoke/{permissionID}</code> HTTP Method POST Request/Response format Authentication Username Password Runtime server/tooling Parameter Description <code>{permissionID}</code> The ID of the permission that needs to be revoked for all user roles."},{"location":"ref/permission-apis/#curl-command-syntax_5","title":"curl command syntax","text":"<pre><code>curl -X POST https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/permissions/revoke/{permissionID}\n</code></pre>"},{"location":"ref/permission-apis/#sample-curl-command_5","title":"Sample curl command","text":"<pre><code>curl -X POST https://localhost:9443/permissions/revoke/8dc31fec-8364-3082-9f88-c7ca7d979873\n</code></pre>"},{"location":"ref/permission-apis/#sample-output_5","title":"Sample output","text":"<pre><code>{ \"code\": 4, \"type\": \"ok\", \"message\": \"Permission revoke for permissionID e9687c6f-b5b2-3216-b3bd-82e7a8e14367 success.\" }\n</code></pre>"},{"location":"ref/permission-apis/#response_5","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/permission-apis/#revoking-a-specific-permission-for-a-specific-role","title":"Revoking a specific permission for a specific role","text":""},{"location":"ref/permission-apis/#overview_6","title":"Overview","text":"Description Grants or revokes a permission for the specified user role. The permission is passed as an array in the body of the request. API Context <code>/permissions/roles/{roleID}?action=revoke/grant</code> HTTP Method POST Request/Response format <code>application/json</code> Authentication Username Password Runtime server/tooling Parameter Description <code>{roleID}</code> The ID of the user role for which the permission given in the request body needs to be granted or revoked."},{"location":"ref/permission-apis/#curl-command-syntax_6","title":"curl command syntax","text":"<pre><code>curl -X POST 'https://&lt;host&gt;:&lt;port&gt;/permissions/roles/&lt;user&gt;?action=revoke' -H 'content-type: application/json' -d ' { \"appName\":\"&lt;Siddhi-app-name&gt;\", \"permissionString\":\"&lt;permission-string&gt;\"}' -k\n</code></pre>"},{"location":"ref/permission-apis/#sample-curl-command_6","title":"Sample curl command","text":"<pre><code>curl -X POST 'https://localhost:9444/permissions/roles/admin?action=revoke' -H 'content-type: application/json' -d ' { \"appName\":\"MON\", \"permissionString\":\"MON.consumer\"}' -k\n</code></pre>"},{"location":"ref/permission-apis/#sample-output_6","title":"Sample output","text":"<pre><code>{\n\"code\":4,\n\"type\":\"ok\",\n\"message\":\"Action, revoke for permission, Permission[appName\\u003dMON, permissionString\\u003dMON.consumer] successful.\"\n}\n</code></pre>"},{"location":"ref/permission-apis/#response_6","title":"Response","text":"HTTP Status Code <p>Possible codes are 200 and 404.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/public-APIs/","title":"public APIs","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/public-APIs/#public-apis","title":"Public APIs","text":"<p>The following topics list the APIs supported for WSO2 Stream processor from its Worker, Manager, Editor and Dashboard runtimes.</p> <ul> <li>Siddhi Application Management     APIs</li> <li>Event Simulation APIs</li> <li>Status Monitoring APIs</li> <li>Dashboard APIs</li> <li>Authentication APIs</li> <li>Permission APIs</li> <li>Business Rules APIs</li> <li>Store APIs</li> <li>Healthcheck APIs</li> </ul>"},{"location":"ref/rest-api-guide-overview/","title":"REST API Guide Overview","text":"<p>The following REST API categories are supported for WSO2 Streaming Integrator:</p> <ul> <li> <p>Siddhi Application Management APIs</p> <p>The supported APIs allow you to manage Siddhi applications by creating, updating and deleting them as required. You can also fetch a Siddhi applications, enable statistics for them, retrieve them take snapshots of them etc.</p> </li> <li> <p>Permission APIs</p> <p>The supported APIs allow you to add permission strings, getting a permission ID for a given string and checking whether a specific user role is granted a specific permission.</p> </li> <li> <p>Business Rules APIs</p> <p>The supported APIs allow you to manage business rules by creating, updating, and deleting them as required and to fetch information relating to both business rules and business rules templates.</p> </li> <li> <p>Store APIs</p> <p>This category includes a single API that allows you to perform CRUD operations for your data stores.</p> </li> <li> <p>Healthcheck APIs</p> <p>This category allows you to retrieve information relating to the health of your streaming integrator deployment.</p> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/","title":"Siddhi Application Management APIs","text":""},{"location":"ref/siddhi-Application-Management-APIs/#creating-a-siddhi-application","title":"Creating a Siddhi application","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview","title":"Overview","text":"Description Creates a new Siddhi Application. API Context <code>/siddhi-apps</code> HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax","title":"curl command syntax","text":"<pre><code>curl -X POST \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @{appName} -u &lt;SI_USERNAME&gt;:&lt;SI_PASSWORD&gt; -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command","title":"Sample curl command","text":"<pre><code>curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output","title":"Sample output","text":"<p>The response for the sample curl command given above can be one of the following.</p> <ul> <li> <p>If API request is valid and there is no existing Siddhi application     with the given name, a response similar to the following is     generated with response code 201. This response contains a location     header with\u00a0the path of the newly created file from product root     home.</p> <pre><code>{\n\"type\":\"success\",\n\"message\":\"Siddhi App saved succesfully and will be deployed in next deployment cycle\"\n}\n</code></pre> </li> <li> <p>If the API request is valid, but a Siddhi application with the given     name already exists,\u00a0\u00a0a response similar to the following is     generated with response code 409.</p> <pre><code>{\n\"type\": \"conflict\",\n\"message\": \"There is a Siddhi App already exists with same name\" }\n</code></pre> </li> <li> <p>If the API request is invalid due to invalid content in the Siddhi     queries you have included in the request body, \u00a0a response similar     to the following is generated\u00a0is generated with response code 400.</p> <pre><code>{\n\"code\": 800101,\n\"type\": \"validation error\",\n\"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" }\n</code></pre> </li> <li> <p>If the API request is valid, but an exception occurred during file     processing or saving, the following response is generated with     response code 500.</p> <pre><code>{\n\"code\": 800102,\n\"type\": \"file processing error\",\n\"message\": &lt;error-message&gt;\n}\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response","title":"Response","text":"HTTP Status Code <p>Possible codes are 201, 409, 400, and 500.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#updating-a-siddhi-application","title":"Updating a Siddhi Application","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_1","title":"Overview","text":"Description Updates a Siddhi Application. API Context <code>/siddhi-apps</code> HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_1","title":"curl command syntax","text":"<pre><code>curl -X PUT \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @{appName}.siddhi -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_1","title":"Sample curl command","text":"<pre><code>curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_1","title":"Sample output","text":"<ul> <li> <p>If the API request is valid and the specified Siddhi application is successfully updated, the following response is returned with response code 200.</p> <pre><code>{\n  \"type\":\"success\",\n  \"message\":\"Siddhi App updated succesfully and will be deployed in next deployment cycle\"}\n</code></pre> </li> <li> <p>If the API request is invalid due to invalid content in the Siddhi     query, a response similar to the following is returned with response     code 400.</p> <pre><code>{\n\"code\": 800101,\n\"type\": \"validation error\",\n\"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" }\n</code></pre> </li> <li> <p>If the API request is valid, but an exception occurred when saving or     processing files,\u00a0a response similar to the following\u00a0is returned     with response code 500.</p> <pre><code>{\n\"code\": 800102,\n\"type\": \"file processing error\",\n\"message\": &lt;error-message&gt;\n}\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_1","title":"Response","text":"HTTP Status Code <p>Possible codes are 200, 201, 400, and 500.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#deleting-a-siddhi-application","title":"Deleting a Siddhi application","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_2","title":"Overview","text":"Description Delets an existing Siddhi application. API Context <code>/siddhi-apps/{appName}</code> HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description","title":"Parameter Description","text":"Parameter Description <code>{appName}</code> The name of the Siddhi application to be deleted."},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_2","title":"curl command syntax","text":"<pre><code>curl -X DELETE \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_2","title":"Sample curl command","text":"<pre><code>curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_2","title":"Sample output","text":"<p>The response for the sample curl command given above can be one of the following:</p> <ul> <li> <p>If the API request is valid and a Siddhi application with the given     name exists, an empty response is received with response     code 200.</p> <pre><code>http://localhost:9090/siddhi-apps/TestExecutionPlan1\n</code></pre> </li> <li> <p>If the API request is valid, but\u00a0a Siddhi application with the given     name is not deployed, the following response is received with     response code 404.</p> <pre><code>{\n\"type\": \"not found\",\n\"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }\n</code></pre> </li> <li> <p>If the API request is valid, but an exception occurred when deleting     the given Siddhi application,\u00a0the following response is received     with response code 500.</p> <pre><code>{\n\"code\": 800102,\n\"type\": \"file processing error\",\n\"message\": &lt;error-message&gt;\n}\n</code></pre> </li> <li> <p>If the API request is valid, but there are restricted characters in     the given Siddhi application name, the\u00a0following response is     received with response code 400.</p> <pre><code>{\n\"code\": 800101,\n\"type\": \"validation error\",\n\"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" }\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_2","title":"Response","text":"HTTP Status Code <p>200, 404, 500 or 400.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#listing-all-active-siddhi-applications","title":"Listing all active Siddhi applications","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_3","title":"Overview","text":"Description <p>Lists all the currently active Siddhi applications.</p> <p>If the <code>isActive=true</code> parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed.</p> API Context <code>/siddhi-apps</code> HTTP Method GET Request/Response format Request content type : any Response content type : <code>application/json</code> Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_3","title":"curl command syntax","text":"<pre><code>curl -X GET \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_3","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_3","title":"Sample output","text":"<p>Possible responses are as follows:</p> <ul> <li> <p>If the API request is valid and there are Siddhi applications deployed in your WSO2 Streaming Integrator setup, a response similar to the following is returned with response code 200.</p> <pre><code>[\"TestExecutionPlan3\", \"TestExecutionPlan4\"]\n</code></pre> </li> <li> <p>If the API request is valid,\u00a0there are Siddhi applications deployed in your Streaming Integrator setup, and a query parameter is defined in the request,\u00a0a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active.</p> <p>Info</p> <p>If these conditions are met, but the <code>isActive</code> parameter is set to <code>false</code>, the response contains only inactive Siddhi applications.</p> <pre><code>[\"TestExecutionPlan3\"]\n</code></pre> </li> <li> <p>If the API request is valid, but there are no Siddhi applications deployed in your Streaming Integrator setup, the following response is returned.</p> <pre><code>[]\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_3","title":"Response","text":"HTTP Status Code <p>200</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#retrieving-a-specific-siddhi-application","title":"Retrieving a specific Siddhi application","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_4","title":"Overview","text":"Description Retrieves the given Siddhi application. API Context <code>/siddhi-apps/{appName}</code> HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_1","title":"Parameter Description","text":"Parameter Description <code>{appName}</code> The name of the Siddhi application to be retrieved."},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_4","title":"curl command syntax","text":"<pre><code>curl -X GET \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_4","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_4","title":"Sample output","text":"<ul> <li> <p>If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200.</p> <pre><code>{\n\"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" }\n</code></pre> </li> <li> <p>If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404.</p> <pre><code>{\n\"type\": \"not found\",\n\"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_4","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#fetching-the-status-of-a-siddhi-application","title":"Fetching the status of a Siddhi Application","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_5","title":"Overview","text":"Description This fetches the status of the specified Siddhi application. API Context <code>/siddhi-apps/{appName}/status</code> HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime server/runtime"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_2","title":"Parameter Description","text":"Parameter Description <code>{appName}</code> The name of the Siddhi application of which the status needs to be fetched."},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_5","title":"curl command syntax","text":"<pre><code>curl -X GET \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_5","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_5","title":"Sample output","text":"<ul> <li> <p>If the Siddhi application is active, the following is returned with response code 200.</p> <pre><code>{\"status\":\"active\"} </code></pre> </li> <li> <p>If the Siddhi application is inactive, the following is returned with response code 200.</p> <pre><code>{\"status\":\"inactive\"} </code></pre> </li> <li> <p>If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404.</p> <pre><code>{\n\"type\": \"not found\",\n\"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_5","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#taking-a-snapshot-of-a-siddhi-application","title":"Taking a snapshot of a Siddhi Application","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_6","title":"Overview","text":"Description This takes a snapshot of the specific Siddhi application. API Context <code>/siddhi-apps/{appName}/backup </code> HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime server/runtime"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_3","title":"Parameter Description","text":"Parameter Description <code>{appName}</code> The name of the Siddhi application of which a snapshot needs to be taken."},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_6","title":"curl command syntax","text":"<pre><code>curl -X POST \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_6","title":"Sample curl command","text":"<pre><code>curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_6","title":"Sample output","text":"<p>The output can be one of the following:</p> <ul> <li> <p>If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201.</p> <pre><code>{\"revision\": \"89489242494242\"} </code></pre> </li> <li> <p>If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404.</p> <pre><code>{\n\"type\": \"not found\",\n\"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }\n</code></pre> </li> <li> <p>If the API request is valid, but an exception has occurred when backing up the state at Siddhi level,\u00a0an output similar to the following is returned with response code 500.</p> <pre><code>        {\n\"code\": 800102,\n\"type\": \"file processing error\",\n\"message\": &lt;error-message&gt;\n}\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_6","title":"Response","text":"HTTP Status Code <p>201, 404, or 500.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#restoring-a-siddhi-application-via-a-snapshot","title":"Restoring a\u00a0Siddhi Application via a snapshot","text":"<p>Info</p> <p>In order to call this API, you need to have already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshot is taken, see Taking a snapshot of a Siddhi application.</p>"},{"location":"ref/siddhi-Application-Management-APIs/#overview_7","title":"Overview","text":"<p>Description</p> <p>This restores a Siddhi application using a previously taken snapshot of the same Siddhi Application.</p> <p>API Context</p> <ul> <li>To restore without considering the version : <code>/siddhi-apps/{appName}/restore</code></li> <li>To restore a specific version : <code>/siddhi-apps/{appName}/restore?version=</code></li> </ul> <p>HTTP Method</p> POST <p>Request/Response format</p> application/json Authentication Basic Username admin Password admin <p>Runtime</p> server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_4","title":"Parameter Description","text":"Parameter Description <code>{appName}</code> The name of the Siddhi application that needs to be restored."},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_7","title":"curl command syntax","text":"<pre><code>curl -X POST \"http://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_7","title":"Sample curl command","text":"<pre><code>curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_7","title":"Sample output","text":"<p>The above sample curl command can generate either one of the following responses:</p> <ul> <li> <p>If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200.</p> <pre><code>{\n\"type\": \"success\",\n\"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" }\n</code></pre> </li> <li> <p>If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system.</p> <pre><code>{\n\"type\": \"success\",\n\"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" }\n</code></pre> </li> <li> <p>If the API request is valid, but no Siddhi application is deployed with the given name,\u00a0the following response is returned with response code 404.</p> <pre><code>{\n\"type\": \"not found\",\n\"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }\n</code></pre> </li> <li> <p>If the API request is valid, but an exception occurred when restoring the state at Siddhi level,\u00a0the following response is returned with response code 500.</p> <pre><code>{\n\"code\": 800102,\n\"type\": \"file processing error\",\n\"message\": &lt;error-message&gt;\n}\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_7","title":"Response","text":"HTTP Status Code <p>200, 404 or 500.</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#returning-real-time-statistics-of-a-streaming-integrator-node","title":"Returning real-time statistics of a Streaming Integrator node","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_8","title":"Overview","text":"Description Returns the real-time statistics of a Streaming Integrator node. <p>API Context</p> <code>/statistics</code> HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin <p>Runtime</p> server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_8","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://&lt;HOST_NAME&gt;&gt;:&lt;PORT&gt;/statistics\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_8","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_8","title":"Sample output","text":"<pre><code>{\n  \"workerMetrics\": {\n    \"processCPU\": 0.0,\n    \"systemCPU\": 0.0,\n    \"loadAverage\": 0.0,\n    \"memoryUsage\": 0.0\n  },\n  \"runningStatus\": \"Reachable\",\n  \"isStatsEnabled\": false,\n  \"clusterID\": \"Single Node Deployments\",\n  \"lastSyncTime\": \"n/a\",\n  \"lastSnapshotTime\": \"Thu, 01 Jan 1970 05:30:00 IST\",\n  \"isInSync\": false,\n  \"message\": \"Metrics are disabled.\"\n}\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#response_8","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-node-statistics","title":"Enabling/disabling node statistics","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_9","title":"Overview","text":"Description Enables/disables the real-time statistics of a Streaming Integrator node. <p>API Context</p> <code>/statistics</code> HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin <p>Runtime</p> server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_9","title":"curl command syntax","text":"<pre><code>curl -X PUT \"https://&lt;HOST_NAME&gt;:&lt;PORT/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"statsEnable\\\":\\\"true\\\"}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_9","title":"Sample curl command","text":"<pre><code>curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"statsEnable\\\":\\\"true\\\"}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_9","title":"Sample output","text":"<ul> <li> <p>If the statistics are successfully enabled, the following response is returned.</p> <p><pre><code>  {\n    \"code\":4,\n    \"type\":\"ok\",\n    \"message\":\"Successfully enabled the metrics.\"\n  }  \n</code></pre> - If the statistics are already enabled, the following response is returned.</p> <pre><code>  {\n    \"code\":4,\n    \"type\":\"ok\",\n    \"message\":\"Metrics are enabled already.\"\n  }\n</code></pre> </li> <li> <p>If the statistics are successfully disabled, the following response is returned.</p> <pre><code>  {\n    \"code\":4,\n    \"type\":\"ok\",\n    \"message\":\"Successfully disabled the metrics.\"\n  }\n</code></pre> </li> <li> <p>If the statistics are already disabled, the following response is returned.</p> <pre><code>  {\n    \"code\":4,\n    \"type\":\"ok\",\n    \"message\":\"Metrics are disabled already.\"\n  }\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_9","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#returning-general-details-of-a-streaming-integrator-node","title":"Returning general details of a Streaming Integrator node","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_10","title":"Overview","text":"Description Returns general details of a Streaming Integrator node. API Context <code>/system-details</code> HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin <p>Runtime</p> server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_10","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/system-details\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_10","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_10","title":"Sample output","text":"<pre><code>{\n  \"carbonId\": \"wso2-si\",\n  \"javaRuntimeName\": \"Java(TM) SE Runtime Environment\",\n  \"javaVMVersion\": \"25.152-b16\",\n  \"javaVMVendor\": \"Oracle Corporation\",\n  \"javaHome\": \"/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/jre\",\n  \"javaVersion\": \"1.8.0_152\",\n  \"osName\": \"Mac OS X\",\n  \"osVersion\": \"10.14.5\",\n  \"userHome\": \"/Users/wso2user\",\n  \"userTimezone\": \"Asia/Colombo\",\n  \"userName\": \"wso2user\",\n  \"userCountry\": \"LK\",\n  \"repoLocation\": \"/Users/wso2user/wso2si/deployment\",\n  \"serverStartTime\": \"Thu, 06 Aug 2020 15:45:51 IST\"\n}\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#response_10","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#returning-detailed-statistics-of-all-siddhi-applications","title":"Returning detailed statistics of all Siddhi applications","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_11","title":"Overview","text":"Description Returns the detailed statistics of all the Siddhi applications currently deployed in the Streaming Integrator setup. API Context <code>/siddhi-apps/statistics </code> HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin <p>Runtime</p> server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_11","title":"curl command syntax","text":"<pre><code>curl -X GET \"https://&lt;HOST_NAME&gt;&gt;:&lt;PORT&gt;/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_11","title":"Sample curl command","text":"<pre><code>curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_11","title":"Sample output","text":"<ul> <li> <p>If Siddhi applications exist, a response similar to the following is returned with details of each Siddhi application. This response is returned with the 200 code.</p> <pre><code>  [\n    {\n      \"appName\": \"TestExecutionPlan1\",\n      \"status\": \"active\",\n      \"age\": 89981329,\n      \"isStatEnabled\": \"DETAIL\",\n      \"siddhiStatEnabledLevel\": { \"name\": \"DETAIL\", \"intLevel\": 500 }\n    },\n    {\n      \"appName\": \"TestExecutionPlan2\",\n      \"status\": \"active\",\n      \"age\": 89981639,\n      \"isStatEnabled\": \"DETAIL\",\n      \"siddhiStatEnabledLevel\": { \"name\": \"DETAIL\", \"intLevel\": 500 }\n    },\n    {\n      \"appName\": \"TestExecutionPlan3\",\n      \"status\": \"active\",\n      \"age\": 87053026,\n      \"isStatEnabled\": \"DETAIL\",\n      \"siddhiStatEnabledLevel\": { \"name\": \"DETAIL\", \"intLevel\": 500 }\n    }\n  ]\n</code></pre> </li> <li> <p>If Siddhi applications do not exist, the following response is returned with the 404 response code.</p> <pre><code>  {\n    \"type\":\"not-found\",\n    \"message\":\"There are no any Siddhi App exist.\"\n  }\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_11","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-the-statistics-of-a-specific-siddhi-application","title":"Enabling/disabling the statistics of a specific Siddhi application","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_12","title":"Overview","text":"Description Enables/disables statistics for a specified Siddhi application. API Context <code>/siddhi-apps/{appName}/statistics</code> HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin <p>Runtime</p> server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_5","title":"Parameter Description","text":"Parameter Description <code>appName</code> The name of the Siddhi application for which the statistics need to be enabled/disabled."},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_12","title":"curl command syntax","text":"<pre><code>curl -X PUT \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps/{appName}/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"statsEnable\\\": \\\"true\\\"}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_12","title":"Sample curl command","text":"<pre><code>curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"statsEnable\\\": \\\"true\\\"}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_12","title":"Sample output","text":"<ul> <li> <p>If statistics are successfully enabled, the following response is returned.</p> <pre><code>  {\n    \"type\":\"success\",\n    \"message\":\"Sucessfully updated Aiddhi App : TestSiddhiApp\"\n  }\n</code></pre> </li> <li> <p>If statistics are already enabled, the following response is returned.</p> <pre><code>  {\"type\":\"success\",\"message\":\"Stats level is already set to :class StatsEnable {\\n    enabledStatLevel: DETAIL\\n} for siddhi appTestSiddhiApp\"}   \n</code></pre> </li> <li> <p>If statistics are successfully disabled, the following response is returned.</p> <pre><code>  {\n    \"type\":\"success\",\n    \"message\":\"Sucessfully updated Aiddhi App : TestSiddhiApp\"\n  }\n</code></pre> </li> <li> <p>If statistics are already disabled, the following response is returned.</p> <pre><code>  {\"type\":\"success\",\"message\":\"Stats level is already set to :class StatsEnable {\\n    enabledStatLevel: OFF\\n} for siddhi appTestSiddhiApp\"}\n</code></pre> </li> <li> <p>If no Siddhi application of the specified name exists, the following response is returned with the 404 code.</p> <pre><code>  {\n    \"type\":\"not-found\",\n    \"message\":\"There is no Siddhi App exist with provided name : TestSiddhiApp\"\n  }\n</code></pre> </li> </ul>"},{"location":"ref/siddhi-Application-Management-APIs/#response_12","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-the-statistics-of-all-siddhi-applications","title":"Enabling/disabling the statistics of all Siddhi applications","text":""},{"location":"ref/siddhi-Application-Management-APIs/#overview_13","title":"Overview","text":"Description Enables/disables statistics for all the Siddhi applications. API Context <code>/siddhi-apps/statistics </code> HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin <p>Runtime</p> server/tooling"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_13","title":"curl command syntax","text":"<pre><code>curl -X PUT \"https://&lt;HOST_NAME&gt;:&lt;PORT&gt;/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"statsEnable\\\": \\\"true\\\"}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_13","title":"Sample curl command","text":"<pre><code>curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"statsEnable\\\": \\\"true\\\"}\" -u admin:admin -k\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_13","title":"Sample output","text":"<pre><code>{\n  \"type\":\"success\",\n  \"message\":\"All siddhi apps Sucessfully updated.\"\n}\n</code></pre>"},{"location":"ref/siddhi-Application-Management-APIs/#response_13","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/status-monitoring-apis/","title":"Status monitoring apis","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/store-APIs/","title":"Store APIs","text":""},{"location":"ref/store-APIs/#query-records-in-siddhi-store","title":"Query records in Siddhi store","text":""},{"location":"ref/store-APIs/#overview","title":"Overview","text":"Description Queries records in the Siddhi store. For more information, see [Integrating Stores - Managing Stored Data via REST API](../Guides/storage-Integration.md#performing-crud-operations-via-rest-api). API Context <code>/stores/query</code> HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime server/tooling"},{"location":"ref/store-APIs/#curl-command-syntax","title":"curl command syntax","text":"<pre><code>curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k\n</code></pre>"},{"location":"ref/store-APIs/#sample-curl-command","title":"Sample curl command","text":"<pre><code>curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"ApiRequestSummary\", \"query\" : \"from API_REQUEST_SUMMARY within 1586249325000L, 1586335725000L per \\\"days\\\" select userId, apiPublisher, sum(totalRequestCount) as net_total_requests group by userId, apiPublisher order by net_total_requests DESC;\" }' -k\n</code></pre>"},{"location":"ref/store-APIs/#sample-output","title":"Sample output","text":"<pre><code>{\"records\":[[\"admin\",\"admin\",66]]}\n</code></pre>"},{"location":"ref/store-APIs/#response","title":"Response","text":"HTTP Status Code <p>200 or 404</p> <p>For descriptions of the HTTP status codes, see HTTP Status Codes .</p>"},{"location":"ref/stream-Processor-REST-API-Guide/","title":"stream Processor REST API Guide","text":"<p>Note</p> <p>This page is still a work in progress!</p>"},{"location":"ref/stream-Processor-REST-API-Guide/#stream-processor-rest-api-guide","title":"Stream Processor REST API Guide","text":"<p>The following topics cover information relating to the public APIs exposed from WSO2 Stream Processor.</p> <ul> <li>Public APIs</li> <li>HTTP Status Codes</li> </ul>"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/","title":"Worker Runtime - REST APIs Permission Model","text":"<p>There are two sets of REST APIs available in worker runtime. Stream Processor APIs and Event Simulator APIs have following permission model. You need to have appropriate permission to invoke these APIS.</p>"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#stream-processor-apis","title":"Stream Processor\u00a0 APIs","text":"Method API Context Required Permission POST /siddhi-apps <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p> PUT /siddhi-apps <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p> DELETE /siddhi-apps/{appName} <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p> GET /siddhi-apps <p>PermissionString - siddhiApp.manage or siddhiApp.view</p> <p>AppName - SAPP</p> GET /siddhi-apps/{appName} <p>PermissionString - siddhiApp.manage or siddhiApp.view</p> <p>AppName - SAPP</p> GET /siddhi-apps/{appName}/status <p>PermissionString - siddhiApp.manage or siddhiApp.view</p> <p>AppName - SAPP</p> POST /siddhi-apps/{appName}/backup <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p> POST <p>/siddhi-apps/{appName}/restore</p> <p>/siddhi-apps/{appName}/restore?version=</p> <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p> GET /statistics <p>PermissionString - siddhiApp.manage or siddhiApp.view</p> <p>AppName - SAPP</p> PUT /statistics <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p> GET /system-details <p>PermissionString - siddhiApp.manage or siddhiApp.view</p> <p>AppName - SAPP</p> GET /siddhi-apps/statistics <p>PermissionString - siddhiApp.manage or siddhiApp.view</p> <p>AppName - SAPP</p> PUT /siddhi-apps/{appName}/statistics <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p> PUT /siddhi-apps/statistics <p>PermissionString - siddhiApp.manage</p> <p>AppName - SAPP</p>"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#event-simulator-apis","title":"Event Simulator APIs","text":"Method API Context Required Permission POST /simulation/single <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/feed <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> GET /simulation/feed <p>PermissionString - simulator.manage or simulator.view</p> <p>AppName - SIM</p> PUT /simulation/feed/{simulationName} <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> GET /simulation/feed/{simulationName} <p>PermissionString - simulator.manage or simulator.view</p> <p>AppName - SIM</p> DELETE /simulation/feed/{simulationName} <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/feed/{simulationName}?action=run <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/feed/{simulationName}?action=pause <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/feed/{simulationName}?action=resume <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/feed/{simulationName}?action=stop <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/feed/{simulationName}?action=resume <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> GET /simulation/feed/{simulationName}/status <p>PermissionString - simulator.manage or simulator.view</p> <p>AppName - SIM</p> POST /simulation/files <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> GET /simulation/files <p>PermissionString - simulator.manage or simulator.view</p> <p>AppName - SIM</p> PUT /simulation/files/{fileName} <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> DELETE /simulation/files/{fileName} <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/ connectToDatabase <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/ connectToDatabase / retrieveTableNames <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> POST /simulation/ connectToDatabase/{tableName}/ retrieveColumnNames <p>PermissionString - simulator.manage</p> <p>AppName - SIM</p> <pre><code>    simulator\n</code></pre>"},{"location":"ref/working-with-data-providers/","title":"Working with Data Providers","text":"<p>Data providers are the sources from which information is fetched to be displayed in widgets. This section describes how to configure the data providers that are currently supported in WSO2 Stream Processor are as follows. These configurations determine the parameters that are available to be configured for each data provider when creating widgets via the Widget Generation wizard. For more information, see Generating Widgets .</p> <ul> <li>RDBMS Batch Data     Provider</li> <li>RDBMS Streaming Data     Provider</li> <li>Siddhi Store Data     Provider</li> <li>Web Socket Provider</li> </ul>"},{"location":"ref/working-with-data-providers/#rdbms-batch-data-provider","title":"RDBMS Batch Data Provider","text":"<p>This data provider queries static tables. The following configuration is an example of an RDBMS Batch Data Provider:</p> <pre><code>    \"config\": {\n\"datasourceName\": \"Twitter_Analytics\",\n\"queryData\": {\n\"query\": \"select type as Sentiment, count(TweetID) as Rate from sentiment where PARSEDATETIME(timestamp, 'yyyy-mm-dd hh:mm:ss','en') &gt; CURRENT_TIMESTAMP()-86400 group by type\"\n},\n\"tableName\": \"sentiment\",\n\"incrementalColumn\": \"Sentiment\",\n\"publishingInterval\": 60\n}\n</code></pre>"},{"location":"ref/working-with-data-providers/#rdbms-streaming-data-provider","title":"RDBMS Streaming Data Provider","text":"<p>This data provider queries dynamic tables. Here, the newer records are published as soon as the table is updated.\u00a0The following configuration is an example of an RDBMS Streaming Data Provider:</p> <pre><code>    \"configs\": {\n\"type\": \"RDBMSStreamingDataProvider\",\n\"config\": {\n\"datasourceName\": \"Twitter_Analytics\",\n\"queryData\": {\n\"query\": \"select id,TweetID from sentiment\"\n},\n\"tableName\": \"sentiment\",\n\"incrementalColumn\": \"id\",\n\"publishingInterval\": 5,\n\"publishingLimit\": 5,\n\"purgingInterval\": 6,\n\"purgingLimit\": 6,\n\"isPurgingEnable\": false\n}\n}\n</code></pre>"},{"location":"ref/working-with-data-providers/#siddhi-store-data-provider","title":"Siddhi Store Data Provider","text":"<p>This data provider runs a siddhi store query. The following configuration is an example of a Siddhi Store Data Provider:  </p> <p>Info</p> <p>The Siddhi application included in this query must not contain any source configurations.</p> <pre><code>    \"configs\":{\n\"type\":\"SiddhiStoreDataProvider\",\n\"config\":{\n\"siddhiApp\":\"@App:name(\\\"HTTPAnalytics\\\") define stream ProcessedRequestsStream(timestamp long, serverName string, serviceName string, serviceMethod string, responseTime double, httpRespGroup string, userAgent string, requestIP string); define aggregation RequestAggregation from ProcessedRequestsStream select serverName, serviceName, serviceMethod, httpRespGroup, count() as numRequests, avg(responseTime) as avgRespTime group by serverName, serviceName, serviceMethod, httpRespGroup aggregate by timestamp every sec...year;\",\n\"queryData\":{\n\"query\":\"from RequestAggregation within \\\"2018-**-** **:**:**\\\" per \\\"days\\\" select AGG_TIMESTAMP, serverName, avg(avgRespTime) as avgRespTime\"\n},\n\"publishingInterval\":60,\n\"timeColumns\": \"AGG_TIMESTAMP\"\n}\n}\n</code></pre>"},{"location":"ref/working-with-data-providers/#web-socket-provider","title":"Web Socket Provider","text":"<p>This data provider\u00a0utilizes web siddhi-io-web socket sink to provide data to the clients. It\u00a0creates endpoints as follows for the web socket sinks to connect and publish information.</p> <pre><code>      wss://host:port/websocket-provider/{topic}\n</code></pre> <p>Info</p> <p>The host and port will be the host and port of the Portal Web application</p> <p>The following configuration is an example of a web socket data provider.</p> <pre><code>    {\nconfigs: {\ntype: 'WebSocketProvider', config: {\nsubscriberTopic: 'sampleStream', mapType: 'json' }\n}\n}\n</code></pre>"},{"location":"samples/AggregateDataIncrementally/","title":"Aggregating Data Incrementally","text":""},{"location":"samples/AggregateDataIncrementally/#purpose","title":"Purpose","text":"<p>This example demonstrates how to get running statistics using Siddhi. The sample Siddhi application aggregates the data relating to the raw material purchases of a sweet production factory.</p> <p>Before you begin:</p> <ol> <li>Install MySQL.</li> <li>Add the MySQL JDBC driver to your Streaming Integrator library as follows:<ol> <li>Download the JDBC driver from the MySQL site. </li> <li>Extract the MySQL JDBC Driver zip file you downloaded. Then use the <code>jarbundle</code> tool in the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory to convert the jars in it into OSGi bundles. To do this, issue one of the following commands:<ul> <li>For Windows: <code>&lt;SI_TOOLING_HOME&gt;/bin/jartobundle.bat &lt;PATH_OF_DOWNLOADED_JAR&gt; &lt;PATH_OF_CONVERTED_JAR&gt;</code></li> <li>For Linux: <code>&lt;SI_TOOLING_HOME&gt;/bin/jartobundle.sh &lt;PATH_OF_DOWNLOADED_JAR&gt; &lt;PATH_OF_CONVERTED_JAR</code></li> </ul> </li> <li>Copy the converted bundles to the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</li> </ol> </li> <li>Create a data store named <code>sweetFactoryDB</code> in MySQL with relevant access privileges.</li> <li>Replace the values for the <code>jdbc.url</code>, <code>username</code>, and <code>password</code> parameters in the sample.     e.g., <ul> <li><code>jdbc.url</code> - <code>jdbc:mysql://localhost:3306/sweetFactoryDB</code></li> <li><code>username</code> - <code>root</code></li> <li><code>password</code> - <code>root</code></li> </ul> </li> <li>In the Streaming Integrator Tooling, save the sample Siddhi application.</li> </ol>"},{"location":"samples/AggregateDataIncrementally/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample Siddhi application, open it in Streaming Integrator Tooling and click the Start button (shown below) or click Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>AggregateDataIncrementally.siddhi - Started Successfully!.</code></p>"},{"location":"samples/AggregateDataIncrementally/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, simulate single events for it via the Streaming Integrator Tooling as follows:</p> <ol> <li> <p>To open the Event Simulator, click the Event Simulator icon.</p> <p></p> <p>This opens the event simulation panel.</p> </li> <li> <p>To simulate events for the <code>RawMaterialStream</code> stream of the <code>AggregateDataIncrementally</code>  Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows.</p> Field Value Siddhi App Name <code>AggregateDataIncrementally</code> StreamName <code>RawMaterialStream</code> <p></p> <p>As a result, the attributes of the <code>RawMaterialStream</code> stream appear as marked in the image above.</p> </li> <li> <p>Send four events by entering values as shown below. Click Send after each event.</p> <ul> <li> <p>Event 1</p> <ul> <li> <p>name: <code>chocolate cake</code></p> </li> <li> <p>amount: 100</p> </li> </ul> </li> <li> <p>Event 2</p> <ul> <li> <p>name: <code>chocolate cake</code></p> </li> <li> <p>amount: 200</p> </li> </ul> </li> <li> <p>Event 3</p> <ul> <li> <p>name: <code>chocolate ice cream</code></p> </li> <li> <p>amount: `50</p> </li> </ul> </li> <li> <p>Event 4</p> <ul> <li> <p>name: <code>chocolate ice cream</code></p> </li> <li> <p>amount: <code>150</code></p> </li> </ul> </li> </ul> </li> <li> <p>In the StreamName field, select TriggerStream*. In the triggerId field, enter <code>1</code> as the trigger ID, and then click Send.</p> </li> </ol>"},{"location":"samples/AggregateDataIncrementally/#viewing-the-results","title":"Viewing the Results:","text":"<p>The input and the corresponding output is displayed in the console as follows.</p> <p>Info</p> <p>The timestamp displayed is different because it is derived based on the time at which you send the events.</p> <pre><code>    INFO {io.siddhi.core.stream.output.sink.LogSink} - AggregateDataIncrementally : RawMaterialStatStream : [Event{timestamp=1513612116450, data=[1537862400000, chocolate ice cream, 100.0], isExpired=false}, Event{timestamp=1513612116450, data=[chocolate cake, 150.0], isExpired=false}]\n    [INFO {io.siddhi.core.stream.output.sink.LogSink} - AggregateDataIncrementally : RawMaterialStatStream : [Event{timestamp=1513612116450, data=[1537862400000, chocolate ice cream, 100.0], isExpired=false}, Event{timestamp=1513612116450, data=[chocolate cake, 150.0], isExpired=false}]\n</code></pre> Click here to view the sample Siddhi application.<pre><code>@App:name(\"AggregateDataIncrementally\")\n@App:description('Aggregates values every second until year and gets statistics')\n\ndefine stream RawMaterialStream (name string, amount double);\n\n@sink(type ='log')\ndefine stream RawMaterialStatStream (AGG_TIMESTAMP long, name string, avgAmount double);\n@store( type=\"rdbms\",\njdbc.url=\"jdbc:mysql://localhost:3306/sweetFactoryDB\",\nusername=\"root\",\npassword=\"root\",\njdbc.driver.name=\"com.mysql.jdbc.Driver\")\n\ndefine aggregation stockAggregation\n\nfrom RawMaterialStream\nselect name, avg(amount) as avgAmount, sum(amount) as totalAmount\ngroup by name\naggregate every sec...year;\ndefine stream TriggerStream (triggerId string);\n@info(name = 'query1')\nfrom TriggerStream as f join stockAggregation as s\nwithin \"2016-06-06 12:00:00 +05:30\", \"2020-06-06 12:00:00 +05:30\"\nper 'hours'\nselect AGG_TIMESTAMP, s.name, avgAmount\ninsert into RawMaterialStatStream;\n</code></pre>"},{"location":"samples/AggregateOverTime/","title":"Aggregating Data Over Time","text":""},{"location":"samples/AggregateOverTime/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to simulate random events via Feed Simulation and calculate running aggregates such as <code>avg</code>, <code>min</code>, <code>max</code>, etc. The aggregation is executed on events within a time window. A sliding time window of 10 seconds is used in this sample. For more information on windows see Siddhi Query Guide - Window. The <code>group by</code> clause helps to perform aggregation on events grouped by a certain attribute. In this sample, the trading information per trader is aggregated and summarized, for a window of 10 seconds.</p> <p>Before you begin:</p> <p>In the Streaming Integrator Tooling, save the sample Siddhi application.</p>"},{"location":"samples/AggregateOverTime/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>AggregateOverTime.siddhi - Started Successfully!.</code></p>"},{"location":"samples/AggregateOverTime/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, simulate random events for it via the Streaming Integrator Tooling as follows:</p> <ol> <li> <p>To open the Event Simulator, click the Event Simulator icon.</p> <p></p> </li> <li> <p>In the Event Simulator panel, click Feed Simulation -&gt; Create.</p> <p></p> </li> <li> <p>In the new panel that opens, enter information as follows:</p> <p></p> <ol> <li> <p>In the Simulation Name field, enter <code>AggregateOverTime</code> as the name for the simulation.</p> </li> <li> <p>Select Random as the simulation source and then click Add Simulation Source.</p> </li> <li> <p>In the Siddhi App Name field, select AggregateOverTime.</p> </li> <li> <p>In the Stream Name field, select Trade Stream.</p> </li> <li> <p>In the trader(STRING) field, select Regex based. Then in the Pattern field that appears, enter <code>(Bob|Jane|Tom)</code> as the pattern.</p> <p>Tip</p> <p>When you use the <code>(Bob|Jane|Tom)</code> pattern, only <code>Bob</code>, <code>Jane</code>, and <code>Tom</code> are selected as values for the <code>trader</code> attribute of the <code>TradeStream</code>. Using a few values for the <code>trader</code> attribute is helpful when you verify the output because the output is grouped by the trader.</p> </li> <li> <p>In the quantity(INT) field, select Primitive based.</p> </li> <li> <p>Save the simulator configuration by clicking Save.</p> </li> </ol> <p>The newly created simulation is now listed under the Active Feed Simulations list as shown below.</p> <p></p> </li> <li> <p>Click the start button next to the AggregateOverTime simulation to start generating random events.</p> <p></p> <p>In the Run or Debug dialog box that appears, select Run and click Start Similation.</p> <p></p> </li> </ol>"},{"location":"samples/AggregateOverTime/#viewing-the-results","title":"Viewing the Results","text":"<p>Once you start the simulator, the output is logged in the console as shown in the sample below. The output reflects the aggregation for the events sent during the last 10 seconds.</p> <p></p> Click here to view the sample Siddhi application.<pre><code>@App:name(\"AggregateOverTime\")\n\n@App:description('Simulate multiple random events and calculate aggregations over time with group by')\n\ndefine stream TradesStream(trader string, quantity int);\n@sink(type='log')\ndefine stream SummarizedTradingInformation(trader string, noOfTrades long, totalTradingQuantity long, minTradingQuantity int, maxTradingQuantity int, avgTradingQuantity double);\n\n--Find count, sum, min, max and avg of quantity per trader, during the last 10 seconds\n@info(name='query1')\nfrom TradesStream#window.time(10 sec)\nselect trader, count() as noOfTrades, sum(quantity) as totalTradingQuantity, min(quantity) as minTradingQuantity, max(quantity) as maxTradingQuantity, avg(quantity) as avgTradingQuantity\ngroup by trader\ninsert into SummarizedTradingInformation;\n</code></pre>"},{"location":"samples/AlertsAndThresholds/","title":"Receiving Email Alerts","text":""},{"location":"samples/AlertsAndThresholds/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to send a single event via Single Simulation, and to generate alerts using filters when the threshold value is exceeded. Furthermore, it shows how to configure WSO2 Streaming Integrator Tooling to publish an alerts via e-mail. An alert is generated as an email when a high value transaction (i.e., where the value is over 5000) takes place.</p> <p>Before you begin:</p> <ul> <li>Enable access to less  secure apps in the gmail account you are using for this example via the Less SecureApps link.</li> <li>In the sample application, change the values for the following parameters in the <code>@sink</code> annotation as follows.<ul> <li><code>username</code> -&gt; <code>business.rules.manager</code> (This is the sender's username.)</li> <li><code>password</code> -&gt; <code>business-rules</code> (This is the sender's password.)</li> <li><code>address</code>-&gt; <code>business.rules.manager@wso2.com</code> (This is the sender's address.)</li> <li><code>to</code> -&gt; <code>template-manager@wso2.com</code> (This is the receiver's address.)</li> <li><code>subject</code> -&gt; <code>Alert for large value transaction: cardNo:{{ creditCardNo }}</code> (This is the subject of the email.) Once you update the values for the above parameters, save the sample Siddhib application.</li> </ul> </li> </ul>"},{"location":"samples/AlertsAndThresholds/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>AlertsAndThresholds.siddhi - Started Successfully!.</code></p>"},{"location":"samples/AlertsAndThresholds/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, simulate a single event for it via the Streaming Integrator Tooling as follows:</p> <ol> <li> <p>To open the Event Simulator, click the Event Simulator icon.</p> <p></p> <p>This opens the event simulation panel.</p> </li> <li> <p>To simulate events for the <code>TransactionStream</code> stream of the <code>AlertsAndThresholds</code>  Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows.</p> <p></p> Field Value Siddhi App Name <code>AlertsAndThresholds</code> StreamName <code>TransactionStream</code> <p>As a result, the attributes of the <code>Transactiontream</code> stream appear as marked in the image above.</p> </li> <li> <p>Enter values for the attributes as follows:</p> <p></p> Attribute Value creditCardNo <code>1234567898765432</code> country <code>SL</code> item <code>mobile</code> quantity <code>100</code> price <code>5000</code> <p>Info</p> <p>To generate an email alert, you need to simulate an event where the transaction value (i.e., <code>quantity</code> * <code>price</code>) exceeds <code>5000</code>. This is indicated by the <code>[quantity * price  &gt; 5000]</code> filter connected to the <code>TransactionStream</code> input stream.</p> </li> <li> <p>Click Send.</p> </li> </ol>"},{"location":"samples/AlertsAndThresholds/#viewing-the-results","title":"Viewing the Results","text":"<p>To view the results, check the receiver gmail inbox (i.e., gmail specified via the <code>to</code> parameter in the sink configuration). The following is displayed.</p> <pre><code>Subject: Alert for large value transaction: cardNo:1234567898765432\nContent:\ncreditCardNo:\"1234567898765432\",\ncountry:\"SL\",\nitem:\"mobile\",\nquantity:100,\nprice:5000\n</code></pre> Click here to view the sample Siddhi application.<pre><code>@App:name(\"AlertsAndThresholds\")\n@App:description('Simulate a single event and receive alerts as e-mail when a predefined threshold value is exceeded')\n\ndefine stream TransactionStream(creditCardNo string, country string, item string, quantity int, price double);\n\n@sink(type='email',\nusername ='business.rules.manager',\naddress ='business.rules.manager@wso2.com',\npassword= 'business-rules',\nsubject='Alert for large value transaction: cardNo:{{ creditCardNo }}',\nto='receive.alert.account1@gmail.com, receive.alert.account2@gmail.com',\nport = '465',\nhost = 'smtp.gmail.com',\nssl.enable = 'true',\nauth = 'true',\n@map(type='text'))\ndefine stream AlertStream(creditCardNo string, country string, item string, quantity int, price double);\n\n--Filter events when quantity * price &gt; 5000 condition is satisfied\n@info(name='query1')\nfrom TransactionStream[quantity * price  &gt; 5000]\nselect *\ninsert into AlertStream;\n</code></pre>"},{"location":"samples/AmazonS3SinkSample/","title":"Publishing Aggregated Events to the Amazon AWS S3 Bucket","text":""},{"location":"samples/AmazonS3SinkSample/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to publish aggregated events to Amazon AWS S3 bucket via the siddhi-io-s3 sink extension. In this sample the events received to the <code>StockQuoteStream</code> stream are aggregated by the <code>StockQuoteWindow</code> window, and then published to the Amazon S3 bucket specified via the <code>bucket.name</code> parameter.</p> <p>Before you begin:</p> <ul> <li>Create an AWS account and set the credentials following the instructions in the AWS Developer Guide.</li> <li>Save the sample Siddhi application in Streaming Integrator Tooling.</li> </ul>"},{"location":"samples/AmazonS3SinkSample/#executing-the-sample","title":"Executing the Sample:","text":"<p>To execute the sample, follow the procedure below:</p> <ol> <li> <p>In Streaming Integrator Tooling, click Open and then click AmazonS3SinkSample.siddhi in the workspace directory. Then update it as follows:</p> <ol> <li> <p>Enter the credential.provider class name as the value for the <code>credential.provider</code> parameter. If the class is not specified, the default credential provider chain is used.</p> </li> <li> <p>For the <code>bucket.name</code> parameter, enter <code>AWSBUCKET</code> as the value.</p> </li> <li> <p>Save the Siddhi application.</p> </li> </ol> </li> <li> <p>Start the Siddhi application by clicking the Start button (shown below) or by clicking by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>AmazonS3SinkSample.siddhi - Started Successfully!.</code></p> </li> </ol>"},{"location":"samples/AmazonS3SinkSample/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, simulate random events for it via the Streaming Integrator Tooling as follows:</p> <ol> <li> <p>To open the Event Simulator, click the Event Simulator icon.</p> <p></p> </li> <li> <p>In the Event Simulator panel, click Feed Simulation -&gt; Create.</p> <pre><code>![Feed Simulation tab](https://wso2.github.io/docs-si/images/aggregate-over-time-sample/feed-simulation-tab.png)</code></pre> </li> <li> <p>In the new panel that opens, enter information as follows.</p> <p></p> <ol> <li> <p>In the Simulation Name field, enter <code>AmazonS3SinkSample</code> as the name for the simulation.</p> </li> <li> <p>Select Random as the simulation source and then click Add Simulation Source.</p> </li> <li> <p>In the Siddhi App Name field, select AmazonS3SinkSample.</p> </li> <li> <p>In the Stream Name field, select StockQuoteStream.</p> </li> <li> <p>In the symbol(STRING) field, select Regex based. Then in the Pattern field that appears, enter <code>(IBM|WSO2|Dell)</code> as the pattern.</p> <p>Tip</p> <p>When you use the <code>(IBM|WSO2|Dell)</code> pattern, only <code>IBM</code>, <code>WSO2</code>, and <code>Dell</code> are selected as values for the <code>symbol</code> attribute of the <code>StockQuoteStream</code>. Using a few values for the <code>symbol</code> attribute is helpful when you verify the output because the output is grouped by the symbol.</p> </li> <li> <p>In the price(DOUBLE) field, select Static value.</p> </li> <li> <p>In the quantity(INT) field, select Primitive based.</p> </li> <li> <p>Save the simulator configuration by clicking Save. Th.e simulation is added to the list of saved feed simulations as shown below.</p> <p></p> </li> </ol> </li> <li> <p>To simulate random events, click the Start button next to the AmazonS3SinkSample simulator.</p> </li> </ol>"},{"location":"samples/AmazonS3SinkSample/#viewing-results","title":"Viewing results","text":"<p>Once the events are sent, check the S3 bucket. Objects are created with 3 events in each.</p> Click here to view the sample Siddhi application.<pre><code>@App:name(\"AmazonS3SinkSample\")\n@App:description(\"Publish events to Amazon AWS S3\")\n\n\ndefine window StockQuoteWindow(symbol string, price double, quantity int) lengthBatch(3) output all events;\n\ndefine stream StockQuoteStream(symbol string, price double, quantity int);\n\n@sink(type='s3', bucket.name='&lt;BUCKET_NAME&gt;', object.path='stocks',\ncredential.provider='com.amazonaws.auth.profile.ProfileCredentialsProvider', node.id='zeus',\n@map(type='json', enclosing.element='$.stocks',\n@payload(\"\"\"{\"symbol\": \"{{ symbol }}\", \"price\": \"{{ price }}\", \"quantity\": \"{{ quantity }}\"}\"\"\")))\ndefine stream StorageOutputStream (symbol string, price double, quantity int);\n\nfrom StockQuoteStream\ninsert into StockQuoteWindow;\n\nfrom StockQuoteWindow\nselect *\ninsert into StorageOutputStream;\n</code></pre>"},{"location":"samples/CDCWithListeningMode/","title":"Capturing MySQL Inserts via CDC","text":""},{"location":"samples/CDCWithListeningMode/#purpose","title":"Purpose:","text":"<p>This sample demonstrates how to capture change data from MySQL using Siddhi. The change events that can be captured include <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code>.</p> <p>Before you begin:</p> <ol> <li>Ensure that MySQL is installed on your computer.</li> <li>Add the MySQL JDBC driver to the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory as follows:<ol> <li>Download the JDBC driver from the MySQL website.</li> <li>Unzip the archive.</li> <li>Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> JAR and place it in the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</li> </ol> </li> <li>Configure MySQL to enable binary logging.         If you are using MySQL 8.0, use the following query to check the binlog status: <code>SELECT variable_value as \"BINARY LOGGING STATUS (log-bin) ::\"         FROM performance_schema.global_variables WHERE variable_name='log_bin';</code></li> <li>Enable state persistence in siddhi applications. To do this, open the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file and set the <code>state.persistence enabled=true</code> property.</li> <li>Create a database named <code>production</code> by issuing the following command. <code>create database production;</code></li> <li>Create a user named <code>wso2sp</code> with <code>wso2</code> as the password, and with <code>SELECT</code>, <code>RELOAD</code>, <code>SHOW DATABASES</code>, <code>REPLICATION SLAVE</code>, <code>REPLICATION CLIENT</code> privileges. To do this, issue the following command. <code>GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'wso2sp' IDENTIFIED BY 'wso2';</code></li> <li>Change the database by issuing the following command. <code>use production;</code></li> <li>Create a table named <code>SweetProductionTable</code>. <code>CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));</code></li> <li>Save the sample Siddhi application in Streaming Integrator Tooling.</li> </ol>"},{"location":"samples/CDCWithListeningMode/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>CDCWithListeningMode.siddhi -  Started Successfully!</code></p> <p>Note</p> <p>If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again.</p>"},{"location":"samples/CDCWithListeningMode/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, insert a record to the <code>SweetProductionTable</code> table you created by issuing the following command:</p> <p><code>insert into SweetProductionTable values('chocolate',100.0);</code></p>"},{"location":"samples/CDCWithListeningMode/#viewing-the-results","title":"Viewing the results","text":"<p>This insert is logged in the Streaming Integrator console as follows.</p> <p></p> <p>Info</p> <p>Optionally, you can use this sample to also capture <code>update</code> and <code>delete</code> operations. - delete operation events include <code>before_name</code> and <code>before amount</code> keys. - update operation events include the <code>before_name</code>, <code>name</code>, <code>before_amount</code>, <code>amount</code> keys.</p> Click here to view the sample Siddhi application.<pre><code>@App:name('CDCWithListeningMode')\n@App:description('Capture MySQL Inserts using cdc source listening mode.')\n\n\n@source(type = 'cdc', url = 'jdbc:mysql://localhost:3306/production', username = 'wso2sp', password = 'wso2', table.name = 'SweetProductionTable', operation = 'insert',\n@map(type = 'keyvalue'))\ndefine stream insertSweetProductionStream (name string, amount double);\n\n@sink(type = 'log')\ndefine stream logStream (name string, amount double);\n\n@info(name = 'query')\nfrom insertSweetProductionStream\nselect name, amount\ninsert into logStream;\n</code></pre>"},{"location":"samples/CDCWithPollingMode/","title":"Capturing MySQL Inserts and Updates via CDC Polling Mode","text":""},{"location":"samples/CDCWithPollingMode/#purpose","title":"Purpose:","text":"<p>This sample demonstrates how to use the polling mode of the CDC source. In this example, you are capturing the inserts to a MySQL table.</p> <p>By changing the database type, you can also try out this example for the following databases.</p> <ul> <li> <p>Oracle</p> </li> <li> <p>H2</p> </li> <li> <p>MS SQL Server</p> </li> <li> <p>Postgresql</p> </li> <li> <p>MongoDB</p> </li> </ul> <p>Before you begin:</p> <ol> <li>Ensure that MySQL is installed on your computer.</li> <li>Add the MySQL JDBC driver to the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory as follows:<ol> <li>Download the JDBC driver from the MySQL website.</li> <li>Unzip the archive.</li> <li>Copy the <code>mysql-connector-java-5.1.45-bin.jar</code> JAR and place it in the <code>&lt;SI_HOME&gt;/lib</code> directory.</li> </ol> </li> <li>Create a database named <code>production</code> by issuing the following command. <code>CREATE SCHEMA production;</code></li> <li>Change the database by issuing the following command. <code>use production;</code></li> <li>Create a table named <code>SweetProductionTable</code> by issuing the following command. <code>CREATE TABLE SweetProductionTable (last_update TIMESTAMP, name VARCHAR(20),amount double(10,2));</code></li> <li> <p>If you want to capture the changes from the last point of time the Siddhi application was stopped, enable state persistence by setting the <code>state.persistence enabled=true</code> pproperty in the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file. If you do not enable state persistence, only the changes since the Siddhi application started are captured.</p> </li> <li> <p>In the sample Siddhi application, update the <code>username</code> and <code>password</code> parameters in the source configuration by adding the username and password you use to log in to MySQL as the values. Then save the sample Siddhi application in Streaming Integrator Tooling.</p> </li> </ol>"},{"location":"samples/CDCWithPollingMode/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>CDCWithPollingMode.siddhi - Started Successfully!</code></p> <p>Note</p> <p>If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again.</p>"},{"location":"samples/CDCWithPollingMode/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, insert a record to the <code>SweetProductionTable</code> table you created by issuing the following command.</p> <p><code>insert into SweetProductionTable(name,amount) values('chocolate',100.0);</code></p>"},{"location":"samples/CDCWithPollingMode/#viewing-the-results","title":"Viewing the results","text":"<p>The insert operation is logged in the Streaming Integrator console as shown below.</p> <p></p> <p>Info</p> <p>You can also update the existing row and observe the change data events.</p> <p>Tip</p> <p>For updates, the previous values of the row are not returned with the event. Use listening mode to obtain such details.</p> <p>The polling mode can also be used with Oracle, MS-SQL server, Postgres, H2.</p> Click here to view the sample Siddhi application.<p>```sql @App:name(\"CDCWithPollingMode\") @App:description(\"Capture MySQL Inserts and Updates using cdc source polling mode.\")</p> <p>@source(type = 'cdc',     url = 'jdbc:mysql://localhost:3306/production?useSSL=false',     mode = 'polling',     jdbc.driver.name = 'com.mysql.jdbc.Driver',     polling.column = 'last_update',     polling.interval = '1',     username = '',     password = '',     table.name = 'SweetProductionTable',     @map(type = 'keyvalue' )) define stream insertSweetProductionStream (name string, amount double);</p> <p>@sink(type = 'log') define stream logStream (name string, amount double);</p> <p>@info(name = 'query') from insertSweetProductionStream select name, amount insert into logStream;     ```</p>"},{"location":"samples/CSVCustomMapping/","title":"Receiving and Publishing Events in Custom CSV Format","text":""},{"location":"samples/CSVCustomMapping/#purpose","title":"Purpose","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to publish and receive data events processed within Siddhi to files in CSV custom format.</p> <p>Before you begin:</p> <ol> <li>Edit the sample Siddhi application as follows:<ul> <li>In the source configuration, update the value for the <code>dir.uri</code> parameter by replacing <code>{WSO2SIHome}</code> with the absolute path of your WSO2 SI Tooling directory.</li> <li>In the sink configuration, update the value for the <code>file.uri</code> parameter by replacing <code>{WSO2SIHome}</code> with the absolute path of your WSO2 SI Tooling directory. If required, you can provide a different path to publish the output to a location of your choice.</li> </ul> </li> <li>Save the sample Siddhi application in Streaming Integrator Tooling.</li> </ol>"},{"location":"samples/CSVCustomMapping/#executing-and-testing-the-sample","title":"Executing and testing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>CSVCustomMapping.siddhi - Started Successfully!</code></p>"},{"location":"samples/CSVCustomMapping/#viewing-the-results","title":"Viewing the Results","text":"<ul> <li> <p>The source gets the input from the <code>SI_HOME&gt;/samples/artifacts/CSVMappingWithFile/new/example.csv</code> file and produces the event. This file has data in below format.</p> <p><code>1,WSO2,23.5</code> <code>2,IBM,2.5</code></p> </li> <li> <p>The sink gets the input from the source output and publishes the output in the <code>outputOfCustom.csv</code> file. The data is published in this file in the following format.</p> <p><code>WSO2,1,100.0</code> <code>IBM,2,2.5</code></p> </li> </ul> Click here to view the sample Siddhi application.<pre><code>@App:name(\"CSVCustomMapping\")\n@App:description('Publish and receive data events processed within Siddhi to files in CSV custom format.')\n\n\n\n@source(type='file',\ndir.uri='file://{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new',\naction.after.process='NONE',\n@map(type='csv', @attributes(id='0', name='1', amount='2')))\ndefine stream IntputStream (name string, id int,  amount double);\n\n@sink(type='file', file.uri='/{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/outputOfCustom.csv' , @map(type='csv',@payload(id='1', name='0', amount='2')))\ndefine stream OutputStream (name string, id int, amount double);\n\nfrom IntputStream\nselect *\ninsert into OutputStream;\n</code></pre>"},{"location":"samples/CSVDefaultMapping/","title":"Publishing and Receiving CSV Events via Files","text":""},{"location":"samples/CSVDefaultMapping/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to Publish and receive data events processed within Siddhi to files in CSV default format.</p>"},{"location":"samples/CSVDefaultMapping/#prerequisites","title":"Prerequisites:","text":"<ul> <li> <p>Edit the uri '{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/example.csv' by replacing {WSO2SIHome} with the absolute path of your WSO2SP home directory. You can also change the path for 'file.uri' in the sink, if you want to publish your event file to a different location.</p> </li> <li> <p>Save this sample. If there is no syntax error, the following messages would be shown on the console:</p> <ul> <li>CSVDefaultMapping.siddhi successfully deployed.</li> </ul> </li> </ul>"},{"location":"samples/CSVDefaultMapping/#executing-testing-the-sample","title":"Executing &amp; testing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>CSVDefaultMapping.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/CSVDefaultMapping/#viewing-the-results","title":"Viewing the Results:","text":"<ul> <li>Source takes input from the '{WSO2SP-HOME}/samples/artifacts/CSVMappingWithFile/new/example.csv' then produce the event. example.csv has data in below format         1,WSO2,23.5         2,IBM,2.5</li> <li>Sink takes input from source output and produces the output to outputOfCustom.csv in CSV custom format.     outputOfCustom.csv's data appear in below format         1,WSO2,100.0         2,IBM,2.5</li> </ul> Click here to view the sample Siddhi application.<pre><code>@App:name(\"CSVDefaultMapping\")\n@App:description('Publish and receive data events processed within Siddhi to files in CSV default format.')\n\n\n@source(type='file',\ndir.uri='file://{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new',\naction.after.process='NONE',\n@map(type='csv'))\ndefine stream InputStream (id int, name string, amount double);\n\n@sink(type='file', file.uri='/{WSO2SIHome}/samples/artifacts/CSVMappingWithFile/new/outputOfDefault.csv' , @map(type='csv'))\ndefine stream OutputStream (id int, name string, amount double);\n\nfrom InputStream\nselect *\ninsert into OutputStream;\n</code></pre>"},{"location":"samples/ClusTreeTestApp/","title":"Performing Streaming Learning Using a Clustree Model","text":""},{"location":"samples/ClusTreeTestApp/#purpose","title":"Purpose","text":"<p>This sample demonstrates how to perform unsupervised streaming learning on a set of data points using a Clustree model.</p> <p>Before you begin:</p> <ol> <li>Download <code>siddhi-gpl-execution-streamingml-x.x.x.jar</code> from here and place it in the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</li> <li>Copy the <code>&lt;SI_TOOLING_HOME&gt;/samples/artifacts/ClusTreeSample/clusTreeTestFeed.json</code> file and place it in the <code>&lt;SI_TOOLING_HOME&gt;/wso2/server/deployment/simulation-configs</code> directory.</li> <li>Copy the <code>&lt;SI_TOOLING_HOME&gt;/samples/artifacts/ClusTreeSample/clusTreeFileTest.csv</code> file and place it in the <code>&lt;SI_TOOLING_HOME&gt;/wso2/server/deployment/csv-files</code> directory.</li> <li>Save the sample Siddhi application in Streaming Integrator Tooling.</li> </ol>"},{"location":"samples/ClusTreeTestApp/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>ClusTreeTestApp.siddhi - Started Successfully!</code></p>"},{"location":"samples/ClusTreeTestApp/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, simulate multiple events via CSV file in the Streaming Integrator Tooling as follows:</p> <ol> <li> <p>To open the Event Simulator, click the Event Simulator icon.</p> <p></p> <p>This opens the event simulation panel.</p> </li> <li> <p>Open event simulator by clicking on the second icon or press Ctrl+Shift+I.</p> </li> <li>In the Feed Simulation tab of the panel you can see that the clusTreeTestFeed.csv file is loaded.</li> <li>Press the play button to start simulation.</li> </ol>"},{"location":"samples/ClusTreeTestApp/#viewing-the-results","title":"Viewing the Results","text":"<p>After clicking the play button see the output on the console, that are produced according to the simulation from csv file.</p> Click here to view the sample Siddhi application.<pre><code>@App:name(\"ClusTreeTestApp\")\n\n\ndefine stream InputStream (x double, y double);\n\n@sink(type='log')\ndefine stream logStream (closestCentroidCoordinate1 double,closestCentroidCoordinate2 double,x double, y double);\n\n@info(name = 'query1')\nfrom InputStream#streamingml:clusTree(2, 10, 20, 5, 50, x, y)\nselect closestCentroidCoordinate1, closestCentroidCoordinate2, x, y\ninsert into logStream;\n</code></pre>"},{"location":"samples/DataPreprocessing/","title":"Pre-processing Data Received via TCP","text":""},{"location":"samples/DataPreprocessing/#purpose","title":"Purpose","text":"<p>This example demonstrates how to receive events via the TCP transport and carry out data pre-processing with numerous Siddhi extensions (e.g., string extension, time extension). In this sample, a composite ID is obtained using string concatenation and the time format of the incoming event is changed from <code>yyyy/MM/dd HH:mm:ss</code> to <code>dd-MM-yyyy HH:mm:ss</code>.</p> <p>For more information about Siddhi extensions, see Siddhi Extensions.</p> <p>Before you begin:</p> <p>Save the sample Siddhi application in Streaming Integrator Tooling.</p>"},{"location":"samples/DataPreprocessing/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following messages appear in the console.</p> <ul> <li> <p><code>Tcp Server started in 0.0.0.0:9892</code></p> </li> <li> <p><code>DataPreprocessing.siddhi - Started Successfully!</code></p> </li> </ul> <p>Note</p> <p>If you want to edit the Siddhi application after you have started it, stop it first, make your edits, save it and then start it again.</p>"},{"location":"samples/DataPreprocessing/#testing-the-sample","title":"Testing the Sample","text":"<p>To test this Siddhi application, navigate to the <code>&lt;SI_TOOLING_HOME&gt;/samples/sample-clients/tcp-client</code> directory and run the <code>ant</code> command as follows.</p> <p><code>ant -Dtype=json -DfilePath={WSO2SIHome}/samples/artifacts/DataPreprocessing/data_preprocessing_events.txt -DeventDefinition='{\"event\":{\"id\":\"{0}\",\"value\":{1},\"property\":{2},\"plugId\":{3},\"householdId\":{4},\"houseId\":{5},\"currentTime\":\"{6}\"}}' -Durl=tcp://localhost:9892/SmartHomeStream</code></p>"},{"location":"samples/DataPreprocessing/#viewing-the-results","title":"Viewing the Results","text":"<p>Once the <code>DataProcessing</code> Siddhi application receives events from the TCP client, the following messages are displayed in the Streaming Integrator Tooling console:</p> <p>INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621173211, data=[HouseholdID:1::UniqueID:0001, 12.12, 13-08-2001 23:49:33], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621174202, data=[HouseholdID:2::UniqueID:0002, 13.12, 13-08-2004 23:49:34], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621175208, data=[HouseholdID:3::UniqueID:0003, 13.12, 13-08-2006 23:49:35], isExpired=false} INFO {io.siddhi.core.stream.output.sink.LogSink} - DataPreprocessing : ProcessedStream : Event{timestamp=1513621176214, data=[HouseholdID:4::UniqueID:0004, 14.12, 13-08-2008 23:49:36], isExpired=false}</p> Click here to view the sample Siddhi application.<pre><code>@App:name(\"DataPreprocessing\")\n\n@App:description('Collect data via TCP transport and pre-process')\n\n\n@Source(type = 'tcp',\ncontext='SmartHomeStream',\n@map(type='json'))\ndefine stream SmartHomeStream (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string);\n\n@sink(type='log')\ndefine stream ProcessedStream (compositeID string, value float, formattedTime string);\n\n--Process smart home data by concatenating the IDs and formatting the time\n@info(name='query1')\nfrom SmartHomeStream\nselect str:concat(\"HouseholdID:\", convert(householdId, \"string\"), \"::\", \"UniqueID:\", id) as compositeID, value, str:concat(currentTime, \" \", time:currentTime()) as formattedTime\ninsert into ProcessedStream;\n</code></pre>"},{"location":"samples/ExtremaBottomK/","title":"Counting the Frequency of Values with BottomK","text":""},{"location":"samples/ExtremaBottomK/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to use the <code>siddhi-execution-extrema</code> extension with the <code>bottomK</code> function.</p> <p>Before you begin:</p> <p>Save the sample Siddhi application in Streaming Integrator Tooling.</p>"},{"location":"samples/ExtremaBottomK/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>ExtremaBottomK.siddhi - Started Successfully!</code></p>"},{"location":"samples/ExtremaBottomK/#testing-the-sample","title":"Testing the Sample","text":"<p>The <code>ExtremaBottomK</code> Siddhi application can be tested in three ways as follows.</p>"},{"location":"samples/ExtremaBottomK/#option-1-publish-events-via-curl","title":"Option 1: Publish events via CURL","text":"<p>You can publish events in the <code>JSON</code> format to the HTTP endpoint via CURL commands. The CURL commands should be in the format of the example given below. The values for <code>name</code> and <code>amount</code> parameters can change.</p> <p><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20}}\"  http://localhost:8006/productionStream --header \"Content-Type:application/json\"</code></p>"},{"location":"samples/ExtremaBottomK/#option2-publish-events-with-postman","title":"Option2: Publish events with Postman","text":"<ol> <li> <p>Install the <code>Postman</code> application from Chrome web store.</p> </li> <li> <p>Launch the application.</p> </li> <li> <p>Make a <code>POST</code> request to the <code>http://localhost:8006/productionStream</code> endpoint. Set the <code>Content-Type</code> to <code>application/json</code> and set the request body in <code>JSON</code> format as follows.</p> <pre><code>    {\n       \"event\":{\n          \"name\":\"sugar\",\n          \"amount\":20\n       }\n    }\n</code></pre> </li> <li> <p>Send some more events in the same format, with different values for the <code>name</code> and <code>amount</code> parameters.</p> </li> </ol>"},{"location":"samples/ExtremaBottomK/#option3-publish-events-with-http-sample-client","title":"Option3: Publish events with http sample client","text":"<p>Navigate to the <code>&lt;WSO2SIHome&gt;/samples/sample-clients/http-client</code> directory and run the following command.</p> <p><code>ant -Dtype=json -DfilePath={WSO2SIHome}/samples/artifacts/ExtreamBottomK/ExtremaBottomKEvents.txt -DeventDefinition='{\"event\":{\"name\":\"{0}\",\"amount\":{1}}}' -Durl=http://localhost:8006/productionStream</code></p>"},{"location":"samples/ExtremaBottomK/#viewing-the-results","title":"Viewing the Results","text":"<p>The output is logged in the Streaming Integrator Tooling console as follows.</p> <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498254202, data=[sugar, 20, sugar, 1, null, null, null, null], isExpired=false}, Event{timestamp=1529498254202, data=[cake, 10, cake, 1, sugar, 1, null, null], isExpired=false}]&lt;br/&gt;&lt;br/&gt;\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498262769, data=[cake, 10, cake, 1, sugar, 1, null, null], isExpired=false}, Event{timestamp=1529498262769, data=[toffee, 65, toffee, 1, cake, 1, sugar, 1], isExpired=false}]&lt;br/&gt;&lt;br/&gt;\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498270897, data=[toffee, 65, toffee, 1, cake, 1, sugar, 1], isExpired=false}, Event{timestamp=1529498270897, data=[cake, 74, toffee, 1, sugar, 1, cake, 2], isExpired=false}]&lt;br/&gt;&lt;br/&gt;\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ExtremaBottomK : outputStream : [Event{timestamp=1529498278304, data=[cake, 74, toffee, 1, sugar, 1, cake, 2], isExpired=false}, Event{timestamp=1529498278304, data=[toffee, 25, sugar, 1, toffee, 2, cake, 2], isExpired=false}]\n</code></pre> Click here to view the sample Siddhi application.<pre><code>@App:name(\"ExtremaBottomK\")\n@App:Description('Demonstrates how to use the siddhi-execution-extrema with bottomK function')\n\n\n@Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',\n@map(type='json'))\ndefine stream inputStream (name string, amount long);\n\n@sink(type='log')\ndefine stream outputStream(name string, amount long, bottom1Element string, bottom1Frequency long, bottom2Element string, bottom2Frequency long, bottom3Element string, bottom3Frequency long);\n\nfrom inputStream#extrema:bottomK(name, 3)\ninsert all events into outputStream;\n</code></pre>"},{"location":"samples/GCSSinkSample/","title":"Publishing Events to a GCS Bucket","text":""},{"location":"samples/GCSSinkSample/#purpose","title":"Purpose","text":"<p>This example shows how to publish events to a GCS bucket via the siddhi-io-gcs sink extension. The Siddhi application receives events by consuming them from the <code>http://localhost:8006/inputStream</code> endpoint. These events are then sent to the <code>GCS</code> sink that aggregates and commits the events to a Google Cloud Storage bucket.</p> <p>Before you begin:</p> <ol> <li>Create an account in Google Cloud.</li> <li>Download the credential file that is generated through the GCP console and save it in a directory of your choice. For more information, see Google Cloud Authentication Documentation.</li> <li>Save the sample Siddhi application in Streaming Integrator Tooling.</li> </ol>"},{"location":"samples/GCSSinkSample/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample, follow the steps below:</p> <ol> <li> <p>Enter values for the <code>credential.path</code> and the <code>bucket.name</code> parameters of the GCS Sink definition in the sample Siddhi application.</p> <p>Info</p> <p>You can either set the <code>credential.path</code> parameter in the sink configuration as shown below or can set a system variable with<code>GOOGLE_APPLICATION_CREDENTIALS</code> as the name with the path to the credential file.</p> </li> <li> <p>Start the sample by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>GCSSinkSample.siddhi - Started Successfully!</code></p> </li> </ol>"},{"location":"samples/GCSSinkSample/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, open a terminal and issue the following curl command</p> <p><code>curl -H \"Content-Type: application/json\" -d '{\"event\":{\"key\":\"testFile\",\"payload\":\"message\", \"test\":\"wjson\"}}' http://localhost:8006/inputStream</code></p> Click here to view the sample Siddhi application.<pre><code>@App:name(\"GCSSinkSample\")\n@App:description(\"Publish Events to a GCS bucket using a http-source\")\n\n\n@Source(type = 'http',\nreceiver.url='http://localhost:8006/inputStream', @map(type='json'))\ndefine stream inputStream(key string, payload string, suffix string);\n\n@sink(type='google-cloud-storage', credential.path='&lt;credential.path&gt;', bucket.name='&lt;bucket.name&gt;',\nobject.name='test-object-{{ suffix }}',\n@map(type='text'))\ndefine stream outputStream(key string, payload string, suffix string);\n\nfrom inputStream\nselect *\ninsert into outputStream;\n</code></pre>"},{"location":"samples/GeoDistanceCalculation/","title":"Publishing and Receiving CSV Events via Files","text":""},{"location":"samples/GeoDistanceCalculation/#purpose","title":"Purpose","text":"<p>This example demonstrates how to calculate the distance between two locations via the <code>siddhi-gpl-execution-geo</code> extension.</p> <p>Before you begin:</p> <ol> <li>Download the siddhi-gpl-execution-geo-x.x.x.jar and place it in the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</li> <li>Save this sample in Streaming Integrator Tooling.</li> </ol>"},{"location":"samples/GeoDistanceCalculation/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>GeoDistanceCalculation.siddhi - Started Successfully!</code></p>"},{"location":"samples/GeoDistanceCalculation/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample application, simulate a single event for it as follows:</p> <ol> <li> <p>To open the Event Simulator, click the Event Simulator icon.</p> <p></p> <p>This opens the event simulation panel.</p> </li> <li> <p>To simulate events for the <code>LocationPointsStream</code> stream of the <code>GeoDistanceCalculation</code> Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows.</p> Field Value Siddhi App Name <code>GeoDistanceCalculation</code> StreamName <code>LocationPointsStream</code> <p></p> <p>As a result, the attributes of the <code>GeoDistanceCalculation</code> stream appear in the panel.</p> </li> <li> <p>Enter attribute values as follows.</p> </li> </ol> <p>.</p> Attribute Value latitude <code>8.116553</code> longitude <code>77.523679</code> prevLatitude <code>9.850047</code> prevLongitude <code>98.597177</code> <ol> <li>Send the event</li> </ol>"},{"location":"samples/GeoDistanceCalculation/#viewing-the-results","title":"Viewing the Results","text":"<p>The following output is logged inthe Streaming Integrator console for the single event you simulated.</p> <p><code>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - GeoDistanceCalculation: Event :, StreamEvent{ timestamp=1513616078228, beforeWindowData=null, onAfterWindowData=null, outputData=[2322119.848252557], type=CURRENT, next=null}</code></p> Click here to view the sample Siddhi application.<pre><code>@App:name(\"GeoDistanceCalculation\")\n\n@App:description('This will demonstrate the distance between two locations')\n\ndefine stream LocationPointsStream (latitude double, longitude double, prevLatitude double, prevLongitude double);\n\n@sink(type='log')\ndefine stream DistanceStream (distance double);\n\n@info(name = 'query1')\nfrom LocationPointsStream\nselect geo:distance(latitude, longitude, prevLatitude, prevLongitude) as distance\ninsert into DistanceStream;\n</code></pre>"},{"location":"samples/GplNLPFindNameEntityType/","title":"Extracting Values from a String","text":""},{"location":"samples/GplNLPFindNameEntityType/#purpose","title":"Purpose:","text":"<p>Through this app, NamedEntity of given type \"organization\" is extracted from the provided string.</p>"},{"location":"samples/GplNLPFindNameEntityType/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Download siddhi-gpl-execution-nlp-x.x.x.jar from the following link and copy the jar to  {WSO2SIHome}/lib http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/nlp/siddhi-gpl-execution-nlp/</li> <li>Save this sample. If there is no syntax error, the following messages would be shown on the console<ul> <li>Siddhi App GplNLPFindNameEntityType.siddhi successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/GplNLPFindNameEntityType/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>GplNLPFindNameEntityType.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/GplNLPFindNameEntityType/#testing-the-sample","title":"Testing the Sample:","text":"<p>You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows:     * Siddhi App Name  : GplNLPFindNameEntityType     * Stream Name     : InputStream 3. Enter following string in the message and send <code>ABC factory produces 20 donuts per day</code></p>"},{"location":"samples/GplNLPFindNameEntityType/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - GplNLPFindNameEntityType: StreamEvent{ timestamp=1513573692406, beforeWindowData=null, onAfterWindowData=null, outputData=[ABC factory produces 20 donuts per day., ABC], type=CURRENT, next=null} ABC has been recognized as an organization.</p>"},{"location":"samples/GplNLPFindNameEntityType/#note","title":"Note:","text":"<p>Stop this Siddhi application, once you are done with the execution</p> <pre><code>@App:name('GplNLPFindNameEntityType')\n@App:Description(\"NamedEntity of given type 'organization' is extracted from the provided string\")\n\n\ndefine stream InputStream (message string);\n\n@sink(type='log')\ndefine stream outputStream (message string, match string);\n\nfrom InputStream#nlp:findNameEntityType( 'ORGANIZATION', true, message )\nselect *\ninsert into FindNameEntityTypeResult;\n\nfrom FindNameEntityTypeResult\nselect *\ninsert into outputStream;\n</code></pre>"},{"location":"samples/HelloKafka/","title":"Consuming Events from a Kafka Topic and Publishing to Another Kafka Topic","text":""},{"location":"samples/HelloKafka/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to use the Kafka transport in Siddhi to receive and publish events. Events which are in JSON format are consumed from one Kafka topic and written to another Kafka topic in XML format.</p>"},{"location":"samples/HelloKafka/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>The following steps must be executed to enable WSO2 SP to receive and publish events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding.</p> <ol> <li> <p>Download the Kafka broker from here: https://www.apache.org/dyn/closer.cgi?path=/kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz</p> </li> <li> <p>Convert and copy the Kafka client jars from the <code>{KafkaHome}/libs</code> directory to the <code>{WSO2SIHome}/libs</code> directory as follows.</p> <ol> <li> <p>Create a directory named <code>{Source}</code> in a preferred location in your machine and copy the following JARs to it from the <code>{KafkaHome}/libs</code> directory.</p> <ul> <li>kafka_2.11-0.10.2.1.jar</li> <li>kafka-clients-0.10.2.1.jar</li> <li>metrics-core-2.2.0.jar</li> <li>scala-library-2.11.8.jar</li> <li>scala-parser-combinators_2.11-1.0.4.jar</li> <li>zkclient-0.10.jar</li> <li>zookeeper-3.4.9.jar</li> </ul> </li> <li> <p>Create another directory named <code>{Destination}</code> in a preferred location in your machine.</p> </li> <li>To convert all the Kafka jars you copied into the <code>{Source}</code> directory, issue the following command,<ul> <li>For Windows: <pre><code>{WSO2SIHome}/bin/jartobundle.bat &lt;{Source} Directory Path&gt; &lt;{Destination} Directory Path&gt;\n</code></pre></li> <li>For Linux: <pre><code>sh {WSO2SIHome}/bin/jartobundle.sh &lt;{Source} Directory Path&gt; &lt;{Destination} Directory Path&gt;\n</code></pre></li> </ul> </li> <li>Add the OSGI converted kafka libs from <code>{Destination}</code> directory to <code>{WSO2SIHome}/lib</code>.</li> <li>Add the original Kafka libs from <code>{Source}</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code>.</li> <li>Navigate to <code>{KafkaHome}</code> and start zookeeper node using following command. <pre><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties\n</code></pre></li> <li>Navigate to <code>{KafkaHome}</code> and start Kafka server node using following command. <pre><code>sh bin/kafka-server-start.sh config/server.properties\n</code></pre></li> <li>Start the server using following command . <pre><code>sh streaming-integrator-tooling.sh\n</code></pre></li> <li>Save this sample.</li> </ol> </li> </ol> </li> </ol>"},{"location":"samples/HelloKafka/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>HelloKafka.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/HelloKafka/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Navigate to <code>{WSO2SIHome}/samples/sample-clients/kafka-consumer</code> and run the following command.     <pre><code>ant -DtopicList=kafka_result_topic -Dtype=xml -DpartitionList=0\n</code></pre></li> <li>Navigate to <code>{WSO2SIHome}/samples/sample-clients/kafka-producer</code> and run the following command.     <pre><code>ant -DtopicName=kafka_topic -DfilePath={WSO2SIHome}/samples/artifacts/HelloKafka/kafka_sample.txt\n</code></pre>     This command would publish the events in <code>kafka_sample</code> file to the Source Kafka Topic (named 'kafka_topic').</li> </ol>"},{"location":"samples/HelloKafka/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output events received by Sink Kafka Topic (named <code>kafka_result_topic</code>) being logged on the <code>kafka-consumer</code> console. Note how the events have been converted from JSON to XML type. This conversion happens due to the Sink configuration's map type being XML.</p> <pre><code>@App:name(\"HelloKafka\")\n@App:description('Consume events from a Kafka Topic and publish to a different Kafka Topic')\n\n\n/*\nSweetProductionStream definition. It receives events from \"kafka_topic\" in json format. Events in this stream will\nhave information about the name of the sweet and how much of it is produced.\n*/\n@source(type='kafka',\ntopic.list='kafka_topic',\npartition.no.list='0',\nthreading.option='single.thread',\ngroup.id=\"group\",\nbootstrap.servers='localhost:9092',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n/*\nSuppose that the factory packs sweets by taking last 3 sweet productions disregarding their individual amount.\nTotalStream will have total of the last 3 events in SweetProductionStream. This is calcuklated as follows; the sum of\n1st, 2nd, and 3rd events of SweetProductionStream will be the 1st event of TotalStream and the sum of 4th, 5th, and\n6th events of SweetProductionStream will be the 2nd event of TotalStream\n*/\ndefine stream TotalStream (total double);\n\n/*\nThis stream counts the event number of TotalStream and sends that count along with total. This will help us find out\nthe batch which has a low total weight by using the count as batch number as we will see in the LowProductionAlertStream\n*/\ndefine stream TotalStreamWithBatch(batchNumber long, total double);\n\n/*\nThis stream will send an alert into kafka_result_topic if any batch has a total weight less than 10. Batch number of\nthe low weight batch and the actual weight will be sent out.\n*/\n@sink(type='kafka',\ntopic='kafka_result_topic',\nbootstrap.servers='localhost:9092',\npartition.no='0',\n@map(type='xml'))\ndefine stream LowProductionAlertStream (batchNumber long, lowTotal double);\n\n\n--summing events in SweetProductionStream in batches of 3 and sending to TotalStream\n@info(name='query1')\nfrom SweetProductionStream#window.lengthBatch(3)\nselect sum(amount) as total\ninsert into TotalStream;\n\n--count is included to indicate batch number\nfrom TotalStream\nselect count() as batchNumber, total\ninsert into TotalStreamWithBatch;\n\n--filtering out events with total less than 10 to create alert\nfrom TotalStreamWithBatch[total &lt; 10]\nselect batchNumber, total as lowTotal\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/HttpRequestResponseSample/","title":"Publishing HTTP Requests, Receiving Responses, and Processing Them","text":""},{"location":"samples/HttpRequestResponseSample/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in JSON default format, Receive response from the http server and process the response using siddhi.</p>"},{"location":"samples/HttpRequestResponseSample/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Save this sample. If there is no syntax error, the following message is shown on the console:<ul> <li>Siddhi App HttpRequestResponseSample successfully deployed.</li> </ul> </li> </ul>"},{"location":"samples/HttpRequestResponseSample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:</li> <li>HttpRequestResponseSample.siddhi - Started Successfully!</li> <li>'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'.</li> </ol>"},{"location":"samples/HttpRequestResponseSample/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/http-server and run \"ant\" command without any arguments.</li> <li> <p>Send events using one or more of the following methods.</p> <ul> <li> <p>Send events with http server through the event simulator:</p> <ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specifiy the values as follows:<ul> <li>Siddhi App Name  : HttpRequestResponseSample</li> <li>Stream Name     : SweetProductionStream</li> </ul> </li> <li>In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event.</li> <li>Send more events as desired.</li> </ol> </li> <li> <p>Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"HttpRequestResponseSample\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install the 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> <li>When publishing the events, http-request sink will send the request to the http server and the server will echo the received request as the response with a 200 http status code. Then,</li> <li>That successful response will be received by the defined http-response source which has the relevant http status code.</li> <li>Received response will be converted to a siddhi event using using json default mapping and pushed to the ResponseStream.</li> </ul> </li> </ol>"},{"location":"samples/HttpRequestResponseSample/#viewing-the-results","title":"Viewing the Results:","text":"<p>The received responses will be logged in the terminal/editor console as following.</p> <ul> <li>INFO {io.siddhi.core.stream.output.sink.LogSink} - RequestResponse : responseStream : Event{timestamp=1555358941592, data=[toffees, 75.6], isExpired=false}</li> </ul>"},{"location":"samples/HttpRequestResponseSample/#notes","title":"Notes:","text":"<p>If the message \"LowProductionAlertStream' stream could not connect to 'localhost:8080\", it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"HttpRequestResponseSample\")\n@App:description(\"Publish http requests, receive their responses and process them\")\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http-request', sink.id='production-request', publisher.url='http://localhost:8080/abc',\n@map(type='json'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@sink(type='log')\n@source(type='http-response' , sink.id='production-request', http.status.code='200',\n@map(type='json'))\ndefine stream ResponseStream(name string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/IBMMessageQueue/","title":"Consuming Messages from IBM Message Queues","text":""},{"location":"samples/IBMMessageQueue/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to consume events from IBM Message Queue and publish messages in to a IBM Queue</p>"},{"location":"samples/IBMMessageQueue/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that there is a running IBM MQ instance.</li> <li>Create a queue manager named ESBQManager, Queue named  Queue1 and channel named Channel1</li> <li>Download com.ibm.mq.allclient_9.0.5.0_1.0.0.jar and javax.jms-api-2.0.1.jar and copy to /lib directory."},{"location":"samples/IBMMessageQueue/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>IBMMessageQueueSample.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/IBMMessageQueue/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -&gt; 'Single Simulation' -&gt; Select 'IBMMessageQueueSample' as 'Siddhi App Name' -&gt; Select 'SweetProductionSinkStream' as 'Stream Name' -&gt; Provide attribute values -&gt; Send</li> </ol>"},{"location":"samples/IBMMessageQueue/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the console.</p> <pre><code>@App:name(\"IBMMessageQueueSample\")\n\n@App:description('Consume event from IBM Message Queue')\n\n\n\n\n@source(type='ibmmq',\ndestination.name='Queue1',\nhost='192.168.56.3',\nport='1414',\nchannel='Channel1',\nqueue.manager = 'ESBQManager',\nusername = 'mqm',\npassword = '1920',\n@map(type='xml'))\ndefine stream SweetProductionSourceStream(name string);\n\n@sink(type='ibmmq',\ndestination.name='Queue1',\nhost='192.168.56.3',\nport='1414',\nchannel='Channel1',\nqueue.manager = 'ESBQManager',\nusername = 'mqm',\npassword = '1920',\n@map(type='xml'))\ndefine stream SweetProductionSinkStream(name string);\n\n@sink(type='log')\ndefine stream logStream(name string);\n\nfrom SweetProductionSourceStream\nselect *\ninsert into logStream;\n</code></pre>"},{"location":"samples/JoinWithStoredData/","title":"Joining Streaming Data with Stored Data in RDBMS","text":""},{"location":"samples/JoinWithStoredData/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to perform join on streaming data with the data stored in RDBMS. The sample depicts a scenario, where a transaction by a credit card with which fraudulent activity has been previously done. The credit card numbers, which were noted for fraudulent activities are stored in an RDBMS table.</p>"},{"location":"samples/JoinWithStoredData/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that MySQL is installed on your machine</li> <li>Create a database named 'fraudDB' in MySQL. This database is referred to with 'jdbc:mysql://localhost:3306/fraudDB' url.</li> <li>Create a table named 'FraudTable': CREATE TABLE FraudTable (creditCardNo VARCHAR(20));</li> <li>Insert some values to the table : INSERT INTO FraudTable VALUES (\"143-90099-23433\");</li> <li>In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials</li> <li>Save this sample</li> </ol>"},{"location":"samples/JoinWithStoredData/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>JoinWithStoredData.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/JoinWithStoredData/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -&gt; 'Single Simulation' -&gt; Select 'JoinWithStoredData' as 'Siddhi App Name' -&gt; Select 'TradeStream' as 'Stream Name' -&gt; Provide attribute values -&gt; Send</li> <li>Send at-least one event with the single event simulator, where the creditCardNo matches a creditCardNo value in the data we previously inserted to the FraudTable. This would satisfy the 'on' condition of our join query</li> </ol>"},{"location":"samples/JoinWithStoredData/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output for suspicious trades on the console.</p> <pre><code>@App:name(\"JoinWithStoredData\")\n\n@App:description('Join streaming data with data stored in an RDBMS table')\n\n\n\n@Store(type=\"rdbms\",\njdbc.url=\"jdbc:mysql://localhost:3306/fraudDB\",\nusername=\"root\",\npassword=\"root\" ,\njdbc.driver.name=\"com.mysql.jdbc.Driver\")\n@PrimaryKey(\"creditCardNo\")\ndefine table FraudTable (creditCardNo string);\n\ndefine stream TradeStream(creditCardNo string, trader string, tradeInfo string);\n\n@sink(type='log')\ndefine stream SuspiciousTradeStream(creditCardNo string, suspiciousTrader string, suspiciousInfoTrade string);\n\n--Perform a join on credit card number, to capture transactions with credit cards that have previously been used for fraudulent activity\n@info(name='query1')\nfrom TradeStream as t join FraudTable as f\non t.creditCardNo == f.creditCardNo\nselect t.creditCardNo, t.trader as suspiciousTrader, t.tradeInfo as suspiciousInfoTrade\ninsert into SuspiciousTradeStream;\n</code></pre>"},{"location":"samples/MapExtensionSample/","title":"Inserting and Accessing Data in a Map","text":""},{"location":"samples/MapExtensionSample/#purpose","title":"Purpose:","text":"<p>This function creates a map and added values and checks whether values are available</p>"},{"location":"samples/MapExtensionSample/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Save this sample. If there is no syntax error, the following messages would be shown on the console<ul> <li>Siddhi App MapExtensionSample successfully deployed.</li> </ul> </li> </ul>"},{"location":"samples/MapExtensionSample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>MapExtensionSample.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/MapExtensionSample/#testing-the-sample","title":"Testing the Sample:","text":"<p>You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows:     * Siddhi App Name  : MapExtensionSample     * Stream Name     : SweetProductionStream 3. Enter following values in the fields and send     * name: chocolate cake     * amount: 50.50</p>"},{"location":"samples/MapExtensionSample/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MapExtensionSample: Event: , StreamEvent{ timestamp=1513384974698, beforeWindowData=null, onAfterWindowData=null, outputData=[true, false], type=CURRENT, next=null}</p> <pre><code>@APP:name(\"MapExtensionSample\")\n\n@App:description('Insert values into a map and access')\n\n\n@sink(type='log')\ndefine stream CheckedMapStream(isMap1 bool, isMap2 bool);\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@info(name = 'query1')\nfrom SweetProductionStream\nselect name, amount, map:create() as tmpMap\ninsert into tmpStream;\n\n@info(name = 'query2')\nfrom tmpStream  select name, amount, map:put(tmpMap,name,amount) as map1\ninsert into outputStream;\n\n@info(name = 'query3')\nfrom outputStream\nselect map:isMap(map1) as isMap1, map:isMap(name) as isMap2\ninsert into CheckedMapStream;\n</code></pre>"},{"location":"samples/MathExtensionSample/","title":"Rounding up Amounts via the Math Function","text":""},{"location":"samples/MathExtensionSample/#purpose","title":"Purpose:","text":"<p>This function returns the smallest (closest to negative infinity) double value that is greater than or equal to the  p1 argument, and is equal to a mathematical integer. This function wraps thejava.lang.Math.ceil() method.</p>"},{"location":"samples/MathExtensionSample/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Save this sample. If there is no syntax error, the following messages would be shown on the console<ul> <li>Siddhi App MathExtensionSample successfully deployed.</li> </ul> </li> </ul>"},{"location":"samples/MathExtensionSample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>MathExtensionSample.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/MathExtensionSample/#testing-the-sample","title":"Testing the Sample:","text":"<p>You can publish data event to the file, through event simulator 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows:     * Siddhi App Name  : MathExtensionSample     * Stream Name      : SweetProductionStream 3. Enter following values in the fields and send     * name: chocolate cake     * amount: 50.50 4. Enter following values in the fields and send     * name: coffee cake     * amount: 50.30</p>"},{"location":"samples/MathExtensionSample/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console.\\</p> <p>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MathExtensionSample: Event :, StreamEvent{ timestamp=1513381581963, beforeWindowData=null, onAfterWindowData=null, outputData=[chocolate cake, 51.0], type=CURRENT, next=null}\\</p> <p>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - MathExtensionSample: Event :, StreamEvent{ timestamp=1513381917721, beforeWindowData=null, onAfterWindowData=null, outputData=[chocolate cake, 51.0], type=CURRENT, next=null}</p> <pre><code>@App:name(\"MathExtensionSample\")\n\n@App:description('Rounds up the sweets amount')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream AmountCorrectionStream(name string, amount double);\n\nfrom SweetProductionStream\nselect name, math:ceil(amount) as amount\ninsert into AmountCorrectionStream;\n</code></pre>"},{"location":"samples/PatternMatching/","title":"Identifying Event Patterns Based On Order of Event Arrival","text":""},{"location":"samples/PatternMatching/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to detect patterns with Siddhi pattern concept. In the sample we capture a pattern where the temperature of a room increases by 5 degrees within 2 minutes</p>"},{"location":"samples/PatternMatching/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample</li> </ol>"},{"location":"samples/PatternMatching/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>PatternMatching.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/PatternMatching/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start.</p>"},{"location":"samples/PatternMatching/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -&gt; 'Single Simulation' -&gt; Select 'PatternMatching' as 'Siddhi App Name' -&gt; Select 'RoomTemperatureStream' as 'Stream Name' -&gt; Provide attribute values -&gt; Send</li> <li>To generate an alert, send one event, followed by another event (within 2 mins) where the temperature of the second event shows an increment by 5 degrees or more (eg. Temperature of event 1 = 17.0, Temperature of event 2 = 30.0). Note that these two events may or may not be consecutive.</li> </ol>"},{"location":"samples/PatternMatching/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the console. Output will be shown if the second temperature input is incremented by 5 degrees or more.</p>"},{"location":"samples/PatternMatching/#note","title":"Note:","text":"<p>Stop this Siddhi application, once you are done with the execution</p> <pre><code>@App:name(\"PatternMatching\")\n\n@App:description('Identify event patterns based on the order of event arrival')\n\n\ndefine stream RoomTemperatureStream(roomNo string, temp double);\n\n@sink(type='log')\ndefine stream RoomTemperatureAlertStream(roomNo string, initialTemp double, finalTemp double);\n\n--Capture a pattern where the temperature of a room increases by 5 degrees within 2 minutes\n@info(name='query1')\nfrom every( e1 = RoomTemperatureStream ) -&gt; e2 = RoomTemperatureStream [e1.roomNo == roomNo and (e1.temp + 5.0) &lt;= temp]\nwithin 2 min\nselect e1.roomNo, e1.temp as initialTemp, e2.temp as finalTemp\ninsert into RoomTemperatureAlertStream;\n</code></pre>"},{"location":"samples/PmmlModelProcessor/","title":"Making Predictions via PMML Model","text":""},{"location":"samples/PmmlModelProcessor/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure a pretrained PMML model to predict required raw materials from sweet production events and view the output on the console.</p>"},{"location":"samples/PmmlModelProcessor/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Download siddhi-gpl-execution-pmml-x.x.x.jar from the following link and copy the jar to  {WSO2SIHome}/lib  http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/pmml/siddhi-gpl-execution-pmml/</li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:<ul> <li>Siddhi App PmmlModelProcessor successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PmmlModelProcessor/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Replace  with the SweetProductionRatePrediction.pmml file's absolute path - {WSO2SIHome}/samples/artifacts/PMMLModelProcessorSample/SweetProductionRatePrediction.pmml <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.<ul> <li>PmmlModelProcessor.siddhi - Started Successfully!</li> </ul> </li>"},{"location":"samples/PmmlModelProcessor/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start.     * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop')     * Start the application and check whether the specified events from the jms provider appear on the console.</p>"},{"location":"samples/PmmlModelProcessor/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods. * You may send events to SweetProductionStream, via event simulator:     1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.     2. In the Single Simulation tab of the panel, specify the values as follows:         * Siddhi App Name  : PmmlModelProcessor         * Stream Name     : SweetProductionStream     3. In the name and amount fields, enter 'candy', 20, 23 respectively and then click Send to send the event.     4. Send more events as desired.</p> <ul> <li> <p>Send events to the http endpoint defined by 'publish.url' in the Sink configuration through the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PmmlModelProcessor\",\"data\": ['candy', 20, 23]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> </ul>"},{"location":"samples/PmmlModelProcessor/#notes_1","title":"Notes:","text":"<ul> <li>Accepted values for the 'name' field should be either candy, caramel-bar, peanut-butter-cup or truffle for the specific pretrained PMML model.</li> </ul>"},{"location":"samples/PmmlModelProcessor/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the prediction as output on the in the console.</p>"},{"location":"samples/PmmlModelProcessor/#note","title":"Note:","text":"<p>Stop this Siddhi application, once you are done with the execution.</p> <p>-- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with SP editor.</p> <pre><code>@App:name(\"PmmlModelProcessor\")\n@App:description('Use a pretrained PMML model to predict required raw materials. View the output on the console.')\n@Source(type = 'tcp', context='SweetProductionStream',\n@map(type='binary'))\ndefine stream SweetProductionStream (name string, currentHourAmount  double, previousHourAmount double );\n\n@sink(type='log')\ndefine stream PredictionStream (name string, currentHourAmount double, previousHourAmount double, Predicted_nextHourAmount string);\n\nfrom SweetProductionStream#pmml:predict('&lt;SweetProductionRatePrediction_model_path&gt;')\nselect *\ninsert into PredictionStream;\n</code></pre>"},{"location":"samples/PublishEmailInTextFormat/","title":"Publishing Text Events via Email","text":""},{"location":"samples/PublishEmailInTextFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to use siddhi-io-email for publishing events to files.</p>"},{"location":"samples/PublishEmailInTextFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Replace the Sink configuration values for following options.<ul> <li>username: senders username (Ex:- 'example123')</li> <li>address: senders address (Ex:- 'example123@wso2.com')</li> <li>password: senders password</li> <li>to: receivers address (Ex:- 'example987@wso2.com')</li> <li>subject: subject of the email</li> </ul> </li> <li>you need to enable access to \"less secure apps\" in the sender's gmail account via \"https://myaccount.google.com/lesssecureapps\" link</li> <li>Save this sample</li> </ol>"},{"location":"samples/PublishEmailInTextFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>PublishEmailInTextFormat.siddhi - Started Successfully!.</li> </ul> </li> </ol>"},{"location":"samples/PublishEmailInTextFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Click on 'Event Simulator' (double arrows on left tab)</li> <li>Click 'Single Simulation' (this will be already selected)</li> <li>Select PublishEmailInTextFormat as 'Siddhi App Name'</li> <li>Select SweetProductionStream as 'StreamName'</li> <li>Provide attribute values<ul> <li>name: chocolate cake</li> <li>amount: 10.10</li> </ul> </li> <li>Click on the start button (Arrow symbol) next to the newly created simulator</li> </ol>"},{"location":"samples/PublishEmailInTextFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>Check the receiver gmail inbox (The gmail referred to in 'to' Sink configuration) to see the alert similar to the following.</p> <p>Subject: Test Siddhi-io-email-{{ name }} Content: name:\"chocolate cake\", hourlyTotal:10.1, currentHour: <pre><code>@App:name(\"PublishEmailInTextFormat\")\n@APP:description(\"Demonstrates how to use siddhi-io-email for publishing events to files.\")\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='email', @map(type='text') ,\nusername ='&lt;senders email user name&gt;',\naddress ='&lt;senders email address&gt;',\npassword= '&lt;senders email password&gt;',\nsubject='Test Siddhi-io-email-{{ name }}', to='&lt;receivers email address&gt;',\nport = '465',\nhost = 'smtp.gmail.com',\nssl.enable = 'true',\nauth = 'true')                define stream LowProductionAlertStream(name string, hourlyTotal double, currentHour  double);\n\n@sink(type='log')\ndefine stream EmailLogStream(name string, hourlyTotal double, currentHour  double);\n\nfrom SweetProductionStream#window.time(1 min)\nselect name, sum(amount) as hourlyTotal,\nconvert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour\ninsert into LowProductionAlertStream;\n\nfrom LowProductionAlertStream\ninsert into EmailLogStream;\n</code></pre>"},{"location":"samples/PublishEmailInXmlFormat/","title":"Publishing Emails in XML Format","text":""},{"location":"samples/PublishEmailInXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to use siddhi-io-email for publishing events to files.</p>"},{"location":"samples/PublishEmailInXmlFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Replace the Sink configuration values for following options.<ul> <li>username: senders username (Ex:- 'example123')</li> <li>address: senders address (Ex:- 'example123@wso2.com')</li> <li>password: senders password</li> <li>to: receivers address (Ex:- 'example987@wso2.com')</li> <li>subject: subject of the email</li> </ul> </li> <li>You need to enable access to \"less secure apps\" in the sender's gmail account via \"https://myaccount.google.com/lesssecureapps\" link</li> <li>Save this sample</li> </ol>"},{"location":"samples/PublishEmailInXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console<ul> <li>PublishEmailInXmlFormat.siddhi - Started Successfully!.</li> </ul> </li> </ol>"},{"location":"samples/PublishEmailInXmlFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Click on 'Event Simulator' (double arrows on left tab)</li> <li>Click 'Single Simulation' (this will be already selected)</li> <li>Select PublishEmailInXmlFormat as 'Siddhi App Name'</li> <li>Select SweetProductionStream as 'StreamName'</li> <li>Provide attribute values<ul> <li>name: chocolate cake</li> <li>amount: 10.10</li> </ul> </li> <li>Click on the start button (Arrow symbol) next to the newly created simulator</li> </ol>"},{"location":"samples/PublishEmailInXmlFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>Check the receiver gmail inbox (The gmail referred to in 'to' Sink configuration) to see the alert as follows.</p> <pre><code>Subject: &lt;subject of the email&gt;\nContent: &lt;events&gt;&lt;event&gt;&lt;name&gt;chocolate cake&lt;/name&gt;&lt;hourlyTotal&gt;10.1&lt;/hourlyTotal&gt;&lt;currentHour&gt;0.0&lt;/currentHour&gt;&lt;/event&gt;&lt;/events&gt;</code></pre>"},{"location":"samples/PublishEmailInXmlFormat/#note","title":"Note:","text":"<p>current hour depends on the system's timestamp</p> <pre><code>@App:name(\"PublishEmailInXmlFormat\")\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='email', @map(type='xml') ,\nusername ='&lt;senders email user name&gt;',\naddress ='&lt;senders email address&gt;',\npassword= '&lt;senders email password&gt;',\nsubject='Test Siddhi-io-email-{{ name }}', to='&lt;receivers email address&gt;',\nport = '465',\nhost = 'smtp.gmail.com',\nssl.enable = 'true',\nauth = 'true')                define stream LowProductionAlertStream(name string, hourlyTotal double, currentHour  double);\n\n@sink(type='log')\ndefine stream EmailLogStream(name string, hourlyTotal double, currentHour  double);\n\nfrom SweetProductionStream#window.time(1 min)\nselect name, sum(amount) as hourlyTotal,\nconvert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour\ninsert into LowProductionAlertStream;\n\nfrom LowProductionAlertStream\ninsert into EmailLogStream;\n</code></pre>"},{"location":"samples/PublishEventsToFile/","title":"Publishing JSON Events to Files","text":""},{"location":"samples/PublishEventsToFile/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to publish data events in to files in Json format.</p>"},{"location":"samples/PublishEventsToFile/#prerequisites","title":"Prerequisites:","text":"<ul> <li> <p>Edit the uri '{WSO2SIHome}/samples/artifacts/PublishEventsToFile/files/sink/{{ name }}.txt' by replacing {WSO2SIHome} with the absolute path of your WSO2SP home directory. You can also change the path for 'file.uri' in the sink, if you want to publish your event file to a different location.</p> </li> <li> <p>Save this sample. If there is no syntax error, the following messages would be shown on the console:</p> <ul> <li>Siddhi App PublishEventsToFile successfully deployed.</li> </ul> </li> </ul>"},{"location":"samples/PublishEventsToFile/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishEventsToFile.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/PublishEventsToFile/#testing-the-sample","title":"Testing the Sample:","text":"<ul> <li>You can publish data events to the file through the event simulator:<ol> <li>Click on 'Event Simulator' (double arrows on left tab)</li> <li>Click 'Single Simulation' (this will be already selected)</li> <li>Select PublishEventsToFile as 'Siddhi App Name'</li> <li>Select SweetProductionStream as 'StreamName'</li> <li>Click on the start button (Arrow symbol) next to the newly created simulator</li> <li>Provide attribute values<ul> <li>name: toffees</li> <li>amount: 66.71</li> </ul> </li> <li>Send event</li> <li>Provide attribute values<ul> <li>name: toffees</li> <li>amount: 200</li> </ul> </li> <li>Send event</li> </ol> </li> </ul>"},{"location":"samples/PublishEventsToFile/#viewing-the-results","title":"Viewing the Results:","text":"<p>Navigate to the path defined by file.uri ({WSO2SIHome}/samples/artifacts/0038/files/sink), where you can see a .txt file named after the event (e.g., toffees.txt) and open it.</p> <ul> <li>You can see the data events that you sent: {\"event\":{\"name\":\"toffees\",\"amount\":66.71}} {\"event\":{\"name\":\"toffees\",\"amount\":200.0}}</li> </ul> <pre><code>@App:name(\"PublishEventsToFile\")\n\n@App:description('Publish data events processed within Siddhi to files in Json format.')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='file', @map(type='json'), file.uri='{WSO2SIHome}/samples/artifacts/PublishEventsToFile/files/sink/{{ name }}.txt')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@info(name='query1') from SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/","title":"Publishing Events to a Google Pub/Sub Topic","text":""},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling using googlepubsub sink in Siddhi to publish events. Events which are in TEXT format are published to a googlepubsub topic.</p>"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Create a Google Cloud Platform account.</li> <li>Sign in to Google Account and set up a GCP Console project and enable the API.</li> <li>Create a service account and download a private key as JSON.</li> <li>Place your json file in any system property.</li> <li>Save the sample.</li> <li>If there is no syntax error, the following message is shown on the console:<ul> <li>Siddhi App SendGooglePubSubMessage successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'. If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>SendGooglePubSubMessage.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Send events through one or more of the following methods.<ul> <li>You may send events to googlepubsub sink, via event simulator<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name  : SendGooglePubSubMessage</li> <li>Stream Name      : FooStream</li> </ul> </li> <li>In the message field, enter the following and then click Send to send the event.<ul> <li>message: Hello</li> </ul> </li> <li>Send some more events.</li> </ol> </li> </ul> </li> </ol>"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#viewing-the-results","title":"Viewing the Results:","text":"<ul> <li>See the output on the terminal: 2019-03-14_12-50-21_966] INFO {io.siddhi.core.stream.output.sink.LogSink} - SendEvent : BarStream : Event{timestamp=1552548021825, data=[Hello], isExpired=false}</li> </ul>"},{"location":"samples/PublishGooglePubSubMessagesInTextFormat/#notes","title":"Notes:","text":"<p>Make sure the the credential file is correct and user have write access to make api calls. Stop this Siddhi application.</p> <pre><code>@App:name(\"SendGooglePubSubMessage\")\n\n@App:description('Send events to a Google Pub/Sub Topic')\n\n\n\n@sink(type='googlepubsub', topic.id = 'topic75',\ncredential.path = '/../sp.json',\nproject.id = 'sp-path-1547649404768',\n@map(type='text'))\ndefine stream FooStream (message string);\n\n@sink(type = 'log')\ndefine stream BarStream(message string);\n\n@info(name = 'query1')\nfrom FooStream\nselect message insert into BarStream;\n</code></pre>"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/","title":"Sending Custom JSON Events via HTTP","text":""},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#purpose","title":"Purpose","text":"<p>This application demonstrates how to configure the Streaming Integrator to send sweet production events via HTTP transport in JSON format using custom mapping.</p> <p>Before executing the sample:</p> <p>Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. <code>- Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed.</code></p>"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#executing-the-sample","title":"Executing the Sample","text":"<ol> <li> <p>Start the Siddhi application by clicking Run =&gt; Run.</p> </li> <li> <p>If the Siddhi application starts successfully, the following messages are shown on the console.</p> <ul> <li> <p><code>PublishHTTPInJsonFormatWithCustomMapping.siddhi - Started Successfully!</code></p> </li> <li> <p><code>'http' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:8080/abc</code></p> </li> </ul> </li> </ol>"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#testing-the-sample","title":"Testing the Sample","text":"<ol> <li> <p>Open a terminal and navigate to <code>&lt;SI_HOME&gt;/samples/sample-clients/http-server</code>.  Then run the <code>ant</code> command without any arguments.</p> </li> <li> <p>To send events, follow one or more of the following methods.</p> <ul> <li> <p>Send events with http server through the event simulator:</p> <p>a. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</p> <p>b. In the Single Simulation tab of the panel, specify the values as follows.</p> <pre><code>- Siddhi App Name`: `PublishHttpInJsonFormatWithCustomMapping\n- Stream Name`: `SweetProductionStream</code></pre> <p>c. In the id and amount fields, enter <code>toffees</code> and <code>123.5</code> respectively and then click Send to send the event.</p> <p>d.  Send more events as required.</p> </li> <li> <p>Send events to the http endpoint defined by 'publish.url' in the Sink configuration through the curl command:</p> <p>a. Open a new terminal and issue the following command</p> <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHTTPInJsonFormatWithCustomMapping\",\"data\": ['toffees', 123.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</code></pre> <p>b. If there is no error, the following message appears in the terminal.</p> <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</code></pre> </li> <li> <p>Publish events with Postman:</p> <p>a. Install the 'Postman' application from Chrome web store.</p> <p>b. Launch the application.</p> <p>c. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows.</p> <pre><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHTTPInJsonFormatWithCustomMapping\",\"data\": ['toffees', 75.6]}</code></pre> <p>d. Click Send. If there is no error, the following messages appear in the terminal.</p> <pre><code>* \"status\": \"OK\"\n\n* \"message\": \"Single Event simulation started successfully\"</code></pre> </li> </ul> </li> </ol>"},{"location":"samples/PublishHTTPInJsonFormatWithCustomMapping/#viewing-the-results","title":"Viewing the Results","text":"<p>See the output on the terminal:</p> <pre><code>[java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"id\":\"toffees\",\"amount\":123.5}}\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"id\":\"toffees\",\"amount\":123.5}} ,\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length]\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [43]]\n</code></pre> <p>Note</p> <p>If the message \"'HTTP' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:8080/abc\" does not appear, it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following:     1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop').     2. In this Siddhi application's source configuration, change the port from 8080 to an unused port.     3. Start the application and check whether the specified messages appear on the console.</p> Click here to view the complete sample Siddhi application<pre><code>@App:name(\"PublishHTTPInJsonFormatWithCustomMapping\")\n\n@App:description('Send events via HTTP transport in JSON format with custom mapping.')\n\n\ndefine stream SweetProductionStream (id string, amount double);\n\n@sink(type='http', publisher.url='http://localhost:8080/abc',\n@map(type='json' , @payload( \"{'name': {{ id }}, 'amount': {{ amount }}}\")))\ndefine stream LowProductionAlertStream (id string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishHl7InER7Format/","title":"Publishing ER7 Events via HL7","text":""},{"location":"samples/PublishHl7InER7Format/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send Hl7 events in ER7 format via MLLP protocol and log the events in hl7Stream and the acknowledgement message to the output console.</p>"},{"location":"samples/PublishHl7InER7Format/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html)</li> <li>Save this sample. If there is no syntax error, the following message is shown on the console:<ul> <li>Siddhi App PublishHl7InER7Format successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishHl7InER7Format/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>In the HAPI testpanel create a receiving connection with port that provided in the siddhi app.</li> <li>Start the listener.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHl7InER7Format.siddhi - Started Successfully!</li> <li>'Hl7' sink at 'hl7Stream' stream successfully connected to 'localhost:4000'.</li> <li>Executing HL7Sender: HOST: localhost, PORT: 4000 for stream PublishHl7InER7Format:hl7Stream.</li> </ul> </li> </ol>"},{"location":"samples/PublishHl7InER7Format/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name  : PublishHl7InER7Format</li> <li>Stream Name      : er7Stream</li> </ul> </li> <li>In the payload, enter 'MSH|~\\&amp;|||||20190211145413.131+0530||ADTA01|10601|T|2.3' and then click Send to send the event.</li> <li>Send more events as desired.</li> </ol> <pre><code>@App:name('PublishHl7InER7Format')\n@App:description('This publishes the HL7 messages in ER7 format, receives and logs the acknowledgement message in the console using MLLP protocol and custom text mapping.')\n\n\ndefine stream er7Stream (payload string);\n\n@sink(type = 'hl7', uri='localhost:4000', tls.enabled='false', hl7.encoding='er7', @map(type='text', @payload('{{ payload }}')))\ndefine stream hl7Stream (payload String);\n\n@info(name='query1')\nfrom er7Stream\nselect payload\ninsert into hl7Stream;\n</code></pre>"},{"location":"samples/PublishHl7InXmlFormat/","title":"Publishing XML messages via HL7","text":""},{"location":"samples/PublishHl7InXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send Hl7 events in XML format via MLLP protocol and log the events in hl7Stream and the acknowledgment message to the output console.</p>"},{"location":"samples/PublishHl7InXmlFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html)</li> <li>Save this sample. If there is no syntax error, the following message is shown on the console:<ul> <li>Siddhi App PublishHl7InXmlFormat successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishHl7InXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>In the HAPI testpanel create a receiving connection with port that provided in the siddhi app.</li> <li>Start the listener.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHl7InXmlFormat.siddhi - Started Successfully!</li> <li>'Hl7' sink at 'hl7Stream' stream successfully connected to 'localhost:4000'.</li> </ul> </li> </ol>"},{"location":"samples/PublishHl7InXmlFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specifiy the values as follows:<ul> <li>Siddhi App Name   :   PublishHl7InXmlFormat</li> <li>Stream Name   :   xmlStream</li> </ul> </li> <li>In the MSH1, MSH2, MSH3HD1,MSH4HD1, MSH5HD1, MSH6HD1, MSH7, MSH8, CM_MSG1, CM_MSG2, MSH10, MSH11, MSH12 fields enter '|', '^~\\&amp;', 'sendingSystemA', 'senderFacilityA', 'receivingSystemB' , 'receivingFacilityB', '20080925161613', ' ', 'ADT', 'A01', 'S123456789', 'P', '2.3' respectively and then click Send to send the event.</li> <li>Send more events as desired.</li> </ol> <pre><code>@App:name('PublishHl7InXmlFormat')\n@App:description('This publishes the HL7 messages in XML format, receives and logs the acknowledgement message in the console using MLLP protocol and custom xml mapping.')\n\n\ndefine stream xmlStream(MSH1 string, MSH2 string, MSH3HD1 string, MSH4HD1 string, MSH5HD1 string, MSH6HD1 string, MSH7 string, MSH8 string, CM_MSG1 string, CM_MSG2 string,MSH10 string,MSH11 string, MSH12 string);\n\n@sink(type = 'hl7', uri = 'localhost:4000', hl7.encoding = 'xml', @map(type = 'xml', enclosing.element=\"&lt;ADT_A01  xmlns='urn:hl7-org:v2xml'&gt;\", @payload('&lt;MSH&gt;&lt;MSH.1&gt;{{ MSH1 }}&lt;/MSH.1&gt;&lt;MSH.2&gt;{{ MSH2 }}&lt;/MSH.2&gt;&lt;MSH.3&gt;&lt;HD.1&gt;{{ MSH3HD1 }}&lt;/HD.1&gt;&lt;/MSH.3&gt;&lt;MSH.4&gt;&lt;HD.1&gt;{{ MSH4HD1 }}&lt;/HD.1&gt;&lt;/MSH.4&gt;&lt;MSH.5&gt;&lt;HD.1&gt;{{ MSH5HD1 }}&lt;/HD.1&gt;&lt;/MSH.5&gt;&lt;MSH.6&gt;&lt;HD.1&gt;{{ MSH6HD1 }}&lt;/HD.1&gt;&lt;/MSH.6&gt;&lt;MSH.7&gt;{{ MSH7 }}&lt;/MSH.7&gt;&lt;MSH.8&gt;{{ MSH8 }}&lt;/MSH.8&gt;&lt;MSH.9&gt;&lt;CM_MSG.1&gt;{{ CM_MSG1 }}&lt;/CM_MSG.1&gt;&lt;CM_MSG.2&gt;{{ CM_MSG2 }}&lt;/CM_MSG.2&gt;&lt;/MSH.9&gt;&lt;MSH.10&gt;{{ MSH10 }}&lt;/MSH.10&gt;&lt;MSH.11&gt;{{ MSH11 }}&lt;/MSH.11&gt;&lt;MSH.12&gt;{{ MSH12 }}&lt;/MSH.12&gt;&lt;/MSH&gt;')))\ndefine stream hl7Stream(MSH1 string, MSH2 string, MSH3HD1 string, MSH4HD1 string, MSH5HD1 string, MSH6HD1 string, MSH7 string, MSH8 string, CM_MSG1 string, CM_MSG2 string,MSH10 string,MSH11 string, MSH12 string);\n\n@info(name='query1')\nfrom xmlStream\nselect *\ninsert into hl7Stream;\n</code></pre>"},{"location":"samples/PublishHttpInJsonFormat/","title":"Publishing JSON Events via HTTP","text":""},{"location":"samples/PublishHttpInJsonFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in JSON default format and log the events in LowProductionAlertStream to the output console.</p>"},{"location":"samples/PublishHttpInJsonFormat/#prerequisites","title":"Prerequisites:","text":"<ul> <li>Save this sample. If there is no syntax error, the following message is shown on the console:<ul> <li>Siddhi App PublishHttpInJsonFormat successfully deployed. </li> </ul> </li> </ul>"},{"location":"samples/PublishHttpInJsonFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHttpInJsonFormat.siddhi - Started Successfully!</li> <li>'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpInJsonFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open a terminal and navigate to {WSO2SIHome}/samples/sample-clients/http-server and run the \"ant\" command without any arguments.</li> <li> <p>Send events through one or more of the following methods:</p> <ul> <li> <p>Send events with http server through the event simulator:</p> <ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name  : PublishHttpInJsonFormat</li> <li>Stream Name     : SweetProductionStream</li> </ul> </li> <li>In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event.</li> <li>Send some more events as desired.</li> </ol> </li> <li> <p>Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInJsonFormat\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install the 'Postman' application from the Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> </ul> </li> </ol>"},{"location":"samples/PublishHttpInJsonFormat/#viewing-the-results","title":"Viewing the Results:","text":"<ul> <li>See the output on the terminal: <pre><code>[java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}}\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Events: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}} ,\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set: [Http_method, Content-type, Content-length]\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set: [[POST], [application/json], [42]]\n</code></pre></li> </ul>"},{"location":"samples/PublishHttpInJsonFormat/#notes","title":"Notes:","text":"<p>If you get the message \"LowProductionAlertStream' stream could not connect to 'localhost:8080\", it could be due to port 8080 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file. 3. Start the application and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishHttpInJsonFormat\")\n\n@App:description('Send events via HTTP transport using JSON format')\n\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http', publisher.url='http://localhost:8080/abc',\n@map(type='json'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@info(name='query1') from SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishHttpInXmlFormat/","title":"Publishing XML Events via HTTP","text":""},{"location":"samples/PublishHttpInXmlFormat/#purpose","title":"Purpose","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via HTTP transport in XML default format and log the events in LowProductionAlertStream to the output console.</p> <p>Before executing the sample:</p> <p>Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. <code>- Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed.</code></p>"},{"location":"samples/PublishHttpInXmlFormat/#executing-the-sample","title":"Executing the Sample","text":"<ol> <li> <p>Start the Siddhi application by clicking Run =&gt; Run.</p> </li> <li> <p>If the Siddhi application starts successfully, the following messages are shown on the console.</p> <ul> <li> <p><code>PublishHttpInXmlFormat.siddhi - Started Successfully!</code></p> </li> <li> <p><code>'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:8080/abc'.</code></p> </li> </ul> </li> </ol>"},{"location":"samples/PublishHttpInXmlFormat/#testing-the-sample","title":"Testing the Sample","text":"<ol> <li> <p>Open a terminal and navigate to <code>&lt;SI_HOME&gt;/samples/sample-clients/http-server</code>.  Then run the <code>ant</code> command without any arguments.</p> </li> <li> <p>Send events using one or more of the following methods:</p> <ul> <li> <p>Send events with http server through the event simulator:</p> <p>a. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</p> <p>b. In the Single Simulation tab of the panel, specify the values as follows:</p> <pre><code>- Siddhi App Name: PublishHttpInXmlFormat\n- Stream Name: SweetProductionStream</code></pre> <p>c. In the name and amount fields, enter <code>toffees</code> and <code>75.6</code> respectively and then click Send to send the event.</p> <p>d. Send more events as required.</p> </li> <li> <p>Send events to the HTTP endpoint defined by <code>publish.url</code> in the Sink configuration using the curl command:</p> <p>a. Open a new terminal and issue the following command.</p> <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormat\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</code></pre> <p>b. If there is no error, the following message appears in the terminal.</p> <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</code></pre> </li> <li> <p>Publish events with Postman:</p> <p>a. Install the 'Postman' application from Chrome web store.</p> <p>b. Launch the application.</p> <p>c. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to <code>text/plain</code> and set the request body in text as follows.</p> <pre><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormat\",\"data\": ['toffees', 75.6]}</code></pre> <p>d. Click Send. If there is no error, the following messages appear in the console.     - <code>\"status\": \"OK\",</code>     - <code>\"message\": \"Single Event simulation started successfully\"</code></p> </li> </ul> </li> </ol>"},{"location":"samples/PublishHttpInXmlFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal.</p> <p>Note</p> <p>If the message <code>LowProductionAlertStream</code> stream could not connect to <code>localhost:8080</code>, it could be because port <code>8080</code> defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following:     1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop').     2. Change the port from 8080 to an unused port in this Siddhi application's source configuration and in the http-server file.     3. Start the application and check whether the expected output appears in the console.</p> Click here to view the complete sample Siddhi application<pre><code>@App:name(\"PublishHttpInXmlFormat\")\n\n@App:description('Send events via HTTP transport using XML format')\n\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http', publisher.url='http://localhost:8080/abc',\n@map(type='xml'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/","title":"Publishing Custom XML Events via HTTP","text":""},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#purpose","title":"Purpose","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator to send sweet production events via HTTP transport in XML format using custom mapping. Map the input events($.item.id) to stream events(name) and log the events in the <code>LowProducitonAlertStream</code> stream on the output console.</p> <p>Before executing the sample:</p> <p>Save the sample Siddhi application. Is there is no syntax error, the following message appears in the console. <code>- Siddhi App PublishHTTPInJsonFormatWithCustomMapping successfully deployed.</code></p>"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#executing-the-sample","title":"Executing the Sample","text":"<ol> <li> <p>Start the Siddhi application by clicking Run =&gt; Run.</p> </li> <li> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>PublishHttpInXmlFormatWithCustomMapping.siddhi - Started Successfully!</code></p> </li> </ol> <p>Note</p> <p>If you edit this application while it's running, save the application and then click Run =&gt; Run. If the <code>PublishHttpInXmlFormatWithCustomMapping.siddhi - Started Successfully!</code> message does not appear, it could be due to port 8080 which is defined in the Siddhi application already being used by a different program. To resolve this issue, do the following:     * Stop this Siddhi application (click 'Run' on the menu bar -&gt; 'Stop')..     * Change the port 8080 to an unused port in this Siddhi application's source configuration..     * Start the application and check whether the specified messages appear on the console.</p>"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#testing-the-sample","title":"Testing the Sample","text":"<ol> <li> <p>Open a terminal and navigate to <code>&lt;SI_HOME&gt;/samples/sample-clients/http-server</code>.  Then run the <code>ant</code> command without any arguments.</p> </li> <li> <p>Send events using one or more of the following methods:</p> <ul> <li> <p>Send events with the HTTP server through the event simulator:</p> <p>a. Open event simulator by clicking on the second icon or press Ctrl+Shift+I.</p> <p>b. In the Single Simulation tab of the panel, select values as follows:     - Siddhi App Name: <code>PublishHttpInXmlFormatWithCustomMapping</code>     - Stream Name: <code>SweetProductionStream</code></p> <p>c. In the name field and amount fields, enter <code>toffee</code> and <code>50.0</code> respectively. Then click Send to send the event.</p> <p>d. Send more events.</p> </li> <li> <p>**Send events to the HTTP endpoint defined via the <code>publish.url</code> in the Sink configuration by issuing the following CURL command:</p> <p><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpInXmlFormatWithCustomMapping\",\"data\": ['toffee', 67.43]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</code></p> <p>If there is no error, the following message appears in the terminal:</p> <p><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</code></p> </li> </ul> </li> </ol>"},{"location":"samples/PublishHttpInXmlFormatWithCustomMapping/#viewing-the-results","title":"Viewing the Results","text":"<ul> <li> <p>If you send events through the event simulator, the following output is logged. [java] [org.wso2.si.http.server.HttpServerListener]: <code>Event Name Arrived: &lt;events&gt;&lt;event&gt;&lt;name&gt;toffee&lt;/name&gt;&lt;amount&gt;50.0&lt;/amount&gt;&lt;/event&gt;&lt;/events&gt;</code> [java] [org.wso2.si.http.server.HttpServerMain]: <code>Received Event Names:&lt;events&gt;&lt;event&gt;&lt;name&gt;toffee&lt;/name&gt;&lt;amount&gt;50.0&lt;/amount&gt;&lt;/event&gt;&lt;/events&gt; ,</code> [java] [org.wso2.si.http.server.HttpServerMain]: <code>Received Event Headers key set:[Http_method, Transfer-encoding, Content-type]</code> [java] [org.wso2.si.http.server.HttpServerMain]: <code>Received Event Headers value set:[[POST], [chunked], [application/xml]]</code></p> </li> <li> <p>If you send events through event CURL commands, the following output is logged: [java] [org.wso2.si.http.server.HttpServerListener]: <code>Event Name Arrived: &lt;events&gt;&lt;event&gt;&lt;name&gt;toffee&lt;/name&gt;&lt;amount&gt;67.43&lt;/amount&gt;&lt;/event&gt;&lt;/events&gt;</code> [java] [org.wso2.si.http.server.HttpServerMain]: <code>Received Event Names:&lt;events&gt;&lt;event&gt;&lt;name&gt;toffee&lt;/name&gt;&lt;amount&gt;67.43&lt;/amount&gt;&lt;/event&gt;&lt;/events&gt; ,</code> [java] [org.wso2.si.http.server.HttpServerMain]: <code>Received Event Headers key set:[Http_method, Transfer-encoding, Content-type]</code> [java] [org.wso2.si.http.server.HttpServerMain]: <code>Received Event Headers value set:[[POST], [chunked], [application/xml]]</code></p> </li> </ul> <p>Note</p> <p>Stop this Siddhi application once you are done with the execution.</p> Click here to view the complete sample Siddhi application<pre><code>@App:name(\"PublishHttpInXmlFormatWithCustomMapping\")\n@App:description('Send events via HTTP transport using xml formatwith custom mapping')\n\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http', publisher.url='http://localhost:8080/abc',\n@map(type='xml', @payload( \"&lt;StockData&gt;&lt;Symbol&gt;{{ name }}&lt;/Symbol&gt;&lt;Price&gt;{{ amount }}&lt;/Price&gt;&lt;/StockData&gt;\")))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishHttpOAuthRequest/","title":"Publishing HTTP Events to to an OAuth-protected Endpoint","text":""},{"location":"samples/PublishHttpOAuthRequest/#purpose","title":"Purpose:","text":"<pre><code>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected\n endpoint.</code></pre>"},{"location":"samples/PublishHttpOAuthRequest/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>Replace the Sink configuration values for following options.</p> <ul> <li>publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc')</li> <li>consumer.key  : consumer key for the http request (Ex:- 'abcdef')</li> <li>consumer.secret: consumer secret for the http request (Ex:- 'abcdef')</li> <li>token.url     : URL of the token end point (Ex:- 'https://localhost:8005/token')</li> <li>method        : method type (Eg:- POST)</li> </ul> <p>optional (You can fill this if it is different from default values) - header (Authorization header)  : access token for the API endpoint request (Ex:- 'abcdef') &lt;= by default it automatically get the access token using the password/client-credential/refresh grant. here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') &lt;= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password :  API trust store password (wso2carbon) &lt;= by default it set as wso2carbon</p> </li> <li> <p>Save this sample. If there is no syntax error, the following message is shown on the console:</p> <ul> <li>Siddhi App PublishHttpOAuthRequest successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequest/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHttpOAuthRequest.siddhi - Started Successfully!</li> <li>'Http' sink at LowProductionAlertStream stream successfully connected to 'https://localhost:8005/abc'.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequest/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods: * Send events with http server through the event simulator:     1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.     2. In the Single Simulation tab of the panel, specify the values as follows:         * Siddhi App Name  : PublishHttpOAuthRequest         * Stream Name     : SweetProductionStream     3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event.     4. Send some more events as desired.</p> <ul> <li> <p>Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequest\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install the 'Postman' application from the Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> </ul>"},{"location":"samples/PublishHttpOAuthRequest/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: <pre><code>Siddhi App test successfully deployed.\nINFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc\n[java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}}\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} ,\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length]\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]\n</code></pre></p>"},{"location":"samples/PublishHttpOAuthRequest/#notes","title":"Notes:","text":"<p>If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem,  do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application again and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishHttpOAuthRequest\")\n@App:description(\"Send HTTP events to an OAuth-protected endpoint\")\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http', method=\"xxxxx\", publisher.url='https://localhost:8005/abc',\nheaders=\"'Authorization:  Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\",\ntoken.url='https://localhost:8005/token',@map(type='json', @payload( \"{'name': {{ name }}, 'amount': {{ amount }}}\")))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishHttpOAuthRequestResponse/","title":"Publishing HTTP Events via an OAuth-protected Endpoint","text":""},{"location":"samples/PublishHttpOAuthRequestResponse/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to sending an HTTP event via an OAuth protected endpoint and catching its response.</p>"},{"location":"samples/PublishHttpOAuthRequestResponse/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>Replace the Sink configuration values for following options.</p> <ul> <li>publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc')</li> <li>consumer.key  : consumer key for the http request (Ex:- 'abcdef')</li> <li>consumer.secret: consumer secret for the http request (Ex:- 'abcdef')</li> <li>token.url     : URL of the token end point (Ex:-'https://localhost:8005/token')</li> <li>method        : method type (Eg:- POST)</li> </ul> <p>optional (You can fill this if it is different from default values ) - header (Authorization header)  : access token for the http request (Ex:- 'abcdef') &lt;= by default it automatically get the access token using the password/client-credential/refresh grant. Here the  grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') &lt;= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password :  API trust store password (wso2carbon) &lt;= by default it set as wso2carbon</p> </li> <li> <p>Save this sample. If there is no syntax error, the following message is shown on the console:</p> <ul> <li>Siddhi App PublishHttpOAuthRequestResponse successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestResponse/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHttpOAuthRequestResponse.siddhi - Started Successfully!</li> <li>'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestResponse/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods: * Send events with http server through the event simulator:     1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.     2. In the Single Simulation tab of the panel, specify the values as follows:         * Siddhi App Name  : PublishHttpOAuthRequestResponse         * Stream Name     : SweetProductionStream     3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event.     4. Send some more events as desired.</p> <ul> <li> <p>Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestResponse\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install the 'Postman' application from the Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> </ul>"},{"location":"samples/PublishHttpOAuthRequestResponse/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: <pre><code>Siddhi App test successfully deployed.\nINFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc\n[java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}}\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} ,\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length]\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]\n</code></pre></p>"},{"location":"samples/PublishHttpOAuthRequestResponse/#notes","title":"Notes:","text":"<p>If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application again and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishHttpOAuthRequestResponse\")\n@App:description(\"sending an HTTP event via an OAuth protected endpoint and catching its response.\")\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http-request', sink.id='abc', publisher.url='https://localhost:8005/abc',\nheaders=\"'Authorization:  Bearer xxxxxxxxxxxxx'\", method=\"xxxxx\", consumer.key=\"xxxxxx\",\nconsumer.secret=\"xxxxxxx\",token.url='https://localhost:8005/token', @map(type='json'),\n@payload( \"{'name': {{ name }}, 'amount': {{ amount }}}\"))\ndefine stream LowProductionAlertStream (name String, amount double);\n\n@source(type='http-response' , sink.id='abc', http.status.code='200',\n@map(type='text', regex.A='((.|\n)*)', @attributes(message='A[1]')))\ndefine stream ResponseLowProductionAlertStream(message string);\n\n@sink(type='log')\ndefine stream loggerStream(message string);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n\n@info(name='query2')\nfrom ResponseLowProductionAlertStream\nselect *\ninsert into loggerStream;\n</code></pre>"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/","title":"Publishing HTTP Events to an OAuth-protected Endpoint","text":""},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint. Here we use password grant type.</p>"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>Replace the Sink configuration values for following options.</p> <ul> <li>oauth.username: sender's oauth username (Ex:- 'username123')</li> <li>oauth.password : sender's oauth password (Ex:- 'password123')</li> <li>publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc')</li> <li>consumer.key  : consumer key for the http request (Ex:- 'abcdef')</li> <li>consumer.secret: consumer secret for the http request (Ex:- 'abcdef')</li> <li>token.url     : URL of the token end point (Ex:- 'https://localhost:8005/token')</li> <li>method        : method type (Eg:- POST)</li> </ul> <p>optional (You can fill this if it is different from default values) - header (Authorization header)  : access token for the http request (Ex:- 'abcdef') &lt;= by default it automatically get the access token using the password/client-credential/refresh grant. Here the  grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') &lt;= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password :  API trust store password (wso2carbon) &lt;= by default it set as wso2carbon</p> </li> <li> <p>Save this sample. If there is no syntax error, the following message is shown on the console:</p> <ul> <li>Siddhi App PublishHttpOAuthRequestWithOAuthUser successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHttpOAuthRequestWithOAuthUser.siddhi - Started Successfully!</li> <li>'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods: * Send events with http server through the event simulator:     1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.     2. In the Single Simulation tab of the panel, specify the values as follows:         * Siddhi App Name  : PublishHttpOAuthRequestWithOAuthUser         * Stream Name     : SweetProductionStream     3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event.     4. Send some more events as desired.</p> <ul> <li> <p>Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithOAuthUser\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install the 'Postman' application from the Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> </ul>"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: <pre><code>Siddhi App test successfully deployed.\nINFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc\n[java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}}\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} ,\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length]\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]\n</code></pre></p>"},{"location":"samples/PublishHttpOAuthRequestWithOAuthUser/#notes","title":"Notes:","text":"<p>If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application again and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishHttpOAuthRequestWithOAuthUser\")\n@App:description(\"Send HTTP events to an OAuth-protected endpoint. Here we use password grant type.\")\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='http', oauth.username=\"xxxx\", oauth.password=\"xxxx\",\npublisher.url='https://localhost:8005/abc', headers=\"'Authorization:  Bearer xxxxxxxxx'\",\nconsumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token',\nmethod=\"xxxxxx\", @map(type='json', @payload( \"{'name': {{ name }}, 'amount': {{ amount }}}\")))\ndefine stream LowProductionAlertStream ( name string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/","title":"Publishing HTTP Events to an OAuth-protected Endpoint while Using a Refresh Token Grant Type","text":""},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send HTTP events to an OAuth-protected endpoint. Here we use refresh token grant type.</p>"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>Replace the Sink configuration values for following options.</p> <ul> <li>publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc')</li> <li>consumer.key  : consumer key for the http request (Ex:- 'abcdef')</li> <li>consumer.secret: consumer secret for the http request (Ex:- 'abcdef')</li> <li>token.url     : URL of the token end point (Ex:- 'https://localhost:8005/token')</li> <li>refresh.token : Refresh token for the http request (Ex:- 'abcdef')</li> <li>method        : method type (Eg:- POST)</li> </ul> <p>optional (You can fill this if it is different from default values) - header (Authorization header)  : access token for the http request (Ex:- 'abcdef') &lt;= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') &lt;= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password :  API trust store password (wso2carbon) &lt;= by default it set as wso2carbon</p> </li> <li> <p>Save this sample. If there is no syntax error, the following message is shown on the console:</p> <ul> <li>Siddhi App PublishHttpOAuthRequestWithRefreshToken successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHttpOAuthRequestWithRefreshToken.siddhi - Started Successfully!</li> <li>'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods: * Send events with http server through the event simulator:     1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.     2. In the Single Simulation tab of the panel, specify the values as follows:         * Siddhi App Name  : PublishHttpOAuthRequestWithRefreshToken         * Stream Name     : SweetProductionStream     3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event.     4. Send some more events as desired.</p> <ul> <li> <p>Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithRefreshToken\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install the 'Postman' application from the Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> </ul>"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: <pre><code>Siddhi App test successfully deployed.\nINFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc\n[java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}}\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} ,\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length]\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]\n</code></pre></p>"},{"location":"samples/PublishHttpOAuthRequestWithRefreshToken/#notes","title":"Notes:","text":"<p>If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application again and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishHttpOAuthRequestWithRefreshToken\")\n@App:description(\"Send HTTP events to an OAuth-protected endpoint. Here we use refresh token grant type.\")\n\ndefine stream SweetProductionStream (name string, amount double);\n@sink(type='http', method=\"xxxxx\", refresh.token=\"refreshToken\", publisher.url='https://localhost:8005/abc',\nheaders=\"'Authorization:  Bearer xxxxxxxxx'\", consumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\",\ntoken.url='https://localhost:8005/token', @map(type='json'), @payload( \"{'name': {{ name }}, 'amount': {{ amount }}}\"))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/","title":"Publishing HTTP Events to to an OAuth-protected Endpoint without an Access Token","text":""},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send a HTTP events to an OAuth-protected endpoint without access token</p>"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>Replace the Sink configuration values for following options.</p> <ul> <li>publisher.url : publisher URL (Ex:- 'https://localhost:8005/abc')</li> <li>consumer.key  : consumer key for the http request (Ex:- 'abcdef')</li> <li>consumer.secret: consumer secret for the http request (Ex:- 'abcdef')</li> <li>token.url     : URL of the token end point (Ex:- 'https://localhost:8005/token')</li> <li>method        : method type (Eg:- POST)</li> </ul> <p>optional (You can fill this if it is different from default values) - header (Authorization header)  : access token for the http request (Ex:- 'abcdef') &lt;= by default it automatically get the access token using the password/client-credential/refresh grant. Here the grantype depends on the user input. - https.truststore.file : API trust store path (Ex:- '/user/../../../client-truststore.jks') &lt;= by default it get from product pack (Ex:- ${carbon.home}/resources/security/client-truststore.jks) - https.truststore.password :  API trust store password (wso2carbon) &lt;= by default it set as wso2carbon</p> </li> <li> <p>Save this sample. If there is no syntax error, the following message is shown on the console:</p> <ul> <li>Siddhi App PublishHttpOAuthRequestWithoutAccessToken successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>PublishHttpOAuthRequestWithoutAccessToken.siddhi - Started Successfully!</li> <li>'Http' sink at 'LowProductionAlertStream' stream successfully connected to 'https://localhost:8005/abc'.</li> </ul> </li> </ol>"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods: * Send events with http server through the event simulator:     1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.     2. In the Single Simulation tab of the panel, specify the values as follows:         * Siddhi App Name  : PublishHttpOAuthRequestWithoutAccessToken         * Stream Name     : SweetProductionStream     3. In the name and amount fields, enter 'toffees' and '75.6' respectively and then click Send to send the event.     4. Send some more events as desired.</p> <ul> <li> <p>Send events to the HTTP endpoint defined by 'publish.url' in the Sink configuration using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishHttpOAuthRequestWithoutAccessToken\",\"data\": ['toffees', 75.6]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'</li> </ul> </li> <li>If there is no error, the following messages are shown on the terminal:<ul> <li>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</li> </ul> </li> </ol> </li> <li> <p>Publish events with Postman:</p> <ol> <li>Install the 'Postman' application from the Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:</li> <li>Click 'send'. If there is no error, the following messages are shown on the console:<ul> <li>\"status\": \"OK\",</li> <li>\"message\": \"Single Event simulation started successfully\"</li> </ul> </li> </ol> </li> </ul>"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: <pre><code>Siddhi App test successfully deployed.\nINFO {org.wso2.extension.siddhi.io.http.sink.HttpSink} - Request sent successfully to https://localhost:8005/abc\n[java] [org.wso2.si.http.server.HttpServerListener] : Event Name Arrived: {\"event\":{\"name\":\"toffees\",\"amount\":75.6}}\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Names:{\"event\":{\"name\":\"toffees\",\"amount\":75.6}} ,\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers key set:[Http_method, Content-type, Content-length]\n[java] [org.wso2.si.http.server.HttpServerMain] : Received Event Headers value set:[[POST], [application/json], [42]]\n</code></pre></p>"},{"location":"samples/PublishHttpOAuthRequestWithoutAccessToken/#notes","title":"Notes:","text":"<p>If you get the message \"Error when pushing events to Siddhi debugger engine', it could be due to time out problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"401\", it could be due to passing invalid parameter problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Recheck the all parameter you are passing and change the correct parameter 3. Start the application and check whether the expected output appears on the console.</p> <p>If you get the message \"500\", it could be due to Internal server error problem, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Start the application again and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishHttpOAuthRequestWithoutAccessToken\")\n@App:description(\"Send a HTTP events to an OAuth-protected endpoint without access token\")\n\ndefine stream SweetProductionStream (name string, amount double);\n@sink(type='http',method=\"xxxxxx\", publisher.url='https://localhost:8005/abc', headers=\"'Content-Type: xxxxx'\",\nconsumer.key=\"xxxxxxxxxx\", consumer.secret=\"xxxxxxxxxxx\", token.url='https://localhost:8005/token',\n@map(type='json', @payload( \"{'name': {{ name }}, 'amount': {{ amount }}}\")))\ndefine stream LowProductionAlertStream ( name string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishJmsInKeyvalueFormat/","title":"Publishing Key-value events via JMS","text":""},{"location":"samples/PublishJmsInKeyvalueFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via JMS transport in Keyvalue format.</p>"},{"location":"samples/PublishJmsInKeyvalueFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Setup ActiveMQ<ul> <li>Download <code>activemq-client-5.x.x.jar</code> (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar).</li> <li>Download <code>apache-activemq-5.x.x-bin.zip</code> (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip)</li> <li>ActiveMQ <code>activemq-client-5.x.x.jar</code> lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI).</li> <li>Unzip the <code>apache-activemq-5.x.x-bin.zip</code> and copy the following ActiveMQ libs in <code>apache-activemq-5.x.x/lib</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code> and <code>{WSO2SIHome}/lib</code>.<ul> <li>hawtbuf-1.9.jar</li> <li>geronimo-j2ee-management_1.1_spec-1.0.1.jar</li> <li>geronimo-jms_1.1_spec-1.1.1.jar</li> </ul> </li> </ul> </li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>Siddhi App PublishJmsInKeyvalueFormat successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInKeyvalueFormat/#note","title":"Note:","text":"<p>To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command:     - For Linux:     <pre><code>./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;/activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     - For Windows:     <pre><code>./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;\\activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     * Provide privileges if necessary using <code>chmod +x icf-provider.(sh|bat)</code>.     * Also, this will register the <code>InitialContextFactory</code> implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create <code>activemq-client-5.x.x</code> directory in the <code>&lt;Output Jar Path&gt;</code> with OSGi converted and original jars:     - <code>activemq-client-5.x.x.jar</code> (Original Jar)     - <code>activemq-client-5.x.x_1.0.0.jar</code> (OSGi converted Jar)     Also, following messages would be shown on the terminal.         <pre><code>- INFO: Executing 'jar uf &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar -C &lt;absolute_path&gt;/activemq-client-5.x.x /internal/CustomBundleActivator.class'\n[timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader\n- INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle\n- INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar\n</code></pre> 3. You can find the osgi converted libs in <code>activemq-client-5.x.x</code> folder. You can copy <code>activemq-client-5.x.x/activemq-client-5.x.x_1.0.0.jar</code> to <code>{WSO2SIHome}/lib</code> and <code>activemq-client-5.x.x/activemq-client-5.x.x.jar</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code>.</p>"},{"location":"samples/PublishJmsInKeyvalueFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{apache-activemq-5.x.x}</code> unzipped directory and start ActiveMQ server node using <code>bin/activemq</code>.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>PublishJmsInKeyvalueFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInKeyvalueFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open a terminal and navigate to <code>{WSO2SIHome}/samples/sample-clients/jms-consumer</code> and run the following comman.     <pre><code>ant -Dtype='keyvalue'\n</code></pre></li> <li>Send events through one or more of the following methods.</li> </ol>"},{"location":"samples/PublishJmsInKeyvalueFormat/#option-1-send-events-to-jms-sink-via-event-simulator","title":"Option 1: Send events to jms sink, via event simulator","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name: PublishJmsInKeyvalueFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the name and amount fields, enter the following and then click Send to send the event.     <pre><code>name: chocolate cake\namount: 50.50\n</code></pre></li> <li>Send some more events.</li> </ol>"},{"location":"samples/PublishJmsInKeyvalueFormat/#option-2-publish-events-with-curl-command-to-the-simulator-http-endpoint","title":"Option 2: Publish events with Curl command to the simulator http endpoint","text":"<ol> <li>Open a new terminal and issue the following command:     <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInKeyvalueFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal:     <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInKeyvalueFormat/#option-3-publish-events-with-postman-to-the-simulator-http-endpoint","title":"Option 3: Publish events with Postman to the simulator http endpoint","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInKeyvalueFormat\",\"data\": ['chocolate cake', 50.50]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInKeyvalueFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal of <code>{WSO2SIHome}/samples/sample-clients/jms-consumer</code>: <pre><code>[java] [io.siddhi.core.stream.output.sink.LogSink] : JmsReceiver : logStream : Event{timestamp=1513607495863, data=['chocolate cake', 50.50], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"PublishJmsInKeyvalueFormat\")\n@App:description('Send events via JMS transport using Keyvalue format')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n@sink(type='jms',\nfactory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory',\nprovider.url='tcp://localhost:61616',\ndestination='jms_result_topic',\nconnection.factory.type='topic',\nconnection.factory.jndi.name='TopicConnectionFactory',\n@map(type='keyvalue'))\ndefine stream LowProductionAlertStream(name string, amount double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishJmsInXmlFormat/","title":"Publishing XML Events via JMS","text":""},{"location":"samples/PublishJmsInXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via JMS transport in Xml format.</p>"},{"location":"samples/PublishJmsInXmlFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Setup ActiveMQ<ul> <li>Download <code>activemq-client-5.x.x.jar</code> (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar).</li> <li>Download <code>apache-activemq-5.x.x-bin.zip</code> (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip).</li> <li>ActiveMQ <code>activemq-client-5.x.x.jar</code> lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI).</li> <li>Unzip the <code>apache-activemq-5.x.x-bin.zip</code> and copy the following ActiveMQ libs in <code>apache-activemq-5.x.x/lib</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code> and <code>{WSO2SIHome}/lib</code>.<ul> <li>hawtbuf-1.9.jar</li> <li>geronimo-j2ee-management_1.1_spec-1.0.1.jar</li> <li>geronimo-jms_1.1_spec-1.1.1.jar</li> </ul> </li> </ul> </li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>Siddhi App PublishJmsInXmlFormat successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInXmlFormat/#note","title":"Note:","text":"<p>To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command:     - For Linux:     <pre><code>./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;/activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     - For Windows:     <pre><code>./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;\\activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     * Provide privileges if necessary using <code>chmod +x icf-provider.(sh|bat)</code>.     * Also, this will register the <code>InitialContextFactory</code> implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create <code>activemq-client-5.x.x</code> directory in the <code>&lt;Output Jar Path&gt;</code> with OSGi converted and original jars:     - <code>activemq-client-5.x.x.jar</code> (Original Jar)     - <code>activemq-client-5.x.x_1.0.0.jar</code> (OSGi converted Jar)     Also, following messages would be shown on the terminal     <pre><code>- INFO: Executing 'jar uf &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar -C &lt;absolute_path&gt;/activemq-client-5.x.x /internal/CustomBundleActivator.class'\n[timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader\n- INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle\n- INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar\n</code></pre> 3. You can find the osgi converted libs in <code>activemq-client-5.x.x</code> folder. You can copy <code>activemq-client-5.x.x/activemq-client-5.x.x_1.0.0.jar</code> to <code>{WSO2SIHome}/lib</code> and <code>activemq-client-5.x.x/activemq-client-5.x.x.jar</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code>.</p>"},{"location":"samples/PublishJmsInXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{apache-activemq-5.x.x}</code> unzipped directory and start ActiveMQ server node using <code>bin/activemq</code>.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>PublishJmsInXmlFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInXmlFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open a terminal and navigate to <code>{WSO2SIHome}/samples/sample-clients/jms-consumer</code> and run <code>ant</code> command without arguments.</li> <li>Send events through one or more of the following methods.</li> </ol>"},{"location":"samples/PublishJmsInXmlFormat/#option-1-send-events-to-jms-sink-via-event-simulator","title":"Option 1: Send events to jms sink, via event simulator","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name  : PublishJmsInXmlFormat</li> <li>Stream Name      : SweetProductionStream</li> </ul> </li> <li>In the name and amount fields, enter the following and then click Send to send the event.     <pre><code>name: chocolate cake\namount: 50.50\n</code></pre></li> <li>Send some more events.</li> </ol>"},{"location":"samples/PublishJmsInXmlFormat/#option-2-publish-events-with-curl-command-to-the-simulator-http-endpoint","title":"Option 2: Publish events with Curl command to the simulator http endpoint","text":"<ol> <li>Open a new terminal and issue the following command:     <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInXmlFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal:     <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInXmlFormat/#option-3-publish-events-with-postman-to-the-simulator-http-endpoint","title":"Option 3: Publish events with Postman to the simulator http endpoint","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishJmsInXmlFormat\",\"data\": ['chocolate cake', 50.50]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/PublishJmsInXmlFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal of <code>{WSO2SIHome}/samples/sample-clients/jms-consumer</code>: <pre><code>[java] [io.siddhi.core.stream.output.sink.LogSink] : JmsReceiver : logStream : Event{timestamp=1513607495863, data=['chocolate cake', 50.50], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"PublishJmsInXmlFormat\")\n@App:description('Send events via JMS transport using XML format')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n@sink(type='jms',\nfactory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory',\nprovider.url='tcp://localhost:61616',\ndestination='jms_result_topic',\nconnection.factory.type='topic',\nconnection.factory.jndi.name='TopicConnectionFactory',\n@map(type='xml'))\ndefine stream LowProductionAlertStream(name string, amount double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/","title":"Publishing Avro Events via Kafka","text":""},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via the Kafka transport in Avro format using Confluent Schema Registry.</p>"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>To install Kafka, follow the steps below:</p> <ol> <li> <p>In Streaming Integrator Tooling, click Tools, and then click Extension Installer.</p> </li> <li> <p>In the Extension Installer dialog box that opens, search for <code>Kafka</code>. Then click Install in the row that appears. A message appears to confirm whether you want to proceed to install the extension. Click Install.</p> </li> <li> <p>Restart Streaming Integrator Tooling.</p> </li> </ol> </li> <li> <p>Download confluent-5.2.1 from the Confluent website. Then unzip the file you downloaded.</p> <p>Tip</p> <p>Download the product Confluent PLatform. For this sample, the deployment type selected was Manual.</p> </li> <li> <p>Save the sample <code>PublishKafkaInAvroFormatUsingSchemaRegistry</code> Siddhi application.</p> <p>If there is no syntax error, the following message is logged in the terminal.</p> <pre><code>* -Siddhi App PublishKafkaInAvroFormatUsingSchemaRegistry successfully deployed.\n</code></pre> </li> </ol>"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#executing-the-sample","title":"Executing the Sample:","text":"<p>To execute the sample, follow the steps below:</p> <ol> <li> <p>First, start a zoo keeper node. To do this, navigate to the <code>&lt;KAFKA_HOME&gt;</code> directory and issue the following command.</p> <p><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties</code></p> </li> <li> <p>Next, start a Kafka server node. To do this, issue the following command from the same directory.</p> <p><code>sh bin/kafka-server-start.sh config/server.properties</code></p> </li> <li> <p>Start the schema registry node by navigating to the <code>&lt;CONFLUENT_HOME&gt;</code> directory and issuing the following command:</p> <p><code>sh bin/schema-registry-start ./etc/schema-registry/schema-registry.properties</code></p> <p>This starts the Confluent client in <code>localhost:8081</code> port.</p> </li> <li> <p>Post the avro schema to the schema registry by issuing the following CURL command.</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" --data '{ \"schema\": \"{ \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"sweetProduction\\\",\\\"namespace\\\": \\\"sweetProduction\\\", \\\"fields\\\":[{ \\\"name\\\": \\\"name\\\", \\\"type\\\": \\\"string\\\" },{ \\\"name\\\": \\\"amount\\\", \\\"type\\\": \\\"double\\\" }]}\"}' http://localhost:8081/subjects/sweet-production/versions\n</code></pre> </li> </ol> <p>The sample Siddhi application specifies <code>http://localhost:8081/subjects/sweet-production/versions</code> as the URI of the schema registry. The above CURL command defines the Avro schema and posts it to this schema registry so that the schema is applied to the output events generated in the <code>LowProductionAlertStream</code> when they are published to the <code>kafka_result_topic</code> kafka topic.</p> <p>For more information about how to configure an Avro mapper, see Siddhi Documentation - Avro Sink Mapper</p> <ol> <li> <p>Navigate to the <code>&lt;SI_TOOLING_HOME&gt;/samples/sample-clients/kafka-avro-consumer</code> directory and run the <code>ant</code> command without arguments.</p> </li> <li> <p>Start the <code>PublishKafkaInAvroFormatUsingSchemaRegistry</code> Siddhi application you saved by opening it in Streaming Integrator Tooling and clicking the Start button in the toolbar.</p> <p>If the Siddhi application starts successfully, the following messages are logged in the terminal:</p> <ul> <li> <p><code>PublishKafkaInAvroFormatUsingSchemaRegistry.siddhi - Started Successfully!</code></p> </li> <li> <p><code>Kafka version : 2.2.0</code></p> </li> <li> <p><code>Kafka commitId : 05fcfde8f69b0349</code></p> </li> <li> <p><code>Kafka producer created.</code></p> </li> </ul> </li> </ol>"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#testing-the-sample","title":"Testing the Sample:","text":"<p>To test this sample, send events following one or more of the methods given below:</p> <p>Option 1 - Send events to the kafka sink via the Event Simulator:</p> <ol> <li> <p>In Streaming Integrator Studio, open the Event Simulator by clicking on the Event Simulator icon in the left panel or pressing Ctrl+Shift+I.</p> </li> <li> <p>In the Single Simulation tab of the panel, specify the values as follows:</p> Field Value Siddhi App Name <code>PublishKafkaInAvroFormatUsingSchemaRegistry</code> Stream Name <code>SweetProductionStream</code> </li> <li> <p>Once you select the stream, the name and amount fields appear. Enter <code>chocolate cake</code> as the name and <code>50.50</code> as the value. Then click Send to send the event.</p> </li> <li> <p>Simulate a few more events for the <code>SweetProductionStream</code> stream by repeating the above steps.</p> </li> </ol> <p>Option 2 - Publish events with Curl to the simulator HTTP endpoint:</p> <ol> <li> <p>Open a new terminal and issue the following command:</p> <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInAvroFormatUsingSchemaRegistry\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre> <p>When the message is successfully sent, the following message is logged in the terminal:</p> <pre><code>\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"\n</code></pre> </li> </ol> <p>Option 3 - Publish events with Postman to the simulator HTTP endpoint:</p> <ol> <li> <p>Install the Postman application from the Chrome web store.</p> </li> <li> <p>Launch the Postman application.</p> </li> <li> <p>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to <code>text/plain</code> and set the request body in text as follows:</p> <p><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInAvroFormatUsingSchemaRegistry\",\"data\": ['chocolate cake', 50.50]}</code></p> </li> <li> <p>Click send. When the message is successfully sent, the following messages are logged in the terminal.</p> <ul> <li><code>\"status\": \"OK\",</code></li> <li><code>\"message\": \"Single Event simulation started successfully\"</code></li> </ul> </li> </ol>"},{"location":"samples/PublishKafkaInAvroFormatUsingSchemaRegistry/#viewing-the-results","title":"Viewing the Results:","text":"<p>You can view the following output in the terminal in which you ran the ant build for <code>&lt;SI_HOME&gt;/samples/sample-clients/kafka-avro-consumer</code>. <pre><code>[java] [org.wso2.extension.siddhi.io.kafka.source.KafkaConsumerThread] : Event received in Kafka Event Adaptor with offSet: 2, key: null, topic: kafka_result_topic, partition: 0\n[java] [io.siddhi.core.stream.output.sink.LogSink] : KafkaSample : logStream : Event{timestamp=1546973831995, data=[chocolate cake, 50.5], isExpired=false}\n</code></pre></p> <p>Note</p> <p>If the message <code>'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092</code> does not appear, the reason can be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do as follows: 1. Stop the Siddhi application (i.e., by clicking the stop button for the Siddhi application in the top panel). 2. In the source configuration of the Siddhi application, change port 9092 to an unused port. 3. Start the Siddhi application and check whether the specified messages appear in the terminal.</p> <p>The complete Siddhi application used in this sample is as follows:</p> <pre><code>@App:name(\"PublishKafkaInAvroFormatUsingSchemaRegistry\")\n\n@App:description('Send events via Kafka transport using Avro format')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='kafka',\ntopic='kafka_result_topic',\nbootstrap.servers='localhost:9092',\nis.binary.message='true',\n@map(type='avro',schema.registry='http://localhost:8081',schema.id='1'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishKafkaInBinaryFormat/","title":"Publishing Binary Events via Kafka","text":""},{"location":"samples/PublishKafkaInBinaryFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Binary format.</p>"},{"location":"samples/PublishKafkaInBinaryFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Setup Kafka.<ul> <li>Kafka libs to be added and converted to OSGI from {KafkaHome}/libs are as follows.<ul> <li>kafka_2.11-0.10.0.0.jar</li> <li>kafka-clients-0.10.0.0.jar</li> <li>metrics-core-2.2.0.jar</li> <li>scala-library-2.11.8.jar</li> <li>zkclient-0.8.jar</li> <li>zookeeper-3.4.6.jar</li> </ul> </li> <li>Add the OSGI converted kafka libs to <code>{WSO2SIHome}/lib</code>.</li> <li>Add the kafka libs to <code>{WSO2SIHome}/samples/sample-clients/lib</code>.</li> </ul> </li> <li>Save this sample.</li> <li>If there is no syntax error, the following messages would be shown on the console.     <pre><code>Siddhi App PublishKafkaInBinaryFormat successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/PublishKafkaInBinaryFormat/#note","title":"Note:","text":"<p>To convert Kafka libs to OSGI, 1. Create a folder (Eg: Kafka) and copy Kafka libs to be added from <code>{KafkaHome}/libs</code>. 2. Create another folder(Eg: Kafka-osgi, This folder will have the libs that converted to OSGI). 3. Navigate to <code>{WSO2SIHome}/bin</code> and issue the follwing command.     * For Linux:         <pre><code>./jartobundle.sh &lt;path/kafka&gt; &lt;path/kafka-osgi&gt;\n</code></pre>     * For Windows:         <pre><code>./jartobundle.bat &lt;path/kafka&gt; &lt;path/kafka-osgi&gt;\n</code></pre> 4. If converted successfully then for each lib, following messages would be shown on the terminal.     <pre><code>- INFO: Created the OSGi bundle &lt;kafka-lib-name&gt;.jar for JAR file &lt;absolute_path&gt;/kafka/&lt;kafka-lib-name&gt;.jar\n</code></pre> 5. You can find the osgi converted libs in kafka-osgi folder. You can copy that to <code>{WSO2SIHome}/lib</code>.</p>"},{"location":"samples/PublishKafkaInBinaryFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{KafkaHome}</code> and start zookeeper node using following command.     <pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties\n</code></pre></li> <li>Navigate to <code>{KafkaHome}</code> and start kafka server node using following command.     <pre><code>bin/kafka-server-start.sh config/server.properties\n</code></pre></li> <li>Navigate to <code>{WSO2SIHome}/samples/sample-clients/kafka-consumer</code> and run <code>ant</code> command with following arguments.     <pre><code>ant -DisBinaryMessage=true -DtopicList=kafka_result_topic -Dtype=binary\n</code></pre></li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>- PublishKafkaInBinaryFormat.siddhi - Started Successfully!\n- Kafka version : 0.10.0.0\n- Kafka commitId : 23c69d62a0cabf06\n- Kafka producer created.\n</code></pre></li> </ol>"},{"location":"samples/PublishKafkaInBinaryFormat/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/PublishKafkaInBinaryFormat/#send-events-with-kafka-server-through-event-simulator","title":"Send events with kafka server, through event simulator:","text":"<ol> <li>To open event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: PublishKafkaInBinaryFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the batchNumber field and lowTotal fields, enter '1', '85.5' respectively and then click Send to send the event.</li> <li>Send some more events.</li> </ol>"},{"location":"samples/PublishKafkaInBinaryFormat/#publish-events-with-curl-command","title":"Publish events with curl command:","text":"<p>Open a new terminal and issue the following command. <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInBinaryFormat\", \"data\": [1, 85.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></p>"},{"location":"samples/PublishKafkaInBinaryFormat/#publish-events-with-postman","title":"Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in json format as follows,     <pre><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInBinaryFormat\",\"data\": [1, 85.5]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages would be shown on the console.     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/PublishKafkaInBinaryFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>It will print the results in binary format.</p>"},{"location":"samples/PublishKafkaInBinaryFormat/#notes","title":"Notes:","text":"<p>If the message \"'Kafka' sink at <code>'LowProducitonAlertStream' has successfully connected to 'http://localhost:9092'</code> does not appear, it could be due to port 9092, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 9092 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.</p> <pre><code>@App:name(\"PublishKafkaInBinaryFormat\")\n@App:description('Send events via Kafka transport using Binary format')\n\n\ndefine stream SweetProductionStream (batchNumber long, lowTotal double);\n\n@sink(type='kafka',\ntopic='kafka_result_topic',\nbootstrap.servers='localhost:9092',\nis.binary.message='true',\n@map(type='binary'))\ndefine stream LowProductionAlertStream (batchNumber long, lowTotal double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishKafkaInCustomAvroFormat/","title":"Publishing Custom Avro Events via Kafka","text":""},{"location":"samples/PublishKafkaInCustomAvroFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in Avro format with custom mapping</p>"},{"location":"samples/PublishKafkaInCustomAvroFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Set up Kafka as follows:<ol> <li>Create a folder called kafka and another folder called kafka-osgi.</li> <li>Copy the following files from {KafkaHome}/libs to the kafka folder you just created:<ul> <li>kafka_2.11-0.10.0.0.jar</li> <li>kafka-clients-0.10.0.0.jar</li> <li>metrics-core-2.2.0.jar</li> <li>scala-library-2.11.8.jar</li> <li>zkclient-0.8.jar</li> <li>zookeeper-3.4.6.jar</li> </ul> </li> <li>Copy these same files to the {WSO2SIHome}/samples/sample-clients/lib folder.</li> <li>Navigate to {WSO2SIHome}/bin and issue the following command:<ul> <li>For Linux: ./jartobundle.sh  <li>For Windows: ./jartobundle.bat   If converted successfully, the following messages are shown on the terminal for each lib file: <li>INFO: Created the OSGi bundle .jar for JAR file /kafka/.jar <li>Copy the OSGi-converted kafka libs from the kafka-osgi folder to {WSO2SIHome}/lib.</li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:<ul> <li>-Siddhi App PublishKafkaInAVroFormat successfully deployed.</li> </ul> </li>"},{"location":"samples/PublishKafkaInCustomAvroFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to {KafkaHome} and start the zookeeper node using bin/zookeeper-server-start.sh config/zookeeper.properties</li> <li>Navigate to {KafkaHome} and start the kafka server node using bin/kafka-server-start. Navigate to {ConfluentHome} and start the schema registry node using, bin/schema-registry-start ./etc/schema-registry/schema-registry.properties</li> <li>Post the avro schema to schema registry using, curl -X POST -H \"Content-Type: application/json\"  --data '{ \"schema\": \"{ \\\"type\\\": \\\"record\\\", \\\"name\\\": \\\"sweetProduction\\\",\\\"namespace\\\": \\\"sweetProduction\\\", \\\"fields\\\":[{ \\\"name\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\" },{ \\\"name\\\": \\\"Amount\\\", \\\"type\\\": \\\"double\\\" }]}\"}'  http://localhost:8081/subjects/sweet-production/versions sh config/server.properties</li> <li>Navigate to {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer and run the 'ant' command as follows: ant -Dtype=avro -DisBinaryMessage=true</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:         - PublishKafkaInCustomAvroFormat.siddhi - Started Successfully!         - Kafka version : 0.10.0.0         - Kafka commitId : 23c69d62a0cabf06         - Kafka producer created.</li> </ol>"},{"location":"samples/PublishKafkaInCustomAvroFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods.</p> <p>Option 1 - Send events to the kafka sink via the event simulator: 1. Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, specify the values as follows:     * Siddhi App Name  : PublishKafkaInCustomAvroFormat     * Stream Name      : SweetProductionStream 3. In the name and amount fields, enter the following values and then click Send to send the event.     * name: chocolate cake     * amount: 50.50 4. Send some more events.</p> <p>Option 2 - Publish events with Curl to the simulator HTTP endpoint: 1. Open a new terminal and issue the following command:     * curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInCustomAvroFormat\",\"data\": [\"chocolate cake\", 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain' 2. If there is no error, the following messages are shown on the terminal:     *  {\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}</p> <p>Option 3 - Publish events with Postman to the simulator HTTP endpoint: 1. Install the 'Postman' application from the Chrome web store. 2. Launch the Postman application. 3. Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows: {\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInCustomAvroFormat\",\"data\": ['chocolate cake', 50.50]} 4. Click 'send'. If there is no error, the following messages are shown on the console:     *  \"status\": \"OK\",     *  \"message\": \"Single Event simulation started successfully\"</p>"},{"location":"samples/PublishKafkaInCustomAvroFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal of {WSO2SIHome}/samples/sample-clients/kafka-avro-consumer: <pre><code>[java] [org.wso2.extension.siddhi.io.kafka.source.KafkaConsumerThread] : Event received in Kafka Event Adaptor with offSet: 0, key: null, topic: kafka_result_topic, partition: 0\n[java] [io.siddhi.core.stream.output.sink.LogSink] : KafkaSample : logStream : Event{timestamp=1546973319457, data=[chocolate cake, 50.5], isExpired=false}\n</code></pre></p>"},{"location":"samples/PublishKafkaInCustomAvroFormat/#notes","title":"Notes:","text":"<p>If the message \"'Kafka' sink at 'LowProductionAlertStream' has successfully connected to http://localhost:9092\" does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console.</p> <pre><code>@App:name(\"PublishKafkaInCustomAvroFormat\")\n\n@App:description('Send events via Kafka transport using Custom Avro format')\n\n\ndefine stream SweetProductionStream (sweetName string, sweetAmount double);\n\n@sink(type='kafka',\ntopic='kafka_result_topic',\nis.binary.message='true',\nbootstrap.servers='localhost:9092',\n@map(type='avro',schema.def=\"\"\"{\"type\":\"record\",\"name\":\"stock\",\"namespace\":\"stock\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"amount\",\"type\":\"double\"}]}\"\"\",\n@payload(\"\"\"{\"name\": \"{{ sweetName }}\", \"amount\": {{ sweetAmount }}}\"\"\")))\ndefine stream LowProductionAlertStream (sweetName string, sweetAmount double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishKafkaInJsonFormat/","title":"Publishing JSON Events via Kafka","text":""},{"location":"samples/PublishKafkaInJsonFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via Kafka transport in JSON format.</p>"},{"location":"samples/PublishKafkaInJsonFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Set up Kafka as follows:<ol> <li>Create a folder called <code>kafka</code> and another folder called <code>kafka-osgi</code>.</li> <li>Copy the following files from <code>{KafkaHome}/libs</code> to the kafka folder you just created:<ul> <li>kafka_2.11-0.10.0.0.jar</li> <li>kafka-clients-0.10.0.0.jar</li> <li>metrics-core-2.2.0.jar</li> <li>scala-library-2.11.8.jar</li> <li>zkclient-0.8.jar</li> <li>zookeeper-3.4.6.jar</li> </ul> </li> <li>Copy these same files to the <code>{WSO2SIHome}/samples/sample-clients/lib</code> folder.</li> <li>Navigate to <code>{WSO2SIHome}/bin</code> and issue the following command:<ul> <li>For Linux:     <pre><code>./jartobundle.sh &lt;path/kafka&gt; &lt;path/kafka-osgi&gt;\n</code></pre></li> <li>For Windows:     <pre><code>./jartobundle.bat &lt;path/kafka&gt; &lt;path/kafka-osgi&gt;\n</code></pre>    If converted successfully, the following messages are shown on the terminal for each lib file:    <pre><code>- INFO: Created the OSGi bundle &lt;kafka-lib-name&gt;.jar for JAR file &lt;absolute_path&gt;/kafka/&lt;kafka-lib-name&gt;.jar\n</code></pre></li> </ul> </li> <li>Copy the OSGi-converted kafka libs from the <code>kafka-osgi</code> folder to <code>{WSO2SIHome}/lib</code>.</li> </ol> </li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>Siddhi App PublishKafkaInJsonFormat successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/PublishKafkaInJsonFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{KafkaHome}</code> and start the zookeeper node using following command.     <pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties\n</code></pre></li> <li>Navigate to <code>{KafkaHome}</code> and start the kafka server node using following command.     <pre><code>bin/kafka-server-start.sh config/server.properties\n</code></pre></li> <li>Navigate to <code>{WSO2SIHome}/samples/sample-clients/kafka-consumer</code> and run the <code>ant</code> command without arguments.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>- PublishKafkaInJsonFormat.siddhi - Started Successfully!\n- Kafka version : 0.10.0.0\n- Kafka commitId : 23c69d62a0cabf06\n- Kafka producer created.\n</code></pre></li> </ol>"},{"location":"samples/PublishKafkaInJsonFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods.</p>"},{"location":"samples/PublishKafkaInJsonFormat/#option-1-send-events-to-the-kafka-sink-via-the-event-simulator","title":"Option 1: Send events to the kafka sink via the event simulator:","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name: PublishKafkaInJsonFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the batchNumber and lowTotal fields, enter the following values and then click Send to send the event.     <pre><code>batchNumber: 1\nlowTotal: 50.50\n</code></pre></li> <li>Send some more events.</li> </ol>"},{"location":"samples/PublishKafkaInJsonFormat/#option-2-publish-events-with-curl-to-the-simulator-http-endpoint","title":"Option 2: Publish events with Curl to the simulator HTTP endpoint:","text":"<ol> <li>Open a new terminal and issue the following command:     <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInJsonFormat\",\"data\": [1, 50.50]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal:     <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/PublishKafkaInJsonFormat/#option-3-publish-events-with-postman-to-the-simulator-http-endpoint","title":"Option 3: Publish events with Postman to the simulator HTTP endpoint:","text":"<ol> <li>Install the 'Postman' application from the Chrome web store.</li> <li>Launch the Postman application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"PublishKafkaInJsonFormat\",\"data\": [1, 50.50]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/PublishKafkaInJsonFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal of <code>{WSO2SIHome}/samples/sample-clients/kafka-consumer</code>: <pre><code>[java] [org.wso2.si.sample.kafka.consumer.KafkaReceiver] : Event received in Kafka Event Adaptor: {\"event\":{\"name\":\"chocolate cake\",\"amount\":50.50}}, offSet: 0, key: null, topic: kafka_result_topic, partition: 0\n[java] [org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] : Committed offset 1 for partition kafka_result_topic-0\n</code></pre></p>"},{"location":"samples/PublishKafkaInJsonFormat/#notes","title":"Notes:","text":"<p>If the message \"'Kafka' sink at <code>'LowProductionAlertStream' has successfully connected to http://localhost:9092'</code> does not appear, it could be that port 9092 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following, 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. In this Siddhi application's source configuration, change port 9092 to an unused port. 3. Start the application and check whether the specified messages appear on the console.</p> <pre><code>@App:name(\"PublishKafkaInJsonFormat\")\n@App:description('Send events via Kafka transport using JSON format')\n\n\ndefine stream SweetProductionStream (batchNumber long, lowTotal double);\n\n@sink(type='kafka',\ntopic='kafka_result_topic',\nbootstrap.servers='localhost:9092',\n@map(type='json'))\ndefine stream LowProductionAlertStream (batchNumber long, lowTotal double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishMqttInXmlFormat/","title":"Publishing XML Events via MQTT","text":""},{"location":"samples/PublishMqttInXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via MQTT transport in XML format and view the output on the mqtt-consumer.</p>"},{"location":"samples/PublishMqttInXmlFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample. The following message appears on the console.     <pre><code>Siddhi App PublishMqttInXmlFormat successfully deployed.\n</code></pre></li> <li>Setting up MQTT Mosquitto broker in Ubuntu Linux.<ol> <li>Add the mosquitto repository using the following commands.     <pre><code>sudo apt-add-repository ppa:mosquitto-dev/mosquitto-ppa\nsudo apt-get update\n</code></pre></li> <li>Execute the following command to install the Mosquitto broker package.     ```bash     sudo apt-get install mosquitto     ````</li> <li>Install Mosquitto developer libraries to develop MQTT clients.     <pre><code>sudo apt-get install libmosquitto-dev\n</code></pre></li> <li>Execute the following command to install Mosquitto client packages.     <pre><code>sudo apt-get install mosquitto-clients\n</code></pre></li> <li>Ensure that the Mosquitto broker is running.     <pre><code>sudo service mosquitto status\n</code></pre></li> </ol> </li> </ol>"},{"location":"samples/PublishMqttInXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Open a terminal, navigate to the <code>{WSO2SIHome}/samples/sample-clients/mqtt-consumer</code> directory and run the <code>ant</code> command.<ul> <li>If you use the default topic <code>mqtt_topic</code> and URL <code>tcp://localhost:1883</code>, in your program use the <code>ant</code> command without any arguments.</li> <li>However, if you use a different topic, run the <code>ant</code> command with appropriate argument. e.g. <code>ant -Dtopic=mqttTest</code></li> </ul> </li> <li>Start the Siddhi application by clicking 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages appear on the console     <pre><code>PublishMqttInXmlFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/PublishMqttInXmlFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open the event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: PublishMqttInXmlFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the name field and amount fields, enter 'toffee' and '45.24' respectively. Then click Send to send the event.</li> <li>Send some more events.</li> </ol>"},{"location":"samples/PublishMqttInXmlFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output in the terminal of <code>{WSO2SIHome}/samples/sample-clients/mqtt-consumer</code>. You will get the output as follows (example for 3 events):</p> <pre><code>[java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper\n[java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462478777, data=[chocolate, 78.34], isExpired=false}\n[java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper\n[java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462520264, data=[sweets, 34.57], isExpired=false}\n[java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper\n[java] [io.siddhi.core.stream.output.sink.LogSink] : PublishMqttInXmlFormatTest : logStream : Event{timestamp=1512462534053, data=[coffee, 12.7], isExpired=false}\n</code></pre>"},{"location":"samples/PublishMqttInXmlFormat/#notes","title":"Notes:","text":"<p>If you need to edit this application while it is running, stop the application -&gt; Save -&gt; Start. If the message \"LowProductionAlertStream' stream could not connect to 'localhost:1883\", it could be due to port 1883, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on the menu bar -&gt; 'Stop'). * Change the port 1883 to an unused port. To do this you need to add a new listener in mosquitto.conf (e.g., listener port 1884,listener port 1885). * Start the application and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishMqttInXmlFormat\")\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='mqtt', url= 'tcp://localhost:1883',topic='mqtt_topic',\n@map(type='xml'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishPrometheusMetricsHTTPServer/","title":"Publishing Consumed Events to Prometheus Metrics and Exposing then via HTTP","text":""},{"location":"samples/PublishPrometheusMetricsHTTPServer/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to use siddhi-io-prometheus for publishing events through http server.</p>"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>The following steps must be executed to enable WSO2 SP to publish events to Prometheus</p> <ol> <li>Download Prometheus server 'prometheus-2.5.0' from here : https://prometheus.io/download/#prometheus    And extract the file in a preferred location.</li> <li> <p>Open the prometheus.yml file from {prometheus} directory and add the following text under \"scrape_configs:\"     \" - job_name: 'server'         honor_labels: true         static_configs:         - targets: ['localhost:9080']\"</p> <ul> <li>you can replace values for 'localhost:9080' in targets according to your preferred host and port.   In that case, the 'server.url' option must be included in the Sink definition with the same host and port values.</li> </ul> </li> <li> <p>Download and copy the prometheus client jars to the {WSO2SIHome}/lib directory as follows.</p> <ol> <li>Download the following jars from https://mvnrepository.com/artifact/io.prometheus and copy them to {WSO2SIHome}/lib directory.<ul> <li>simpleclient_pushgateway-0.5.0.jar</li> <li>simpleclient_common-0.5.0.jar</li> <li>simpleclient-0.5.0.jar</li> <li>simpleclient_httpserver-0.5.0.jar</li> </ul> </li> </ol> </li> <li>Navigate to {prometheus} and start prometheus server using ' ./prometheus --config.file=\"prometheus.yml\" ' command</li> <li>Start the editor WSO2 SP by giving this command in the terminal : sh editor.sh</li> <li>Save this sample</li> </ol> </li> </ol>"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console,<ul> <li>PublishPrometheusMetricHTTPServer.siddhi - Started Successfully!</li> <li>SweetProductionStream has successfully connected at http://localhost:9080</li> </ul> </li> </ol>"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#testing-the-sample","title":"Testing the Sample:","text":"<ul> <li>Send events through the event simulator:<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name  : PublishPrometheusMetricHTTPServer</li> <li>Stream Name     : SweetProductionStream</li> </ul> </li> <li>In the name and amount fields, enter 'toffees' and '55.4' respectively and then click Send to send the event.</li> <li>Send some more events as follows,         ['gingerbread', 72.3],         ['toffees', 32.4],         ['lollipop', 23.8].</li> </ol> </li> </ul>"},{"location":"samples/PublishPrometheusMetricsHTTPServer/#viewing-the-results","title":"Viewing the Results:","text":"<ol> <li> <p>See the output events through one or more of the following methods:</p> <ul> <li>Open the url \"http://localhost:9080/metrics\" in your browser. The folllowing output will be displayed,<ul> <li>\"# HELP SweetProductionStream help for counter SweetProductionStream\"</li> <li>\"# TYPE SweetProductionStream counter\"</li> <li>\"SweetProductionStream{Name=\"gingerbread\",} 72.3\"</li> <li>\"SweetProductionStream{Name=\"lollipop\",} 23.8\"</li> <li>\"SweetProductionStream{Name=\"toffees\",} 87.8\"\"</li> </ul> </li> </ul> </li> <li> <p>Send http request to the endpoint \"http://localhost:9090\" to the running prometheus server using the curl command:</p> <ol> <li>Open a new terminal and issue the following command:<ul> <li>curl -X GET http://localhost:9090/</li> </ul> </li> <li>If there is no error, the result will be shown on the terminal in JSON Format similar to the following : <pre><code>{\"status\":\"success\",\n\"data\":{\n\"resultType\":\"vector\",\n\"result\":[\n{   \"metric\":{\"Name\":\"gingerbread\",\n\"__name__\":\"SweetProductionStream\",\n\"instance\":\"localhost:9080\",\n\"job\":\"server\"},\n\"value\":[1*********.***,\"72.3\"]\n},\n{   \"metric\":{\"Name\":\"lollipop\",\n\"__name__\":\"SweetProductionStream\",\n\"instance\":\"localhost:9080\",\n\"job\":\"server\"},\n\"value\":[1*********.***,\"23.8\"]\n},\n{   \"metric\":{\"Name\":\"toffees\",\n\"__name__\":\"SweetProductionStream\",\n\"instance\":\"localhost:9080\",\n\"job\":\"server\"},\n\"value\":[1*********.***,\"87.8\"]\n}\n]}\n}\n</code></pre></li> </ol> </li> </ol> <pre><code>@App:name(\"PublishPrometheusMetrics\")\n\n@App:description('Publish consumed events to Prometheus metrics and expose them via http server.')\n\n\ndefine stream FooStream(Name string, amount double);\n\n@sink(type='log', priority='WARN', prefix ='received sweet products')\ndefine stream LogStream(Name string, amount double);\n\n@sink(type='prometheus' , job='SweetProduction', publish.mode='server', metric.type='counter', value.attribute = \"amount\", @map(type='keyvalue'))\ndefine stream SweetProductionStream(Name string, amount double);\n\n@info(name='Add-events')\nfrom FooStream\nselect *\ninsert into SweetProductionStream;\n\n@info(name='Log-events')\nfrom FooStream\nselect *\ninsert into LogStream;\n</code></pre>"},{"location":"samples/PublishRabbitmqInXmlFormat/","title":"Publishing XML Events via RabbitMQ","text":""},{"location":"samples/PublishRabbitmqInXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via RabbitMQ transport in XML format, and view the output on the rabbitmq-consumer.</p>"},{"location":"samples/PublishRabbitmqInXmlFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample. The following message appears on the console.     <pre><code>Siddhi App PublishRabbitmqInXmlFormat successfully deployed.\n</code></pre></li> <li>Setting up RabbitMQ broker in Ubuntu Linux.<ol> <li>To download and install RabbitMQ, run the following commands:     <pre><code>sudo apt-get update\nsudo apt-get install rabbitmq-server\n</code></pre></li> <li>To manage the maximum amount of connections upon launch, open and edit the following configuration file using a nano command:     <pre><code>sudo nano /etc/default/rabbitmq-server\n</code></pre></li> <li>To enable RabbitMQ Management Console, run the following command:     <pre><code>sudo rabbitmq-plugins enable rabbitmq_management\n</code></pre></li> <li>To start the service, issue the following command:     <pre><code>invoke-rc.d rabbitmq-server start\n</code></pre></li> </ol> </li> </ol>"},{"location":"samples/PublishRabbitmqInXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Open a terminal and navigate to the <code>{WSO2SIHome}/samples/sample-clients/rabbitmq-consumer</code> directory and run the <code>ant</code> command.<ul> <li>If you use the default exchange <code>RABBITMQSAMPLE</code> and URI <code>amqp://guest:guest@localhost:5672</code> in your program use <code>ant</code> command without any arguments.</li> <li>However, if you use different exchange names or URIs, run the <code>ant</code> command with appropriate arguments. e.g., <code>ant -Dexchange=rabbitMqtest</code></li> </ul> </li> <li>Start the Siddhi application by clicking 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages appear on the console. <pre><code>PublishRabbitmqInXmlFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/PublishRabbitmqInXmlFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Open the event simulator by clicking the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: PublishRabbitmqInXmlFormat</li> <li>Stream Name: SweetProductiontream</li> </ul> </li> <li>In the name field and amount fields, enter 'toffee' and '45.24' respectively and then click Send to send the event.</li> <li>Send some more events.</li> </ol>"},{"location":"samples/PublishRabbitmqInXmlFormat/#viewing-the-results","title":"Viewing the Results:","text":"<ul> <li>See the output in the terminal of <code>{WSO2SIHome}/samples/sample-clients/rabbitmq-consumer</code>. You will get the output as follows (sample output for 4 events): <pre><code>[java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper\n[java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448790922, data=[toffee, 9.78], isExpired=false}\n[java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper\n[java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448813975, data=[lollipop, 12.6], isExpired=false}\n[java] [org.apache.axiom.om.util.StAXUtils] : XMLStreamReader is org.apache.axiom.util.stax.dialect.SJSXPStreamReaderWrapper\n[java] [io.siddhi.core.stream.output.sink.LogSink] : PublishRabbitmqInXmlFormatTest : logStream : Event{timestamp=1512448830488, data=[Pop, 72.6], isExpired=false}\n</code></pre></li> </ul>"},{"location":"samples/PublishRabbitmqInXmlFormat/#notes","title":"Notes:","text":"<p>If you need to edit this application while it is running, stop the application -&gt; Save -&gt; Start. If the message <code>'LowProducitonAlertStream' stream could not connect to 'localhost:5672'</code>, it could be due to port 5672, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (click 'Run' on menu bar -&gt; 'Stop'). * Change the port 5672 to an unused port. * Start the application and check whether the expected output appears on the console. RabbitMq consumer was written according to this example (go through the RabbitMQReceiver application).</p> <pre><code>@App:name(\"PublishRabbitmqInXmlFormat\")\n@APP:description(\"demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via RabbitMQ transport in XML format, and view the output on the rabbitmq-consumer\")\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type ='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'RABBITMQSAMPLE',\n@map(type='xml'))\ndefine stream LowProducitonAlertStream (name string, amount double);\n\nfrom SweetProductionStream\nselect *\ninsert into LowProducitonAlertStream;\n</code></pre>"},{"location":"samples/PublishTcpInBinaryFormat/","title":"Publishing Binary Events via TCP","text":""},{"location":"samples/PublishTcpInBinaryFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in binary format and view the output on the console.</p>"},{"location":"samples/PublishTcpInBinaryFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/PublishTcpInBinaryFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Open a terminal and navigate to the <code>{WSO2SIHome}/samples/sample-clients/tcp-server</code> directory. Run the command:     <pre><code>ant -Dcontext=LowProductionAlertStream\n</code></pre></li> <li>Start the Siddhi application by clicking 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages appear on the console.     <pre><code>* PublishTcpInBinaryFormat.siddhi - Started Successfully!\n* 'tcp' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:9892'.\n</code></pre></li> <li>Open the event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: PublishTcpInBinaryFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the 'name' and 'amount' fields, enter 'toffee' and '45.24' respectively, and then click Send to send the event.</li> <li>Send some more events.</li> <li>Check the output in the terminal of {WSO2SIHome}/samples/sample-clients/tcp-server. You will see output similar to the following:     <pre><code>[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false}\n[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false}\n[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false}\n</code></pre></li> </ol>"},{"location":"samples/PublishTcpInBinaryFormat/#notes","title":"Notes:","text":"<p>If you need to edit this application while it is running, stop the application -&gt; Save -&gt; Start. If the message <code>'LowProducitonAlertStream' stream could not connect to 'localhost:9892'</code>, it could be due to port 9892, which is defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -&gt; 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishTcpInBinaryFormat\")\n@App:description('Send events via TCP transport using binary format')\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream',\n@map(type='binary'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishTcpInJsonFormat/","title":"Publishing JSON Events via TCP","text":""},{"location":"samples/PublishTcpInJsonFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in JSON format, and view the output on the console.</p>"},{"location":"samples/PublishTcpInJsonFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/PublishTcpInJsonFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Open a terminal and navigate to the <code>{WSO2SIHome}/samples/sample-clients/tcp-server</code> directory and run the following command:     <pre><code>ant -Dtype=json -Dcontext=LowProductionAlertStream\n</code></pre></li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages appear on the console.     <pre><code>* PublishTcpInJsonFormat.siddhi - Started Successfully!\n* 'tcp' sink at 'LowProducitonAlertStream' stream successfully connected to 'localhost:9892'.\n</code></pre></li> <li>Open the event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: PublishTcpInJsonFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the 'name' field and 'amount' field, enter 'toffee' and '45.24' respectively and click Send to send the event.</li> <li>Send some more events.</li> <li>See the output in the terminal of <code>{WSO2SIHome}/samples/sample-clients/tcp-server</code>. You can see output similar to the following:     <pre><code>[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false}\n[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false}\n[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false}\n</code></pre></li> </ol>"},{"location":"samples/PublishTcpInJsonFormat/#notes","title":"Notes:","text":"<p>If you need to edit this application while it is running, stop the application -&gt; Save -&gt; Start. If you see the message <code>'LowProducitonAlertStream' stream could not connect to 'localhost:9892'</code>, it could be due to port 9892, defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -&gt; 'Stop'). * Change the port 9892 to an unused port in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishTcpInJsonFormat\")\n@App:description('Send events via TCP transport using json format')\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream',\n@map(type='json'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishTcpInTextFormat/","title":"Publishing Text Events via TCP","text":""},{"location":"samples/PublishTcpInTextFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via TCP transport in text format, and view the output on the server terminal.</p>"},{"location":"samples/PublishTcpInTextFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/PublishTcpInTextFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Open a terminal and navigate to the <code>{WSO2SIHome}/samples/sample-clients/tcp-server</code> directory. Run the <code>ant</code> command with argument <code>-Dtype=text</code>.</li> <li>Start the Siddhi application by clicking 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages appear on the console.     <pre><code>* PublishTcpInTextFormat.siddhi - Started Successfully!\n* 'tcp' sink at 'LowProductionAlertStream' stream successfully connected to 'localhost:9892'.\n</code></pre></li> <li>Open event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: PublishTcpInTextFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the 'name' field and 'amount' field, enter 'toffee' and '45.24' respectively and click Send to send the event.</li> <li>Send some more events.</li> <li>See the output in the terminal of <code>{WSO2SIHome}/samples/sample-clients/tcp-server</code>. You can see output similar to the following:     <pre><code>[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446413468, data=[toffee, 45.25], isExpired=false}\n[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446425113, data=[coffee, 9.78], isExpired=false}\n[java] [org.wso2.si.tcp.server.TCPServer] : Event{timestamp=1512446442300, data=[chocolate, 78.23], isExpired=false}\n</code></pre></li> </ol>"},{"location":"samples/PublishTcpInTextFormat/#notes","title":"Notes:","text":"<p>If you need to edit this application while it is running, stop the application -&gt; Save -&gt; Start. If you see the message <code>'LowProductionAlertStream' stream could not connect to 'localhost:9892'</code>, it could be due to port 9892, defined in the Siddhi application. This port is already being used by a different program. To resolve this issue, please do the following: * Stop this Siddhi application (click 'Run' on menu bar -&gt; 'Stop'). * Change the port 9893 to an unused port in this Siddhi application's source configuration and also change the port number in the tcp-server file. * Start the application and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishTcpInTextFormat\")\n@App:description('Send events via TCP transport using text format')\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='tcp', url='tcp://localhost:9892/LowProductionAlertStream',\n@map(type='text'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/PublishWebSocketInXmlFormat/","title":"Publishing XML Events via WebSocket","text":""},{"location":"samples/PublishWebSocketInXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send sweet production events via WebSocket transport in XML default format and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/PublishWebSocketInXmlFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample. If there is no syntax error, the following message is shown on the console: <pre><code>Siddhi App PublishWebSocketInXmlFormat successfully deployed.\n</code></pre></p>"},{"location":"samples/PublishWebSocketInXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Open a terminal and navigate to the <code>{WSO2SIHome}/samples/sample-clients/websocket-receiver</code> directory and run the <code>ant</code> command.<ul> <li>If you use the default host 'localhost' and port '8025' in your program use <code>ant</code> command without any arguments.</li> <li>However, if you use different host or port, run the <code>ant</code> command with appropriate arguments. e.g., <code>ant -Dport=9025</code></li> </ul> </li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>PublishWebSocketInXmlFormat.siddhi - Started Successfully!\n</code></pre></li> <li>Open the event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: PublishWebSocketInXmlFormat</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the 'name' and 'amount' fields, enter 'toffee' and '45.24' respectively, and then click Send to send the event.</li> <li>Send some more events.</li> <li>Check the output in the terminal of <code>{WSO2SIHome}/samples/sample-clients/websocket-receiver</code>. You will see output similar to the following: <pre><code>WebSocketSample : logStream : Event{timestamp=1517982716368, data=[toffee, 45.25], isExpired=false}\nWebSocketSample : logStream : Event{timestamp=1517982792293, data=[coffee, 9.78], isExpired=false}\nWebSocketSample : logStream : Event{timestamp=1517982828856, data=[chocolate, 78.23], isExpired=false}\n</code></pre></li> </ol>"},{"location":"samples/PublishWebSocketInXmlFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal.</p>"},{"location":"samples/PublishWebSocketInXmlFormat/#notes","title":"Notes:","text":"<p>If the message <code>'LowProductionAlertStream' stream could not connect to 'localhost:8025'</code>, it could be due to port 8025 defined in the Siddhi application is already being used by a different program. To resolve this issue, do the following: 1. Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). 2. Change the port from 8025 to an unused port in this Siddhi application's source configuration and in the websocket-receiver file. 3. Start the application and check whether the expected output appears on the console.</p> <pre><code>@App:name(\"PublishWebSocketInXmlFormat\")\n@App:description('Send events via WebSocket transport using XML format')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='websocket', url='ws://localhost:8025/abc',\n@map(type='xml'))\ndefine stream LowProductionAlertStream (name string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveAndCount/","title":"Receiving Events via HTTP Transport","text":""},{"location":"samples/ReceiveAndCount/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events via HTTP transport and view the output on the console. The count of all events arriving to the stream is calculated.</p>"},{"location":"samples/ReceiveAndCount/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveAndCount/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li> <p>Start the Siddhi application by clicking on 'Run'.</p> </li> <li> <p>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Source Listener has created for url http://localhost:8006/productionStream\n* ReceiveAndCount.siddhi - Started Successfully!\n</code></pre></p> </li> </ol>"},{"location":"samples/ReceiveAndCount/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.</p>"},{"location":"samples/ReceiveAndCount/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/ReceiveAndCount/#publish-events-to-the-http-endpoint-defined-by-receiverurl-in-source-configuration","title":"Publish events to the http endpoint defined by <code>receiver.url</code> in Source configuration.","text":""},{"location":"samples/ReceiveAndCount/#publish-events-with-the-client","title":"Publish events with the client:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/http-client</code> and run <code>ant</code> command as follows:</p> <p>Run <code>ant</code> command in the terminal. If you want to publish custom number of events, you need to run \"ant\" command as follows. <pre><code>ant -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveAndCount/#publish-events-with-curl-command","title":"Publish events with curl command:","text":"<p>Publish few events in json format to the http endpoint as follows (The values for name, age and country attributes can be changed as desired). <pre><code>curl -X POST -d \"{\\\"event\\\":{\\\"name\\\":\\\"Cake\\\",\\\"amount\\\":20.12}}\" http://localhost:8006/productionStream --header \"Content-Type:application/json\"\n</code></pre></p>"},{"location":"samples/ReceiveAndCount/#publish-events-with-postman","title":"Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows.     <pre><code>{\n\"event\": {\n\"name\": \"Cake\",\n\"amount\": 20.12\n}\n}\n</code></pre></li> </ol>"},{"location":"samples/ReceiveAndCount/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the console. Note how the count increments with every event you send.</p> <pre><code>@App:name(\"ReceiveAndCount\")\n@App:description('Receive events via HTTP transport and view the output on the console')\n\n\n@Source(type = 'http',\nreceiver.url='http://localhost:8006/productionStream',\nbasic.auth.enabled='false',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream TotalCountStream (totalCount long);\n\n-- Count the incoming events\n@info(name='query1')\nfrom SweetProductionStream\nselect count() as totalCount\ninsert into TotalCountStream;\n</code></pre>"},{"location":"samples/ReceiveEmailInXmlFormat/","title":"Receiving XML Events via Email","text":""},{"location":"samples/ReceiveEmailInXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to use <code>siddhi-io-email</code> for receiving events from emails.</p>"},{"location":"samples/ReceiveEmailInXmlFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>Add relevant <code>siddhi-io-email</code> and <code>siddhi-map-xml</code> jars to the <code>{WSO2Home}/lib</code> folder if not exist.</p> </li> <li> <p>Make sure you have provide less secure access to the sender's email account. eg: For gmail this can be done by visiting https://myaccount.google.com/lesssecureapps.</p> </li> <li> <p>Edit the siddhi app by providing following details.</p> <ul> <li><code>receiver_email_username</code></li> <li><code>receiver_email_password</code></li> </ul> </li> <li> <p>Give the subject of the email to . For further information search.terms refer https://wso2-extensions.github.io/siddhi-io-email/api/1.0.9/. <li> <p>Save this sample.</p> </li>"},{"location":"samples/ReceiveEmailInXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>ReceiveEmailInXmlFormat.siddhi - Started Successfully!\n</code></pre></li> <li>Send an email with the body in following format to receiver's email address.     <pre><code>&lt;events&gt;\n&lt;event&gt;\n&lt;name&gt;WSO2&lt;/name&gt;\n&lt;amount&gt;34.56&lt;/amount&gt;\n&lt;/event&gt;\n&lt;/events&gt;\n</code></pre></li> <li>The message should be logged in the console.</li> </ol> <pre><code>@App:name(\"ReceiveEmailInXmlFormat\")\n\n\n@source(type='email', @map(type='xml'),\nusername='&lt;receiver_username&gt;',\npassword='&lt;receiver_email_password&gt;',\nstore = 'imap' ,\nhost = 'imap.gmail.com',\nfolder = 'INBOX',\nssl.enable = 'true' ,\npolling.interval = '30' ,\nsearch.term = 'subject:&lt;subject_of_mail&gt;' ,\ncontent.type = 'text/plain')\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, hourlyTotal double, currentHour double);\n\nfrom SweetProductionStream\nselect name, sum(amount) as hourlyTotal,\nconvert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveEventsFromFile/","title":"Receiving Events via File","text":""},{"location":"samples/ReceiveEventsFromFile/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to use <code>siddhi-io-file</code> for receiving.</p>"},{"location":"samples/ReceiveEventsFromFile/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Edit this sample file by replacing <code>{WSO2SIHome}</code> with the absolute path of your WSO2SI home directory.</li> <li>Save this sample.</li> </ol>"},{"location":"samples/ReceiveEventsFromFile/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>ReceiveEventsFromFile.siddhi - Started Successfully!\n</code></pre></li> <li>Check the directories <code>{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed</code> and <code>new</code>.</li> <li>All the files which were in the directory <code>new</code> should have been moved to <code>consumed</code> directory.</li> </ol>"},{"location":"samples/ReceiveEventsFromFile/#note","title":"Note:","text":"<p>If the sample is not running and producing output, do the following first. * Move all the files in <code>{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed</code> directory to <code>{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/new</code>. * Delete all the files in consumed and sink directories.</p>"},{"location":"samples/ReceiveEventsFromFile/#viewing-the-results","title":"Viewing the Results:","text":"<p>Processed output events will be logged in the console as follows: <pre><code>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - ReceiveEventsFromFile: event, StreamEvent{ timestamp=1513847875990, beforeWindowData=null, onAfterWindowData=null, outputData=[apache, 80.0, 2.0], type=CURRENT, next=null}\nINFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - ReceiveEventsFromFile: event, StreamEvent{ timestamp=1513847876004, beforeWindowData=null, onAfterWindowData=null, outputData=[cloudbees, 134.4, 2.0], type=CURRENT, next=null}\n</code></pre></p> <pre><code>@App:name('ReceiveEventsFromFile')\n\n\n@source(type='file', mode='text.full',\ndir.uri='file:/{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/new',\naction.after.process='move',\ntailing='false',\nmove.after.process='file:/{WSO2SIHome}/samples/artifacts/ReceiveEventsFromFile/files/consumed',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\nfrom SweetProductionStream#window.time(1 min)\nselect name, sum(amount) as hourlyTotal, convert(time:extract('HOUR', time:currentTimestamp(), 'yyyy-MM-dd hh:mm:ss'), 'double') as currentHour\ninsert into LowProductionAlertStream;\n\nfrom LowProductionAlertStream#log('event')\ninsert into LogStream;\n</code></pre>"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/","title":"Receiving Messages from a Google Pub/Sub Topic","text":""},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling using googlepubsub source in Siddhi to consume events. Events which are in TEXT format are consumed from a googlepubsub topic.</p>"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Create a Google Cloud Platform account.</li> <li>Sign in to Google Account and set up a GCP Console project and enable the API.</li> <li>Create a service account and download a private key as JSON.</li> <li>Place your json file in any system folder and provide the path for the credential.path.</li> <li>Save the sample.</li> <li>If there is no syntax error, the following message is shown on the console:             * -Siddhi App ReceiveGooglePubSubMesssage successfully deployed.</li> </ol>"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console,</li> <li>ReceiveGooglePubSubMesssage.siddhi - Started Successfully!</li> </ol>"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Receive events through the following, You may listen to the events coming to a topic after the subscription is made.</li> </ol>"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveEvent : BarStream : Event{timestamp=1552548124599, data=[message:\"Hello\"], isExpired=false} (Encoded)</p>"},{"location":"samples/ReceiveGooglePubSubMessagesInTextFormat/#notes","title":"Notes:","text":"<pre><code>Make sure the the credential file is correct and user have write access to make api calls.</code></pre> <p>Stop this Siddhi application</p> <pre><code>@App:name(\"ReceiveGooglePubSubMesssage\")\n\n@App:description('Consume messages from a googlepubsub Topic.')\n\n\n@App:name(\"ReceiveEvent\")\n@App:description(\"Listen for messages received for a topic in Google Pub Sub Server.\")\n\n\n@source(type ='googlepubsub', project.id = 'sp-path-1547649404768', topic.id = 'topic75',\ncredential.path = '/../sp.json',\nsubscription.id = 'sub75', @map(type = 'text'))\n\ndefine stream FooStream (message string); @sink(type='log')\ndefine stream BarStream(message string);\n\n@info(name='EventsPassthroughQuery')\nfrom FooStream select message insert into BarStream;\n</code></pre>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/","title":"Receiving Custom JSON Events via HTTP","text":""},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the <code>SweetProductionStream</code> via HTTP transport in JSON format using custom mapping and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Source Listener has created for url http://localhost:8006/productionStream\n* ReceiveHTTPInJsonFormatWithDefaultMapping.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#option-1-publish-events-with-http-sample-client","title":"Option 1: Publish events with http sample client:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/http-client</code> and run <code>ant</code> command as follows:</p> <p>Run <code>ant</code> command in the terminal as follows: <pre><code>ant -DcustomMapping=true\n</code></pre> If you want to publish custom number of events, you need to run \"ant\" command as follows: <pre><code>ant -DcustomMapping=true -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#option-2-publish-events-with-curl-command","title":"Option 2: Publish events with curl command:","text":"<p>Publish few events in json format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). <pre><code>curl -X POST -d \"{\\\"item\\\": {\\\"id\\\":\\\"Sugar\\\",\\\"amount\\\": 300.5}}\"  http://localhost:8006/productionStream --header \"Content-Type:application/json\"\n</code></pre></p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#option-3-publish-events-with-postman","title":"Option 3: Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'POST' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows. <pre><code>{\n\"item\": {\n\"id\": \"sugar\",\n\"amount\": 20.0\n}\n}\n</code></pre></li> </ol>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.</p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithCustomMapping/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console. <pre><code>ReceiveHTTPInJsonFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveHTTPInJsonFormatWithCustomMapping\")\n\n@App:description('Receive events via HTTP transport in JSON format with custom mapping and view the output on the console')\n@source(type = 'http',\nreceiver.url='http://localhost:8006/productionStream',\nbasic.auth.enabled='false',@map(type='json', @attributes( name = '$.item.id', amount = '$.item.amount')))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/","title":"Receiving JSON Events via HTTP","text":""},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the <code>SweetProductionStream</code> via HTTP transport in JSON format using default mapping and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Source Listener has created for url http://localhost:8006/productionStream\n* ReceiveHTTPInXMLFormatWithDefaultMapping.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#option-1-publish-events-with-http-sample-client","title":"Option 1: Publish events with http sample client:","text":"<p>Navigate to {WSO2SIHome}/samples/sample-clients/http-client and run \"ant\" command as follows:</p> <p>Run <code>ant</code> command in the terminal. If you want to publish custom number of events, you need to run <code>ant</code> command as follows. <pre><code>ant -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#option-2-publish-events-with-curl-command","title":"Option 2: Publish events with curl command:","text":"<p>Publish few events in json format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). <pre><code>curl -X POST -d \"{\\\"event\\\": {\\\"name\\\":\\\"sugar\\\",\\\"amount\\\": 20.5}}\"  http://localhost:8006/productionStream --header \"Content-Type:application/json\"\n</code></pre></p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#option-3-publish-events-with-postman","title":"Option 3: Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'POST' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/json' and set the request body in json format as follows, <pre><code>{\n\"event\": {\n\"name\": \"sugar\",\n\"amount\": 20.0\n}\n}\n</code></pre></li> </ol>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console</p>"},{"location":"samples/ReceiveHTTPInJsonFormatWithDefaultMapping/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console. <pre><code>ReceiveHTTPInXMLFormatWithDefaultMapping : LowProductionAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name('ReceiveHTTPInJsonFormatWithDefaultMapping')\n@App:description('Receive events via HTTP transport in JSON format with default mapping and view the output on the console')\n\n\n@Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProductionAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/","title":"Receiving XML Events via HTTP","text":""},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the <code>SweetProductionStream</code> via HTTP transport in XML default format and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#executing-the-sample","title":"Executing the Sample:","text":"<p>1) Start the Siddhi application by clicking on 'Run'. 2) If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Source Listener has created for url http://localhost:8006/productionStream\n* ReceiveHTTPInXMLFormatWithDefaultMapping.siddhi - Started Successfully!\n</code></pre></p>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#option-1-publish-events-with-http-sample-client","title":"Option 1: Publish events with http sample client:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/http-client</code> and run <code>ant</code> command as follows:</p> <p>Run <code>ant</code> command in the terminal  as follows: <pre><code>ant -Dtype=xml\n</code></pre> If you want to publish custom number of events, you need to run <code>ant</code> command as follows: <pre><code>ant -Dtype=xml -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#option-2-publish-events-with-curl-command","title":"Option 2: Publish events with curl command:","text":"<p>Publish few events in xml format to the http endpoint as follows (The values for name and amount attributes can be changed as desired). <pre><code>curl -X POST -d '&lt;events&gt;&lt;event&gt;&lt;name&gt;sugar&lt;/name&gt;&lt;amount&gt;300&lt;/amount&gt;&lt;/event&gt;&lt;/events&gt;' http://localhost:8006/productionStream --header \"Content-Type:application/xml\"\n</code></pre></p>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#option-3-publish-events-with-postman","title":"Option 3: Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/xml' and set the request body in xml format as follows. <pre><code>&lt;events&gt;\n&lt;event&gt;\n&lt;name&gt;sugar&lt;/name&gt;\n&lt;amount&gt;200&lt;/amount&gt;\n&lt;/event&gt;\n&lt;/events&gt;\n</code></pre></li> </ol>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console</p>"},{"location":"samples/ReceiveHTTPInXMLFormatWithDefaultMapping/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console if you use option 2 or 3 to publish events. <pre><code>ReceiveHTTPInXMLFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveHTTPInXMLFormatWithDefaultMapping\")\n@App:description('Receive events via HTTP transport in XML format with default mapping and view the output on the console.')\n\n\n@Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',\n@map(type='xml'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/","title":"Receiving Custom XML Events via HTTP","text":""},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#purpose","title":"Purpose","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the <code>SweetProductionStream</code> via HTTP transport in XML custom format and log the  events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Source Listener has created for url http://localhost:8006/productionStream\n* ReceiveHTTPInXMLFormatWithCustomMapping.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#option-1-publish-events-with-http-sample-client","title":"Option 1: Publish events with http sample client:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/http-client</code> and run <code>ant</code> command as follows:</p> <p>Run <code>ant</code> command in the terminal  as follows: <pre><code>ant -Dtype=xml -DcustomMapping=true\n</code></pre> If you want to publish custom number of events, you need to run \"ant\" command as follows: <pre><code>ant -Dtype=xml -DcustomMapping=true -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#option-2-publish-events-with-curl-command","title":"Option 2: Publish events with curl command:","text":"<pre><code>curl -X POST -d '&lt;events&gt;&lt;item&gt;&lt;id&gt;sugar&lt;/id&gt;&lt;amount&gt;300&lt;/amount&gt;&lt;/item&gt;&lt;/events&gt;' http://localhost:8006/productionStream --header \"Content-Type:application/xml\"\n</code></pre>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#option-3-publish-events-with-postman","title":"Option 3: Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to 'http://localhost:8006/productionStream' endpoint. Set the Content-Type to 'application/xml' and set the request body in xml format as follows. <pre><code>&lt;events&gt;\n&lt;item&gt;\n&lt;id&gt;sugar&lt;/id&gt;\n&lt;amount&gt;200&lt;/amount&gt;\n&lt;/item&gt;\n&lt;/events&gt;\n</code></pre></li> </ol>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start. If the message \"Source Listener has created for url http://localhost:8006/productionStream\" does not appear,it could be due to port 8006, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 8006 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console</p>"},{"location":"samples/ReceiveHTTPinXMLFormatWithCustomMapping/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console. <pre><code>ReceiveHTTPinXMLFormatWithCustomMapping : LowProducitonAlertStream : Event{timestamp=1511939868628, data=[sugar, 300.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveHTTPinXMLFormatWithCustomMapping\")\n\n@App:description('Receive events via HTTP transport in XML format with custom mapping and view the output on the console.')\n\n@Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false',\n@map(type='xml',@attributes( name = \"item/id\", amount = \"item/amount\")))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveHl7InER7Format/","title":"Receiving ER7 Events via HL7","text":""},{"location":"samples/ReceiveHl7InER7Format/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive Hl7 events in ER7 format to the hl7Stream via MLLP protocol and log the events in er7Stream to the output console.</p>"},{"location":"samples/ReceiveHl7InER7Format/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html)</li> <li>Save this sample. If there is no syntax error, the following message is shown on the console:</li> <li> <ul> <li>Siddhi App ReceiveHl7InER7Format successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/ReceiveHl7InER7Format/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.<ul> <li>Starting SimpleServer running on port 4000</li> <li>ReceiveHl7InER7Format.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/ReceiveHl7InER7Format/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>In the HAPI testpanel create a sending connection with port that provided in the siddhi app.</li> <li>Send this message 'MSH|~\\&amp;|sendingSystemA|senderFacilityA|receivingSystemB|receivingFacilityB|20080925161613||ADTA01|589888ADT30502184808|P|2.3' from the testpanel</li> </ol>"},{"location":"samples/ReceiveHl7InER7Format/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console if you publish events. ReceiveHl7InER7Format : er7Stream : Event{timestamp=1552530948958, data=[MSH|~\\&amp;|||||20190211145413.131+0530||ADTA01|10601|T|2.3 ], isExpired=false}</p> <pre><code>@App:name('ReceiveHl7InER7Format')\n@App:description('This receives the HL7 messages and sends the acknowledgement message to the client using the MLLP protocol and text mapping.')\n\n\n@source(type = 'hl7', port = '4000', hl7.encoding = 'er7', @map(type = 'text'))\ndefine stream hl7stream(payload string);\n\n@sink(type='log')\ndefine stream er7Stream (payload string);\n\n@info(name='query1')\nfrom hl7stream\nselect *\ninsert into er7Stream;\n</code></pre>"},{"location":"samples/ReceiveHl7InXmlFormat/","title":"Receiving Custom XML Messages via HL7","text":""},{"location":"samples/ReceiveHl7InXmlFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive Hl7 events in XML format to the hl7Stream via MLLP protocol and log the events in xmlStream to the output console.</p>"},{"location":"samples/ReceiveHl7InXmlFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Install the HAPI testpanel. (Reference: https://hapifhir.github.io/hapi-hl7v2/hapi-testpanel/install.html)</li> <li>Save this sample. If there is no syntax error, the following message is shown on the console:<ul> <li>Siddhi App ReceiveHl7InXmlFormat successfully deployed.</li> </ul> </li> </ol>"},{"location":"samples/ReceiveHl7InXmlFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.<ul> <li>Starting SimpleServer running on port 4000</li> <li>ReceiveHl7InXmlFormat.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/ReceiveHl7InXmlFormat/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>In the HAPI testpanel create a sending connection with port that provided in the siddhi app.</li> <li>Send this message 'MSH|~\\&amp;|sendingSystemA|senderFacilityA|receivingSystemB|receivingFacilityB|20080925161613||ADTA01|589888ADT30502184808|P|2.3' from the testpanel</li> </ol>"},{"location":"samples/ReceiveHl7InXmlFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console if you publish events. ReceiveHl7InXmlFormat : er7Stream : Event{timestamp=1552532452870, data=[589888ADT30502184808, sendingSystemA], isExpired=false}</p> <pre><code>@App:name('ReceiveHl7InXmlFormat')\n@App:description('This receives the HL7 messages and sends the acknowledgement message to the client using the MLLP protocol and custom xml mapping.')\n\n\n@source(type = 'hl7', port = '4000', hl7.encoding = 'xml', @map(type = 'xml', namespaces = 'ns=urn:hl7-org:v2xml', @attributes(MSH10 = \"ns:MSH/ns:MSH.10\", MSH3HD1 = \"ns:MSH/ns:MSH.3/ns:HD.1\")))\ndefine stream hl7stream (MSH10 string, MSH3HD1 string);\n\n@sink(type='log')\ndefine stream xmlStream (MSH10 string, MSH3HD1 string);\n\n@info(name='query1')\nfrom hl7stream\nselect *\ninsert into xmlStream;\n</code></pre>"},{"location":"samples/ReceiveJMSInJsonFormat/","title":"Receiving JSON Events via JMS","text":""},{"location":"samples/ReceiveJMSInJsonFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the <code>SweetProductionStream</code> via Jms transport in Json format using default mapping and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveJMSInJsonFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Setup ActiveMQ<ul> <li>Download <code>activemq-client-5.x.x.jar</code> (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar)</li> <li>Download <code>apache-activemq-5.x.x-bin.zip</code> (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip)</li> <li>ActiveMQ <code>activemq-client-5.x.x.jar</code> lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI).</li> <li>Unzip the <code>apache-activemq-5.x.x-bin.zip</code> and copy the following ActiveMQ libs in <code>apache-activemq-5.x.x/lib</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code> and <code>{WSO2SIHome}/lib</code>.<ul> <li>hawtbuf-1.9.jar</li> <li>geronimo-j2ee-management_1.1_spec-1.0.1.jar</li> <li>geronimo-jms_1.1_spec-1.1.1.jar</li> </ul> </li> </ul> </li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>Siddhi App ReceiveJMSInJsonFormat successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/ReceiveJMSInJsonFormat/#note","title":"Note:","text":"<p>To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command:     * For Linux:     <pre><code>./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;/activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     * For Windows:     <pre><code>./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;\\activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     * Provide privileges if necessary using <code>chmod +x icf-provider.(sh|bat)</code>.     * Also, this will register the <code>InitialContextFactory</code> implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create 'activemq-client-5.x.x' directory in the  with OSGi converted and original jars:     - <code>activemq-client-5.x.x.jar</code> (Original Jar)     - <code>activemq-client-5.x.x_1.0.0.jar</code> (OSGi converted Jar)     Also, following messages would be shown on the terminal.         <pre><code>- INFO: Executing 'jar uf &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar -C &lt;absolute_path&gt;/activemq-client-5.x.x /internal/CustomBundleActivator.class'\n[timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader\n- INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle\n- INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar\n</code></pre> 3) You can find the osgi converted libs in <code>activemq-client-5.x.x</code> folder. You can copy <code>activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar</code> to <code>{WSO2SIHome}/lib</code> and <code>activemq-client-5.x.x/activemq-client-5.x.x.jar</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code>."},{"location":"samples/ReceiveJMSInJsonFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{apache-activemq-5.x.x}</code> unzipped directory.</li> <li>Provide required permissions by executing,     <pre><code>chmod +x bin/activemq\n</code></pre></li> <li>Create system wide configuration defaults, by executing,     <pre><code>sudo bin/activemq setup /etc/default/activemq\n</code></pre></li> <li>Start ActiveMQ server node using 'bin/activemq start'.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>ReceiveJMSInJsonFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveJMSInJsonFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/jms-producer</code> and run <code>ant</code> command without arguments.</p>"},{"location":"samples/ReceiveJMSInJsonFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the editor console. <pre><code>- INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveJMSInJsonFormat : OutputStream : Event{timestamp=1513617090756, data=[Cream Sandwich, 790.7842348407036], isExpired=false}\n</code></pre></p> <pre><code>@App:name('ReceiveJMSInJsonFormat')\n@App:description('Receive events via JMS provider in JSON format with default mapping and view the output on the console.')\n\n\n@source(type='jms',\nfactory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory',\nprovider.url='tcp://localhost:61616',\ndestination='jms_result_topic',\nconnection.factory.type='topic',\nconnection.factory.jndi.name='TopicConnectionFactory',\ntransport.jms.SubscriptionDurable='true',\ntransport.jms.DurableSubscriberClientID='wso2SPclient1',\n@map(type='json'))\ndefine stream SweetProductionStream(name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream(name string, amount double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveJMSInKeyvalueFormat/","title":"Receiving Key Value Events via JMS","text":""},{"location":"samples/ReceiveJMSInKeyvalueFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the <code>SweetProductionStream</code> via Jms transport in Keyvalue and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Setup ActiveMQ.<ul> <li>Download <code>activemq-client-5.x.x.jar</code> (http://central.maven.org/maven2/org/apache/activemq/activemq-client/5.9.0/activemq-client-5.9.0.jar).</li> <li>Download <code>apache-activemq-5.x.x-bin.zip</code> (http://archive.apache.org/dist/activemq/apache-activemq/5.9.0/apache-activemq-5.9.0-bin.zip).</li> <li>ActiveMQ <code>activemq-client-5.x.x.jar</code> lib to be added and converted to OSGI (See Note: To convert ActiveMQ lib to OSGI).</li> <li>Unzip the <code>apache-activemq-5.x.x-bin.zip</code> and copy the following ActiveMQ libs in <code>apache-activemq-5.x.x/lib</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code> and <code>{WSO2SIHome}/lib</code>.<ul> <li>hawtbuf-1.9.jar</li> <li>geronimo-j2ee-management_1.1_spec-1.0.1.jar</li> <li>geronimo-jms_1.1_spec-1.1.1.jar</li> </ul> </li> </ul> </li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>Siddhi App ReceiveJMSInKeyvalueFormat successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#note","title":"Note:","text":"<p>To convert ActiveMQ lib to OSGI, 1. Navigate to {WSO2SIHome}/bin and run the following command:     * For Linux:     <pre><code>./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;/activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     * For Windows:     <pre><code>./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory &lt;Downloaded Jar Path&gt;\\activemq-client-5.x.x.jar &lt;Output Jar Path&gt;\n</code></pre>     * Provide privileges if necessary using <code>chmod +x icf-provider.(sh|bat)</code>     * Also, this will register the <code>InitialContextFactory</code> implementation according to the OSGi JNDI spec. 2. If converted successfully then it will create <code>activemq-client-5.x.x</code> directory in the <code>&lt;Output Jar Path&gt;</code> with OSGi converted and original jars:     * <code>activemq-client-5.x.x.jar</code> (Original Jar)     * <code>activemq-client-5.x.x_1.0.0.jar</code> (OSGi converted Jar)     Also, following messages would be shown on the terminal         <pre><code>- INFO: Executing 'jar uf &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar -C &lt;absolute_path&gt;/activemq-client-5.x.x /internal/CustomBundleActivator.class'\n[timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader\n- INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle\n- INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file &lt;absolute_path&gt;/activemq-client-5.x.x/activemq-client-5.x.x.jar\n</code></pre> 3. You can find the osgi converted libs in <code>activemq-client-5.x.x</code> folder. You can copy <code>activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar</code> to <code>{WSO2SIHome}/lib</code> and <code>activemq-client-5.x.x/activemq-client-5.x.x.jar</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code>.</p>"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{apache-activemq-5.x.x}</code> unzipped directory and start ActiveMQ server node using <code>bin/activemq start</code>.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>ReceiveJMSInKeyvalueFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/jms-producer</code> and run the following command. <pre><code>ant -Dtype='keyvalue'\n</code></pre></p>"},{"location":"samples/ReceiveJMSInKeyvalueFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the editor console. <pre><code>- INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveJMSInKeyvalueFormat : OutputStream : Event{timestamp=1513617090756, data=[Cream Sandwich, 790.7842348407036], isExpired=false}\n</code></pre></p> <pre><code>@App:name('ReceiveJMSInKeyvalueFormat')\n@App:description('Receive events via JMS provider in Keyvalue format with default mapping and view the output on the console.')\n\n\n@source(type='jms',\nfactory.initial='org.apache.activemq.jndi.ActiveMQInitialContextFactory',\nprovider.url='tcp://localhost:61616',\ndestination='jms_result_topic',\nconnection.factory.type='topic',\nconnection.factory.jndi.name='TopicConnectionFactory',\n@map(type='keyvalue'))\ndefine stream SweetProductionStream(name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream(name string, amount double);\n\n@info(name='EventsPassthroughQuery')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveKafkaInBinaryFormat/","title":"Receiving Binary Events via Kafka","text":""},{"location":"samples/ReceiveKafkaInBinaryFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via Kafka transport in Binary Format and log the events in LowProductionAlertStream to the output console.</p>"},{"location":"samples/ReceiveKafkaInBinaryFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>The following steps must be executed to enable WSO2 SI to receive events via the Kafka transport. Since you need to shut down the server to execute these steps, get a copy of these instructions prior to proceeding.<ol> <li>Download the Kafka broker from here: https://archive.apache.org/dist/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz.</li> <li>Convert and copy the Kafka client jars from the <code>{KafkaHome}/libs</code> directory to the <code>{WSO2SIHome}/libs</code> directory as follows.<ol> <li>Create a directory named <code>{Source}</code> in a preferred location in your machine and copy the following JARs to it from the <code>{KafkaHome}/libs</code> directory.<ul> <li>kafka_2.11-0.10.0.0.jar</li> <li>kafka-clients-0.10.0.0.jar</li> <li>metrics-core-2.2.0.jar</li> <li>scala-library-2.11.8.jar</li> <li>zkclient-0.8.jar</li> <li>zookeeper-3.4.6.jar</li> </ul> </li> <li>Create another directory named <code>{Destination}</code> in a preferred location in your machine.</li> <li>To convert all the Kafka jars you copied into the <code>{Source}</code> directory, issue the following command,<ul> <li>For Windows:     <pre><code>{WSO2SIHome}/bin/jartobundle.bat &lt;{Source} Directory Path&gt; &lt;{Destination} Directory Path&gt;\n</code></pre></li> <li>For Linux:     <pre><code>sh {WSO2SIHome}/bin/jartobundle.sh &lt;{Source} Directory Path&gt; &lt;{Destination} Directory Path&gt;\n</code></pre></li> </ul> </li> <li>Add the OSGI converted kafka libs from <code>{Destination}</code> directory to <code>{WSO2SIHome}/lib</code>.</li> <li>Add the original Kafka libs from <code>{Source}</code> to <code>{WSO2SIHome}/samples/sample-clients/lib</code>.</li> <li>Navigate to <code>{KafkaHome}</code> and start zookeeper node using following command.     <pre><code>sh bin/zookeeper-server-start.sh config/zookeeper.properties\n</code></pre></li> <li>Navigate to <code>{KafkaHome}</code> and start Kafka server node using following command. <pre><code>sh bin/kafka-server-start.sh config/server.properties\n</code></pre></li> </ol> </li> </ol> </li> <li>Save this sample.</li> </ol>"},{"location":"samples/ReceiveKafkaInBinaryFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.         <pre><code>* ReceiveKafkaInBinaryFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveKafkaInBinaryFormat/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start.</p>"},{"location":"samples/ReceiveKafkaInBinaryFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/kafka-producer</code> and run \"ant\" command as follows: <pre><code>ant -DnoOfEventsToSend=5 -DtopicName=kafka_sample_topic -DisBinaryMessage=true\n</code></pre></p>"},{"location":"samples/ReceiveKafkaInBinaryFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveKafkaInBinaryFormat : LowProductionAlertStream : Event{timestamp=1513282182570, data=[\"Cupcake\", 1665.0], isExpired=false}\n</code></pre></p>"},{"location":"samples/ReceiveKafkaInBinaryFormat/#note","title":"Note:","text":"<ul> <li>Stop this Siddhi application, once you are done with the execution.</li> <li>Stop Kafka server and Zookeeper server individually by executing Ctrl+C.</li> </ul> <pre><code>@App:name(\"ReceiveKafkaInBinaryFormat\")\n@App:description('Receive events via Kafka transport in Binary format and view the output on the console')\n\n\n@source(type='kafka',\ntopic.list='kafka_sample_topic',\npartition.no.list='0',\nthreading.option='single.thread',\ngroup.id='group',\nis.binary.message='true',\nbootstrap.servers='localhost:9092',\n@map(type='binary'))\ndefine stream SweetProductionStream(id string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream(id string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/","title":"Receiving Custom Text Events via Kafka","text":""},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the <code>SweetProductionStream</code> via Kafka transport in Text format using custom mapping and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Setup Kafka.<ul> <li>Kafka libs to be added and converted to OSGI from <code>{KafkaHome}/libs</code> are as follows.<ul> <li>kafka_2.11-0.10.0.0.jar</li> <li>kafka-clients-0.10.0.0.jar</li> <li>metrics-core-2.2.0.jar</li> <li>scala-library-2.11.8.jar</li> <li>zkclient-0.8.jar</li> <li>zookeeper-3.4.6.jar</li> </ul> </li> <li>Add the OSGI converted kafka libs to <code>{WSO2SIHome}/lib</code>.</li> <li>Add the kafka libs to <code>{WSO2SIHome}/samples/sample-clients/lib</code>.</li> </ul> </li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>* Siddhi App PublishKafkaInJsonFormat successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#note","title":"Note:","text":"<p>To convert Kafka libs to OSGI, 1. Create a folder (eg: kafka) and copy Kafka libs to be added from <code>{KafkaHome}/libs</code>. 2. Create another folder (eg: kafka-osgi, This folder will have the libs that converted to OSGI). 3. Navigate to <code>{WSO2SIHome}/bin</code> and issue the following command.     - For Linux:         <pre><code>./jartobundle.sh &lt;path/kafka&gt; &lt;path/kafka-osgi&gt;\n</code></pre>     - For Windows:         <pre><code>./jartobundle.bat &lt;path/kafka&gt; &lt;path/kafka-osgi&gt;\n</code></pre> 4. If converted successfully then for each lib, following messages would be shown on the terminal.     <pre><code>- INFO: Created the OSGi bundle &lt;kafka-lib-name&gt;.jar for JAR file &lt;absolute_path&gt;/kafka/&lt;kafka-lib-name&gt;.jar\n</code></pre> 5. You can find the osgi converted libs in kafka-osgi folder. You can copy that to <code>{WSO2SIHome}/lib</code>.</p>"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{KafkaHome}</code> and start zookeeper node using <code>bin/zookeeper-server-start.sh config/zookeeper.properties</code>.</li> <li>Navigate to <code>{KafkaHome}</code> and start kafka server node using <code>bin/kafka-server-start.sh config/server.properties</code>.</li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>- ReceiveKafkaInTextFormatWithCustomMapping.siddhi - Started Successfully!\n- Kafka version : 0.10.0.0\n- Kafka commitId : b8642491e78c5a13\n- Adding partition 0 for topic: kafka_sample_topic\n- Adding partitions [0] for topic: kafka_sample_topic\n- Subscribed for topics: [kafka_sample_topic]\n- Kafka Consumer thread starting to listen on topic/s: [kafka_sample_topic] with partition/s: [0]\n- Discovered coordinator 10.100.7.56:9092 (id: 2147483647 rack: null) for group group\n</code></pre></li> </ol>"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/kafka-producer</code> and run <code>ant</code> command as follows: <pre><code>ant -Dtype=text -DcustomMapping=true\n</code></pre></p>"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. <pre><code>- INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveKafkaInTextFormatWithCustomMapping: LowProductionAlertStream : Event{timestamp=1513282182570, data=[\"Cupcake\", 1665.0], isExpired=false}\n</code></pre></p>"},{"location":"samples/ReceiveKafkaInTextFormatWithCustomMapping/#note_1","title":"Note:","text":"<ul> <li>Stop this Siddhi application, once you are done with the execution.</li> <li>Stop Kafka server and Zookeeper server individually by executing Ctrl+C.</li> </ul> <pre><code>@App:name(\"ReceiveKafkaInTextFormatWithCustomMapping\")\n@App:description('Receive events via Kafka transport in Text format with custom mapping and view the output on the console')\n\n\n@source(type='kafka',\ntopic.list='kafka_sample_topic',\npartition.no.list='0',\nthreading.option='single.thread',\ngroup.id=\"group\",\nbootstrap.servers='localhost:9092',\n@map(type='text',fail.on.missing.attribute='true', regex.A='(id):(.*)', regex.B='(amount):([-.0-9]+)',\n@attributes(id = 'A[2]', amount = 'B[2]')))\ndefine stream SweetProductionStream(id string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream(id string, amount double);\n\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveMQTTInXMLFormat/","title":"Receiving XML events via MQTT","text":""},{"location":"samples/ReceiveMQTTInXMLFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling  to receive events to the SweetProductionStream via MQTT transport in XML format and log the events in LowProductionAlertStream to the output console.</p>"},{"location":"samples/ReceiveMQTTInXMLFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample. \"<code>Siddhi App ReceiveMQTTInXMLFormat successfully deployed</code>\" message would be shown in the console.</li> <li>Before running this MQTT sample, set up mosquitto server which supports MQTT. This can be done by the following commands:     <pre><code>sudo apt-get update\nsudo apt-get install mosquitto\n</code></pre></li> <li>Install mosquitto client packages by executing following command.     <pre><code>sudo apt-get install mosquitto-clients\n</code></pre></li> <li>After the set up ,start the mosquitto server by running the following command.     <pre><code>sudo service mosquitto start\n</code></pre></li> </ol>"},{"location":"samples/ReceiveMQTTInXMLFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run', the following messages would be shown on the console.     <pre><code>ReceiveMQTTInXMLFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveMQTTInXMLFormat/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/ReceiveMQTTInXMLFormat/#option-1-publish-events-with-the-command-line-publisher","title":"Option 1: Publish events with the command line publisher:","text":"<p>Open a terminal and publish events using following command. (The values for name and amount attributes can be changed as desired). <pre><code>mosquitto_pub -t 'mqtt_topic_input' -m '&lt;events&gt;&lt;event&gt;&lt;name&gt;sugar&lt;/name&gt;&lt;amount&gt;300.0&lt;/amount&gt;&lt;/event&gt;&lt;/events&gt;'\n</code></pre></p>"},{"location":"samples/ReceiveMQTTInXMLFormat/#option-2-publish-events-with-mqtt-sample-client","title":"Option 2: Publish events with mqtt sample client:","text":"<ol> <li>Open a terminal and navigate to <code>&lt;WSO2_SI_HOME&gt;/samples/sample-clients/mqtt-client</code>.</li> <li>Run the following command in the terminal:     <pre><code>ant\n</code></pre>     If you want to publish custom number of events, you need to run <code>ant</code> command as follows.     <pre><code>ant -DnoOfEventsToSend=5\n</code></pre></li> </ol>"},{"location":"samples/ReceiveMQTTInXMLFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console. <pre><code>ReceiveHTTPInXMLFormatWithDefaultMapping : LowProducitonAlertStream : Event{timestamp=1511938781887, data=[sugar, 300.0], isExpired=false}\n</code></pre></p>"},{"location":"samples/ReceiveMQTTInXMLFormat/#note","title":"Note:","text":"<ol> <li>Stop this Siddhi application.</li> <li>Stop the mosquitto server using following command once you are done with the execution.     <pre><code>sudo service mosquitto stop\n</code></pre></li> </ol> <pre><code>@App:name(\"ReceiveMQTTInXMLFormat\")\n@App:description('Receive events via MQTT transport in XML format and view the output on the console.')\n\n\n@source(type='mqtt', url= 'tcp://localhost:1883',topic='mqtt_topic_input', @map(type='xml'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceivePrometheusMetrics/","title":"Receiving Prometheus Metrics","text":""},{"location":"samples/ReceivePrometheusMetrics/#purpose","title":"Purpose:","text":"<p>This application demonstrates how  to retrieve Prometheus metrics that are exported at an HTTP endpoint via the <code>prometheus</code> source.</p>"},{"location":"samples/ReceivePrometheusMetrics/#prerequisites","title":"Prerequisites:","text":"<p>The following steps must be executed to enable WSO2 Streaming Integrator to publish and retrieve events via Prometheus.</p> <ol> <li> <p>To complete the installation of the Prometheus extension in WSO2 Streaming Integrator Tooling, follow the steps below:</p> <ol> <li> <p>Download the following JARs.</p> <ul> <li> <p>simpleclient_common-0.5.0.jar</p> </li> <li> <p>simpleclient-0.5.0.jar</p> </li> <li> <p>simpleclient_httpserver-0.5.0.jar</p> </li> <li> <p>simpleclient_pushgateway-0.5.0.jar</p> </li> </ul> </li> <li> <p>Place the JARs you downloaded in the <code>&lt;SI_TOOLING_HOME&gt;/lib</code> directory.</p> </li> </ol> </li> <li> <p>Start WSO2 Streaming Integrator Tooling, navigate to the <code>&lt;SI_TOOLING_HOME&gt;/bin</code> directory and issue the appropriate command based on your operating system.</p> <ul> <li>For Windows: <code>server.bat --run</code></li> <li>For Linux/MacOS: <code>./server.sh</code></li> </ul> </li> <li> <p>Save the sample <code>EnergyAlertApp</code> Siddhi application.</p> <p>When the Siddhi application is successfully deployed in Streaming Integrator Tooling, the following message is logged in the console.</p> <p><code>\"Siddhi App EnergyAlertApp successfully deployed\"</code></p> </li> <li> <p>Navigate to <code>&lt;SI_TOOLING_HOME&gt;/samples/sample-clients/prometheus-client</code> and issue the <code>ant</code> command as follows.</p> <p><code>ant</code></p> </li> </ol>"},{"location":"samples/ReceivePrometheusMetrics/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li> <p>Start the <code>EnergyAlertApp</code> Siddhi application you saved by opening it and then clicking the Start button in the toolbar.</p> <p>When the Siddhi application is successfully started, the following messages are logged in the terminal.</p> <ul> <li><code>ReceivePrometheusMetrics.siddhi - Started Successfully!</code></li> <li><code>PowerConsumptionStream has successfully connected at http://localhost:9080</code></li> </ul> </li> </ol>"},{"location":"samples/ReceivePrometheusMetrics/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following are logged in the terminal. <pre><code>- INFO {io.siddhi.core.stream.output.sink.LogSink} - HIGH POWER CONSUMPTION : Event{timestamp=1*********, data=[server001, F3Room2, **, **], isExpired=false}\n- INFO {io.siddhi.core.stream.output.sink.LogSink} - HIGH POWER CONSUMPTION : Event{timestamp=1*********, data=[server002, F2Room2, **, **], isExpired=false}\n</code></pre></p> <p>The complete sample Siddhi Application is as follows.</p> <pre><code>@App:name(\"EnergyAlertApp\")\n@App:description(\"Use siddhi-io-prometheus retrieve and analyse Prometheus metrics in Siddhi\")\n\n\n@source(type='prometheus' , target.url='http://localhost:9080/metrics',metric.type='counter', metric.name='total_device_power_consumption_WATTS', scrape.interval='5',\n@map(type='keyvalue'))\ndefine stream devicePowerStream(deviceID string, roomID string, value int);\n\n@sink(type='log', priority='WARN', prefix ='High power consumption')\ndefine stream AlertStream (deviceID string, roomID string, initialPower int, finalPower int);\n\n@sink(type='log', priority='WARN', prefix ='Logging purpose')\ndefine stream LogStream (deviceID string, roomID string, power int);\n\n@info(name='power increase pattern')\nfrom every( e1=devicePowerStream ) -&gt; e2=devicePowerStream[ e1.deviceID == deviceID and (e1.value + 5) &lt;= value]\nwithin 1 min\nselect e1.deviceID, e1.roomID, e1.value as initialPower, e2.value as finalPower\ninsert into AlertStream;\n</code></pre>"},{"location":"samples/ReceiveRabbitmqInJSONFormat/","title":"Receiving JSON Events via RabbitMQ","text":""},{"location":"samples/ReceiveRabbitmqInJSONFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the <code>SweetProductionStream</code> via rabbitmq broker in JSON format using default mapping and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample.</li> <li>Install the rabbitmq server using the following command     <pre><code>sudo apt-get install rabbitmq-server.\n</code></pre></li> <li>To enable RabbitMQ Management Console, run the following:     <pre><code>sudo rabbitmq-plugins enable rabbitmq_management.\n</code></pre></li> <li>To start the service, issue the following command:     <pre><code>invoke-rc.d rabbitmq-server start\n</code></pre></li> </ol>"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* ReceiveRabbitmqInJSONFormat.siddhi - Started Successfully!\n</code></pre> Check whether the exchange 'rabbitmq_sample' is created in the rabbitmq server or not. To check that you can visit http://localhost:15672/.</li> </ol>"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#testing-the-sample","title":"Testing the Sample:","text":""},{"location":"samples/ReceiveRabbitmqInJSONFormat/#publish-events-with-rabbitmq-sample-publisher","title":"Publish events with rabbitmq sample publisher:","text":"<p>Open a terminal and issue command from the <code>{WSO2SIHome}/samples/sample-clients/rabbitmq-producer</code> and run <code>ant</code> command. If you want to publish custom number of events, you need to run <code>ant</code> command as follows: <pre><code>ant -DnoOfEventsToPublish=5\n</code></pre></p>"},{"location":"samples/ReceiveRabbitmqInJSONFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233900122, data=[Lollipop, 6186.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233901122, data=[Donut, 7904.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233902124, data=[Honeycomb, 4495.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveRabbitmqInJSONFormat : LowProducitonAlertStream : Event{timestamp=1513233903125, data=[Donut, 1393.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveRabbitmqInJSONFormat\")\n@app:description(\"Receives the events from the rabbitmq broker using the AMQP protocol.\")\n\n@source(type='rabbitmq', uri = 'amqp://guest:guest@localhost:5672', exchange.name = 'rabbitmq_sample',  @map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProductionAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/","title":"Receiving Custom Text Events via TCP","text":""},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator to receive events to the SweetProductionStream via TCP transport in Text default format and log the events in LowProductionAlertStream to the  output  console.</p>"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Tcp Server started in 0.0.0.0:9892\n* ReceiveTCPInTextFormatWithCustomMapping.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/tcp-client</code> and run <code>ant</code> command as follows: Run <code>ant</code> command in the terminal as follows: <pre><code>ant -Dtype=text -DcustomMapping=true\n</code></pre> If you want to publish custom number of events, you need to run <code>ant</code> command as follows. <pre><code>ant -Dtype=text -DcustomMapping=true -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#notes","title":"Notes:","text":"<p>If the message <code>Source Listener has created for url tcp://localhost:9892/SweetProductionStream</code> does not appear, it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following. * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.</p>"},{"location":"samples/ReceiveTCPInTextFormatWithCustomMapping/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following messages would be shown on the console continuousely. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971370311, data=[\"Eclair\", 132.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971371299, data=[\"Ice\", 6733.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971372300, data=[\"Lollipop\", 742.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971373300, data=[\"Marshmallow\", 8781.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971374301, data=[\"Marshmallow\", 5105.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971375301, data=[\"Froyo\", 5531.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPInTextFormatWithCustomMapping : LowProductionAlertStream : Event{timestamp=1512971376301, data=[\"Gingerbread\", 2193.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveTCPInTextFormatWithCustomMapping\")\n@App:description('Receive events via TCP transport in text format with custom mapping and view the output on the console.')\n\n\n@Source(type = 'tcp', context='SweetProductionStream',\n@map(type='text', fail.on.unknown.attribute = 'true', regex.A='((?&lt;=id:)(.*)(?=\n))',regex.B='([-0-9]+)', @attributes(name ='A',amount= 'B')))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveTCPinBinaryFormat/","title":"Receiving Binary Events via TCP","text":""},{"location":"samples/ReceiveTCPinBinaryFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in binary default format and log the events in LowProductionAlertStream to the output console.</p>"},{"location":"samples/ReceiveTCPinBinaryFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveTCPinBinaryFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Tcp Server started in 0.0.0.0:9892\n* ReceiveTCPinBinaryFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveTCPinBinaryFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/tcp-client</code> and run <code>ant</code> command as follows: Run <code>ant</code> command in the terminal as follows: <pre><code>ant -Dtype=binary\n</code></pre> If you want to publish custom number of events, you need to run <code>ant</code> command as follows: <pre><code>ant -Dtype=binary -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveTCPinBinaryFormat/#notes","title":"Notes:","text":"<p>If the message <code>Source Listener has created for url tcp://localhost:9892/SweetProductionStream</code> does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following. * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.</p>"},{"location":"samples/ReceiveTCPinBinaryFormat/#viewing-the-results","title":"Viewing the Results:","text":"<pre><code>[2017-12-11 15:55:12,682]  INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987912665, data=[Cupcake, 10.550546580727683], isExpired=false}\n[2017-12-11 15:55:13,671]  INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987913669, data=[Donut, 98.45360926759642], isExpired=false}\n[2017-12-11 15:55:14,672]  INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987914670, data=[Eclair, 48.77465755572478], isExpired=false}\n[2017-12-11 15:55:15,672]  INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987915671, data=[Froyo, 28.209321491656706], isExpired=false}\n[2017-12-11 15:55:16,673]  INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinBinaryFormat : LowProductionAlertStream : Event{timestamp=1512987916671, data=[Gingerbread, 110.46772110205492], isExpired=false}\n</code></pre> <pre><code>@App:name(\"ReceiveTCPinBinaryFormat\")\n@App:description('Receive events via TCP transport in binary format with default mapping and view the output on the console.')\n\n\n@Source(type = 'tcp', context='SweetProductionStream',\n@map(type='binary'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveTCPinJSONFormat/","title":"Receiving JSON Events via TCP","text":""},{"location":"samples/ReceiveTCPinJSONFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in JSON format using default mapping and log the events in LowProductionAlertStream to the output console.</p>"},{"location":"samples/ReceiveTCPinJSONFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveTCPinJSONFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Tcp Server started in 0.0.0.0:9892\n* ReceiveTCPinJSONFormat - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveTCPinJSONFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/tcp-client</code> and run <code>ant</code> command as follows: <pre><code>ant -Dtype=application/json\n</code></pre> If you want to publish custom number of events, you need to run <code>ant</code> command as follows. <pre><code>ant -Dtype=application/json -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveTCPinJSONFormat/#notes","title":"Notes:","text":"<p>If the message <code>Tcp Server started in 0.0.0.0:9892</code> does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.</p>"},{"location":"samples/ReceiveTCPinJSONFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following messages would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049050858, data=[Gingerbread, 6664.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049051858, data=[Cream Sandwich, 6190.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049052859, data=[Gingerbread, 9725.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049053859, data=[Donut, 7777.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinJSONFormat : LowProductionAlertStream : Event{timestamp=1513049054860, data=[Honeycomb, 8818.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveTCPinJSONFormat\")\n@App:description('Receive events via TCP transport in JSON format with default mapping and view the output on the console.')\n\n\n@Source(type = 'tcp',\ncontext='SweetProductionStream',\n@map(type='json'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveTCPinTextFormat/","title":"Receiving Text Events via TCP","text":""},{"location":"samples/ReceiveTCPinTextFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the <code>SweetProductionStream</code> via TCP transport in text default format and log the events in <code>LowProductionAlertStream</code> to the  output  console.</p>"},{"location":"samples/ReceiveTCPinTextFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveTCPinTextFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Tcp Server started in 0.0.0.0:9892\n* ReceiveTCPinTextFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveTCPinTextFormat/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/tcp-client</code> and run <code>ant</code> command as follows: <pre><code>ant -Dtype=text\n</code></pre> If you want to publish custom number of events, you need to run <code>ant</code> command as follows. <pre><code>ant -Dtype=text -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/ReceiveTCPinTextFormat/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start. If the message \"Tcp Server started in 0.0.0.0:9892\" does not appear,it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console.</p>"},{"location":"samples/ReceiveTCPinTextFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following messages would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990726372, data=[Eclair, 2171.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990727362, data=[Froyo, 1155.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990728363, data=[Gingerbread, 8840.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990729363, data=[Marshmallow, 7400.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveTCPinTextFormat : LowProductionAlertStream : Event{timestamp=1512990730364, data=[Cupcake, 889.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveTCPinTextFormat\")\n@App:description('Receive events via TCP transport in text format with default mapping and view the output on the console.')\n\n\n@Source(type = 'tcp', context='SweetProductionStream',\n@map(type='text'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProductionAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/ReceiveWebSocketInXMLFormat/","title":"Receiving XML Events via Websocket","text":""},{"location":"samples/ReceiveWebSocketInXMLFormat/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the <code>SweetProductionStream</code> via WebSocket transport in XML default format and log the events in <code>LowProductionAlertStream</code> to the output console.</p>"},{"location":"samples/ReceiveWebSocketInXMLFormat/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample.</p>"},{"location":"samples/ReceiveWebSocketInXMLFormat/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Navigate to <code>{WSO2SIHome}/samples/sample-clients/websocket-producer</code> and run <code>ant</code> command as follows:     Run <code>ant</code> command in the terminal.     <pre><code>ant\n</code></pre>     If you want to publish custom number of events, you need to run <code>ant</code> command as follows.     <pre><code>ant -DnoOfEventsToSend=5\n</code></pre></li> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* ReceiveWebSocketInXMLFormat.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/ReceiveWebSocketInXMLFormat/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985540005, data=[Honeycomb, 2700.3555330804284], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985541009, data=[Froyo, 4195.429933118964], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985542006, data=[Donut, 9625.837679695496], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985543008, data=[Froyo, 1909.568113992198], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - ReceiveWebSocketInXMLFormat : LowProductionAlertStream : Event{timestamp=1517985544012, data=[Lollipop, 291.8985351086241], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"ReceiveWebSocketInXMLFormat\")\n@App:description('Receive events via WebSocket transport in XML format and view the output on the console.')\n\n@Source(type = 'websocket', url='ws://localhost:8025/abc',\n@map(type='xml'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream LowProductionAlertStream (name string, amount double);\n\n-- passthrough data in the SweetProductionStream into LowProducitonAlertStream\n@info(name='query1')\nfrom SweetProductionStream\nselect *\ninsert into LowProductionAlertStream;\n</code></pre>"},{"location":"samples/RegexExecutionSample/","title":"Identifying Sub-sequences in Input Sequences","text":""},{"location":"samples/RegexExecutionSample/#purpose","title":"Purpose:","text":"<p>This function attempts to find the next sub-sequence of the inputSequence that matches the regex pattern. It returns true if such a sub sequence exists, or returns false otherwise</p>"},{"location":"samples/RegexExecutionSample/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample. If there is no syntax error, the following messages would be shown on the console. <pre><code>* Siddhi App RegexExecutionSample successfully deployed.\n</code></pre></p>"},{"location":"samples/RegexExecutionSample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* RegexExecutionSample.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/RegexExecutionSample/#testing-the-sample","title":"Testing the Sample:","text":"<p>You can publish data event to the file, through event simulator.</p> <ol> <li>Open event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name  : RegexExecutionSample</li> <li>Stream Name     : SweetProductionStream</li> </ul> </li> <li>Enter following values in the fields and send,     <pre><code>name: chocolate cake\nstartingIndex: 0\n</code></pre></li> <li>Enter following values in the fields and send     <pre><code>name: coffee cake\nstartingIndex: 0\n</code></pre></li> </ol>"},{"location":"samples/RegexExecutionSample/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - RegexExecutionSample : ChocolateProductStream : Event{timestamp=1513759840093, data=[chocolate cake, true], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - RegexExecutionSample : ChocolateProductStream : Event{timestamp=1513759907324, data=[coffee cake, false], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"RegexExecutionSample\")\n@App:description('Finds if a sweet is a chocolate product.')\n\n\ndefine stream SweetProductionStream (name string, startingIndex int);\n\n@sink(type='log')\ndefine stream ChocolateProductStream(name string, isAChocolateProduct bool);\n\nfrom SweetProductionStream\nselect name, regex:find(\"^chocolate(\\s*[a-zA-Z]+)\", str:lower(name), startingIndex) as isAChocolateProduct\ninsert into ChocolateProductStream;\n</code></pre>"},{"location":"samples/SNMPGetRequestApp/","title":"Receiving Custom Key Value Events via SNMP","text":""},{"location":"samples/SNMPGetRequestApp/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to resive snmp source via SNMP in keyvalue using custom mapping.</p>"},{"location":"samples/SNMPGetRequestApp/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample.</li> <li>Install snmp agent on your network node.<ul> <li>for linux can install snmpd / for windows it can be configured by 'windows features'.</li> <li>configure snmp agent ex:- community string, port, user access.</li> </ul> </li> <li>If there is no syntax error, the following message is shown on the console:<ul> <li> <ul> <li>SNMP-get-request-app successfully deployed.</li> </ul> </li> </ul> </li> </ol>"},{"location":"samples/SNMPGetRequestApp/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>SNMP-set-request-app - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/SNMPGetRequestApp/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output. Following message would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-get-request-app : logStream : Event{timestamp=1************, data=[1:28:33.05, mail@wso2.com], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-get-request-app : logStream : Event{timestamp=1************, data=[1:28:38.05, mail@wso2.com], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"SNMPGetRequestApp\")\n@App:description('listening oid status from agent')\n\n@source(type ='snmp',\n@map(type='keyvalue',    @attributes(sysUpTime= '1.3.6.1.2.1.1.3.0', sysContact = '1.3.6.1.2.1.1.4.0') ),\nhost ='127.0.0.1',\nversion = 'v1',\nrequest.interval = '5000',\ncommunity = 'public',\nagent.port = '2019',\noids='1.3.6.1.2.1.1.3.0, 1.3.6.1.2.1.1.4.0')\ndefine stream inputStream(sysUpTime string, sysContact string);\n\n@sink(type='log')\ndefine stream logStream (sysUpTime string, sysContact string);\n\n-- passthrough data in the inputStream to logStream\n@info(name='query1')\nfrom inputStream\nselect *\ninsert into logStream;\n</code></pre>"},{"location":"samples/SNMPSetRequestApp/","title":"Sending Custom Keyvalue Events via SNMP","text":""},{"location":"samples/SNMPSetRequestApp/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to send snmp set request via SNMP in keyvalue using custom mapping.</p>"},{"location":"samples/SNMPSetRequestApp/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample.</li> <li>Install snmp agent on your network node.<ul> <li>for linux can install snmpd / for windows it can be configured by 'windows features'.</li> <li>configure snmp agent ex:- community string = public,</li> </ul> </li> <li>If there is no syntax error, the following message is shown on the console:<ul> <li> <ul> <li>SNMP-set-request-app successfully deployed.</li> </ul> </li> </ul> </li> </ol>"},{"location":"samples/SNMPSetRequestApp/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:<ul> <li>SNMP-set-request-app - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/SNMPSetRequestApp/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Click on 'Event Simulator' (double arrows on left tab)</li> <li>Click 'Single Simulation' (this will be already selected)</li> <li>Select SNMP-set-request-app as 'Siddhi App Name'</li> <li>Select outputStream as 'StreamName'</li> <li>Provide attribute values<ul> <li>sysLocation : asia-branch-singapore</li> </ul> </li> <li>Click on the start button (Arrow symbol) next to the newly created simulator</li> </ol>"},{"location":"samples/SNMPSetRequestApp/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: INFO {io.siddhi.core.stream.output.sink.LogSink} - SNMP-set-request-app : logStream : Event{timestamp=1*********, data=[asia-branch-singapore], isExpired=false}</p>"},{"location":"samples/SNMPSetRequestApp/#notes","title":"Notes:","text":"<p>Make sure the port number is correct and user have write access to agent</p> <pre><code>@App:name(\"SNMPSetRequestApp\")\n@App:description('setting oids on agent')\n\n\n@Sink(type='snmp',\n@map(type='keyvalue', @payload('1.3.6.1.2.1.1.6.0' = 'sysLocation')),\nhost = '127.0.0.1',\nversion = 'v1',\ncommunity = 'public',\nagent.port = '2019')\ndefine stream outputStream(sysLocation string);\n\n@sink(type='log')\ndefine stream logStream(sysLocation string);\n\n@info(name='query_name')\nfrom outputStream\nselect sysLocation\ninsert into logStream;\n</code></pre>"},{"location":"samples/Script-js-sample/","title":"Using Javascript Functions in Siddhi Applications","text":""},{"location":"samples/Script-js-sample/#purpose","title":"Purpose:","text":"<p>This sample demonstrate how javascript functions can be used in Siddhi Applications.</p>"},{"location":"samples/Script-js-sample/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample.</li> </ol>"},{"location":"samples/Script-js-sample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.      <pre><code>* Script-js-sample.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/Script-js-sample/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>To open event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, select values as follows:<ul> <li>Siddhi App Name: Script-js-sample</li> <li>Stream Name: SweetProductiontream</li> </ul> </li> <li>In the name field and amount fields, enter 'toffee', '45.25' respectively and then click Send to send the event.</li> <li>Send some more events.</li> </ol>"},{"location":"samples/Script-js-sample/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the console. Description of the raw material with the passed details are shown in the logger.You will get the output as follows: <pre><code>[2017-12-20_14-26-03_120] INFO {io.siddhi.core.stream.output.sink.LogSink} - Script-js-sample : logStream : Event{timestamp=1513760163112, data=[toffee, 45.25, There are 45.25kg of toffee in the store], isExpired=false}\n</code></pre></p> <p>Notes: If you need to edit this application while it is running, then Save -&gt; Start.</p> <pre><code>@App:name('Script-js-sample')\n@App:Description('Demonstrate how javascript functions can be used in Siddhi Applications.')\n\n\ndefine stream sweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream logStream (name string, amount double, itemDescription string);\n\ndefine function detailedMaterial[JavaScript] return string {\nvar name = data[0];\nvar amount = data[1];\nvar res = \"There are \"+amount+\"kg of \"+name+ \" in the store\";\nreturn res;\n};\n\nfrom sweetProductionStream select name , amount, detailedMaterial(name,amount) as itemDescription\ninsert into detailedProductionstream;\n\nfrom detailedProductionstream\nselect *\ninsert into logStream;\n</code></pre>"},{"location":"samples/Store-cassandra/","title":"Receive Events via Simulator and Persist in Cassandra Store","text":""},{"location":"samples/Store-cassandra/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to perform Cassandra operations using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as the name of the raw material and amount used for production, can be stored using <code>insertSweetProductionStream</code>. The following streams can be used to search, delete, update, update or insert the existing data in the store:</p> <ul> <li>Search - <code>searchSweetProductionStream</code></li> <li>insert - <code>insertSweetProductionStream</code></li> <li>delete - <code>deleteSweetProductionStream</code></li> <li>update - <code>updateSweetProductionStream</code></li> <li>update or insert - <code>updateOrInsertSweetProductionStream</code></li> <li>contains - <code>containsSweetProductionStream</code> (verifies whether all the attributes that enter in the stream exist in the store)</li> </ul>"},{"location":"samples/Store-cassandra/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that Cassandra version 3 or above is installed on your machine.</li> <li>Add the DataStax Java driver into <code>{WSO2_SI_HOME}/lib</code> as follows:<ol> <li>Download the DataStax Java driver from: http://central.maven.org/maven2/com/datastax/cassandra/cassandra-driver-core/3.3.2/cassandra-driver-core-3.3.2.jar</li> <li>Use the <code>jartobundle</code> tool in <code>{WSO2_SI_Home}/bin</code> to extract and convert the above JARs into OSGi bundles.<ul> <li>For Windows:     <pre><code>&lt;SI_HOME&gt;/bin/jartobundle.bat &lt;PATH_OF_DOWNLOADED_JAR&gt; &lt;PATH_OF_CONVERTED_JAR&gt;\n</code></pre></li> <li>For Linux:     <pre><code>&lt;SI_HOME&gt;/bin/jartobundle.sh &lt;PATH_OF_DOWNLOADED_JAR&gt; &lt;PATH_OF_CONVERTED_JAR&gt;\n</code></pre> Note: The driver given in the above link is a OSGi bundled one. Please skip this step if the jar is already OSGi bunbled.</li> </ul> </li> <li>Copy the converted bundles to the {WSO2_SI_Home}/lib directory.</li> </ol> </li> <li>Create a keyspace named 'production' in Cassandra store: <pre><code>CREATE KEYSPACE \"production\" WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};\n</code></pre></li> <li>In the store configuration of this application, replace 'username' and 'password' values with your Cassandra credentials.</li> <li>Save this sample.</li> </ol>"},{"location":"samples/Store-cassandra/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following message is shown on the console.     <pre><code>Store-cassandra.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/Store-cassandra/#note","title":"Note:","text":"<p>If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again.</p>"},{"location":"samples/Store-cassandra/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events:<ol> <li>Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation'</li> <li>Select 'Store-cassandra' as 'Siddhi App Name' and select 'searchSweetProductionStream' as 'Stream Name'.</li> <li>Provide attribute values, and then click Send.</li> </ol> </li> <li>Send at least one event where the name matches a name value in the data you previously inserted into the <code>SweetProductionTable</code>. This will satisfy the 'on' condition of the join query.</li> <li>Optionally, send events to the other corresponding streams to add, delete, update, insert, and search events.</li> </ol>"},{"location":"samples/Store-cassandra/#notes","title":"Notes:","text":"<ul> <li>After a change in the store, you can use the search stream to see whether the operation is successful.</li> <li>The Primary Key constraint in <code>SweetProductionTable</code> is disabled, because the name cannot be used as a <code>PrimaryKey</code> in a <code>ProductionTable</code>.</li> <li>You can use Siddhi functions to create a unique ID for the received events, which can then be used to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)</li> </ul>"},{"location":"samples/Store-cassandra/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output for raw materials on the console. You can use <code>searchSweetProductionStream</code> to check for inserted, deleted, and updated events.</p> <pre><code>@App:name(\"Store-cassandra\")\n@App:description('Receive events via simulator and persist the received data in the store.')\n\n\ndefine stream insertSweetProductionStream (name string, amount double);\ndefine stream deleteSweetProductionStream (name string);\ndefine stream searchSweetProductionStream (name string);\ndefine stream updateSweetProductionStream (name string, amount double);\ndefine stream updateOrInsertSweetProductionStream (name string, amount double);\ndefine stream containsSweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream logStream(name string, amount double);\n\n@store(type='cassandra' , cassandra.host='localhost', username='cassandra', password='cassandra',keyspace='production',\ncolumn.family='SweetProductionTable')\ndefine table SweetProductionTable (name string, amount double);\n\n/* Inserting event into the cassandra keyspace */\n@info(name='query1')\nfrom insertSweetProductionStream\ninsert into SweetProductionTable;\n\n/* Deleting event from cassandra keyspace */\n@info(name = 'query2')\nfrom deleteSweetProductionStream\ndelete SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating event from cassandra keyspace */\n@info(name = 'query3')\nfrom updateSweetProductionStream\nupdate SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating or inserting event from cassandra keyspace */\n@info(name = 'query4')\nfrom updateOrInsertSweetProductionStream\nupdate or insert into SweetProductionTable\non SweetProductionTable.name == name;\n\n/* Siddhi In in cassandra keyspace */\n@info(name = 'query5')\nfrom containsSweetProductionStream\n[(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable]\ninsert into logStream;\n\n--Perform a join on raw material name so that the data in the store can be viewed\n@info(name='query6')\nfrom searchSweetProductionStream as s join SweetProductionTable as sp\non s.name == sp.name\nselect sp.name, sp.amount\ninsert into logStream;\n</code></pre>"},{"location":"samples/Store-rdbms/","title":"Receiving Events via Simulator and Persisting in RDBMS Store","text":""},{"location":"samples/Store-rdbms/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to perform RDBMS operations using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as the name of the raw material and amount used for production, can be stored using <code>insertSweetProductionStream</code>. The following streams can be used to search, delete, update, or upsert (update or insert) the existing data in the store: * search - <code>searchSweetProductionStream</code> * insert - <code>insertSweetProductionStream</code> * delete - <code>deleteSweetProductionStream</code> * update - <code>updateSweetProductionStream</code> * update or insert - <code>updateOrInsertSweetProductionStream</code> * contains - <code>containsSweetProductionStream</code> (verifies whether all the attributes that enter in the stream exist in the store)</p>"},{"location":"samples/Store-rdbms/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that MySQL is installed on your machine.</li> <li>Add the MySQL JDBC driver into <code>{WSO2_SI_HOME}/lib</code> as follows:<ol> <li>Download the JDBC driver from: https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz</li> <li>Unzip the archive.</li> <li>Copy <code>mysql-connector-java-5.1.45-bin.jar</code> to <code>{WSO2_SI_Home}/lib</code> directory.</li> </ol> </li> <li>Create a database named <code>production</code> in MySQL. This database is referred to with the <code>jdbc:mysql://localhost:3306/production</code> url.</li> <li>Create a table named <code>SweetProductionTable</code> as follows.     <pre><code>CREATE TABLE SweetProductionTable (name VARCHAR(20),amount double(10,2));\n</code></pre></li> <li> <p>Insert some values into the table as follows.     <pre><code>INSERT INTO SweetProductionTable VALUES ('Sugar',23.50);\n</code></pre> Note: You can also use <code>insertSweetProductionStream</code> for this.</p> </li> <li> <p>In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials.</p> </li> <li>Save this sample.</li> </ol>"},{"location":"samples/Store-rdbms/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following message is shown on the console. <pre><code>Store-rdbms.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/Store-rdbms/#note","title":"Note:","text":"<p>If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again.</p>"},{"location":"samples/Store-rdbms/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events:<ol> <li>Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation'</li> <li>Select 'Store-rdbms' as 'Siddhi App Name' and select 'searchSweetProductionStream' as 'Stream Name'.</li> <li>Provide attribute values, and then click Send.</li> </ol> </li> <li>Send at least one event where the name matches a name value in the data you previously inserted into the SweetProductionTable. This will satisfy the 'on' condition of the join query.</li> <li>Optionally, send events to the other corresponding streams to add, delete, update, insert, and search events.</li> </ol>"},{"location":"samples/Store-rdbms/#notes","title":"Notes:","text":"<ul> <li>After a change in the store, you can use the search stream to see whether the operation is successful.</li> <li>The Primary Key constraint in <code>SweetProductionTable</code> is disabled, because the name cannot be used as a <code>PrimaryKey</code> in a <code>ProductionTable</code>.</li> <li>You can use Siddhi functions to create a unique ID for the received events, which can then be used to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)</li> </ul>"},{"location":"samples/Store-rdbms/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output for raw materials on the console. You can use <code>searchSweetProductionStream</code> to check for inserted, deleted, and updated events.</p> <pre><code>@App:name(\"Store-rdbms\")\n@App:description('Receive events via simulator and persist the received data in the store.')\n\n\ndefine stream insertSweetProductionStream (name string, amount double);\ndefine stream deleteSweetProductionStream (name string);\ndefine stream searchSweetProductionStream (name string);\ndefine stream updateSweetProductionStream (name string, amount double);\ndefine stream updateOrInsertSweetProductionStream (name string, amount double);\ndefine stream containsSweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream logStream(name string, amount double);\n\n@Store(type=\"rdbms\",\njdbc.url=\"jdbc:mysql://localhost:3306/production?useSSL=false\",\nusername=\"wso2\",\npassword=\"123\" ,\njdbc.driver.name=\"com.mysql.jdbc.Driver\")\n--@PrimaryKey(\"name\")\n@index(\"amount\")\ndefine table SweetProductionTable (name string, amount double);\n\n/* Inserting event into the mysql database */\n@info(name='query1')\nfrom insertSweetProductionStream\ninsert into SweetProductionTable;\n\n/* Deleting event from mysql database */\n@info(name = 'query2')\nfrom deleteSweetProductionStream\ndelete SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating event from mysql database */\n@info(name = 'query3')\nfrom updateSweetProductionStream\nupdate SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating or inserting event from mysql database */\n@info(name = 'query4')\nfrom updateOrInsertSweetProductionStream\nupdate or insert into SweetProductionTable\non SweetProductionTable.name == name;\n\n/* Siddhi In in mysql database */\n@info(name = 'query5')\nfrom containsSweetProductionStream\n[(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable]\ninsert into logStream;\n\n--Perform a join on raw material name so that the data in the store can be viewed\n@info(name='query6')\nfrom searchSweetProductionStream as s join SweetProductionTable as sp\non s.name == sp.name\nselect sp.name, sp.amount\ninsert into logStream;\n</code></pre>"},{"location":"samples/Store-redis/","title":"Receiving Events and Persisting Them in Redis Store","text":""},{"location":"samples/Store-redis/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to perform CRUD operations using Siddhi queries in Redis stores. The sample depicts a scenario in a sweet production factory. The sweet production details such as name of the raw material, amount used for production can be stored using <code>insertSweetProductionStream</code>. The following streams can be used to search, delete, update or upsert(update or insert) the existing data in the store.</p> <ul> <li>Search - <code>searchSweetProductionStream</code></li> <li>insert - <code>insertSweetProductionStream</code></li> <li>delete - <code>deleteSweetProductionStream</code></li> <li>update - <code>updateSweetProductionStream</code></li> <li>update or insert - <code>updateOrInsertSweetProductionStream</code></li> <li>contains - <code>containsSweetProductionStream</code> (verifies whether all the attributes that enter in the stream exists in the store).</li> </ul>"},{"location":"samples/Store-redis/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Download the Redis from https://redis.io/.</li> <li>Download redis java client 'Jedis' jar (&gt;2.7.0) from https://mvnrepository.com/artifact/redis.clients/jedis and place in <code>&lt;SI_HOME&gt;/lib</code> folder.</li> <li>In <code>redis.conf</code>, provide the requirepass as root.</li> <li>Start redis by redis-server ."},{"location":"samples/Store-redis/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Store-redis.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/Store-redis/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -&gt; 'Single Simulation' -&gt; Select 'Store-redis' as 'Siddhi App Name' -&gt; Select 'searchSweetProductionStream' as 'Stream Name' -&gt; Provide attribute values -&gt; Send.</li> <li>Send at-least one event with the single event simulator, where the name matches a name value in the data we previously inserted to the SweetProductionTable. This would satisfy the 'on' condition of our join query.</li> <li>Likewise the events can be sent to the other corresponding streams to add, delete, update, insert, search events.</li> <li>After a change in the store, using the search stream the developer can see whether the operation is successful.</li> <li>Primary Key constraint SweetProductionTable is disabled, since name cannot be used as a PrimaryKey in ProductionTable. Siddhi functions can be used to create a unique id for the received events which can then be used to apply Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)</li> </ol>"},{"location":"samples/Store-redis/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output for row materials on the console. Inserted, deleted, updated events can be checked by searchSweetProductionStream.</p> <pre><code>@App:name(\"Store-redis\")\n@App:description(\"Receive events via simulator and received data are persisted in store.\")\n\n\ndefine stream insertSweetProductionStream (symbol string, price float, volume long);\ndefine stream deleteSweetProductionStream (symbol string, price float, volume long);\ndefine stream searchSweetProductionStream(symbol string);\ndefine stream updateOrInsertSweetProductionStream(symbol string, price float, volume long);\ndefine stream updateSweetProductionStream (symbol string, price float, volume long);\ndefine stream containsSweetProductionStream (symbol string, price float, volume long);\n\n@Store(type='redis', table.name='SweetProductionTable', host= 'localhost', port='6379', password=\"root\")\n@primaryKey('symbol')\n@index('price')\ndefine table SweetProductionTable (symbol string, price float, volume long);\n\n@sink(type='log')\ndefine stream OutStream(symbol string, price float, volume long);\n\n@info(name='query1')\nfrom insertSweetProductionStream\ninsert into SweetProductionTable;\n\n@info(name = 'query2')\nfrom updateSweetProductionStream\nupdate SweetProductionTable\nset SweetProductionTable.price = price\non SweetProductionTable.symbol == symbol;\n\n@info(name = 'query3')\nfrom deleteSweetProductionStream\ndelete SweetProductionTable\non SweetProductionTable.symbol==symbol ;\n\n@info(name = 'query4')\nfrom updateOrInsertSweetProductionStream#window.timeBatch(1 sec)\nupdate or insert into SweetProductionTable\non SweetProductionTable.symbol==symbol;\n\n@info(name = 'query5')\nfrom containsSweetProductionStream[ (symbol==SweetProductionTable.symbol) in SweetProductionTable]\ninsert into OutStream;\n\n@info(name = 'query6')\nfrom searchSweetProductionStream#window.length(1) join SweetProductionTable on searchSweetProductionStream.symbol==SweetProductionTable.symbol\nselect searchSweetProductionStream.symbol as symbol, SweetProductionTable.price as price,\nSweetProductionTable.volume as volume\ninsert into OutStream;\n</code></pre>"},{"location":"samples/Store-solr/","title":"Receiving Events via Simulator and Persisting in SOLR Store","text":""},{"location":"samples/Store-solr/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to perform CRUD operations using Siddhi queries in Solr stores. The sample depicts a scenario in a sweet production factory. The sweet production details such as name of the raw material, amount used for production can be stored using <code>insertSweetProductionStream</code>. The following streams can be used to search, delete, update or upsert(update or insert) the existing data in the store.</p> <ul> <li>search - <code>searchSweetProductionStream</code></li> <li>insert - <code>insertSweetProductionStream</code></li> <li>delete - <code>deleteSweetProductionStream</code></li> <li>update - <code>updateSweetProductionStream</code></li> <li>update or insert - <code>updateOrInsertSweetProductionStream</code></li> <li>contains - <code>containsSweetProductionStream</code> (verifies whether all the attributes that enter in the stream exists in the store).</li> </ul>"},{"location":"samples/Store-solr/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Download the <code>solr-6.x.x.zip</code> distribution from https://archive.apache.org/dist/lucene/solr/.</li> <li>Start the Solr server in cloud mode using the command <code>{SOLR_HOME}/bin/solr -e cloud</code>. This will create a simple solr cloud in your local machine. When creating the cloud provide the suggested examples values for the each field. Give the collection name as <code>gettingstarted</code>. For the configuration provide <code>basic_configs</code>.</li> </ol>"},{"location":"samples/Store-solr/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Store-solr.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/Store-solr/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events. For this, click on 'Event Simulator' (double arrows on left tab) -&gt; 'Single Simulation' -&gt; Select 'Store-solr' as 'Siddhi App Name' -&gt; Select 'searchSweetProductionStream' as 'Stream Name' -&gt; Provide attribute values -&gt; Send.</li> <li>Send at-least one event with the single event simulator, where the name matches a name value in the data we previously inserted to the SweetProductionTable. This would satisfy the 'on' condition of our join query.</li> <li>Likewise the events can be sent to the other corresponding streams to add, delete, update, insert, search events.</li> <li>After a change in the store, using the search stream the developer can see whether the operation is successful.</li> <li>Primary Key constraint SweetProductionTable is disabled, since name cannot be used as a PrimaryKey in ProductionTable. Siddhi functions can be used to create a unique id for the received events which can then be used to apply Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)</li> </ol>"},{"location":"samples/Store-solr/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output for row materials on the console. Inserted, deleted, updated events can be checked by <code>searchSweetProductionStream</code>.</p> <pre><code>@App:name(\"Store-solr\")\n@App:description('Receive events via simulator and received data are persisted in store.')\n\n\ndefine stream insertSweetProductionStream (name string, amount double);\ndefine stream deleteSweetProductionStream (name string);\ndefine stream searchSweetProductionStream (name string);\ndefine stream updateSweetProductionStream (name string, amount double);\ndefine stream updateOrInsertSweetProductionStream (name string, amount double);\ndefine stream containsSweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream logStream(name string, amount double);\n\n@Store(type='solr', collection='SweetProductionTable', zookeeper.url='localhost:9983', shards='2', replicas='2', schema='name string stored,amount double stored')\n--@PrimaryKey(\"name\")\n@index(\"amount\")\ndefine table SweetProductionTable (name string, amount double);\n\n/* Inserting events*/\n@info(name='query1')\nfrom insertSweetProductionStream\ninsert into SweetProductionTable;\n\n/* Deleting events*/\n@info(name = 'query2')\nfrom deleteSweetProductionStream\ndelete SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating events*/\n@info(name = 'query3')\nfrom updateSweetProductionStream\nselect name,amount\nupdate SweetProductionTable\nset SweetProductionTable.amount = amount\non SweetProductionTable.name == name ;\n\n/* Updating or inserting events */\n@info(name = 'query4')\nfrom updateOrInsertSweetProductionStream\nselect name,amount\nupdate or insert into SweetProductionTable\nset SweetProductionTable.amount = amount\non SweetProductionTable.name == name;\n\n/* Siddhi In (Contains)*/\n@info(name = 'query5')\nfrom containsSweetProductionStream\n[(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable]\ninsert into logStream;\n\n@info(name='query6')\nfrom searchSweetProductionStream as s join SweetProductionTable as sp\non s.name == sp.name\nselect sp.name, sp.amount\ninsert into logStream;\n</code></pre>"},{"location":"samples/StreamingKMeansSample/","title":"Using StreamingML Kmeans for Clustering","text":""},{"location":"samples/StreamingKMeansSample/#purpose","title":"Purpose:","text":"<p>This sample demonstrates how to use siddhi-execution-streamingml kmeans incremental function for clustering.</p>"},{"location":"samples/StreamingKMeansSample/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample. If there is no syntax error, the following messages would be shown on the console. <pre><code>* Siddhi App StreamingKMeansSample successfully deployed.\n</code></pre></p>"},{"location":"samples/StreamingKMeansSample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* StreamingKMeansSample.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/StreamingKMeansSample/#testing-the-sample","title":"Testing the Sample:","text":"<p>You can publish data event to the file, through event simulator. 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows:     * Siddhi App Name  : StreamingKMeansSample     * Stream Name     : SweetProductionStream 3. Enter and send suitable values for the attributes of selected stream.</p>"},{"location":"samples/StreamingKMeansSample/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - StreamingMLExtensionkmeans-incremental-sample : SweetStatePredictionStream : Event{timestamp=1513603080892, data=[12.5, 124.5, 12.5, 124.5], isExpired=false}\n</code></pre></p> <p>First two values of the data array represent the coordinates of the cluster that given product belongs to. <pre><code>(eg: 12.5, 124.5)\n</code></pre></p> <pre><code>@App:name(\"StreamingKMeansSample\")\n@App:Description('Demonstrates how to use siddhi-execution-streamingml kmeans incremental function for clustering.')\n\n\ndefine stream SweetProductionStream(temperature double, density double);\n\n@sink(type='log')\ndefine stream SweetStatePredictionStream(closestCentroidCoordinate1 double, closestCentroidCoordinate2 double, temperature double, density double);\n\n@info(name = 'query1')\nfrom SweetProductionStream#streamingml:kMeansIncremental(2, 0.2, temperature, density)\nselect closestCentroidCoordinate1, closestCentroidCoordinate2, temperature, density\ninsert into SweetStatePredictionStream;\n</code></pre>"},{"location":"samples/StreamingRegressor/","title":"Making Predictions via a Regressor Model","text":""},{"location":"samples/StreamingRegressor/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to train a ML model with CSV data and do predictions based on it. The sample explores a scenario where the buying/ site browsing patterns, and average monthly earnings per person are used to predict how much she can be expected to spend on buying goods from a certain store per month.</p>"},{"location":"samples/StreamingRegressor/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Download <code>siddhi-gpl-execution-streamingml-x.x.x.jar</code> from the following http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/streamingml/siddhi-gpl-execution-streamingml/ and copy the jar to <code>{WSO2SIHome}/lib</code>.</li> <li>Shutdown the server and copy this jar file to <code>{WSO2SIHome}/lib</code> location. Re-start the server.</li> <li>Save this sample.</li> </ol>"},{"location":"samples/StreamingRegressor/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* StreamingRegressor.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/StreamingRegressor/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start.</p>"},{"location":"samples/StreamingRegressor/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Train the ML model with training data set. The <code>{WSO2SIHome}/samples/artifacts/StreamingRegressor/train.csv</code> file is used to create the ML model. To simulate the events in this CSV file, execute the following steps.<ol> <li>Click on 'Event Simulator' (double arrows on left tab).</li> <li>Click 'Feed Simulation' -&gt; 'Create'.</li> <li>Give a name, description and time interval.</li> <li>Select 'CSV file' as the 'Simulation Source'.</li> <li>Click on 'Add Simulation Source'.</li> <li>Select StreamingRegressor as 'Siddhi App Name'.</li> <li>Select 'TrainInputStream' as 'StreamName'.</li> <li>Click on 'Upload' button under 'CSV file' section and upload the <code>{WSO2SIHome}/samples/artifacts/StreamingRegressor/train.csv</code> file.</li> <li>Give ',' as the delimiter.</li> <li>Save the configuration.</li> </ol> </li> <li>Click on the start button (Arrow symbol) next to the newly created simulator, and wait few seconds for the model training to complete</li> <li>Send events to <code>PredictionInputStream</code> using <code>{WSO2SIHome}/samples/artifacts/StreamingRegressor/test.csv</code> file, to see the predicted values per each event in test.csv. CSV simulation can be done as explained above.</li> </ol>"},{"location":"samples/StreamingRegressor/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the predicted values per each event on the console.</p>"},{"location":"samples/StreamingRegressor/#note","title":"Note:","text":"<p>Stop this Siddhi application, once you are done with the execution.</p> <pre><code>@App:name(\"StreamingRegressor\")\n@App:description('Train a machine learning regressor model with CSV data and subsequently do predictions using that model')\n\n\ndefine stream TrainInputStream(avgClothShoppingTimesPerMonth double, avgCosmeticsShoppingTimesPerMonth double, avgEarningsPerMonth double, avgSiteBrowsingPerMonth double, AvgAmountSpentPerMonth double );\n\ndefine stream PredictionInputStream(avgClothShoppingTimesPerMonth double, avgCosmeticsShoppingTimesPerMonth double, avgEarningsPerMonth double, avgSiteBrowsingPerMonth double);\n\n@sink(type='log')\ndefine stream PredictionOutputStream(prediction double, meanSquaredError double);\n\n--Train the ML model\n@info(name = 'query-train')\nfrom TrainInputStream#streamingml:updateAMRulesRegressor('model1', avgClothShoppingTimesPerMonth, avgCosmeticsShoppingTimesPerMonth, avgEarningsPerMonth, avgSiteBrowsingPerMonth, AvgAmountSpentPerMonth)\nselect meanSquaredError\ninsert into TrainOutputStream;\n\n--Predict using the model1 created with query 'query-train'\n@info(name = 'query-predict')\nfrom PredictionInputStream#streamingml:AMRulesRegressor('model1', avgClothShoppingTimesPerMonth, avgCosmeticsShoppingTimesPerMonth, avgEarningsPerMonth, avgSiteBrowsingPerMonth )\nselect prediction, meanSquaredError\ninsert into PredictionOutputStream;\n</code></pre>"},{"location":"samples/StringExtensionSample/","title":"Converting String Values to Lowercase","text":""},{"location":"samples/StringExtensionSample/#purpose","title":"Purpose:","text":"<p>This String function converts the string value to lowercase letters</p>"},{"location":"samples/StringExtensionSample/#prerequisites","title":"Prerequisites:","text":"<p>Save this sample. If there is no syntax error, the following messages would be shown on the console <pre><code>* Siddhi App StringExtensionSample successfully deployed.\n</code></pre></p>"},{"location":"samples/StringExtensionSample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* StringExtensionSample.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/StringExtensionSample/#testing-the-sample","title":"Testing the Sample:","text":"<p>You can publish data event to the file, through event simulator. 1. Open event simulator by clicking on the second icon or press Ctrl+Shift+I. 2. In the Single Simulation tab of the panel, select values as follows:     * Siddhi App Name: StringExtensionSample     * Stream Name: SweetProductionStream 3. Enter following values in the fields and send.     <pre><code>name: CaKe\namount: 55.6\n</code></pre></p>"},{"location":"samples/StringExtensionSample/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - StringExtensionSample : OutputStream : Event{timestamp=1513760993921, data=[cake, 55.6], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"StringExtensionSample\")\n@App:description('Converts the sweet name to lowercase letters.')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream OutputStream(name string, amount double);\n\nfrom SweetProductionStream\nselect str:lower(name) as name, amount\ninsert into OutputStream;\n</code></pre>"},{"location":"samples/SweetProductionDataPreprocessing/","title":"Receiving Data via TCP and Preprocessing","text":""},{"location":"samples/SweetProductionDataPreprocessing/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to receive events via TCP transport and carryout data pre-processing with numerous Siddhi extensions (eg. string extension, time extension). For more information on Siddhi extensions please refer to \"https://wso2.github.io/siddhi/extensions/\". In this sample, a composite ID is obtained using string concatenation and the time format of the incoming event</p>"},{"location":"samples/SweetProductionDataPreprocessing/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that MySQL is installed on your machine.</li> <li>Add the MySQL JDBC driver into <code>{WSO2_SI_HOME}/lib</code> as follows:<ol> <li>Download the JDBC driver from: https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz.</li> <li>Unzip the archive.</li> <li>Copy <code>mysql-connector-java-5.1.45-bin.jar</code> to <code>{WSO2_SI_Home}/lib</code> directory.</li> </ol> </li> <li>Create a database named <code>sampleDB</code> in MySQL. This database is referred to with <code>jdbc:mysql://localhost:3306/sampleDB</code> url.</li> <li>In the store configuration of this application, replace 'username' and 'password' values with your MySQL credentials.</li> <li>Save this sample.</li> </ol>"},{"location":"samples/SweetProductionDataPreprocessing/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.     <pre><code>* Tcp Server started in 0.0.0.0:9892\n* SweetProductionDataPreprocessing.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/SweetProductionDataPreprocessing/#testing-the-sample","title":"Testing the Sample:","text":"<p>Navigate to <code>{WSO2SIHome}/samples/sample-clients/tcp-client</code> and run the <code>ant</code> command as follows. <pre><code>ant -Dtype=binary\n</code></pre></p> <p>If you want to publish custom number of events, you need to run <code>ant</code> command as follows. <pre><code>ant -Dtype=binary -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/SweetProductionDataPreprocessing/#viewing-the-results","title":"Viewing the Results:","text":"<p>Check the <code>ProcessedSweetProductionTable</code> created in <code>sampleDB</code>. You would be able to see the pre-processed data written to the table</p> <pre><code>@App:name(\"SweetProductionDataPreprocessing\")\n@App:description('Collect data via TCP transport and pre-process')\n\n\n@source(type='tcp',\ncontext='SweetProductionStream',\nport='9892',\n@map(type='binary'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@Store(type=\"rdbms\",\njdbc.url=\"jdbc:mysql://localhost:3306/sampleDB\",\nusername=\"root\",\npassword=\"mysql\" ,\njdbc.driver.name=\"com.mysql.jdbc.Driver\")\n@PrimaryKey(\"compositeID\")\ndefine table ProcessedSweetProductionTable (compositeID string, amount double, date string);\n\n--Process smart home data by concatenating the IDs and formatting the time\n@info(name='query1')\nfrom SweetProductionStream\nselect str:concat(str:lower(name), \"::\", time:currentTimestamp()) as compositeID, amount, time:currentDate() as date\ninsert into ProcessedSweetProductionTable;\n</code></pre>"},{"location":"samples/TensorFlowTestApp/","title":"Performing Regression Tasks via an Imported Tensorflow Model","text":""},{"location":"samples/TensorFlowTestApp/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to import a pretrained Tensorflow model WSO2 Streaming Integrator to perform a regression task.</p>"},{"location":"samples/TensorFlowTestApp/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Replace <code>{SI_HOME}</code> with absolute path to the Streaming Integrator Tooling home directory.</li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>* Siddhi App TensorFlowTestApp successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/TensorFlowTestApp/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console:     <pre><code>* TensorFlowTestApp.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/TensorFlowTestApp/#testing-the-sample","title":"Testing the Sample:","text":"<p>Send events through one or more of the following methods.</p>"},{"location":"samples/TensorFlowTestApp/#send-events-to-productioninputstream-via-event-simulator","title":"Send events to <code>ProductionInputStream</code>, via event simulator.","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name: TensorFlowTestApp</li> <li>Stream Name: InputStream</li> </ul> </li> <li> <p>In the x field, enter the following and then click Send to send the event. <pre><code>x: \"double:[1,-2]\"\n</code></pre></p> </li> <li> <p>Send some more events.</p> </li> </ol>"},{"location":"samples/TensorFlowTestApp/#send-events-to-the-simulator-http-endpoint-through-the-curl-command","title":"Send events to the simulator http endpoint through the curl command:","text":"<ol> <li>Open a new terminal and issue the following command:     <pre><code>curl -X POST \\\nhttp://localhost:9390/simulation/single \\\n-H 'content-type: text/plain' \\\n-d '{\"streamName\": \"InputStream\", \"siddhiAppName\": \"TensorFlowTestApp\",\"data\": [\"double:[1,-2]\"]}'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal:     <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/TensorFlowTestApp/#publish-events-with-postman","title":"Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"InputStream\", \"siddhiAppName\": \"TensorFlowTestApp\",\"data\": ['double:[1,-2]']}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/TensorFlowTestApp/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal.</p> <pre><code>@App:name(\"TensorFlowTestApp\")\n\n\ndefine stream InputStream (x string);\n\n@sink(type='log')\ndefine stream OutputStream (outputPoint0 double, outputPoint1 double);\n\n@info(name = 'query1')\nfrom InputStream#tensorFlow:predict('{SI_HOME}/samples/artifacts/TensorflowSample/Regression', 'inputPoint', 'outputPoint', x)\nselect outputPoint0, outputPoint1\ninsert into OutputStream;\n</code></pre>"},{"location":"samples/TextMappingWithInmemoryTransport/","title":"Text Mapping with In-memory Transport","text":""},{"location":"samples/TextMappingWithInmemoryTransport/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to receive events to the SweetProductionStream via TCP transport in binary format and check the custom text mapping and the default text mapping using inMemory transport and log the events in OutputStreams accordingly to the  output  console.</p>"},{"location":"samples/TextMappingWithInmemoryTransport/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample</li> </ol>"},{"location":"samples/TextMappingWithInmemoryTransport/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li> <p>If the Siddhi application starts successfully, the following messages would be shown on the console. </p> <pre><code>* Tcp Server started in 0.0.0.0:9892\n* TextMappingWithInmemoryTransport.siddhi - Started Successfully!\n</code></pre> </li> </ol>"},{"location":"samples/TextMappingWithInmemoryTransport/#testing-the-sample","title":"Testing the Sample:","text":"<p>In order to publish events with TCP client, 1. Go to <code>{WSO2SIHome}/samples/sample-clients/tcp-client/</code> directory. 2. Run ant commant as following.     <pre><code>ant -Dtype=binary\n</code></pre></p> <p>If you want to publish custom number of events, you need to run \"ant\" command as follows. <pre><code>ant -Dtype=binary -DnoOfEventsToSend=5\n</code></pre></p>"},{"location":"samples/TextMappingWithInmemoryTransport/#viewing-the-results","title":"Viewing the Results:","text":"<pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599736271, data=[Jelly Bean, 9.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599737255, data=[Froyo, 1534.87], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599737255, data=[Froyo, 1.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599738255, data=[Jelly Bean, 3030.71], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - Custom Mapper : Event{timestamp=1513599738256, data=[Jelly Bean, 3.0], isExpired=false}\nINFO {io.siddhi.core.stream.output.sink.LogSink} - Default Mapper : Event{timestamp=1513599739256, data=[Cupcake, 3212.83], isExpired=false}\n</code></pre>"},{"location":"samples/TextMappingWithInmemoryTransport/#notes","title":"Notes:","text":"<p>If the message \"Tcp Server started in 0.0.0.0:9892\" does not appear, it could be due to port 9892, defined in the Siddhi application is already being used by a different program. To resolve this issue, please do the following, * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop') * Change the port 9892 to an unused port, in this Siddhi application's source configuration. * Start the application and check whether the specified messages appear on the console</p> <pre><code>@App:name(\"TextMappingWithInmemoryTransport\")\n@App:description('Use inmemory transport to custom text mapping and the default text mapping and view the output on the console.')\n\n\n@source(type='tcp', context='SweetProductionStream', port='9892',\n@map(type='binary'))\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log', prefix='Default Mapper')\ndefine stream DefaultOutputStream (name string, amount double);\n\n@sink(type='log', prefix='Custom Mapper')\ndefine stream CustomOutputStream (name string, amount double);\n\n-- Default text mapping.\n\n@sink(type='inMemory', topic='home', @map(type='text'))\ndefine stream InMemoryDefaultSweetProductionInputData (name string, amount double);\n\n@source(type='inMemory', topic='home', @map(type='text'))\ndefine stream UsageStream (name string, amount double);\n\n-- Custom text mapping.\n\n@sink(type='inMemory', topic='home1', @map(type='text',\n@payload(\"\"\"name:{{ name }},\namount:{{ amount }}\"\"\")))\ndefine stream InMemoryCustomSweetProductionInputData (name string, amount double);\n\n@source(type='inMemory', topic='home1', @map(type='text' , regex.A='((?&lt;=name:)(.*)(?=,))',regex.B='([-0-9]+)',\n@attributes(name = 'A', amount = 'B')))\ndefine stream UsageStream2 (name string, amount double);\n\nfrom SweetProductionStream\nselect *\ninsert into InMemoryDefaultSweetProductionInputData;\n\nfrom UsageStream\nselect *\ninsert into DefaultOutputStream;\n\nfrom SweetProductionStream\nselect *\ninsert into InMemoryCustomSweetProductionInputData;\n\nfrom UsageStream2\nselect *\ninsert into CustomOutputStream;\n</code></pre>"},{"location":"samples/UnitConversionExtentionSample/","title":"Converting Units","text":""},{"location":"samples/UnitConversionExtentionSample/#purpose","title":"Purpose:","text":"<p>This sample demonstrates how to use unit conversion extension for converting units.</p>"},{"location":"samples/UnitConversionExtentionSample/#prerequisites","title":"Prerequisites:","text":"<ul> <li> <p>Save this sample. If there is no syntax error, the following messages would be shown on the console.</p> <pre><code>* Siddhi App UnitConversionExtensionSample successfully deployed.\n</code></pre> </li> </ul>"},{"location":"samples/UnitConversionExtentionSample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages would be shown on the console.  <pre><code>* UnitConversionExtensionSample.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/UnitConversionExtentionSample/#testing-the-sample","title":"Testing the Sample:","text":"<p>You can publish data event to the file, through event simulator</p> <ol> <li>Open event simulator by clicking on the second icon or press Ctrl+Shift+I.</li> <li> <p>In the Single Simulation tab of the panel, select values as follows:</p> <ul> <li>Siddhi App Name: UnitConversionExtensionSample</li> <li>Stream name: SweetProductionStream</li> </ul> </li> <li> <p>Enter and send suitable values for the attributes of selected stream.</p> </li> </ol>"},{"location":"samples/UnitConversionExtentionSample/#viewing-the-results","title":"Viewing the Results:","text":"<p>Messages similar to the following would be shown on the console. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - UnitConversionExtensionSample : WeightConvertedStream : Event{timestamp=1513588858315, data=[Chocolate, 1250.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"UnitConversionExtensionSample\")\n@App:description('Demonstrates how to use unit conversion extension for converting units.')\n\n\ndefine stream SweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream WeightConvertedStream(name string, weightInGrams double);\n\nfrom SweetProductionStream\nselect name, unitconversion:kgTog(amount) as  weightInGrams\ninsert into WeightConvertedStream;\n</code></pre>"},{"location":"samples/execution-geo-sample/","title":"Processing Geo Data","text":""},{"location":"samples/execution-geo-sample/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to retrieve the longitude and latitude based on location details provided.</p> <p>Before you begin:</p> <p>Save the sample Siddhi application in Streaming Integrator Tooling.</p>"},{"location":"samples/execution-geo-sample/#executing-the-sample","title":"Executing the Sample","text":"<p>To execute the sample open the saved Siddhi application in Streaming Integrator Tooling, and start it by clicking the Start button (shown below) or by clicking Run =&gt; Run.</p> <p></p> <p>If the Siddhi application starts successfully, the following message appears in the console.</p> <p><code>execution-geo-sample.siddhi - Started Successfully!</code></p>"},{"location":"samples/execution-geo-sample/#testing-the-sample","title":"Testing the Sample","text":"<p>To test the sample Siddhi application, simulate single events for it via the Streaming Integrator Tooling as follows:</p> <ol> <li> <p>To open the Event Simulator, click the Event Simulator icon.</p> <p></p> <p>This opens the event simulation panel.</p> </li> <li> <p>To simulate events for the <code>geocodeStream</code> stream of the <code>execution-geo-sample</code>  Siddhi application, enter information in the Single Simulation tab of the event simulation panel as follows.</p> <p></p> Field Value Siddhi App Name <code>execution-geo-sample</code> StreamName <code>geocodeStream</code> <p>As a result, attributes specific to the <code>geocodeStream</code> are displayed as marked in the above image.</p> </li> <li> <p>Enter attribute values as follows.</p> <p>.</p> Attribute Value location <code>5 Avenue Anatole France</code> level <code>75007 Paris</code> time <code>France</code> </li> <li> <p>Click Start and Send.</p> </li> </ol>"},{"location":"samples/execution-geo-sample/#viewing-the-results","title":"Viewing the Results","text":"<p>The prediction for the location you provided via the event you simulated is displayed as follows in the Streaming Integrator Tooling console.</p> <p><code>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - sentimentExtensionSample: Event :, StreamEvent{ timestamp=1513623526790, beforeWindowData=null, onAfterWindowData=null, outputData=[48.8583698, 2.2944833, Tour Eiffel, 5 Avenue Anatole France, 75007 Paris, France], type=CURRENT, next=null}</code></p> Click here to view the sample Siddhi application.<pre><code>@App:name(\"execution-geo-sample\")\n@App:description('Use geo data related functionality to retrieve the longitude and latitude for the provided location details.')\n\n-- Please refer to https://docs.wso2.com/display/SP400/Quick+Start+Guide on getting started with streaming-integrator-tooling.\n\ndefine stream geocodeStream (location string, level string, time string);\n\n@sink(type='log')\ndefine stream dataOut(latitude double, longitude double, formattedAddress string);\n\n@info(name = 'query')\nfrom geocodeStream#geo:geocode(location)\nselect latitude, longitude, formattedAddress\ninsert into dataOut;\n</code></pre>"},{"location":"samples/hoeffding-adaptive-tree-sample/","title":"Making Predictions via Hoeffding Classifier Model","text":""},{"location":"samples/hoeffding-adaptive-tree-sample/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to train a Hoeffding Classifier and to predict the sweet category from the sweet production stream in streaming manner.</p>"},{"location":"samples/hoeffding-adaptive-tree-sample/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Download <code>siddhi-gpl-execution-streamingml-x.x.x.jar</code> from the following http://maven.wso2.org/nexus/content/repositories/wso2gpl/org/wso2/extension/siddhi/gpl/execution/streamingml/siddhi-gpl-execution-streamingml/ and copy the jar to <code>{WSO2SIHome}/lib</code>.</li> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console:     <pre><code>* Siddhi App streaming-hoeffding-classifier-sample successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li> <p>If the Siddhi application starts successfully, the following messages would be shown on the console.</p> <pre><code>* streaming-hoeffding-classifier-sample.siddhi - Started Successfully!\n</code></pre> </li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#notes","title":"Notes:","text":"<p>If you edit this application while it's running, stop the application -&gt; Save -&gt; Start. * Stop this Siddhi application (Click 'Run' on menu bar -&gt; 'Stop'). * Start the application and check whether the specified events from the jms provider appear on the console.</p>"},{"location":"samples/hoeffding-adaptive-tree-sample/#testing-the-sample","title":"Testing the Sample:","text":"<p>Note: The Hoeffding Classifier for streaming machine learning needs to be trained prior to perform prediction.</p>"},{"location":"samples/hoeffding-adaptive-tree-sample/#training-phase","title":"Training phase","text":"<p>Send events through one or more of the following methods.</p>"},{"location":"samples/hoeffding-adaptive-tree-sample/#send-events-to-productiontrainstream-via-event-simulator","title":"Send events to ProductionTrainStream, via event simulator:","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name: streaming-hoeffding-classifier-sample</li> <li>Stream Name: ProductionTrainStream</li> </ul> </li> <li> <p>In the name and amount fields, enter the following and then click Send to send the event.     <pre><code>density: 50.4\nsolubility: 30.03\nsweetType: candy\n</code></pre></p> </li> <li> <p>Send more events including upto 3 unique sweet types as specified as the parameter during the training phase.     <pre><code>@info(name = 'query-train')\nfrom ProductionTrainStream#streamingml:updateHoeffdingTree('classifierModel', 3, density, solubility, sweetType )\n</code></pre></p> </li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command","title":"Send events to the simulator http endpoint through the curl command:","text":"<ol> <li>Open a new terminal and issue the following command: <pre><code>* curl -X POST -d '{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [50.4, 30.03, candy]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal: <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#publish-events-with-postman","title":"Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [50.4, 30.03, candy]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#testing-phase","title":"Testing phase","text":"<p>Send events through one or more of the following methods.</p>"},{"location":"samples/hoeffding-adaptive-tree-sample/#you-may-send-events-to-productioninputstream-via-event-simulator","title":"You may send events to ProductionInputStream, via event simulator","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name: streaming-hoeffding-classifier-sample</li> <li>Stream Name: SweetProductionStream</li> </ul> </li> <li>In the name and amount fields, enter the following and then click Send to send the event.     <pre><code>density: 30.4\nemperature: 20.5\n</code></pre></li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command_1","title":"Send events to the simulator http endpoint through the curl command:","text":"<ol> <li>Open a new terminal and issue the following command: <pre><code>curl -X POST -d '{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [30.4, 20.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal: <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#publish-events-with-postman_1","title":"Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"SweetProductionStream\", \"siddhiAppName\": \"streaming-hoeffding-classifier-sample\",\"data\": [30.4, 20.5]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/hoeffding-adaptive-tree-sample/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal. <pre><code>INFO {io.siddhi.core.stream.output.sink.LogSink} - streaming-hoeffding-classifier-sample : PredictionStream : Event{timestamp=1513610806272, data=[30.4, 20.5, candy, 0.0], isExpired=false}\n</code></pre></p> <pre><code>@App:name(\"streaming-hoeffding-classifier-sample\")\n@App:description('Train a streaming Hoeffding Classifier and to predict the type of sweet.')\n\n\ndefine stream ProductionTrainStream (density double, solubility double, sweetType string );\n\ndefine stream SweetProductionStream (density double, solubility double);\n\n@sink(type='log')\ndefine stream PredictionStream (density double, solubility double, prediction string, confidenceLevel double);\n\n@info(name = 'query-train')\nfrom ProductionTrainStream#streamingml:updateHoeffdingTree('classifierModel', 3, density, solubility, sweetType )\nselect *\ninsert into trainOutputStream;\n\n@info(name = 'query-predict')\nfrom SweetProductionStream#streamingml:hoeffdingTreeClassifier('classifierModel', density, solubility )\nselect density, solubility, prediction, confidenceLevel\ninsert into PredictionStream;\n</code></pre>"},{"location":"samples/sample/","title":"List of Streaming Integrator Samples","text":""},{"location":"samples/sample/#consume","title":"Consume","text":"<ul> <li>Recieve TCP in JSON Format</li> <li>Publish Google PubSub Messages In Text Format</li> <li>CSV Default Mapping</li> <li>CDC With Listening Mode</li> <li>Receive Google PubSub Messages In Text Format</li> <li>Receive MQTT In XML Format</li> <li>Receive TCP in Binary Format</li> <li>Receive Kafka In Binary Format </li> <li>Receive HTTP In XML Format With Default Mapping</li> <li>CDC With Polling Mode</li> <li>Publish Http OAuth Request With Refresh Token</li> <li>Publish Http OAuth Request Response</li> <li>Store solr</li> <li>Receive Events From File</li> <li>Receive Email In Xml Format</li> <li>Receive Rabbitmq In JSON Format</li> <li>Store cassandra</li> </ul>"},{"location":"samples/sample/#transform","title":"Transform","text":"<ul> <li>Publish Http OAuth Request</li> <li>Publish Http OAuth Request Without Access Token</li> <li>Receive Hl7 In ER7 Format</li> <li>Data Preprocessing</li> <li>Publish Hl7 In ER7 Format</li> <li>Sweet Production Data Pre-processing</li> <li>CSV Custom Mapping</li> <li>Publish HTTP In Json Format With Custom Mapping</li> <li>Publish Email In Text Format</li> <li>Receive Kafka In Text Format With Custom Mapping</li> <li>Script js sample</li> <li>Publish Kafka In Custom Avro Format</li> <li>Receive Hl7 In Xml Format</li> <li>Regex Execution Sample</li> <li>Publish Hl7 In Xml Format</li> <li>Publish Kafka In Json Format</li> <li>Publish Events To File</li> <li>Receive HTTP in XML Format With Custom Mapping</li> <li>Publish Rabbitmq In Xml Format</li> <li>Math Extension Sample</li> <li>Receive TCP in Text Format</li> <li>Publish Email In Xml Format</li> <li>String Extension Sample</li> <li>Text Mapping With In memory Transport</li> <li>Receive HTTP In Json Format With Default Mapping</li> <li>Hello Kafka</li> <li>Publish Http In Xml Format With Custom Mapping</li> <li>SNMP Set Request App</li> <li>Publish Tcp In Json Format</li> <li>Publish Http OAuth Request With OAuth User</li> <li>Map Extension Sample</li> <li>Publish Jms In Xml Format</li> <li>Receive JMS In Json Format</li> <li>Receive HTTP In Json Format With Custom Mapping</li> </ul>"},{"location":"samples/sample/#publish","title":"Publish","text":"<ul> <li>Publish Tcp In Text Format</li> <li>Receive Hl7 In ER7 Format</li> <li>Publish Google PubSub Messages In Text Format</li> <li>Publish Tcp In Binary Format</li> <li>Publish Web Socket In Xml Format</li> <li>Publish Prometheus Metrics HTTP Server</li> <li>store influxdb</li> <li>Publish Kafka In BinaryFormat</li> <li>Publish Jms In Key value Format</li> <li>Http Request Response Sample</li> <li>Publish Http In Xml Format</li> <li>Publish Http OAuth Request With Refresh Token</li> <li>Publish Http OAuth Request Response</li> <li>Store solr</li> <li>Publish Http In Json Format</li> <li>Store rdbms</li> <li>Store cassandra</li> <li>Publish Kafka In Avro Format Using Schema Registry</li> </ul>"},{"location":"samples/sample/#insights","title":"Insights","text":"<ul> <li>Pmml Model Processor</li> <li>Streaming Regressor</li> <li>Gpl NLP Find Name Entity Type</li> <li>Tensor Flow TestApp</li> <li>Pattern Matching</li> <li>Streaming KMeans Sample</li> <li>Receive Prometheus Metrics</li> <li>hoeffding adaptive tree sample</li> <li>Alerts And Thresholds</li> <li>streaming perceptron sample</li> <li>Geo Distance Calculation</li> </ul>"},{"location":"samples/sample/#aggregate","title":"Aggregate","text":"<ul> <li>Data Preprocessing</li> <li>Aggregate Over Time</li> <li>Receive And Count </li> <li>Aggregate Data Incrementally</li> <li>Store redis</li> <li>store mongodb</li> </ul>"},{"location":"samples/sample/#enrich","title":"Enrich","text":"<ul> <li>Extrema Bottom K</li> <li>Gpl NLP Find Name Entity Type</li> <li>Join With Stored Data</li> <li>Script js sample</li> <li>store hbase</li> <li>Unit Conversion Extention Sample</li> <li>execution geo sample</li> <li>Clus Tree Test App</li> <li>Receive WebSocket In XML Format</li> </ul>"},{"location":"samples/sample/#receive","title":"Receive","text":"<ul> <li>Receive And Count</li> </ul>"},{"location":"samples/sample/#correlate","title":"Correlate","text":"<ul> <li>Publish Mqtt In Xml Format</li> <li>SNMP Get Request App</li> <li>Receive JMS In Key value Format</li> <li>IBM Message Queue</li> </ul>"},{"location":"samples/store-hbase/","title":"Receiving Events via Simulator and Persisting in a Store","text":""},{"location":"samples/store-hbase/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to perform CRUD operations in HBase stores using Siddhi queries. The sample depicts a scenario in a sweet production factory. The sweet production details, such as name of the raw material and amount used for production, can be stored using <code>insertSweetProductionStream</code>. The following streams can be used to search, delete, update, or upsert (update or insert) the existing data in the store. * Search - <code>searchSweetProductionStream</code> * insert - <code>insertSweetProductionStream</code> * delete - <code>deleteSweetProductionStream</code> * update - <code>updateSweetProductionStream</code> * update or insert - <code>updateOrInsertSweetProductionStream</code> * contains - <code>containsSweetProductionStream</code> (verifies whether all the attributes that enter in the stream exists in the store).</p>"},{"location":"samples/store-hbase/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that HBase is installed and the server is running on your machine.</li> <li>Add HBase client into <code>{WSO2_SI_HOME}/lib</code> as follows:<ol> <li>Download the following jars:<ul> <li>HBase client (http://central.maven.org/maven2/org/apache/hbase/hbase-shaded-client/1.3.1/hbase-shaded-client-1.3.1.jar)</li> <li>Apache HTrace core (http://central.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar)</li> </ul> </li> <li>Use the <code>jartobundle</code> tool in <code>{WSO2_SI_Home}/bin</code> to convert the above jar into a OSGi bundle.<ul> <li>For Windows:     <pre><code>&lt;SI_HOME&gt;/bin/jartobundle.bat &lt;PATH_OF_DOWNLOADED_JAR&gt; &lt;PATH_OF_CONVERTED_JAR&gt;\n</code></pre></li> <li>For Linux:     <pre><code>&lt;SI_HOME&gt;/bin/jartobundle.sh &lt;PATH_OF_DOWNLOADED_JAR&gt; &lt;PATH_OF_CONVERTED_JAR&gt;\n</code></pre></li> </ul> </li> <li>Copy the converted bundles to the <code>{WSO2_SI_Home}/lib</code> directory.</li> </ol> </li> <li>Save this sample.</li> </ol>"},{"location":"samples/store-hbase/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following message is shown on the console:     <pre><code>store-hbase.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/store-hbase/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events as follows:<ol> <li>Click on Event Simulator (double arrows on left tab) and then 'Single Simulation'.</li> <li>For the Siddhi App Name, select 'store-hbase'.</li> <li>For the Stream Name, select <code>searchSweetProductionStream</code>.</li> <li>Enter attribute values and click Send.</li> </ol> </li> <li>Send an event where the name matches a name value in the data you just inserted to the <code>SweetProductionTable</code>. This will satisfy the 'on' condition of the join query.</li> </ol>"},{"location":"samples/store-hbase/#notes","title":"Notes:","text":"<ul> <li>You can send events to the other corresponding streams to add, delete, update, insert, and search events.</li> <li>The Primary Key constraint in <code>SweetProductionTable</code> is disabled, because the name cannot be used as a <code>PrimaryKey</code> in a <code>ProductionTable</code>.</li> <li>You can use Siddhi functions to create a unique ID for the received events, which you can then use to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)</li> </ul>"},{"location":"samples/store-hbase/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output for raw materials on the console. You can use <code>searchSweetProductionStream</code> to check for inserted, deleted, and updated events.</p> <pre><code>@App:name(\"store-hbase\")\n@App:description('Receive events via simulator and persist the received data in the store.')\n\n\ndefine stream insertSweetProductionStream (name string, amount double);\ndefine stream deleteSweetProductionStream (name string);\ndefine stream searchSweetProductionStream (name string);\ndefine stream updateSweetProductionStream (name string, amount double);\ndefine stream updateOrInsertSweetProductionStream (name string, amount double);\ndefine stream containsSweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream logStream(name string, amount double);\n\n@Store(type='hbase', hbase.zookeeper.quorum='localhost')\n--@primaryKey('name')\ndefine table SweetProductionTable (name string, amount double);\n\n/* Inserting event into the store */\n@info(name='query1')\nfrom insertSweetProductionStream\ninsert into SweetProductionTable;\n\n/* Deleting event from the store */\n@info(name = 'query2')\nfrom deleteSweetProductionStream\ndelete SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating event from the store */\n@info(name = 'query3')\nfrom updateSweetProductionStream\nupdate SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating or inserting event from the store */\n@info(name = 'query4')\nfrom updateOrInsertSweetProductionStream\nupdate or insert into SweetProductionTable\non SweetProductionTable.name == name;\n\n/* Siddhi In in store */\n@info(name = 'query5')\nfrom containsSweetProductionStream\n[(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable]\ninsert into logStream;\n\n--Perform a join on raw material name so that the data in the store can be viewed\n@info(name='query6')\nfrom searchSweetProductionStream as s join SweetProductionTable as sp\non s.name == sp.name\nselect sp.name, sp.amount\ninsert into logStream;\n</code></pre>"},{"location":"samples/store-influxdb/","title":"Working with an influxDB Store","text":""},{"location":"samples/store-influxdb/#purpose","title":"Purpose:","text":"<p>This application demostrates how to perform InfluxDB operations using Siddhi queries.The following streams can be used to insert, search, delete, and update or insert  data into the InfluxDB store. insert - insertStream,stockStream delete - deleteStream search - searchStream update or insert - updateStream contains - containStream (verifies whether all the attributes that enter in the stream exists in the store).</p>"},{"location":"samples/store-influxdb/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that InfluxDB is installed in your machine (https://portal.influxdata.com/downloads/)</li> <li>Create a database named 'aTimeSeries' in InfluxDB. Replace influxdb.database in store configuration of this application with this database name.</li> <li>In the store configuration of this application, replace username and password with your influxDB credentials.</li> <li>Save this sample.</li> </ol>"},{"location":"samples/store-influxdb/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following message is shown on the console,<ul> <li>store-influxdb.siddhi - Started Successfully!</li> </ul> </li> </ol>"},{"location":"samples/store-influxdb/#note","title":"Note:","text":"<p>If you want to edit this application while it's running, stop the application, make your edits and save the application, and then start it again.</p>"},{"location":"samples/store-influxdb/#testing-the-sample","title":"Testing the Sample :","text":"<ol> <li>Simulate single events:<ol> <li>Click on 'Event Simulator' (double arrows on left tab) and click 'Single Simulation'</li> <li>Select 'store-influxdb' as 'Siddhi App Name' and select 'insertStream' as 'Stream Name'.</li> <li>Provide attribute values, and then click send.</li> </ol> </li> <li>Send events to the other corresponding streams to delete, update or insert, and  search events.</li> </ol>"},{"location":"samples/store-influxdb/#notes","title":"Notes:","text":"<p>After a change in the store, you can use the search stream to see whether the operation is successful.</p>"},{"location":"samples/store-influxdb/#viewing-the-results","title":"Viewing the Results :","text":"<p>You can use searchStream to check for inserted,deleted, and updated events. See the console for output events for searchStream.</p> <pre><code>@App:name(\"store-influxdb\")\n@App:description(\"Perform inserting,deleting,updating Or inserting and reading events from influxDB store\")\n\n\ndefine stream stockStream (symbol string,volume long,price float,time long);\ndefine stream insertStream(symbol string,volume long,price float);\ndefine stream deleteStream(symbol string);\ndefine stream updateStream(symbol string,volume long,price float,time long);\ndefine stream searchStream(symbols string);\ndefine stream containStream(name string,value long);\n\n@sink (type='log') define stream OutputStream (checkName string, checkCategory float, checkVolume long,checkTime long);\n\n@sink (type='log')\ndefine stream logStream(name string,value long);\n\n@Store (type = \"influxdb\",\nurl = \"http://localhost:8086\",\nusername = \"root\",\npassword = \"root\" ,\ninfluxdb.database =\"aTimeSeries\")\n@Index(\"time\",\"symbol\")\ndefine table StockTable(symbol string,volume long,price float,time long) ;    /* Inserting event into influxDB store */\n@info(name='query0')\nfrom insertStream\nselect symbol,volume,price,currentTimeMillis() as time\ninsert into StockTable;\n\n/* Inserting event into influxDB store */\n@info(name = 'query1')  from stockStream select symbol,  volume, price, time\ninsert into StockTable ;  /* deleting events from influxDB store */\n@info(name= 'query2') from deleteStream delete StockTable on StockTable.symbol==symbol;\n\n/* Inserting or updating event into influxDB store */\n@info(name='query3')\nfrom updateStream#window.timeBatch(1 sec)  update or insert into StockTable on StockTable.symbol==symbol and StockTable.time==time;\n\n/* Reading events from influxDB store */\n@info(name = 'query4')\nfrom searchStream#window.length(1) join StockTable on StockTable.symbol==symbols select StockTable.symbol as checkName, StockTable.price as checkCategory,\nStockTable.volume as checkVolume,StockTable.time as checkTime\ninsert into OutputStream;\n\n/* Siddhi In in influxDB store */\n@info(name = 'query6')\nfrom containStream [(StockTable.symbol == name) in StockTable]\ninsert into logStream;\n</code></pre>"},{"location":"samples/store-mongodb/","title":"Receiving Events and Persisting in MongoDB Store","text":""},{"location":"samples/store-mongodb/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to perform CRUD operations using Siddhi queries in MongoDB stores. The sample depicts a scenario in a sweet production factory. The sweet production details, such as name of the raw material and amount used for production, can be stored using <code>insertSweetProductionStream</code>. The following streams can be used to insert, update, search, delete and update or insert the existing data in the store. * Search - <code>searchSweetProductionStream</code> * insert - <code>insertSweetProductionStream`` * delete -</code>deleteSweetProductionStream<code>* update -</code>updateSweetProductionStream<code>* update or insert -</code>updateOrInsertSweetProductionStream<code>* contains -</code>containsSweetProductionStream` (verifies whether all the attributes that enter in the stream exists in the store).</p>"},{"location":"samples/store-mongodb/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Ensure that MongoDB is installed on your machine. (https://docs.mongodb.com/manual/administration/install-community/).</li> <li>If auth is not enabled in the MongoDB instance, skip steps 3 and 4.</li> <li>Create a data store named <code>production</code> in MongoD with relevant access privileges.</li> <li>Create a collection named <code>SweetProductionTable</code> and insert values into <code>SweetProductionTable</code>.</li> <li>Save this sample.</li> </ol>"},{"location":"samples/store-mongodb/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following message is shown on the console:     <pre><code>store-mongodb.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/store-mongodb/#note","title":"Note:","text":"<p>If you want to edit this application, stop the application, make your edits and save, and then start the application again.</p>"},{"location":"samples/store-mongodb/#testing-the-sample","title":"Testing the Sample:","text":"<ol> <li>Simulate single events as follows:<ol> <li>Click on Event Simulator (double arrows on left tab) and then 'Single Simulation'.</li> <li>For the Siddhi App Name, select 'store-mongodb'.</li> <li>For the Stream Name, select 'searchSweetProductionStream'.</li> <li>Enter attribute values and click Send.</li> </ol> </li> <li>Send an event where the name matches a name value in the data you just inserted to the <code>SweetProductionTable</code>. This will satisfy the <code>on</code> condition of the join query.</li> </ol>"},{"location":"samples/store-mongodb/#notes","title":"Notes:","text":"<ul> <li>You can send events to the other corresponding streams to add, delete, update, insert, and search events.</li> <li>The Primary Key constraint in <code>SweetProductionTable</code> is disabled, because the name cannot be used as a <code>PrimaryKey</code> in a <code>ProductionTable</code>.</li> <li>You can use Siddhi functions to create a unique ID for the received events, which you can then use to apply the Primary Key constraint on the data store records. (http://wso2.github.io/siddhi/documentation/siddhi-4.0/#function)</li> </ul>"},{"location":"samples/store-mongodb/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output for raw materials on the console. You can use <code>searchSweetProductionStream</code> to check for inserted, deleted, and updated events.</p> <pre><code>@App:name(\"store-mongodb.siddhi\")\n@App:description('Receive events via simulator and persist the received data in the store.')\n\n\ndefine stream insertSweetProductionStream (name string, amount double);\ndefine stream deleteSweetProductionStream (name string);\ndefine stream searchSweetProductionStream (name string);\ndefine stream updateSweetProductionStream (name string, amount double);\ndefine stream updateOrInsertSweetProductionStream (name string, amount double);\ndefine stream containsSweetProductionStream (name string, amount double);\n\n@sink(type='log')\ndefine stream logStream(name string, amount double);\n\n@Store(type=\"mongodb\",mongodb.uri='mongodb://localhost/production')\n--@PrimaryKey(\"name\")\n@IndexBy(\"amount {background:true}\")\ndefine table SweetProductionTable (name string, amount double);\n\n/* Inserting event into the mongo store */\n@info(name='query1')\nfrom insertSweetProductionStream\ninsert into SweetProductionTable;\n\n/* Deleting event from mongo store */\n@info(name = 'query2')\nfrom deleteSweetProductionStream\ndelete SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating event from mongo store */\n@info(name = 'query3')\nfrom updateSweetProductionStream\nupdate SweetProductionTable\non SweetProductionTable.name == name ;\n\n/* Updating or inserting event from mongo store */\n@info(name = 'query4')\nfrom updateOrInsertSweetProductionStream\nupdate or insert into SweetProductionTable\non SweetProductionTable.name == name;\n\n/* Siddhi In in mongo store */\n@info(name = 'query5')\nfrom containsSweetProductionStream\n[(SweetProductionTable.name == name and SweetProductionTable.amount == amount) in SweetProductionTable]\ninsert into logStream;\n\n--Perform a join on raw material name so that the data in the store can be viewed\n@info(name='query6')\nfrom searchSweetProductionStream as s join SweetProductionTable as sp\non s.name == sp.name\nselect sp.name, sp.amount\ninsert into logStream;\n</code></pre>"},{"location":"samples/streaming-perceptron-sample/","title":"Making Predictions via a Streaming Perceptron Model","text":""},{"location":"samples/streaming-perceptron-sample/#purpose","title":"Purpose:","text":"<p>This application demonstrates how to configure WSO2 Streaming Integrator Tooling to perform binary classification using a streaming Perceptron.</p>"},{"location":"samples/streaming-perceptron-sample/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Save this sample.</li> <li>If there is no syntax error, the following message is shown on the console.     <pre><code>* Siddhi App streaming-perceptron-sample successfully deployed.\n</code></pre></li> </ol>"},{"location":"samples/streaming-perceptron-sample/#executing-the-sample","title":"Executing the Sample:","text":"<ol> <li>Start the Siddhi application by clicking on 'Run'.</li> <li>If the Siddhi application starts successfully, the following messages are shown on the console.     <pre><code>* streaming-perceptron-sample.siddhi - Started Successfully!\n</code></pre></li> </ol>"},{"location":"samples/streaming-perceptron-sample/#testing-the-sample","title":"Testing the Sample:","text":"<p>Note: The Streaming Perceptron for streaming machine learning needs to be trained prior to perform prediction.</p>"},{"location":"samples/streaming-perceptron-sample/#training-phase","title":"Training phase","text":"<p>Send events through one or more of the following methods.</p>"},{"location":"samples/streaming-perceptron-sample/#send-events-to-productiontrainstream-via-event-simulator","title":"Send events to ProductionTrainStream, via event simulator.","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name: streaming-perceptron-sample</li> <li>Stream Name: ProductionTrainStream</li> </ul> </li> <li> <p>In the name and amount fields, enter the following and then click Send to send the event.     <pre><code>density: 50.4\nemperature: 30.03\n</code></pre></p> </li> <li> <p>Send some more events.</p> </li> </ol>"},{"location":"samples/streaming-perceptron-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command","title":"Send events to the simulator http endpoint through the curl command:","text":"<ol> <li>Open a new terminal and issue the following command: <pre><code>curl -X POST -d '{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [50.4, 30.03, true]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal: <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/streaming-perceptron-sample/#publish-events-with-postman","title":"Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"ProductionTrainStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [50.4, 30.03, true]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/streaming-perceptron-sample/#testing-phase","title":"Testing phase","text":"<p>Send events through one or more of the following methods.</p>"},{"location":"samples/streaming-perceptron-sample/#send-events-to-productioninputstream-via-event-simulator","title":"Send events to ProductionInputStream, via event simulator","text":"<ol> <li>Open the event simulator by clicking on the second icon or pressing Ctrl+Shift+I.</li> <li>In the Single Simulation tab of the panel, specify the values as follows:<ul> <li>Siddhi App Name: streaming-perceptron-sample</li> <li>Stream Name: ProductionInputStream</li> </ul> </li> <li>In the name and amount fields, enter the following and then click Send to send the event.     <pre><code>density: 30.4\nemperature: 20.5\n</code></pre></li> <li>Send some more events.</li> </ol>"},{"location":"samples/streaming-perceptron-sample/#send-events-to-the-simulator-http-endpoint-through-the-curl-command_1","title":"Send events to the simulator http endpoint through the curl command:","text":"<ol> <li>Open a new terminal and issue the following command:     <pre><code>curl -X POST -d '{\"streamName\": \"ProductionInputStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [30.4, 20.5]}' http://localhost:9390/simulation/single -H 'content-type: text/plain'\n</code></pre></li> <li>If there is no error, the following messages are shown on the terminal:     <pre><code>{\"status\":\"OK\",\"message\":\"Single Event simulation started successfully\"}\n</code></pre></li> </ol>"},{"location":"samples/streaming-perceptron-sample/#publish-events-with-postman_1","title":"Publish events with Postman:","text":"<ol> <li>Install 'Postman' application from Chrome web store.</li> <li>Launch the application.</li> <li>Make a 'Post' request to the 'http://localhost:9390/simulation/single' endpoint. Set the Content-Type to 'text/plain' and set the request body in text as follows:     <pre><code>{\"streamName\": \"ProductionInputStream\", \"siddhiAppName\": \"streaming-perceptron-sample\",\"data\": [30.4, 20.5]}\n</code></pre></li> <li>Click 'send'. If there is no error, the following messages are shown on the console:     <pre><code>\"status\": \"OK\",\n\"message\": \"Single Event simulation started successfully\"\n</code></pre></li> </ol>"},{"location":"samples/streaming-perceptron-sample/#viewing-the-results","title":"Viewing the Results:","text":"<p>See the output on the terminal: <pre><code>INFO {io.siddhi.core.query.processor.stream.LogStreamProcessor} - streaming-perceptron-sample: LOGGER, StreamEvent{ timestamp=1513596699142, beforeWindowData=null, onAfterWindowData=null, outputData=[34.0, 12.0, false, 0.0], type=CURRENT, next=null}\n</code></pre></p> <pre><code>@App:name(\"streaming-perceptron-sample\")\n@App:description('Train a streaming Perceptron model to predict whether an item passes quality check.')\n\n\ndefine stream ProductionTrainStream (density double, temperature double, qualityCheck_pass bool );\n\ndefine stream ProductionInputStream (density double, temperature double);\n\n@sink(type='log')\ndefine stream PredictedQCStream (density double, temperature double, prediction bool, confidenceLevel double);\n\n@info(name = 'query-train')\nfrom ProductionTrainStream#streamingml:updatePerceptronClassifier('QCmodel', qualityCheck_pass, 0.1, density, temperature)\nselect *\ninsert into trainOutputStream;\n\n@info(name = 'query-predict')\nfrom ProductionInputStream#streamingml:perceptronClassifier('QCmodel', 0.0, 0.5, density, temperature)\nselect *\ninsert into PredictedQCStream;\n</code></pre>"},{"location":"setup/change-hostname-and-context-path/","title":"Changing the Host Name and Context Path of Web UI Applications","text":"<p>When you start any web application of the WSO2 Streaming Integrator (i.e., Streaming Integrator Tooling, the Template Editor, Business Rules, or Policies) the URL to access its user interface is displayed in the start up logs as follows.</p> <p><pre><code>INFO {org.wso2.carbon.siddhi.editor.core.internal.StartupComponent} - Editor Started on : http://&lt;IP_ADDRESS&gt;:&lt;PORT&gt;/&lt;WEB_UI_APPLICATION_NAME&gt;\n</code></pre> e.g., The URL to access the Streaming Integrator Tooling is logged as follows:</p> <pre><code>INFO {org.wso2.carbon.siddhi.editor.core.internal.StartupComponent} - Editor Started on : http://&lt;IP_ADDRESS&gt;:9390/editor\n</code></pre> <p>You can change this URL to display a host name instead of the IP address. You can also change it to display a different name for the web UI application (in the given example, a name different to <code>editor</code>). To do the required configurations to make this change, see the topics below.</p>"},{"location":"setup/change-hostname-and-context-path/#changing-the-ip-address-to-a-host-name","title":"Changing the IP address to a host name","text":"<p>To change the IP address displayed in the URL to a host name:</p> <ol> <li> <p>Open the <code>&lt;SI_TOOLING_HOME&gt;/CONF/server/deployment.yaml</code> file.</p> </li> <li> <p>In the <code>wso2.transport.http:</code> \u2192 <code>listenerConfigurations:</code> section, change the value for the <code>host parameter</code> to the required hostname (e.g., to <code>streaming-integrator</code>) as shown below.</p> <p>Info</p> <p>Note that in this example, you are specifying a host name for the <code>http</code> URLs. If the web UI application you are accessing has an <code>https</code> URL, you need to make this change for the listener configuration with the <code>https</code> scheme.</p> <pre><code>     wso2.transport.http:\ntransportProperties:\n\nlistenerConfigurations:\n-\nid: \"default\"\nhost: \"streaming-integrator\"\nport: 9387\n</code></pre> </li> </ol> <p>Once you change the host to <code>streaming-integrator</code> as shown in the above example, the <code>http</code> URLs to access the Streaming Integrator Tooling and the Template Editor display <code>streaming-integrator</code> as the host name.</p> <p>e.g., the URL to access the Streaming Integrator Tooling is displayed as follows:</p> <pre><code>    INFO {org.wso2.carbon.siddhi.editor.core.internal.StartupComponent} - Editor Started on : http://streaming-integrator:9390/editor\n</code></pre>"},{"location":"setup/change-hostname-and-context-path/#changing-the-web-ui-application-name","title":"Changing the web UI application name","text":"<p>To change the name of the web UI application that appears in its URL, add a section as follows in the <code>&lt;SI_TOOLING_HOME&gt;/CONF/server/deployment.yaml</code> file.</p> <p><pre><code>wso2.carbon-ui-server:\napps:\n# configurations for the Editor app \"editor\": \ncontextPath: \"/tooling\"\n</code></pre> Here, the context path for the editor webUI application (i.e., the Streaming Integrator Studio) is specified as <code>tooling</code>. Therefore, when you start the Streaming Integrator Tooling server, the URL  for the Streaming Integrator Tooling appears as follows.</p> <pre><code>INFO {org.wso2.carbon.siddhi.editor.core.internal.StartupComponent} - Editor Started on : http://&lt;IP_ADDRESS&gt;:9390/tooling\n</code></pre>"},{"location":"setup/configuring-business-rules-deployment/","title":"Configuring Business Rules Deployment","text":""},{"location":"setup/configuring-business-rules-deployment/#configuring-business-rules-deployment","title":"Configuring Business Rules Deployment","text":"<p>The Business Rules Manager derives business rules from the rule templates within template groups. Each rule template has a UUID(Universally Unique Identifier) for the purpose of uniquely identifying it. When you configure a Streaming Integrator node to use a specific rule template, Siddhi applications deployed in the node are derived from the business rules created from that rule template.</p> <p>To configure a Streaming Integrator node to use specific rules templates, follow the steps below:</p> <ol> <li> <p>Open the <code>&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> </li> <li> <p>In the <code>wso2.business.rules.manager</code> -&gt; <code>deployment_configs:</code> section, provide the URL(s) of  the Streaming Integrator node(s) in which you want to deploy Siddhi applications. The required format is <code>&lt;HOST_NAME&gt;:&lt;PORT&gt;</code>.</p> <pre><code> deployment_configs:\n- &lt;NODE1_HOST_NAME&gt;:&lt;NODE1_PORT&gt;\n&lt;NODE2_HOST_NAME&gt;:&lt;NODE2_PORT&gt;\n</code></pre> <p>e.g.,  <pre><code> deployment_configs:\n- localhost:9090\n10.100.4.140:9090\n</code></pre></p> </li> <li> <p>List down the UUIDs of required rule templates under each node. As a result, Siddhi applications are created out of the business rules that are derived from these templates for the required nodes.</p> <p><pre><code> deployment_configs:\n- &lt;NODE1_HOST_NAME&gt;:&lt;NODE1_PORT&gt;:\n- ruleTemplate1_UUID\n- ruleTemplate2_UUID\n- ruleTemplate3_UUID\n</code></pre> e.g., </p> <p><pre><code> deployment_configs:\n- localhost:9090:\n- sweet-production-kpi-analysis\n- stock-exchange-input\n- stock-exchange-output\n</code></pre>    !!! tip    If required, you can enter a specific rule template under multiple nodes as shown below. Before doing so, ensure the you have selected Many for the Instance Count field of the template. For more information, see Creating a Business Rules Template. <code>yaml     deployment_configs:         - &lt;NODE1_HOST_NAME&gt;:&lt;NODE1_PORT&gt;:             - ruleTemplate1_UUID             - ruleTemplate2_UUID             - ruleTemplate3_UUID           &lt;NODE2_HOST_NAME&gt;:&lt;NODE2_PORT&gt;:             - ruleTemplate1_UUID             - ruleTemplate3_UUID           &lt;NODE3_HOST_NAME&gt;:&lt;NODE3_PORT&gt;:             - ruleTemplate2_UUID             - ruleTemplate3_UUID             - ruleTemplate4_UUID</code>    e.g.,    <code>yaml     deployment_configs:         - localhost:9090:             - sweet-production-kpi-analysis             - stock-exchange-input             - stock-exchange-output           10.100.40.169:9090:             - identifying-continuous-production-decrease             - sweet-production-kpi-analysis</code>    In the above example, the <code>sweet-production-kpi-analysis</code> UUID is configured under two Streaming Integrator nodes. Therefore, if you derive a business rule from the <code>sweet-production-kpi-analysis</code> template, the Siddhi applications created from it are deployed in both the nodes.</p> </li> <li> <p>Specify the username and password that are common for all the Streaming Integrator nodes.</p> <pre><code> username: admin\npassword: admin\n</code></pre> </li> </ol> <p>The complete deployment configuration for Business Rules looks as follows.</p> <pre><code>    wso2.business.rules.manager:\ndatasource: BUSINESS_RULES_DB\ndeployment_configs:\n- localhost:9090:\n- stock-data-analysis\n- stock-exchange-input\n- stock-exchange-output\n- identifying-continuous-production-decrease\n- sweet-production-kpi-analysis\nusername: admin\npassword: admin\n</code></pre>"},{"location":"setup/configuring-data-sources/","title":"Configuring Data Sources","text":"<p>In the Streaming Integrator, there are datasources specific to both the Streaming Integrator server and Streaming Integrator Tooling. The data sources of each runtime are defined in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <p>e.g., To configure a data source in the\u00a0server\u00a0runtime, the relevant configurations need to be added in the <code>&lt;SI_Home&gt;/conf/server/deployment.yaml</code> file.</p> <p>Note</p> <p>The embedded H2 databases shipped with your product are suitable for development and testing environments. However, for production environments, it is recommended to use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, MS SQL, etc.</p> <p>To view a sample data source configuration for each database type supported, expand the following sections:</p> <p>Info</p> <p>If the database driver is not an OSGI bundle, then it should be converted to OSGI (using jartobundle.sh) before placing it in the <code>&lt;SI_HOME&gt;|&lt;SI_TOOLING_HOME&gt;/lib</code> directory. For detailed instructions,see Adding Third Party Non OSGi Libraries.</p> <p>e.g., <code>sh WSO2_SI_HOME/bin/jartobundle.sh ojdbc6.jar WSO2_SI_HOME/lib/</code></p> <p>The database should be tuned to handle the total maxPoolSize (The maximum number of threads that should be reserved at any given time to handle events) that is defined in the <code>deployment.yaml</code> file.</p> MySQL<pre><code>    wso2.datasources:\n     dataSources:\n     name: TEST_DB\n     description: The datasource used for test database\n     jndiConfig:\n     definition:\n       type: RDBMS\n       configuration:\n         jdbcUrl: jdbc:mysql://hostname:port/testdb\n         username: root\n         password: root\n         driverClassName: com.mysql.jdbc.Driver\n         minIdle: 5\n         maxPoolSize: 50\n         idleTimeout: 60000\n         connectionTestQuery: SELECT 1\n         validationTimeout: 30000\n         isAutoCommit: false\n</code></pre> <p>** Parameter descriptions*</p> Parameter Description <code>name</code> A unique name for the data source. <code>description</code> A description of the data source. <code>type</code> The database type to which the data source connects. <code>jdbcUrl</code> The string used by the database driver to connect to the database. <code>username</code> the username with which WSO2 Streaming Integrator accesses the database. <code>password</code> The password with which WSO2 Streaming Integrator accesses the database. <code>driverClassName</code> The name of the driver class that establishes the connection to the database. <code>minIdle</code> The minimum number of active connections that can remain idle in the pool at a given time without extra connections being created. e.g., If you specify <code>2</code> as the value for this parameter, the system creates a new connection to the database only if the connection pool currently has only two or less active and idle connections. <code>maxPoolSize</code> The maximum number of total connections that are allowed to exist in the connection pool at a given time. <code>idleTimeout</code> The maximum duration of time (in milliseconds) for which the system allows a connection to the database to be idle before closing it. <code>connectionTestQuery</code> The test query executed on the database to check the validity of the connection. <code>validationTimeout</code> The maximum duration of time (in milliseconds) that is allowed between validation tests carried out for the database connection. <code>isAutoCommit</code> If this parameter is set to <code>true</code>, each database query you perform during a single session is treated as a separate database transaction. As a result, the result of any query is visible to other database sessions soon after it is executed. If this parameter is set to <code>true</code>, the system considers the whole database session as a single transaction. Therefore, the reults of all the queries you execute within a single session are visible only after the session is over. POSTGRES<p><pre><code>wso2.datasources:\n dataSources:\n     description: The datasource used for test database\n     jndiConfig:\n     definition:\n       type: RDBMS\n      configuration:\n        jdbcUrl: jdbc:postgresql://hostname:port/testdb\n        username: root\n        password: root\n        driverClassName: org.postgresql.Driver\n        minIdle: 5\n        maxPoolSize: 50\n        idleTimeout: 60000\n        connectionTestQuery: SELECT 1\n        validationTimeout: 30000\n        isAutoCommit: false\n</code></pre> | Parameter         | Description                                                           | |-----------------------|---------------------------------------------------------------------------| | <code>name</code>                | A unique name for the data source. | | <code>description</code>         | A description of the data source. | | <code>type</code>                | The database type to which the data source connects. | | <code>jdbcUrl</code>             | The string used by the database driver to connect to the database. | | <code>username</code>            | the username with which WSO2 Streaming Integrator accesses the database.| | <code>password</code>            | The password with which WSO2 Streaming Integrator accesses the database. | | <code>driverClassName</code>     | The name of the driver class that establishes the connection to the database. | | <code>minIdle</code>             | The minimum number of active connections that can remain idle in the pool at a given time without extra connections being created. e.g., If you specify <code>2</code> as the value for this parameter, the system creates a new connection to the database only if the connection pool currently has only two or less active and idle connections. |  | <code>maxPoolSize</code>         | The maximum number of total connections that are allowed to exist in the connection pool at a given time. | | <code>idleTimeout</code>         | The maximum duration of time (in milliseconds) for which the system allows a connection to the database to be idle before closing it. | | <code>connectionTestQuery</code> | The test query executed on the database to check the validity of the connection. | | <code>validationTimeout</code>   | The maximum duration of time (in milliseconds) that is allowed between validation tests carried out for the database connection. | | <code>isAutoCommit</code>        | If this parameter is set to <code>true</code>, each database query you perform during a single session is treated as a separate database transaction. As a result, the result of any query is visible to other database sessions soon after it is executed. If this parameter is set to <code>true</code>, the system considers the whole database session as a single transaction. Therefore, the reults of all the queries you execute within a single session are visible only after the session is over. |</p> Oracle<p>There are two ways to configure this database type. If you have a System Identifier (SID), use this (older) format:</p> <p><code>jdbc:oracle:thin:@[HOST][:PORT]:SID</code></p> <pre><code>wso2.datasources:\n dataSources:\n     description: The datasource used for test database\n     jndiConfig:\n     definition:\n       type: RDBMS\n       configuration:\n         jdbcUrl: jdbc:oracle:thin:@hostname:port:SID\n         username: testdb\n         password: root\n         driverClassName: oracle.jdbc.driver.OracleDriver\n         minIdle: 5\n         maxPoolSize: 50\n         idleTimeout: 60000\n         connectionTestQuery: SELECT 1\n         validationTimeout: 30000\n         isAutoCommit: false\n</code></pre> <p>If you have an Oracle service name, use this (newer) format:</p> <p><code>jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE</code></p> <pre><code>wso2.datasources:\n dataSources:\n     description: The datasource used for test database\n     jndiConfig:\n     definition:\n       type: RDBMS\n       configuration:\n         jdbcUrl: jdbc:oracle:thin:@hostname:port/SERVICE\n         username: testdb\n         password: root\n         driverClassName: oracle.jdbc.driver.OracleDriver\n         minIdle: 5\n         maxPoolSize: 50\n         idleTimeout: 60000\n         connectionTestQuery: SELECT 1\n         validationTimeout: 30000\n         isAutoCommit: false\n</code></pre> <p>The Oracle driver need to be converted to OSGi (using <code>jartobundle.sh</code>) before put into <code>SI_HOME/lib</code> directory. For detailed instructions, see Adding Third Party Non OSGi Libraries.</p> <p>If you are using persisted aggregations, you can include the <code>connectionInitSql</code> parameter instead of <code>connectionTestQuery</code> as shown in the following sample configuration.</p> <pre><code>Sample datasource configuration -\n\n - name: TEST_DB\n   description: The datasource used for test database\n   jndiConfig:\n     name: jdbc/TEST_DB\n   definition:\n     type: RDBMS\n     configuration:\n       jdbcUrl: jdbc:oracle:thin:@localhost:1521/ORCLCDB.localdomain\n       username: &lt;username&gt;\n       password: &lt;password&gt;\n       driverClassName: oracle.jdbc.driver.OracleDriver\n       minIdle: 5\n       maxPoolSize: 50\n       idleTimeout: 60000\n       connectionInitSql: ALTER SESSION SET NLS_DATE_FORMAT='RRRR/fmMM/fmDD'\n       validationTimeout: 30000\n       isAutoCommit: false  \n</code></pre> Parameter Description name A unique name for the data source. description A description of the data source. type The database type to which the data source connects. jdbcUrl The string used by the database driver to connect to the database. username the username with which WSO2 Streaming Integrator accesses the database. password The password with which WSO2 Streaming Integrator accesses the database. driverClassName The name of the driver class that establishes the connection to the database. minIdle The minimum number of active connections that can remain idle in the pool at a given time without extra connections being created. e.g., If you specify <code>2</code> as the value for this parameter, the system creates a new connection to the database only if the connection pool currently has only two or less active and idle connections. maxPoolSize The maximum number of total connections that are allowed to exist in the connection pool at a given time. idleTimeout The maximum duration of time (in milliseconds) for which the system allows a connection to the database to be idle before closing it. connectionInitSql The test query executed on the database to check the validity of the connection. You can use this parameter instead of the <code>connectionTestQuery</code> parameter when you are using persisted aggregations. This is because when you use persisted aggregation with an Oracle database, the database connection session time format needs to be changed to <code>RRRR/fmMM/fmDD</code>. This is addressed when you use the <code>connectionInitSql</code> parameter. connectionTestQuery The test query executed on the database to check the validity of the connection. validationTimeout The maximum duration of time (in milliseconds) that is allowed between validation tests carried out for the database connection. isAutoCommit If this parameter is set to <code>true</code>, each database query you perform during a single session is treated as a separate database transaction. As a result, the result of any query is visible to other database sessions soon after it is executed. If this parameter is set to <code>true</code>, the system considers the whole database session as a single transaction. Therefore, the reults of all the queries you execute within a single session are visible only after the session is over. MSSQL<pre><code>wso2.datasources:\n dataSources:\n     description: The datasource used for test database\n     jndiConfig:\n     definition:\n       type: RDBMS\n       configuration:\n         jdbcUrl: jdbc:sqlserver://hostname:port;databaseName=testdb\n         username: root\n         password: root\n         driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver\n         minIdle: 5\n         maxPoolSize: 50\n         idleTimeout: 60000\n         connectionTestQuery: SELECT 1\n         validationTimeout: 30000\n         isAutoCommit: false\n</code></pre> <p>The following tables explain the default data sources configured in the Streaming Integrator components for different purposes, and how to change them.</p>"},{"location":"setup/configuring-data-sources/#rdbms-data-provider","title":"RDBMS data provider","text":"Database Access Requirement                  The RDBMS provider publishes records from RDBMS tables into generated widgets. It can also be configured to purge records in tables. In order to carry out these actions, this provider requires access to read and delete records in user defined tables of the database. For more information about the RDBMS data provider, see Generating Widgets. Required/Optional This is required if you select a datasource when generating the widget or use existing widgets that connect to the RDBMS data provider when you run the dashboard server for the Streaming Integrator. Default Datasource Name <code>SAMPLE_DB</code> Default Database The default <code>H2</code> database location is <code> &lt;DASHBOARD_HOME&gt;/wso2/dashboard/database/SAMPLE_DB</code>. Tables The default database shipped with a sample table named <code>TRANSACTION_TABLE</code> . Schemas and Queries <p>The schema for the sample table is <code>TRANSACTIONS_TABLE (creditCardNo VARCHAR(50), country VARCHAR(50), transaction VARCHAR(50), amount INT)</code></p> <p>You can also view default schemas and queries .</p> Tested Database Types H2, MySQL, Postgres, Mssql, Oracle 11g"},{"location":"setup/configuring-data-sources/#carbon-coordination","title":"Carbon coordination","text":"Database Access Requirement Carbon coordination supports zookeeper and RDBMS based coordination. In RDBMS coordination, database access is required for updating the heartbeats of the nodes. In addition, database access is required to update the coordinator and the other members in the cluster. For more information, see Configuring Cluster Coordination. Required/Optional This is required. However, you can also use Zookeeper coordination instead of RDBMS. Default Datasource Name <code>WSO2_CARBON_DB</code> Tables <code>LEADER_STATUS_TABLE</code>, <code>MEMBERSHIP_EVENT_TABLE</code>, <code>REMOVED_MEMBERS_TABLE</code>, <code>CLUSTER_NODE_STATUS_TABLE</code> Schemas and Queries <p>See information about schemas and queries.</p> Tested Database Types MySQL, Postgres, Mssql, Oracle 11g"},{"location":"setup/configuring-data-sources/#streaming-integrator-core-persistence","title":"Streaming Integrator core - persistence","text":"Database Access Requirement This involves persisting the state of Siddhi Applications periodically in the database. State persistence is enabled by selecting the <code>org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore</code> class in the <code>state.persistence</code> section of the <code>&lt;SI_Home&gt;/conf/&lt;server&gt;/deployment.yaml</code> file. For more information, see Configuring Database and File System State Persistence. Required/Optional This is optional. WSO2 is configured to persist the state of Siddhi applications by default. Default Datasource Name N/A. If state persistence is required, you need to configure the datasource in the <code>&lt;SI_Home&gt;/conf/&lt;server&gt;/deployment.yaml</code> file under <code>state.persistence</code> &gt;<code>config</code> &gt; <code>datasource</code> . Tables N/A. If state persistence is required, you need to specify the table name to be used when persisting the state in the <code>&lt;SI_Home&gt;/conf/&lt;server&gt;/deployment.yaml</code> file under <code>state.persistence</code> &gt; <code>config</code> &gt; <code>table</code> . Schemas and Queries <p>See information about schemas and queries.</p> Tested Database Types H2, MySQL, Postgres, Mssql, Oracle 11g"},{"location":"setup/configuring-data-sources/#streaming-integrator-status-dashboard","title":"Streaming Integrator - Status Dashboard","text":"Database Access Requirement To display information relating to the status of your Streaming Integrator deployment, the Status Dashboard needs to retrieve carbon metrics data, registered Streaming Integrator server details and authentication details within the cluster from the database. For more information, see Monitoring Stream Processor . Required/Optional Required Default Datasource Name <code>WSO2_STATUS_DASHBOARD_DB</code>, <code>WSO2_METRICS_DB</code> Tables <code>METRIC_COUNTER</code>, <code>METRIC_GAUGE</code>, <code>METRIC_HISTOGRAM</code>, <code>METRIC_METER</code>, <code>METRIC_TIMER</code>, <code>WORKERS_CONFIGURATIONS</code>, <code>WORKERS_DETAILS</code> Schemas and Queries <p>See information about schemas and queries.              Tested Database Types H2, MySQL, Mssql, Oracle 11g ( Postgres is tested with Carbon-Metrics only)"},{"location":"setup/configuring-data-sources/#siddhi-rdbms-store","title":"Siddhi RDBMS store","text":"Database Access Requirement It gives the capability of creating the tables at the siddhi application runtime and access the existing tables if a user-defined carbon data source or JNDI property in a siddhi application. Documentation can be found in [Siddhi Extensions Documentation](https://siddhi-io.github.io/siddhi-store-rdbms/api/latest/). Required/Optional Optional Default Datasource Name No such default Datasource. User has to create the datasource in the Siddhi application Tables No such default tables. User has to define the tables Schemas and Queries <p>See information about schemas and queries.</p> Tested Database Types H2, MySQL, Mssql, Oracle 11g, DB2, PostgreSQL"},{"location":"setup/configuring-data-sources/#carbon-dashboards","title":"Carbon Dashboards","text":"Database Access Requirement Carbon Dashboard feature uses its datasource to persist the dashboard related information Required/Optional Optional Default Datasource Name <code>WSO2_DASHBOARD_DB</code> Tables <code>DASHBOARD_RESOURCES</code> Schemas and Queries <p>information about schemas and queries.              Tested Database Types H2, MySQL, Postgres"},{"location":"setup/configuring-data-sources/#business-rules","title":"Business Rules","text":"Database Access Requirement Business Rules feature uses database to persist the derived business rules Required/Optional Mandatory Default Datasource Name <code>BUSINESS_RULES_DB</code> Tables <code>BUSINESS_RULES</code>, <code>RULES_TEMPLATES</code> Schemas and Queries <p>See information about schemas and queries.</p> Tested Database Types H2, MySQL, Oracle 11g"},{"location":"setup/configuring-data-sources/#idp-client","title":"IdP client","text":"Database Access Requirement IdP client access the DB layer to persist the client id and the client secret of dynamic client registration Required/Optional Mandatory for external IdP client Default Datasource Name <code>DB_AUTH_DB</code> Tables <code>OAUTH_APPS</code> Schemas and Queries <p> See information about schemas and queries.</p> Tested Database Types H2, MySQL, Oracle 11g"},{"location":"setup/configuring-data-sources/#permission-provider","title":"Permission \u00a0provider","text":"Database Access Requirement Permission provider will access the DB to persist permissions and role - permission mappings. Required/Optional Mandatory, default is in H2 Default Datasource Name <code>PERMISSIONS_DB</code> Tables <code>PERMISSIONS, ROLE_PERMISSIONS</code> Schemas and Queries <p>See information about schemas and queries.</p> Tested Database Types H2, MySQL, Mssql, Oracle 11g , Postgres                  &lt;/table"},{"location":"setup/defining-Data-Tables/","title":"Defining Data Tables","text":"<p>This section explains how to configure data tables to store the events you need to persist to carry out time series aggregation.</p> <p>The data handled by WSO2 Stream Processor are stored in the following two types of tables:</p> <ul> <li> <p>In-memory tables : If no \u00a0store-backed tables are defined, data     is stored in in-memory tables by default.</p> </li> <li> <p>Store-backed tables : These are tables that are defined by you     in an external database. For a list of database types supported and     instructions to define table for different database types, see     Defining Tables for External Data     Stores .</p> </li> </ul>"},{"location":"setup/defining-Data-Tables/#adding-primary-and-index-keys","title":"Adding primary and index keys","text":"<p>Both in-memory tables and tables backed by external databases support primary and index keys. These are defined to allow stored information to be searched and retrieved in an effective and efficient manner.</p>"},{"location":"setup/defining-Data-Tables/#adding-primary-keys","title":"Adding primary keys","text":"<p>Attribute(s) within the event stream for which the event table is created can be specified as the primary key for the table. The purpose of primary key is to ensure that the value for a selected attribute is unique for each entry in the table. This prevents the duplication of entries saved in the table.</p> <p>Primary keys are configured via the <code>@PrimaryKey</code> annotation. Only one\u00a0@PrimaryKey annotation is allowed per event table.</p> <p>When several attributes are given within Primary key annotation (e.g @PrimaryKey( 'key1', 'key2')), those attributes would act as a composite primary key.</p>"},{"location":"setup/defining-Data-Tables/#syntax","title":"Syntax","text":"<pre><code>    @PrimaryKey('&lt;attribute_1&gt;')\ndefine table &lt;event_table&gt; (&lt;attribute_1&gt; &lt;attribute_type&gt;, &lt;attribute_2&gt; &lt;attribute_type&gt;, &lt;attribute_3&gt; &lt;attribute_type&gt;);\n</code></pre>"},{"location":"setup/defining-Data-Tables/#example","title":"Example","text":"<pre><code>    @PrimaryKey('symbol')\ndefine table StockTable (symbol string, price float, volume long);\n</code></pre> <p>The above configuration ensures that each entry saved in the <code>StockTable</code> event table should have a unique value for the <code>symbol</code> attribute because this attribute is defined as the primary key.</p>"},{"location":"setup/defining-Data-Tables/#adding-indexes","title":"Adding indexes","text":"<p>An attribute within the event stream for which the event table is created can be specified as the primary key for the table. This allows the entries stored within the table to be indexed by that attribute.</p> <p>Indexes are configured via the <code>@Index</code> annotation.</p> <p>An event table can have multiple attributes defined as index attributes. However, only one <code>@Index</code> annotation can be added per event table.</p>"},{"location":"setup/defining-Data-Tables/#syntax_1","title":"Syntax","text":"<p>To index by a single attribute:</p> <pre><code>    @Index('&lt;attribute_1&gt;')\ndefine table &lt;event_table&gt; (&lt;attribute_1&gt; &lt;attribute_type&gt;, &lt;attribute_2&gt; &lt;attribute_type&gt;, &lt;attribute_3&gt; &lt;attribute_type&gt;);\n</code></pre> <p>To index by multiple attributes:</p> <pre><code>    @Index('&lt;attribute_1&gt;''&lt;attribute_2&gt;')\ndefine table &lt;event_table&gt; (&lt;attribute_1&gt; &lt;attribute_type&gt;, &lt;attribute_2&gt; &lt;attribute_type&gt;, &lt;attribute_3&gt; &lt;attribute_type&gt;);\n</code></pre>"},{"location":"setup/defining-Data-Tables/#example_1","title":"Example","text":"<pre><code>    @Index('symbol')\ndefine table StockTable (symbol string, price float, volume long);\n</code></pre> <p>The above configuration ensures that the entries stored in the <code>StockTable</code> event table are indexed by the <code>symbol</code> attribute.</p>"},{"location":"setup/defining-Data-Tables/#defining-tables-for-physical-stores","title":"Defining Tables for Physical Stores","text":""},{"location":"setup/defining-Tables-for-Physical-Stores/","title":"Defining Tables for Physical Stores","text":"<p>This section explains how to define data tables to store data handled by WSO2 Stream Processor in physical databases. T he @store annotation syntax for defining these tables differ based on the database type as well as where the properties are defined.</p> <p>The store properties(such as URL, username and password) can be defined in the following ways:</p> <ul> <li> <p>Inline definition : The data store can be defined within the     Siddhi application as shown in the example below:</p> <pre><code>    @Store(type='hbase', hbase.zookeeper.quorum='localhost')\n@primaryKey('name')\ndefine table SweetProductionTable (name string, amount double);\n</code></pre> <pre><code>!!! info\n\nThis method is not recommended in a production environment because\nis less secure compared to the other methods.</code></pre> </li> <li> <p>As references in the deployment file : In order to do this, the     store configuration needs to be defined for the relevant deployment     environment in the     <code>&lt;SP_HOME&gt;/conf/&lt;PROFILE&gt;/deployment.yaml</code> file     as a ref (i.e., in a separate section siddhi: and subsection refs:)     as shown in the example below.  </p> <pre><code>!!! info\n\nThe database connection is started when a Siddhi application is\ndeployed, and\u00a0disconnected when the Siddhi application is\nundeployed. Therefore, this metho is not recommended if the same\ndatabase is used across multiple Siddhi applications.</code></pre> <pre><code>    siddhi:   refs:\n        -\n           ref:\n             name: 'store1'\n             type: 'rdbms'\n             properties:\n               jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE'\n               username: 'root'\n               password: ${sec:store1.password}\n               field.length='currentTime:100'\n               jdbc.driver.name: 'org.h2.Driver'\n</code></pre> <p>Then you need to refer to that store via the <code>@store</code> annotation as in the Siddhi application as shown in the example below.</p> <pre><code>        @Store(ref='store1')\n@PrimaryKey('id')\n@Index('houseId')\ndefine table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string);\n</code></pre> </li> <li> <p>Using WSO2 data sources configuration : Once a data source     defined in the wso2.datasources section of the file,     <code>&lt;SP_HOME&gt;/conf/&lt;PROFILE&gt;/deployment.yaml,</code> the     same connection can be used across different Siddhi applications.     This is done by specifying the data source to which you need to     connect via the <code>@store</code> annotation\u00a0in the     following format.</p> <pre><code>        @Store(type='&lt;DATABASE_TYPE&gt;', datasource=\u2019&lt;carbon.datasource.name&gt;\u2019)\n</code></pre> <pre><code>!!! info\n\nThe database connection pool is initialized at server startup, and\ndestroyed at server shut down.</code></pre> <p>This is further illustrated by the following example.</p> <pre><code>    @Store(type='rdbms', datasource=\u2019SweetFactoryDB\u2019)@PrimaryKey(\"symbol\")\ndefine table FooTable (symbol string, price float, volume long);\n</code></pre> <p>For more information about definig datasources, see Configuring Datasources .</p> </li> </ul> <p>The following database types are currently supported for WSO2 SP.</p> <p>Tip</p> <p>Before you begin:</p> <p>In order to create and use an event table to store data, the following should be completed:</p> <ul> <li>The required database (MySql,\u00a0MongoDB, Oracle Database, etc)\u00a0should     be downloaded and installed.</li> <li>A database instance should be started.</li> <li>The user IDs used to perform the required table operations should be     granted the relevant privileges.</li> <li> <p>The relevant JDBC Driver must be downloaded and the jar must be put     in\u00a0the <code>&lt;SP_HOME&gt;/lib</code> directory.</p> </li> <li> <p>RDBMS</p> </li> <li>Apache HBase</li> <li>Apache Solr</li> <li>MongoDB</li> </ul>"},{"location":"setup/defining-Tables-for-Physical-Stores/#rdbms","title":"RDBMS","text":"<p>The RDBMS database types that are currently supported are as follows:</p> <ul> <li>H2</li> <li>MySQL</li> <li>Oracle database</li> <li>My SQL Server</li> <li>PostgreSQL</li> <li>IBM DB2</li> </ul>"},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax","title":"Query syntax","text":"<p>The following is the syntax for an RDBMS event table configuration:</p> <pre><code>    @store(type=\"rdbms\", jdbc.url=\"&lt;jdbc.url&gt;\", username=\"&lt;username&gt;\", password=\"&lt;password&gt;\",pool.properties=\"&lt;prop1&gt;:&lt;val1&gt;,&lt;prop2&gt;:&lt;val2&gt;\")\n@PrimaryKey(\"col1\")\n@IndexBy(\"col3\")\ndefine table &lt;table_name&gt;(col1 datatype1, col2 datatype2, col3 datatype3);\n</code></pre>"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters","title":"Parameters","text":"<p>The following parameters are configured in the definition of an RDBMS event table.</p> Parameter Description Required/Optional <code>jdbc.url</code> The JDBC URL via which the RDBMS data store is accessed. Required <code>username</code> The username to be used to access the RDBMS data store. Required <code>password</code> The password to be used to access the RDBMS data store. Required <code>pool.properties</code> Any pool parameters for the database connection must be specified as key value pairs. Required <code>jndi.resource</code> The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account. Optional <code>table.name</code> The name of the RDBMS table created. Optional <code>field.length</code> The number of characters that the values for fields of the <code>STRING</code> type in the table definition must contain. If this is not specified, the default number of characters specific to the database type is considered. Optional <p>In addition to the above parameters, you can add the <code>@primary</code> and <code>@index</code> annotations in the RDBMS table configuration.</p> <ul> <li><code>@primary</code> : This specifies     a list of comma-separated values to be treated as unique fields in     the table. Each record in the table must have a unique combination     of values for the fields specified here.</li> <li><code>@index</code> : This specifies     the fields that must be indexed at the database level. You can     specify multiple values as a come-separated list.</li> </ul>"},{"location":"setup/defining-Tables-for-Physical-Stores/#example","title":"Example","text":"<p>The following is an example of an RDBMS table definition:</p> <pre><code>    @Store(type=\"rdbms\", jdbc.url=\"jdbc:h2:repository/database/ANALYTICS_EVENT_STORE\", username=\"root\", password=\"root\",field.length=\"symbol:254\")\n@PrimaryKey(\"symbol\")\ndefine table FooTable (symbol string, price float, volume long);\n</code></pre>"},{"location":"setup/defining-Tables-for-Physical-Stores/#apache-hbase","title":"Apache HBase","text":""},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax_1","title":"Query syntax","text":"<p>The query syntax to define an HBase table is as follows.</p> <pre><code>    @Store(type=\"hbase\", any.hbase.property=\"&lt;STRING&gt;\", table.name=\"&lt;STRING&gt;\", column.family.name=\"&lt;STRING&gt;\")\n@PrimaryKey(\"PRIMARY_KEY\")\n@Index(\"INDEX\")\n</code></pre>"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters_1","title":"Parameters","text":"<p>The following parameters are configured in the definition of an HBase event table:</p> Parameter Description Required/Optional <p><code>                             table.name                           </code></p> <p>The name with which the table should be persisted in the store. If no table name is specified, the table in the store is assigned the same name as the corresponding Siddhi table.</p> Optional <code>                           column.family.name                         </code> The name of the HBase column family from which data must be stored/referred to. Required any.hbase.property <p>Any property that can be specified for HBase connectivity in hbase-site.xml is also accepted by the HBase Store implementation. The most frequently used properties are...</p> <p>hbase.zookeeper.quorum - The hostname of the server in which the zookeeper node is run.</p> <p>hbase.zookeeper.property.clientPort - The port of the zookeeper node.</p> Required <p>In addition, the following annotations are used in the HBase definition.</p> <ul> <li><code>@primary</code> : This specifies     a list of comma-separated values to be treated as unique fields in     the table. Each record in the table must have a unique combination     of values for the fields specified here. Primary keys allow you to     fetch records in a table by providing a unique reference for each     record. Therefore, if you do not include one or more primary keys in     the table definition, it is not possible to perform table operations     such as searching, updating, deleting and joining. For more     information about table operations, see Managing Stored Data via     Streams and Managing Stored     Data via REST APIs .</li> <li><code>@index</code> : This specifies     the fields that must be indexed at the database level. You can     specify multiple values as a comma separated list.</li> </ul>"},{"location":"setup/defining-Tables-for-Physical-Stores/#example_1","title":"Example","text":"<pre><code>    @Store(type=\u201dhbase\u201d, table.name=\u201dSomeTestTable\u201d, column.family.name=\u201dSomeCF\u201d, hbase.zookeeper.quorum=\u201dlocalhost\u201d, hbase.zookeeper.property.clientPort=\u201d2181\u201d)\n@PrimaryKey(symbol)\ndefine table FooTable (symbol string, price float, volume long);\n</code></pre>"},{"location":"setup/defining-Tables-for-Physical-Stores/#apache-solr","title":"Apache Solr","text":""},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax_2","title":"Query syntax","text":"<p>The query syntax to define an SOLR table is as follows.</p> <pre><code>    @PrimaryKey(\"id\")\n@store(type=\u201csolr\u201d, url=&lt;solr-cloud-zookeeper-url&gt;, collection=&lt;solr-collection-name&gt;, base.config=&lt;config-name&gt;, shards=&lt;no-of-shards&gt;, replicas=&lt;no-of-replicas&gt;, schema=&lt;schema-definition&gt;, commit.async=true|false)\ndefine table Footable (time long, date string);\n</code></pre>"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters_2","title":"Parameters","text":"<p>The following parameters are configured in an SOLR table definition.</p> Parameter Description Required/Optional <code>             collection            </code> The name of the solr collection/table. Required <code>             url            </code> The URL of the zookeeper master of SOLR cloud. Required <p><code>              base.config             </code></p> The default configuration that should be used for the SOLR schema. Optional <code>             shards            </code> The number of shards. Optional <code>             replica            </code> The number of replica. Optional <code>             schema            </code> The SOLR schema definition. Optional <code>             commit.async            </code> <p>If this is set to <code>              true             </code> , the results all the operations carried out for the table (described below) are applied at a specified time interval. If this is set to <code>              false             </code> , the results of the operations are applied soon after they are performed with the vent arrival.</p> <p>e.g., If this is set to <code>              false             </code> , an event selected to be inserted into the table is inserted as soon as it arrives to the event stream.</p> N/A"},{"location":"setup/defining-Tables-for-Physical-Stores/#example_2","title":"Example","text":"<p>This query defines an SOLR table named <code>FooTable</code> in which a schema that consists of the two attributes <code>time</code> (of <code>long</code> type) and date (of the <code>string</code> type) is maintained. The values for both attributes are stored.</p> <pre><code>\"shards='2', replicas='2', schema='time long stored, date string stored', commit.async='true') \" +\n\"define table Footable(time long, date string);\n</code></pre> <pre><code>## MongoDB\n\n#### Query syntax\n\nThe following is the query syntax to define a MongoDB event table.</code></pre> <pre><code>@Store(type=\"mongodb\", mongodb.uri=\"&lt;MONGODB CONNECTION URI&gt;\")\n@PrimaryKey(\"ATTRIBUTE_NAME\")\n@IndexBy(\"&lt;ATTRIBUTE_NAME&gt; &lt;SORTING ORDER&gt; &lt;INDEX OPTIONS&gt;\")\ndefine table &lt;TABLE_NME&gt; (&lt;ATTRIBUTE1_NAME&gt; &lt;ATTRIBUTE1_TYPE&gt;, &lt;ATTRIBUTE2_NAME&gt; &lt;ATTRIBUTE2_TYPE&gt;, &lt;ATTRIBUTE3_NAME&gt; &lt;ATTRIBUTE3_TYPE&gt;, ...);\n</code></pre> <pre><code>The `         mongodb.uri        ` parameter specifies the URI via which\nMongoDB user store is accessed.\n\nIn addition, the following annotations are used in the MongoDB\ndefinition.\n\n-   `                     @primary                   ` : This specifies\n    a list of comma-separated values to be treated as unique fields in\n    the table. Each record in the table must have a unique combination\n    of values for the fields specified here.\n-   `                     @index                   ` : This specifies\n    the fields that must be indexed at the database level. You can\n    specify multiple values as a comma separated list.\n\n#### Example\n\nThe following query defines a MongoDB table named\n`         FooTable        ` with the `         symbol        ` ,\n`         price        ` , and `         volume        ` attributes. The\n`         symbol        ` attribute is considered the primary key and it\nis also indexed.</code></pre> <pre><code>@Store(type=\"mongodb\", mongodb.uri=\"mongodb://admin:admin@localhost:27017/Foo?ssl=true\")\n@PrimaryKey(\"symbol\")\n@IndexBy(\"symbol 1 {background:true}\")\ndefine table FooTable (symbol string, price float, volume long);\n</code></pre>"},{"location":"setup/deploying-si-as-a-scalable-cluster/","title":"Scalable Deployment","text":"<p>Scalable high availability deployment predominantly focuses on scaling the system according to the load or the TPS of the system. This is achieved with the help of horizontal scalability.</p> <p>WSO2 Streaming Integrator uses Siddhi as the streaming language. Siddhi allows you to write Siddhi logic in a stateless way as well as a stateful way.</p> <p>Stateless operations include filters, database operations etc., and stateful operations include window operations, aggregations etc., that keep data in memory to carry out calculations.</p> <p>The deployment options for a scalable streaming integrator depends on the statelessness and the statefulness of Siddhi applications.</p> <p>The following topics provide detailed descriptions of two approaches.</p> <p>System Requirements</p> <p>For system requirements for this deployment, see Installing the Streaming Integrator in a Virtual Machine.</p>"},{"location":"setup/deploying-si-as-a-scalable-cluster/#stateless-scalable-high-availability-ha-deployment","title":"Stateless scalable high availability (HA) deployment","text":"<p>In stateless scenarios, the system does not work with any in-memory state. Thus in order to scale, you can keep adding Streaming Integrator servers to the system and front them with a load balancer that publishes the events in round robin manner.</p> <p>This is depicted in the diagram below.</p> <p></p>"},{"location":"setup/deploying-si-as-a-scalable-cluster/#stateful-scalable-high-availability-ha-deployment","title":"Stateful scalable high availability (HA) deployment","text":"<p>As described before, stateful operations keep state data in memory. Therefore, in order to scale such a system, you need to process specific data on the same node without processing same-state data in different servers. You can achieve this via data partitioning where one bucket of partitioned data is processed only in one specific server.</p> <p>Info</p> <p>In order to scale stateful operations, it is required to have some partitioning attribute available enabling the partitioned data to be processed independently.</p> <p>The following is a high level diagram of event flow and components to achieve scalable, stateful, and highly available deployment.</p> <p></p> <p>The following sections describe each component in detail and how to configure them with WSO2 Streaming Integrator.</p>"},{"location":"setup/deploying-si-as-a-scalable-cluster/#partitioning-layer","title":"Partitioning layer","text":"<p>As shown in the above diagram, first you need to have a partitioning layer. Here, you are using an SI server to achieve it. The function of this layer is to consume events from output sources and then partition the events based on a partitioning condition.</p> <p>In order to partition you can leverage on the Distributed sink extension in WSO2 Streaming Integrator. The following is a sample Siddhi application syntax that defines a stream. It shows how the distributed sink can be applied to partition data. In this example, data is partitioned from tenant domain. For more information, see Siddhi Query Guide - Distributed Sink.</p> <p>Note</p> <p>In the following example, the definition of the <code>Request</code> stream(Request stream) only includes the logicto send events out for load balancers via http for each partition. In addition, there should be a logic to consume eventsfrom outside and direct them to the <code>Request</code> stream.</p> <p>e.g.,</p> <pre><code>// Stream defined with distributed sink with partitioned stratergy      \n@Sink(type = 'http',\n          @map(type='json'),\n          @distribution(strategy='partitioned', partitionKey='userTenantDomain',\n          @destination(publisher.url='http://Ip1:8009/Request'),\n          @destination(publisher.url='http://Ip2:8009/Request')))\ndefine stream Request (meta_clientType string,\n     applicationConsumerKey string,\n     applicationName string,\n     applicationId string,\n     applicationOwner string,\n     apiContext string,\n     apiName string,\n     apiVersion string,\n     apiCreator string,,\n     apiTier string,\n     apiHostname string,\n     username string,\n     userTenantDomain string,,\n     requestTimestamp long,\n     throttledOut bool,\n     responseTime long,\n     backendTime long);</code></pre> <p>According to above distributed sink configuration, events that arrive at the <code>Request</code> stream are partitioned based on the <code>userTenantDomain</code> attribute. Therefore, if there are two tenant domain values <code>fooDomain</code> and <code>barDomain</code>, then events of <code>fooDomain</code> can be published to <code>Ip1</code>, and the events of <code>barDomain</code> can be published to <code>Ip2</code>. The distributed sink ensures that the unique partitioned events are not distributed across the cluster. Here, <code>Ip1</code> and <code>Ip2</code> represent the load balancer IPs. The reason for using load balancers is because the stateful layer also contains two SI servers to handle the high availability. Therefore, you need a load balancer to direct the traffic in a failover manner.</p> <p>According to the above diagram, there are four partitions. Therefore, four load balancers are used.</p> <p>You need the high availability in the partitioning layer. Therefore, you can use two WSO2 Streaming Integrator servers (minimum) as depicted in the diagram.</p>"},{"location":"setup/deploying-si-as-a-scalable-cluster/#stateful-layer","title":"Stateful Layer","text":"<p>The function of this layer is to consume events according to each partition and carry out the rest of the stateful operations. When you have partitioned the date, you can seamlessly handle the scalability of the system as well as address the requirement for high availability of the system. Thus for each partition we can deploy the system as mentioned in two node minimum high available deployment section. Basically for each partition or partitions there will be a separate cluster  of two SI nodes. So if active node fails for a particular partition the other node in the cluster will carry out the  work.</p> <p>In order to configure the stateful layer you can follow the minimum high availability deployment guide. The only  difference in configurations regarding this layer would be , as mentioned since we maintain separate clusters for each partition the Cluster Configuration group id should be different for each cluster.</p> <p>Sample cluster configuration can be as below  :</p> <pre><code>  - cluster.config:\n      enabled: true\n      groupId:  group 1\n      coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy\n      strategyConfig:\n        datasource: WSO2_CLUSTER_DB\n        heartbeatInterval: 5000\n        heartbeatMaxRetry: 5\n        eventPollingInterval: 5000</code></pre>"},{"location":"setup/deploying-si-as-a-single-deployment/","title":"Single Node Deployment","text":"<p>You can deploy the WSO2 Streaming Integrator as a single node deployment to achieve most of the use cases that commonly arise in the streaming integration world. The other deployment options, namely Minimum High Availability (HA) Deployment and Scalable High Available(HA) Deployment are mainly introduced to handle high availability, scalability, and resiliency. However, single node deployment too allows you to achieve resilient deployment as explained in the Resilient Deployment subsection.</p> <p>System Requirements</p> <p>For system requirements for this deployment, see Installing the Streaming Integrator in a Virtual Machine.</p>"},{"location":"setup/deploying-si-as-a-single-deployment/#resilient-deployment","title":"Resilient deployment","text":"<p>Resiliency guarantees the ability to withstand or recover from any system failure and carry out the process without  loosing any data. Streaming integrator has the capability to achieve the above via a broker that can replay data from a certain checkpoint. Kafka is one such broker that you can configure with WSO2 Streaming Integrator to achieve this. The only additional configuration that you need to do in WSO2 Streaming Integrator is state persistence. For detailed instructions, see Configuring Database and File System State Persistence.</p>"},{"location":"setup/deploying-si-as-a-single-deployment/#_1","title":"Single Deployment","text":"<p>If the single Streaming Integrator node fails to receive incoming events and if you have configured state persistence, the single node is able to retrieve the latest snapshot from the database and request the broker to send the events that it was unable to process due to the failure</p> <p>The only disadvantage of this approach is that you need to ensure that there is a reliable mechanism to restart the server once it fails.</p>"},{"location":"setup/deploying-si-as-an-active-active-deployment/","title":"Active-Active Deployment","text":"<p>The recommended deployment for WSO2 Streaming Integrator (SI) is the Minimum HA Deployment. However, that deployment pattern involves using only two nodes and it is not scalable beyond that. If you want to configure WSO2 SI as a scalable deployment, you can use the Active-Active deployment pattern. This section provides an overview of the Active-Active deployment pattern and instructions to configure it.</p>"},{"location":"setup/deploying-si-as-an-active-active-deployment/#overview","title":"Overview","text":"<p>The above diagram represents a deployment where you are not limited to two nodes. You can scale the event processing horizontally by adding more SI  nodes to the deployment. In this deployment, it is recommended to configure the client application to publish events to multiple SI nodes in a Round Robin manner to ensure better fault tolerance. The publishing of events can be carried out by one or more clients.</p> <p>In order to perform aggregations in a distributed manner and achieve the scalability, this setup uses distributed aggregations.</p> <p>Distributed aggregations partially process aggregations in different nodes. This allows you to assign one node to process only a part of an aggregation (regional scaling, etc.). In order to do this all the aggregations must have a physical database and must be linked to the same database.</p> <p>Partitioning aggregations can be enabled at aggregation level and also at a global level. To enable it at the global level, add the following section with the <code>@PartitionById</code> annotation set to <code>true</code> in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>    siddhi:\n     properties:\n       partitionById: true\n       shardId: wso2-si\n</code></pre> <p>If you want to enable for a specific aggregation then the <code>@PartitionById</code> annotation must be added before the aggregation definition as shown in the example below.</p> <p>e.g., To understand how an active-active cluster processes aggregations when aggregations are partitioned and assigned to different nodes, consider the following Siddhi query.  To learn more about Siddhi queries, see Siddhi Query Guide.</p> <pre><code>define stream TradeStream (symbol string, price double, quantity long, ;timestamp long);\n@store(type='rdbms',jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\")\n@PartitionById(enable='true')\ndefine aggregation TradeAggregation\nfrom TradeStream\nselect symbol, avg(price) as avgPrice, sum(quantity) as total\ngroup by symbol\naggregate by timestamp every sec ... year\n</code></pre> <p>This query captures the information relating to a trade. Each transaction represents an event, and the information captured includes the symbol of the product, the price at which it is sold, the quantity sold during the transaction, and the timestamp of the transaction. Each node stores this information in the <code>TEST_DB</code> data store defined in the <code>&lt;SI_WORKER_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <p>Now let's assume that the following input events were generated for the two nodes during a specific hour.</p> <p>Node 1</p> Event symbol price quantity 1 <code>wso2</code> <code>100</code> <code>10</code> 2 <code>wso2</code> <code>100</code> <code>20</code> <p>Node 2</p> Event symbol price quantity 1 <code>wso2</code> <code>100</code> <code>10</code> 2 <code>wso2</code> <code>100</code> <code>30</code> <p>Here, node 1 calculates an hourly total of 30, and node 2 calculates an hourly total of 40. When you retrieve the total for this hour via a retrieval query, the result is 70.</p> <p>You can find the steps to enable aggregation partitioning within the next subsection.</p>"},{"location":"setup/deploying-si-as-an-active-active-deployment/#configuring-an-active-active-cluster","title":"Configuring an active-active cluster","text":"<p>To configure the Streaming Integrator nodes and deploy them as an active-active cluster, edit the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file as follows:</p> <p>Before you begin:</p> <ul> <li>Download two binary packs of the WSO2 Streaming Integrator.</li> <li>Set up a working RDBMS instance to be used by the WSO2 Streaming Integrator cluster.</li> </ul> <ol> <li> <p>For each node, enter a unique ID for the <code>id</code> property under the <code>wso2.carbon</code> section. This is used to identify each node within a cluster. For example, you can add IDs as shown below.</p> <ul> <li> <p>For node 1:     <pre><code>wso2.carbon:\n  id: wso2-si-1\n</code></pre></p> </li> <li> <p>For node 2:     <pre><code>wso2.carbon:\n  id: wso2-si-2\n</code></pre></p> </li> </ul> </li> <li> <p>Enable partitioning aggregations for each node, and assign a unique shard ID for each node. To do this, set the <code>partitionById</code> and <code>shardId</code> parameters as Siddhi properties as shown below.</p> <p>Info</p> <p>Assigning shard IDs to nodes allows the system to identify each unique node when assigning parts of the aggregation. If the shard IDs are not assigned, system uses the unique node IDs (defined in step 1) for this purpose.</p> <ul> <li> <p>For node 1:     <pre><code>siddhi:\n  properties:\n    partitionById: true\n    shardId: wso2-si-1\n</code></pre></p> </li> <li> <p>For node 2:     <pre><code>siddhi:\n  properties:\n    partitionById: true\n    shardId: wso2-si-2\n</code></pre></p> </li> </ul> <p>Tip</p> <ul> <li>To maintain data consistency, do not change the shard IDs after the first configuration. </li> <li>When you enable the aggregation partitioning feature, a new column ID named <code>SHARD_ID</code> is introduced to the aggregation tables. Therefore, you need to do one of the following options after enabling this feature to avoid errors occuring due to the differences in the table schema.<ul> <li>Delete all the aggregation tables for <code>SECONDS</code>, <code>MINUTES</code>, <code>HOURS</code>, <code>DAYS</code>, <code>MONTHS</code>, <code>YEARS</code>. </li> <li>Edit the aggregation tables by adding a new column named SHARD_ID, and add that to the existing primary key list of the table.</li> </ul> </li> </ul> </li> <li> <p>Configure a database, and then update the default configuration for the <code>TEST_DB</code> data source with parameter values suitable for your requirements.</p> </li> </ol> <p>Warning</p> <p>As explained in above the events are processed in multiple active nodes. Eventhough this is usually a stateful operation, you can overcome the node-dependent calculations via distributed aggregation. This allows WSO2 SI to execute scripts that depend on incremental distributed aggregation. However, an active-active deployment can affect alerts because alerts also depend on some in-memory stateful operations such as windows. Due to this, alerts can be generated based on the events received by specific node. Thus the alerts are node-dependent, and you need to disable them to run scripts with distributed incremental aggregation.</p>"},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/","title":"Deploying the Streaming Integrator as a Minimum HA Cluster in AWS ECS","text":""},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#introduction","title":"Introduction","text":"<p>This section explains how to run WSO2 Streaming Integrator as a minimum HA (High Availability) cluster in AWS (Amazon Web Services) ECS(Elastic Container Service).</p> <p>The minimum HA cluster typically has two nodes where one node operates as the active node and the other as the passive node. Each node maintains a communication link with the other node as well as with the database.</p>"},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#assigning-the-active-and-passive-statuses-to-nodes","title":"Assigning the Active and Passive statuses to nodes","text":"<p>When a node is started in the minimum HA cluster mode, it checks the tables in the <code>WSO2_CLUSTER_DB</code> database. This check covers checking whether there are existing members in the cluster. If other nodes already exist as members of the cluster, it checks whether there are heartbeats from the existing member(s) for the last time interval that is of the same length as the specified heartbeat interval. If no heartbeat exists for the specified time interval, the node is added to the cluster as the active node. If not, it is added as the passive node.</p> <p>Once a node becomes the active node, it performs the following:</p> <ul> <li>Inserts its own details in the <code>WSO2_CLUSTER_DB</code> database or updates them if they already exist. The details include <code>nodeId</code>, <code>clusterGroup</code>, <code>propertyMap</code>, and <code>heartbeatTimestamp</code>.</li> <li>Periodically updates the appropriate table of the <code>WSO2_CLUSTER_DB</code> database with its heartbeat (timestamp).</li> <li>Starts the Siddhi applications in runtime and opens all the ports mentioned in the Siddhi applications.</li> <li>Starts the binary and thrift transports.</li> <li>Starts the REST endpoints.</li> <li>Once a new member (i.e., passive node) joins the cluster, the active node checks the <code>WSO2_CLUSTER_DB</code> database for the host and port of ther other member's event syncing server. Once this information is retrieved, it sends the input events received by the cluster to that event syncing server.</li> </ul>"},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#operating-the-nodes","title":"Operating the nodes","text":"<p>When the cluster is running with both the nodes, the active node sends each event received by the cluster to the passive node's event sync server with the event timestamp. It also persists the state (windows, sequences, patterns, aggregations, etc.,) in the database named <code>PERSISTENSE_DB</code>. Each time the state is persisted, the active node sends a control message to the passive node.</p> <p>The passive node saves the events sent to its event sync server in a queue. When it receives the control message from the active node, it trims the queue to keep only the latest events that were not used by the active node to build the state of Siddhi applications at the time of persisting the state.</p>"},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#handling-the-failure-of-the-active-node","title":"Handling the failure of the active node","text":"<p>The passive node continuously monitors the heartbeat of the active node. If the active node fails, the passive node follows the process shown below to start functioning as the active node so that data is not lost due to node failure.</p> <p></p> <p>The following table explains the above steps.</p> Step Description 1. Start Siddhi Application Runtime This is done without opening any ports mentioned in Siddhi applications. This is because the Siddhi application statuses need to be restored to what they were before the node failure so that unprocessed events before the failure are not lost. 2. Restore State This is done by retrieving the states persisted in the <code>PERSISTENSE_DB</code> database. 3. Direct Events in Queue to Streams The unprocessed events that are currently in the queue of the event sync server are directed into the relevant streams of the Siddhi applications. 4. Open Ports, Binary &amp; Thrift Transports, and REST Endpoints Once the above steps are completed, the previously passive and now active node opens the ports, starts the thrift and binary transports, and opens the REST endpoints."},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#setting-up-the-si-cluster-in-aws-ecs","title":"Setting up the SI cluster in AWS ECS","text":"<p>Before you begin:</p> <p>In order to deploy WSO2 Streaming Integrator in AWS ECS, you need to complete the following requisites:  - create an account in AWS.  - Download and install AWS CLI Version 2.  - Download and install Docker. For instructions, see Docker Documentation.</p>"},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#step-1-store-si-docker-images-in-ecr","title":"Step 1: Store SI Docker images in ECR","text":"<ol> <li> <p>Generate an access key and a secret key.</p> <ol> <li> <p>Sign in to the IAM console via the AWS console. Under Access Management, click Users.</p> <p></p> <p>Then click Add User in the page that appears, and enter information as follows.</p> <ol> <li> <p>In the User name field, enter a valid email address. Under Access Type, select the Programmatic Access check box. Then click Next:Permissions.</p> <p></p> </li> <li> <p>Add the user you are creating to a relevant group. In this example, let's create a new group. To do this, click Create Group and open the CreateGroup dialog box. Then enter a name for the group, select the AdministratorAccess check box, and then click Create Group.     </p> <p>Click Next:Tags, and in the next page click Next:Review.</p> </li> <li> <p>Review the information you have entered for the user. If you want to change anything, go back to the relevant previous page. If not, click Create User to save the information and create the user.</p> <p>Once you create the user, you can get the access key and the secret key as shown below.</p> <p></p> </li> </ol> </li> </ol> </li> <li> <p>Issue the following command in the terminal to configure AWS.</p> <p><code>aws configure</code></p> <p>You are prompted for the <code>AWS Access Key ID</code>, <code>AWS Secret Access Key</code>, <code>Default region name</code>, and <code>Default output format</code>. For this example, you can enter the following values.</p> Requested Information Value <code>AWS Access Key ID</code> Enter the access key that you generated when you created the AWS user. <code>AWS Secret Access Key [None]</code> Enter the secret key that you generated when you created the AWS user. <code>Default region name</code> <code>us-east-2</code>. <code>Default output format</code> <code>json</code> </li> <li> <p>Create a repository in the ECR (Elastic Container Registry).</p> <ol> <li> <p>Access AWS ECR (Elastic Container Registry) via the AWS Console.</p> </li> <li> <p>To create a new repository, click Create a repository -&gt; Get Started. The Create Repository page opens.</p> </li> <li> <p>For this example, enter <code>wso2</code> as the repository name and click Create repository to create the repository.</p> <p>The repository is created as shown below.</p> <p></p> </li> <li> <p>To retrieve an authentication token and authenticate your Docker client to your registry, do the following:</p> <ol> <li> <p>In the Repositories page, click View push commands.</p> </li> <li> <p>Copy the command given under Retrieve an authentication token and authenticate your Docker client to your registry.. This command is similar to the following example.     <code>aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 627334729308.dkr.ecr.us-east-2.amazonaws.com/wso2</code></p> </li> <li> <p>Issue the command you copied in the AWS CLI. The following message appears in the CLI.</p> <p><code>Login Succeeded</code></p> </li> </ol> </li> </ol> </li> <li> <p>Build Docker images with SI configurations as follows:</p> <ol> <li> <p>Edit the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file as follows.</p> <ol> <li> <p>In the <code>state.persistence</code> section, update the following parameters.</p> Parameter Value <code>enabled</code> <code>true</code> <code>persistenceStore</code> <code>org.wso2.carbon.streaming.integrator.core.persistence.DBPersistenceStore</code> <code>datasource</code> <code>PERSISTENCE_DB</code> <code>table</code> <code>PERSISTENCE_TABLE</code> Click here to view the updated <code>state.persistence</code> section.<pre><code>state.persistence:\n  enabled: true\n  intervalInMin: 1\n  revisionsToKeep: 2\n  persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.DBPersistenceStore\n  config:\n    datasource: PERSISTENCE_DB\n    table: PERSISTENCE_TABLE\n</code></pre> </li> <li> <p>In the <code>wso2.datasources:</code> section, add two data sources named <code>PERSISTENCE_DB</code> and <code>WSO2_CLUSTER_DB</code> as follows.</p> <pre><code>    - name: PERSISTENCE_DB\n      description: The MySQL datasource used for persistence\n      jndiConfig:\n        name: jdbc/PERSISTENCE_DB\n      definition:\n        type: RDBMS\n        configuration:\n          jdbcUrl: 'jdbc:mysql://wso2db.cxtsxcdgcayr.ap-south-1.rds.amazonaws.com:3306/persistencedb?useSSL=false'\n          username: root\n          password: rootroot\n          driverClassName: com.mysql.jdbc.Driver\n          maxPoolSize: 50\n          idleTimeout: 60000\n          connectionTestQuery: SELECT 1\n          validationTimeout: 30000\n          isAutoCommit: false\n</code></pre> <pre><code>    - name: WSO2_CLUSTER_DB\n      description: The MySQL datasource used for persistence\n      jndiConfig:\n        name: jdbc/WSO2_CLUSTER_DB\n      definition:\n        type: RDBMS\n        configuration:\n          jdbcUrl: 'jdbc:mysql://wso2db.cxtsxcdgcayr.ap-south-1.rds.amazonaws.com:3306/clusterdb?useSSL=false'\n          username: root\n          password: rootroot\n          driverClassName: com.mysql.jdbc.Driver\n          maxPoolSize: 50\n          idleTimeout: 60000\n          connectionTestQuery: SELECT 1\n          validationTimeout: 30000\n          isAutoCommit: false\n</code></pre> </li> <li> <p>To enable clustering and update strategy configuration values, update the <code>cluster.config</code> section as follows:</p> Parameter Value <code>enabled</code> <code>true</code> <code>groupId</code> <code>si</code> <code>coordinationStrategyClass</code> <code>org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy</code> <code>datasource</code> <code>WSO2_CLUSTER_DB</code> <code>heartbeatInterval</code> <code>5000</code> <code>heartbeatMaxRetry</code> <code>5</code> <code>eventPollingInterval</code> <code>5000</code> Click here to view the updated <code>cluster.config:</code> section.<pre><code>    cluster.config:\n      enabled: true\n      groupId:  si\n      coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy\n      strategyConfig:\n        datasource: WSO2_CLUSTER_DB\n        heartbeatInterval: 5000\n        heartbeatMaxRetry: 5\n        eventPollingInterval: 5000\n</code></pre> </li> <li> <p>Uncomment the <code>deployment.config:</code> section and edit the parameters as follows:</p> </li> </ol> Parameter Value <code>type</code> <code>ha</code> <code>host</code> <code>localhost</code> <code>advertisedHost</code> <code>localhost</code> </li> <li> <p>Clone the <code>docker-ei</code> repository by issuing the following command.</p> <p><code>git clone https://github.com/wso2/docker-ei</code></p> </li> <li> <p>In the <code>docker-ei/alpine/streaming-integrator/docker-entrypoint.sh</code> file, enter the following code before the <code># start WSO2 server</code> comment.</p> <p>Info</p> <p>In this example, it is assumed that you are using <code>alpine</code>. You can also use <code>centos</code> or <code>ubuntu</code>.</p> <pre><code>IP=$(ifconfig eth0 | grep \"inet addr\" | cut -d ':' -f 2 | cut -d ' ' -f 1)\nDEPLOYMENT_YAML=${WSO2_SERVER_HOME}/conf/server/deployment.yaml\necho \"$IP\"\nsed -i \"s/localhost/$IP/\" \"$DEPLOYMENT_YAML\"\n</code></pre> <p>This gets the IP address of the host machine and replaces it in the HA configuration in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> </li> <li> <p>Make a copy of the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file you edited and paste it in <code>docker-ei/dockerfiles/alpine/streaming-integrator</code> directory.</p> </li> <li> <p>In the <code>docker-ei/dockerfiles/alpine/streaming-integrator/deployment.yaml</code> file, under <code>wso2.carbon:</code>, change value for the <code>id</code> parameter to <code>wso2-si-1</code>.</p> <p><code>id: wso2-si-1</code></p> </li> <li> <p>To build the Docker image for node 1, navigate to the <code>docker-ei/dockerfiles/alpine/streaming-integrator</code> directory and issue the following command.</p> <p><code>docker build -t wso2si:1.0.0-alpine-ha1 .</code></p> <p>Once the build is successfully executed, a message similar to the following appears in the CLI.</p> <p></p> </li> <li> <p>In the <code>docker-ei/dockerfiles/alpine/streaming-integrator/deployment.yaml</code> file, under <code>wso2.carbon</code>, change value for the <code>id</code> parameter to <code>wso2-si-2</code>.</p> <p><code>id: wso2-si-2</code></p> <p>This is because, now you are using the same repository to build the Docker image for node 2.</p> </li> <li> <p>To build the Docker image for node 2, navigate to the <code>docker-ei/dockerfiles/alpine/streaming-integrator</code> directory and issue the following command.</p> <p><code>docker build -t wso2si:1.0.0-alpine-ha2 .</code></p> <p>Once the build is successfully executed, a message similar to the following appears in the CLI.</p> <p></p> </li> <li> <p>To tag the built images, issue the following commands.</p> <p><code>docker tag wso2si:1.0.0-alpine-ha2 &lt;AWS_ACCOUNT_NUMBER&gt;.dkr.ecr.us-east-2.amazonaws.com/wso2:ha1</code></p> <p><code>docker tag wso2si:1.0.0-alpine-ha1 &lt;AWS_ACCOUNT_NUMBER&gt;.dkr.ecr.us-east-2.amazonaws.com/wso2:ha2</code></p> </li> <li> <p>To push the images, issue the following commands.</p> <p><code>docker push &lt;AWS_ACCOUNT_NUMBER&gt;.dkr.ecr.us-east-2.amazonaws.com/wso2:ha1</code></p> <p><code>docker push &lt;AWS_ACCOUNT_NUMBER&gt;.dkr.ecr.us-east-2.amazonaws.com/wso2:ha2</code></p> </li> </ol> </li> </ol>"},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#step-2-create-rds-for-persisting-and-clustering-requirements","title":"STEP 2: Create RDS for persisting and clustering requirements","text":"<p>To create a Amazon RDS (Relational Database Service) for the purpose of persisting data handled by the cluster, follow the procedure below:</p> <ol> <li> <p>Access Amazon RDS via the AWS console.</p> </li> <li> <p>To add a parameter group and change the value for the <code>max_connections</code> parameter for it, follow the procedure below:</p> <ol> <li> <p>In the left navigator, click Parameter groups. Then, in the Parameter Groups page, click Create Parameter Group to create a new parameter group.</p> <p></p> <p>The Create parameter group page opens.</p> </li> <li> <p>Enter details in the Create parameter group page to create a new parameter group.</p> <p></p> <p>Then click Create. The newly created parameter group appears in the Parameter groups page as follows.</p> <p></p> </li> <li> <p>To edit the parameter group, click on it. Then search for the max_connections parameter, select it, and click Edit parameters. Change the value for the <code>max_connections</code> parameter to <code>300</code> and then click Save changes.</p> </li> </ol> </li> <li> <p>Create an RDS instance as follows:</p> <ol> <li> <p>In the left navigator, click Databases. Then in the Databases page, click Create database to open the Create Database page.</p> <p></p> </li> <li> <p>In the Create Database page, enter details as follows.</p> <ol> <li> <p>Select Standard Create.</p> </li> <li> <p>Under Engine Type, select MySQL.</p> </li> <li> <p>Under Templates, select Free Tier.</p> </li> <li> <p>Under Settings, enter details as instructed within the user interface.</p> <p></p> </li> <li> <p>Under Connectivity, expand the Additional connectivity configuration. Under Publicly accessible, select Yes. This allows you to connect and create the database, and then check on the database values later.</p> <p></p> </li> <li> <p>Expand the Additional Configurations section. In the DB parameter group field, select the parameter group that you previously created.</p> <p></p> </li> <li> <p>Click Create database. The database you created appears in the Databases page.</p> </li> </ol> </li> <li> <p>Enable public access to the database as follows:</p> <ol> <li> <p>In the Databases page, click on the wso2 database you created to view details of it in a separate page.</p> </li> <li> <p>In the Security section, click the VPC.</p> <p></p> </li> <li> <p>In the Security Groups page that opens, click on the relevant security group to view details of it in a separate page.</p> <p></p> </li> <li> <p>In the page with details of your security group, click Edit Inbound Rules.</p> <p></p> <p>This opens the Edit inbound rules page.</p> </li> <li> <p>In the Edit inbound rules page, add two rules as follows.</p> Type Source MYSQL/AURORA Select Custom for the Source field and then select 0.0.0.0/0 as the value. MYSQL/AURORA Select Custom for the Source field and then select ::/0 as the value. <p>Then click Save rules.</p> </li> </ol> </li> </ol> </li> </ol>"},{"location":"setup/deploying-si-as-minimum-ha-cluster-in-aws-ecs/#step-3-set-up-ecs-cluster-tasks-and-services","title":"Step 3: Set up ECS cluster, tasks and services","text":"<ol> <li> <p>To create a VPC (Virtual Private Cloud), a subnet, and a security group, follow the procedure below:</p> <ol> <li> <p>Access the VPC Dashboard via the AWS Console.</p> </li> <li> <p>Click Launch VPC Wizard.</p> </li> <li> <p>In Step 1: Select a VPC Configuration, VPC with a Single Public Subnet is selected by default in the left pane. Click Select for it.</p> <p></p> </li> <li> <p>In Step 2: VPC with a Single Public Subnet, enter <code>si-ha-vpc</code> in the VPC name field. Then click Create VPC.</p> <p></p> </li> </ol> <p>After successfully creating the VPC, you can view it in the Your VPCs page as follows:</p> <p></p> <p>To view the public subnet created, click Subnets in the left navigator. The subnet connected to the VPC is displayed as shown below.</p> <p></p> <p>To view the security group of the VPC, click Security Groups in the left navigator. The security group is displayed as follows.</p> <p></p> <p>Info</p> <p>When you select the security group and view details for it at the bottom of the Security Groups page, note that all incoming traffic is currently enabled for now in the Inbound Rules tab.</p> <p></p> </li> <li> <p>Set up the cluster as follows:</p> <p>To set up the cluster, follow the procedure below:</p> <ol> <li> <p>Access Amazon ECS via the AWS Console.</p> </li> <li> <p>In the left navigator, click Clusters. Then click Create Cluster in the Clusters page.</p> <p></p> </li> <li> <p>In the Create Cluster wizard that opens, select EC2 Linux + Networking and then click Next step.     </p> </li> <li> <p>In Step 2: Configure Cluster of the Create Cluster wizard, enter information as follows:</p> <p></p> <ol> <li> <p>In the Cluster name field, enter a name for your cluster. In this example, let's enter <code>si-ha-cluster</code> as the name.</p> </li> <li> <p>Select the Create an empty cluster check box.</p> </li> <li> <p>Click Create to create the cluster.</p> </li> </ol> </li> </ol> </li> <li> <p>Create a task for Streaming Integrator node 1 as follows:</p> <ol> <li> <p>In the left navigator of Amazon ECS, click Task Definitions. Then in the Task Definitions window, click Create new Task Definition.</p> <p></p> </li> <li> <p>In the Select launch type compatibility page, click FARGATE and then click Next step.</p> <p></p> </li> <li> <p>In Step 2: Configure task and container definitions, enter information in the Create new Task Definition wizard as follows:</p> <ol> <li> <p>In the Task Definition Name field, enter <code>ha-node-1-task</code> as the name of the task definition.</p> </li> <li> <p>In the Task Size section, enter values as follows:</p> Field Value Task memory (GB) <code>0.5GB</code> Task CPU (vCPU) <code>0.25vCPU</code> </li> <li> <p>In the Container Definitions section, click Add container. Then enter information as follows:</p> <ol> <li> <p>In the Container name field, enter <code>node-1-ha-container</code> as the name of the container.</p> </li> <li> <p>In the Image field, enter the image URI.</p> <p>Tip</p> <p>To get the image URI, follow the steps below: 1. Access Amazon ECR via the AWS Console. 2. In the left navigator, click Repositories to open the Repositories window. 3. Click on your repository (which is <code>wso2</code> in this example). The available docker images are displayed in the Images window.          3. In the Port Mappings section, add the following ports. </p> Port Protocol 9893 tcp 9090 tcp 9711 tcp 9611 tcp 7711 tcp 7611 tcp 7443 tcp 7070 tcp 9443 tcp 8006 tcp <p>To add each port, click Add Port Mapping.                        </p> </li> <li> <p>Click Add.</p> </li> <li> <p>Click Create in the Configure task and container definitions page to create the task.</p> </li> </ol> </li> </ol> </li> </ol> </li> <li> <p>Create a task for Streaming Integrator node 2 as follows:</p> <ol> <li> <p>In the left navigator of Amazon ECS, click Task Definitions. Then in the Task Definitions window, click Create new Task Definition.</p> <p></p> </li> <li> <p>In the Select launch type compatibility page, click FARGATE and then click Next step.</p> <p></p> </li> <li> <p>In Step 2: Configure task and container definitions, enter information in the Create new Task Definition wizard as follows:</p> <ol> <li> <p>In the Task Definition Name field, enter <code>ha-node-2-task</code> as the name of the task definition.</p> </li> <li> <p>In the Task Size section, enter values as follows:</p> Field Value Task memory (GB) <code>0.5GB</code> Task CPU (vCPU) <code>0.25vCPU</code> </li> <li> <p>In the Container Definitions section, click Add container. Then enter information as follows:</p> <ol> <li> <p>In the Container name field, enter <code>node-2-ha-container</code> as the name of the container.</p> </li> <li> <p>In the Image field, enter the image URI.</p> </li> <li> <p>In the Port Mappings section, add the following ports. </p> </li> </ol> Port Protocol 9893 tcp 9090 tcp 9711 tcp 9611 tcp 7711 tcp 7611 tcp 7443 tcp 7070 tcp 9443 tcp 8006 tcp <p>To add each port, click Add Port Mapping.                        </p> <ol> <li> <p>Click Add.</p> </li> <li> <p>Click Create in the Configure task and container definitions page to create the task.</p> </li> </ol> </li> </ol> </li> </ol> </li> <li> <p>Create a service for node 1 using the <code>ha-node1-task</code> task as follows:</p> <ol> <li> <p>In Amazon ECS, click Clusters in the left navigator. Then click on your cluster.</p> <p></p> <p>This opens the Create Service wizard.</p> </li> <li> <p>In the Step 1: Configure service page, select/enter information as follows:</p> <p></p> Field Value Launch Type FARGATE Task Definition ha-node-1-task Service Name <code>ha-node1-service</code> Number of tasks <code>1</code> <p>Then click Next step.</p> </li> <li> <p>In the Step 2: Configure network, select/enter information as follows:</p> <p></p> <ol> <li> <p>In the Cluster VPC field, select the VPC that you previously created.</p> </li> <li> <p>In the Subnets field, select the subnet that you previously created for your VPC.</p> </li> <li> <p>Click Edit to select the security group. </p> <p>When you click Edit, the Configure security groups dialog box opens. Here, select the Select existing security group option and then select the security group that you previously created and connected to your VPC. Then click Save to save he information you entered in this dialog box.</p> <p></p> <p>Once you are back in the Step 2: Configure network page of the Create Service wizard, click Next step.</p> </li> </ol> </li> <li> <p>Skip Step 3: Set Auto Scaling (optional) by clicking Next step in the page without making any changes. In Step 4: Review, check the information, and click Create Service. </p> <p>Once the service is successfully created, click View Service to view it. You can also access it by clicking Clusters in the left navigator, and then clicking si-ha-cluster -&gt; Services tab -&gt; ha-node1-service.</p> </li> <li> <p>To view the running service and node logs, follow substeps below: </p> <ol> <li> <p>Click Clusters in the left navigator. Then click si-ha-cluster -&gt; Services tab -&gt; ha-node1-service to view the <code>ha-node1-service</code> service you created.</p> </li> <li> <p>Click on the Tasks tab. The task is displayed as shown below.</p> <p></p> </li> <li> <p>Click on the task to view it in a separate page. In that page, click on the View logs in CloudWatch. The following logs should be available to indicate that the node is started in the Active mode and it is persisting events.</p> <pre><code>    [2020-03-20 20:18:48,283]  INFO {org.wso2.carbon.streaming.integrator.core.ha.HAManager} - HA Deployment: Starting up as Active Node\n    [2020-03-20 20:18:52,261]  INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 44.893 sec\n    [2020-03-20 20:19:46,595]  INFO {org.wso2.carbon.streaming.integrator.core.persistence.PersistenceManager} - Siddhi apps persisted successfully\n</code></pre> </li> </ol> </li> </ol> </li> <li> <p>Create a service for node 2 using the <code>ha-node2-task</code> task by following the same procedure you followed in the previous step (i.e., step 5) to create a service for node 1. However, make sure that the task definition is <code>ha-node2-task</code>. The service name can be <code>ha-node2-service</code>.</p> <p></p> <p>When you view logs in CloudWatch for node 2, the following is displayed.</p> <pre><code>    [2020-03-20 20:38:14,390]  INFO {org.wso2.carbon.streaming.integrator.core.ha.HAManager} - HA Deployment: Starting up as Passive Node\n    [2020-03-20 20:38:18,287]  INFO {org.wso2.carbon.kernel.internal.CarbonStartupHandler} - WSO2 Streaming Integrator started in 46.604 sec\n</code></pre> </li> <li> <p>To check whether the cluster is working, check logs for node 1 as well as the error trace nodes.</p> <p>Logs similar to the following should be printed for node 1.</p> <pre><code>    [2020-03-20 20:38:15,409]  INFO {org.wso2.carbon.streaming.integrator.core.ha.HAEventListener} - memberAdded event received for node id : wso2-si-2\n    [2020-03-20 20:38:15,455]  INFO {org.wso2.carbon.streaming.integrator.core.ha.HAEventListener} - Active node retrieved node details of passive node\n</code></pre> </li> </ol> <p>Also, the following log should be printed without errors by the Error traces.</p> <pre><code> [2020-03-20 20:38:46,587]  INFO {org.wso2.carbon.streaming.integrator.core.persistence.PersistenceManager} - Siddhi apps persisted successfully\n</code></pre>"},{"location":"setup/deploying-si-as-minimum-ha-cluster/","title":"Minimum High Availability (HA) Deployment","text":"<p>The minimum high availability deployment mainly focuses on providing high availability that ensures the prevention of data loss if the system suffers a failure due to one or more unforeseeable reasons. One of the main adavantages of this deployment pattern is that it uses minimum amount of infrastructure resources possible. Thus deployment pattern is run with only two Streaming integration servers.</p> <p>In the minimum HA setup, one node is assigned as the active node while the other node is assigned as the passive node. Only the active node processes the incoming events and publishes the outgoing events. Internally, the active node  publishes the events to the passive node, but the passive node does not process or send any events outside as mentioned  earlier. In a scenario where the active node fails, the passive node is activated. Then the passive node starts receiving events and then publishing them from where the active node left off. Once the terminated (previously active) node restarts , it operates in the passive state. In the passive node, sources are in an inactive mode where they do not receive events into the system. </p> <p>Info</p> <p>In the passive node, databridge ports and Siddhi Store Query API endpoint are closed, but the admin API are accessible.</p> <p>For a two-node minimum HA cluster to work, only the active node should receive events. By design, you can only send  events to the active node. To achieve this, you can use a load balancing mechanism that sends events in a failover manner as depicted in the diagram below.</p> <p></p> <p>Before you begin:</p> <p>Before configuring a minimum HA cluster, you need to complete the following prerequisites:   - For each WSO2 SI instance, you need a CPU with four cores, and a total memory of 4GB. For more information, see Installing the Streaming Integrator in a Virtual Machine.    - Download and install two binary packs of WSO2 SI..    - Download, install and start a working RDBMS instance to be used for clustering the two nodes.    - Download the MySQL connector from here.     Extract and find the <code>mysql-connector-java-5.*.*-bin.jar</code>, and place it in the <code>&lt;SI_HOME&gt;/lib</code> directory of both nodes.    - In order to retrieve the state of the Siddhi Applications deployed in the system (in case of a scenario where both the     nodes fail), enable state persistence for both the nodes by specifying the same datasource/file location.     For detailed instructions, see Configuring Database and File System State Persistence.    - A client-side data publishing mechanism (such as a load balancer) that works in a failover manner must be available     to publish events to one of the available nodes (i.e., to the active node).</p>"},{"location":"setup/deploying-si-as-minimum-ha-cluster/#configuring-a-minimum-ha-cluster","title":"Configuring a minimum HA cluster","text":"<p>There are three main configurations that are required to setup a minimum HA cluster. They are as follows:</p> <ul> <li> <p>Cluster Configuration</p> </li> <li> <p>Persistent configuration</p> </li> <li> <p>HA configuration</p> </li> </ul> <p>Note</p> <ul> <li>The configurations given below need to be done in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file for both   the WSO2 SI nodes in the cluster.</li> <li>If you need to run both SI instances in the same host, make sure that you do a port offset to change the default    ports in one of the hosts. For more information about the default ports, see Configuring Default Ports.</li> </ul> <p>To configure the HA cluster, follow the steps below:</p> <ol> <li> <p>For each node, enter a unique ID for the <code>id</code> property under the <code>wso2.carbon</code> section (e.g., id: wso2-si). This is used to identify each node within a cluster.</p> </li> <li> <p>To allow the two nodes to use same persistence storage, you need to configure RDBMS persistence configuration under <code>state.persistence</code>. The following is a configuration for db-based file persistence.</p> <p>Info</p> <p>This step covers persistent configuration. For this purpose, you can use MySQL, MSSQL, POSTGRES and Oracle database types. For more information about the supported database types, see Configuring Data Sources.</p> <pre><code> - state.persistence:\n     enabled: true\n     intervalInMin: 1\n     revisionsToKeep: 2\n     persistenceStore: org.wso2.carbon.streaming.integrator.core.persistence.DBPersistenceStore\n     config:\n       datasource: PERSISTENCE_DB   # A datasource with this name should be defined in wso2.datasources namespace\n       table: PERSISTENCE_TABLE\n</code></pre> <p>The datasource named <code>PERSISTENCE_DB</code> in the above configuration can be defined in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code>  file under <code>wso2.datasources</code>. The following is a sample datasource configuration.</p> <pre><code>- name: PERSISTENCE_DB\n      description: The MySQL datasource used for persistence\n      jndiConfig:\n        name: jdbc/PERSISTENCE_DB\n      definition:\n        type: RDBMS\n        configuration:\n          jdbcUrl: 'jdbc:mysql://localhost:3306/PERSISTENCE_DB?useSSL=false'\n          username: root\n          password: root\n          driverClassName: com.mysql.jdbc.Driver\n          maxPoolSize: 50\n          idleTimeout: 60000\n          connectionTestQuery: SELECT 1\n          validationTimeout: 30000\n          isAutoCommit: false\n</code></pre> </li> <li> <p>To allow the two nodes in the cluster to coordinate effectively, configure carbon coordination by updating the <code>cluster.config</code> section of the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> as follows:</p> <p>Info</p> <p>This step covers cluster configuration.</p> <p>a. To enable the cluster mode, set the <code>enabled</code> property to <code>true</code>.</p> <pre><code>`enabled: true`</code></pre> <p>b. In order to cluster the two nodes together, enter the same ID as the group ID for both nodes (e.g., <code>groupId: group-1</code>).</p> <p>c. Enter the ID of the class that defines the coordination strategy for the cluster (e.g., <code>coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy</code>).</p> <p>d. In the <code>strategyConfig</code> section, enter information as follows:</p> <ul> <li> <p><code>datasource</code>:</p> <p>Enter the name of the configured datasource shared by the nodes in the cluster as shown in the example below. Data handled by the cluster are persisted here.</p> <p>The following is a sample datasource configuration for a MySQL datasource that should appear under the <code>dataSources</code> subsection of the <code>wso2.datasources</code> section in the  <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <pre><code>Sample MySQL datasource\n- name: WSO2_CLUSTER_DB\n  description: The MySQL datasource used for Cluster Coordination\n  jndiConfig:\n    name: jdbc/WSO2ClusterDB\n  definition:\n    type: RDBMS\n    configuration:\n      jdbcUrl: 'jdbc:mysql://localhost:3306/WSO2_CLUSTER_DB?useSSL=false'\n      username: root\n      password: root\n      driverClassName: com.mysql.jdbc.Driver\n      maxPoolSize: 50\n      idleTimeout: 60000\n      connectionTestQuery: SELECT 1\n      validationTimeout: 30000\n      isAutoCommit: false\n</code></pre> <p>For detailed instructions on how to configure a datasource, see Configuring Datasources.</p> </li> <li> <p><code>heartbeatInterval</code>:</p> </li> </ul> <p>Define the time interval (in milliseconds) at which the heartbeat pulse should occur for each node.  Recommended value for it is 5000 milliseconds.</p> <ul> <li><code>heartbeatMaxRetry</code>:</li> </ul> <p>Define the number of times to tghe system should retry to hear the heartbeat of the active node (that indicates that the node is alive) before the passive node becomes active. The recommended value is five times.</p> <ul> <li><code>eventPollingInterval</code>:</li> </ul> <p>Define the time interval (in milliseconds) at which each node should listen for changes that occur in the cluster. Recommended value for it is 5000 milliseconds.</p> <p>The following is a sample cluster configuration.</p> <pre><code>   - cluster.config:\n       enabled: true\n       groupId:  si\n       coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy\n       strategyConfig:\n         datasource: WSO2_CLUSTER_DB\n         heartbeatInterval: 5000\n         heartbeatMaxRetry: 5\n         eventPollingInterval: 5000\n</code></pre> </li> <li> <p>Next, add the <code>deployment.config</code> section to the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file with the following configurations. (HA configuration)</p> <p>Info</p> <p>This step covers HA configuration.</p> <ul> <li> <p><code>type</code>: To enable two-node minimum HA, set the type property to <code>ha</code>.</p> </li> <li> <p><code>passiveNodeDetailsWaitTimeOutMillis</code>: The time duration (in miliseconds) to wait until the details of the passive node are available in the database so that the active node can retrieve them. Once this time duration elapses, a timeout occurs and the system sleeps for a time duration specified via the <code>passiveNodeDetailsRetrySleepTimeMillis</code> parameter.</p> </li> <li> <p><code>passiveNodeDetailsRetrySleepTimeMillis</code>: The time duration (in milliseconds) to sleep before retying to retrieve details of the passive node again. This applies when the system has timed out after an attempt to retrieve these details.</p> </li> <li> <p><code>eventByteBufferQueueCapacity</code>: The size of the queue that is used to keep events in the passive node.</p> </li> <li> <p><code>byteBufferExtractorThreadPoolSize</code>: The number of worker threads that read events from the queue in the passive node.</p> </li> <li> <p>To configure the TCP server via which event synchronization is carried out from the active node to the passive node,     add a subsection named <code>eventSyncServer</code> and enter information as follows:</p> <ul> <li> <p><code>host</code>: The hostname of the server in which the TCP server is spawned.</p> </li> <li> <p><code>port</code>: The number of the port in which the TCP server is run.</p> </li> <li> <p><code>advertisedHost</code>: This is specified when the host can be different from the actual server host.</p> </li> <li> <p><code>advertisedPort</code>: This is specified when the port can be different from the actual port of the server.</p> </li> <li> <p><code>bossThreads</code>: The number of boss threads to be allocated for the TCP server to handle the connections. The default value is <code>10</code>.</p> </li> <li> <p><code>workerThreads</code>: The number of worker threads to be allocated for the TCP server to handle the connections. The default value is 10.</p> </li> </ul> </li> <li> <p>. To configure the TCP client via which requests are sent to the SI cluster, add a subsection named     <code>eventSyncClientPool</code> and add information as follows:</p> <ul> <li> <p><code>maxActive</code>: The maximum number of active connections that must be allowed in the TCP client pool. The default value is <code>10</code>.</p> </li> <li> <p><code>maxTotal</code>: The maximum number of total connections that must be allowed in the TCP client pool. The default value is <code>10</code>.</p> </li> <li> <p><code>maxIdle</code>: The maximum number of idle connections that must be allowed in the TCP client pool. The default value is <code>10</code>.</p> </li> <li> <p><code>maxWait</code>: The maximum amount of time (in milliseconds) that the client pool must wait for an idle object in the connection pool. The default value is <code>6000</code>.</p> </li> <li> <p><code>minEvictableIdleTimeInMillis</code>: The minimum number of milliseconds that an object can sit idle in the pool before it is eligible for eviction. The default value is <code>120000</code>.</p> </li> </ul> <p>Info</p> <p>In a container environment, you can use an advertised host and an advertised port to avoid exposing the actual host and port.</p> </li> </ul> <p>The following is sample HA configuration.</p> <pre><code>```\n    - deployment.config:\n        type: ha\n        passiveNodeDetailsWaitTimeOutMillis: 300000\n        passiveNodeDetailsRetrySleepTimeMillis: 500\n        eventByteBufferQueueCapacity: 20000\n        byteBufferExtractorThreadPoolSize: 5\n        eventSyncServer:\n            host: localhost\n            port: 9893\n            advertisedHost: localhost\n            advertisedPort: 9893\n            bossThreads: 10\n            workerThreads: 10\n        eventSyncClientPool:\n            maxActive: 10\n            maxTotal: 10\n            maxIdle: 10\n            maxWait: 60000\n            minEvictableIdleTimeMillis: 120000\n```</code></pre> </li> </ol>"},{"location":"setup/deploying-si-as-minimum-ha-cluster/#starting-the-cluster","title":"Starting the cluster","text":"<p>To start the minimum HA cluster you configured, follow the steps below:</p> <ol> <li> <p>Save the required Siddhi applications in the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory of both nodes. In order to ensure that the Siddhi applications are completely synchronized between the active and  the passive node, they must be added to the <code>siddhi-files</code> directory before the server startup. However, the synchronization can take place effectively even if the Siddhi applications are added while the server is running. </p> </li> <li> <p>Start both servers by navigating to the <code>&lt;SI_HOME&gt;/bin</code> directory and issuing one of the following commands (depending on your operating system:</p> <ul> <li>For Windows: <code>server.bat</code></li> <li>For Linux/Mac OS : <code>./server.sh</code></li> </ul> </li> </ol> <p>If the cluster is correctly configured, the following CLI logs can be viewed without any error logs:</p> <ul> <li> <p>In the active node:</p> <pre><code>    [2018-09-09 23:56:54,272]  INFO {org.wso2.carbon.streaming.integrator.core.internal.ServiceComponent} - WSO2 Streaming Integrator Starting in Two Node Minimum HA Deployment\n    [2018-09-09 23:56:54,294]  INFO {org.wso2.carbon.streaming.integrator.core.ha.HAManager} - HA Deployment: Starting up as Active Node\n</code></pre> </li> <li> <p>In the passive node:</p> <pre><code>    [2018-09-09 23:58:44,178]  INFO {org.wso2.carbon.streaming.integrator.core.internal.ServiceComponent} - WSO2 Streaming Integrator Starting in Two Node Minimum HA Deployment\n    [2018-09-09 23:58:44,199]  INFO {org.wso2.carbon.streaming.integrator.core.ha.HAManager} - HA Deployment: Starting up as Passive Node\n</code></pre> </li> </ul> <p>Info</p> <p>When deploying Siddhi applications in a two node minimum HA cluster, it is recommended to use a content synchronization mechanism since Siddhi applications must be deployed to both server nodes. You can use a common shared file system such as Network File System (NFS). You need to mount the <code>&lt;SI_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory of the two nodes to the shared file system.</p> <p>Info</p> <p>To start two WSO2 SI Nodes in the same machine, <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file -&gt; <code>wso2.transport.http</code> namespace -&gt; <code>listenerConfigurations</code> section must be updated to listen to different ports. The <code>offset</code> property in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> -&gt; <code>wso2.carbon section</code> section -&gt; <code>ports</code> subsection should also be changed in one SI instance to avoid conflicts when starting both servers.</p>"},{"location":"setup/deployment-guide/","title":"Deployment","text":"<p>WSO2 Streaming Integrator(SI) is designed to deploy enterprise grade critical systems which demands high  reliability, availability and performance. Thus, WSO2 SI is designed with support for deployment options which can  handle increasing system loads (scalability) and provide near-zero downtime and by eliminate single point of failure  (high availability). Furthermore, some of the key characteristics of WSO2 SI deployment architecture are ease of  installation and deployment, high maintainability and DevOps friendliness.</p> <p>Mainly following deployment options can be followed, </p> <ul> <li>Single Node Deployment</li> <li>Minimum High Available(HA) Deployment</li> <li>Scalable High Available(HA) Deployment</li> </ul>"},{"location":"setup/installing-Stream-Processor-Using-Docker/","title":"Installing Stream Processor Using Docker","text":"<p>Tip</p> <p>Before you begin:</p> <ul> <li>You need the following system requrements:<ul> <li>3 GHz Dual-core Xeon/Opteron (or latest)</li> <li>8 GB RAM</li> <li>10 GB free disk space</li> </ul> </li> <li>Install Docker by following the instructions provided     here .</li> </ul> <p>WSO2 provides open source Docker images to run WSO2 Stream Processor in Docker Hub. You can view these images from here .</p>"},{"location":"setup/installing-Stream-Processor-Using-Docker/#downloading-and-installing-wso2-stream-processor","title":"Downloading and installing WSO2 Stream Processor","text":"<p>Issue the following commands to pull tghe required WSO2 Stream Processor profile with updates from the Docker image.</p> Profile Command worker <code>docker pull wso2/wso2sp-worker</code> manager <code>docker pull wso2/wso2sp-manager</code> editor <code>docker pull wso2/wso2sp-editor</code> dashboard <code>docker pull wso2/wso2sp-dashboard</code>"},{"location":"setup/installing-Stream-Processor-Using-Docker/#running-wso2-stream-processor","title":"Running WSO2 Stream Processor","text":"<p>To run WSO2 SP, follow the steps below:</p> <ol> <li> <p>To start each WSO2 Stream Processor profile in a Docker container,     issue the following commands:</p> <ul> <li> <p>**For dashboard:     **</p> <pre><code>        docker run -it -p 9643:9643  wso2/wso2sp-dashboard\n</code></pre> </li> <li> <p>**For editor:     **</p> <pre><code>            docker run -it \\\n            -p 9390:9390 \\\n            -p 9743:9743 \\\n            wso2/wso2sp-editor\n</code></pre> </li> <li> <p>**For manager:     **</p> <pre><code>            docker run -it wso2/wso2sp-manager\n</code></pre> </li> <li> <p>For worker:</p> <pre><code>            docker run -it wso2/wso2sp-worker\n</code></pre> </li> </ul> </li> <li> <p>Once the container is started, access the UIs of each profile\u00a0via     the following URLs on your favourite browser. You can enter     <code>admin</code> as both the username and the password.</p> <ul> <li>Dashboard<ul> <li>Business Rules :     <code>https://localhost:9643/business-rules</code></li> <li>Dashboard Portal :     <code>https://localhost:9643/portal</code></li> <li>Status Dashboard :     <code>https://localhost:9643/monitoring</code></li> </ul> </li> <li>Editor<ul> <li>Steam Processor Studio :     <code>https://localhost:9390/editor</code></li> <li>Template Editor :     <code>https://localhost:930/template-editor</code></li> </ul> </li> </ul> </li> </ol>"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/","title":"Installing Stream Processor Using Kubernetes","text":"<p>To install WSO2 Stream Processor using Kubernetes, follow the steps below:</p> <ol> <li>Install     Git     and Kubernetes     client     (tested with v1.10). As a result, an already setup Kubernetes     cluster with     NGINX Ingress     Controller     is enabled.</li> <li> <p>A pre-configured Network File System (NFS) is used as the persistent     volume for artifact sharing and persistence.     In the NFS server instance, create a Linux system user account named     <code>wso2carbon</code> with <code>802</code> as     the user ID, and a system group named <code>wso2</code>     with <code>802</code> as the group ID. Add the     <code>wso2carbon</code> user to the     <code>wso2</code> group.</p> <pre><code>    groupadd --system -g 802 wso2\n    useradd --system -g 802 -u 802 wso2carbon\n</code></pre> </li> </ol>"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/#fully-distributed-deployment-of-wso2-stream-processor","title":"Fully distributed deployment of WSO2 Stream Processor","text":"<p>To set up a fully distributed deployment of WSO2 Stream Processor with Kubernetes, follow the steps below:</p> <p>Clone the Kubernetes resources for WSO2 Stream Processor Git repository .</p> <p>Info</p> <p>The local copy of the Git repository is referred to as <code>&lt;KUBERNETES_HOME&gt;</code> from here onwards.</p> <pre><code>    git clone https://github.com/wso2/kubernetes-sp.git\n</code></pre> <p>Setup a Network File System (NFS) to be used for persistent storage as follows.  </p> <ol> <li> <p>Create and export unique directories within the NFS server instance     for each Kubernetes Persistent Volume resource defined in the     <code>&lt;KUBERNETES_HOME&gt;/pattern-distributed/volumes/persistent-volumes.yaml</code>     file.</p> <pre><code>        sudo chown -R wso2carbon:wso2 &lt;directory_name&gt;\n</code></pre> </li> <li> <p>Grant ownership to the <code>wso2carbon</code> user     and <code>wso2</code> group, for each of the     previously created directories by issuing the following command.</p> <pre><code>        chmod -R 700 &lt;directory_name&gt;\n</code></pre> </li> <li> <p>Then, update each Kubernetes Persistent Volume resource with the     corresponding NFS server IP (NFS_SERVER_IP), and the NFS server     directory path (NFS_LOCATION_PATH) of the directory you exported.</p> </li> </ol> <p>Setup product database(s) using MySQL in Kubernetes. Here, a NFS is needed for persisting MySQL DB data.</p> <p>Create and export a directory within the NFS server instance.</p> <p>Provide read-write-execute permissions to other users for the created folder.</p> <p>Then, update the Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP) and exported, NFS server directory path (NFS_LOCATION_PATH) in <code>&lt;KUBERNETES_HOME&gt;/pattern-distributed/extras/rdbms/volumes/persistent-volumes.yaml</code> .</p> <p>For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance.</p> <p>Navigate to the <code>&lt;KUBERNETES_HOME&gt;/pattern-distributed/scripts</code> directory as follows.</p> <pre><code>cd &lt;KUBERNETES_HOME&gt;/pattern-distributed/scripts</code></pre> <p>Deploy the Kubernetes resources by executing the <code>&lt;KUBERNETES_HOME&gt;/pattern-distributed/scripts/deploy.sh</code> script as follows.</p> <pre><code>./deploy.sh --wso2-username=&lt;WSO2_USERNAME&gt; --wso2-password=&lt;WSO2_PASSWORD&gt; --cluster-admin-password=&lt;K8S_CLUSTER_ADMIN_PASSWORD&gt;WSO2_USERNAME: Your WSO2 username</code></pre> <ul> <li> <p><code>WSO2_USERNAME</code> : Your WSO2 username</p> </li> <li> <p><code>WSO2_PASSWORD</code> : Your WSO2 password</p> </li> <li> <p><code>K8S_CLUSTER_ADMIN_PASSWORD</code> : Kubernetes     cluster admin password</p> </li> </ul> <p>Access product management consoles.  </p> <p>Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing  </p> <p>The external IP can be found under the ADDRESS column of the output.  </p> <p>Add the above host as an entry in the <code>/etc/hosts</code> file as shown below:  </p> <p>\\&lt;EXTERNAL-IP&gt; wso2sp-dashboard</p> <pre><code>&lt;EXTERNAL-IP&gt; wso2sp-manager-1\n&lt;EXTERNAL-IP&gt; wso2sp-manager-2</code></pre> <p>Try navigating to https://wso2sp-dashboard/monitoring from your favorite browser.</p>"},{"location":"setup/installing-as-a-windows-service/","title":"Installing as a Windows Service","text":"<p>Note</p> <p>Before you begin:</p> <ul> <li>See our compatibility matrix to find out if this version of the product is fully tested on your OS.</li> </ul>"},{"location":"setup/installing-as-a-windows-service/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install JDK and set up the <code>JAVA_HOME</code> environment variable.</li> <li>Download and install a service wrapper library to use for running WSO2 API Manager as a Windows service. WSO2 recommends Yet Another Java Service Wrapper ( YAJSW ) versions 11.03 or 12.14, and several WSO2 products provide a default <code>wrapper.conf</code> file in their <code>&lt;PRODUCT_HOME&gt;/bin/yajsw/</code> directory. The instructions below describe how to set up this file.</li> </ul> <p>Important</p> <p>Please note that JDK 11 might not be compatible with YAJSW 11.03. Use JDK 8 for YAJSW 11.03 and JDK 11 for YAJSW 12.14.</p>"},{"location":"setup/installing-as-a-windows-service/#setting-up-the-yajsw-wrapper-configuration-file","title":"Setting up the YAJSW wrapper configuration file","text":"<p>The configuration file used for wrapping Java Applications by YAJSW is <code>wrapper.conf</code> , which is located in the <code>&lt;YAJSW_HOME&gt;/conf/</code> directory. The configuration file in <code>&lt;PRODUCT_HOME&gt;/bin/yajsw/</code> directory of many WSO2 products can be used as a reference for this. Following is the minimal <code>wrapper.conf</code> configuration for running a WSO2 product as a Windows service. Open your <code>wrapper.conf</code> file in <code>&lt;YAJSW_HOME&gt;/conf/</code> directory, set its properties as follows, and save it.</p> <p>Info</p> <p>If you want to set additional properties from an external registry at runtime, store sensitive information like usernames and passwords for connecting to the registry in a properties file and secure it with secure vault.</p> <p>Note</p> <p>Manual Configurations</p> <p>Add the following class path to the <code>wrapper.conf</code> file manually to avoid errors in the WSO2 API Manager Management Console:</p> <pre><code>wrapper.java.classpath.3 = ${carbon_home}/repository/components/plugins/commons-lang_2.6.0.wso2v1.jar </code></pre> <p>Minimal wrapper.conf configuration</p> <pre><code>    #********************************************************************\n# working directory\n#********************************************************************\nwrapper.working.dir=${carbon_home}/\n    # Java Main class.\n# YAJSW: default is \"org.rzo.yajsw.app.WrapperJVMMain\"\n# DO NOT SET THIS PROPERTY UNLESS YOU HAVE YOUR OWN IMPLEMENTATION\n# wrapper.java.mainclass=\n#********************************************************************\n# tmp folder\n# yajsw creates temporary files named in_.. out_.. err_.. jna..\n# per default these are placed in jna.tmpdir.\n# jna.tmpdir is set in setenv batch file to &lt;yajsw&gt;/tmp\n#********************************************************************\nwrapper.tmp.path = ${jna_tmpdir}\n#********************************************************************\n# Application main class or native executable\n# One of the following properties MUST be defined\n#********************************************************************\n# Java Application main class\nwrapper.java.app.mainclass=org.wso2.carbon.bootstrap.Bootstrap\n    # Log Level for console output.  (See docs for log levels)\nwrapper.console.loglevel=INFO\n    # Log file to use for wrapper output logging.\nwrapper.logfile=${wrapper_home}\\/log\\/wrapper.log\n    # Format of output for the log file.  (See docs for formats)\n#wrapper.logfile.format=LPTM\n# Log Level for log file output.  (See docs for log levels)\n#wrapper.logfile.loglevel=INFO\n# Maximum size that the log file will be allowed to grow to before\n#  the log is rolled. Size is specified in bytes.  The default value\n#  of 0, disables log rolling by size.  May abbreviate with the 'k' (kB) or\n#  'm' (mB) suffix.  For example: 10m = 10 megabytes.\n# If wrapper.logfile does not contain the string ROLLNUM it will be automatically added as suffix of the file name\nwrapper.logfile.maxsize=10m\n    # Maximum number of rolled log files which will be allowed before old\n#  files are deleted.  The default value of 0 implies no limit.\nwrapper.logfile.maxfiles=10\n# Title to use when running as a console\nwrapper.console.title=WSO2 Carbon\n    #********************************************************************\n# Wrapper Windows Service and Posix Daemon Properties\n#********************************************************************\n# Name of the service\nwrapper.ntservice.name=WSO2CARBON\n    # Display name of the service\nwrapper.ntservice.displayname=WSO2 Carbon\n    # Description of the service\nwrapper.ntservice.description=Carbon Kernel\n    #********************************************************************\n# Wrapper System Tray Properties\n#********************************************************************\n# enable system tray\nwrapper.tray = true\n# TCP/IP port. If none is defined multicast discovery is used to find the port\n# Set the port in case multicast is not possible.\nwrapper.tray.port = 15002\n#********************************************************************\n# Exit Code Properties\n# Restart on non zero exit code\n#********************************************************************\nwrapper.on_exit.0=SHUTDOWN\n    wrapper.on_exit.default=RESTART\n    #********************************************************************\n# Trigger actions on console output\n#********************************************************************\n# On Exception show message in system tray\nwrapper.filter.trigger.0=Exception\n    wrapper.filter.script.0=${wrapper_home}/scripts/trayMessage.gv\n    wrapper.filter.script.0.args=Exception\n    #********************************************************************\n# genConfig: further Properties generated by genConfig\n#********************************************************************\nplaceHolderSoGenPropsComeHere=\nwrapper.java.command = java\n    wrapper.java.classpath.1 = ${carbon_home}/bin/*.jar\n    wrapper.java.classpath.2 = ${carbon_home}/lib/commons-lang-*.jar\n    wrapper.java.classpath.3 = ${carbon_home}/lib/*.jar\n    wrapper.app.parameter.1 = org.wso2.carbon.bootstrap.Bootstrap\n    wrapper.app.parameter.2 = RUN\n    wrapper.java.additional.1 = -Xbootclasspath/a:${carbon_home}/lib/xboot/*.jar\n    wrapper.java.additional.2 = -Xms256m\n    wrapper.java.additional.3 = -Xmx1024m\n    wrapper.java.additional.4 = -XX:MaxPermSize=256m\n    wrapper.java.additional.5 = -XX:+HeapDumpOnOutOfMemoryError\n    wrapper.java.additional.6 = -XX:HeapDumpPath=${carbon_home}/repository/logs/heap-dump.hprof\n    wrapper.java.additional.7 = -Dcom.sun.management.jmxremote\n    wrapper.java.additional.8 = -Dcarbon.registry.root=\\/\nwrapper.java.additional.9 = -Dcarbon.home=${carbon_home}\nwrapper.java.additional.10 = -Dwso2.server.standalone=true\nwrapper.java.additional.11 = -Djava.command=${java_home}/bin/java\n    wrapper.java.additional.12 = -Djava.io.tmpdir=${carbon_home}/tmp\n    wrapper.java.additional.13 = -Dcatalina.base=${carbon_home}/lib/tomcat\n    wrapper.java.additional.14 = -Djava.util.logging.config.file=${carbon_home}/repository/conf/etc/logging-bridge.properties\n    wrapper.java.additional.15 = -Dcarbon.config.dir.path=${carbon_home}/repository/conf\n    wrapper.java.additional.16 = -Dcarbon.logs.path=${carbon_home}/repository/logs\n    wrapper.java.additional.17 = -Dcomponents.repo=${carbon_home}/repository/components/plugins\n    wrapper.java.additional.18 = -Dconf.location=${carbon_home}/repository/conf\n    wrapper.java.additional.19 = -Dcom.atomikos.icatch.file=${carbon_home}/lib/transactions.properties\n    wrapper.java.additional.20 = -Dcom.atomikos.icatch.hide_init_file_path=true\nwrapper.java.additional.21 = -Dorg.apache.jasper.runtime.BodyContentImpl.LIMIT_BUFFER=true\nwrapper.java.additional.22 = -Dcom.sun.jndi.ldap.connect.pool.authentication=simple\n    wrapper.java.additional.23 = -Dcom.sun.jndi.ldap.connect.pool.timeout=3000\nwrapper.java.additional.24 = -Dorg.terracotta.quartz.skipUpdateCheck=true\nwrapper.java.additional.25 = -Dorg.apache.jasper.compiler.Parser.STRICT_QUOTE_ESCAPING=false\nwrapper.java.additional.26 = -Dfile.encoding=UTF8\n    wrapper.java.additional.27 = -DworkerNode=false\nwrapper.java.additional.28 = -Dhttpclient.hostnameVerifier=DefaultAndLocalhost\n    wrapper.java.additional.29 = -Dcarbon.new.config.dir.path=${carbon_home}/repository/resources/conf\n</code></pre>"},{"location":"setup/installing-as-a-windows-service/#setting-up-carbon_home","title":"Setting up CARBON_HOME","text":"<p>Extract WSO2 API Manager that you want to run as a Windows service, and then set the Windows environment variable <code>CARBON_HOME</code> to the extracted product directory location which is for example wso2am-2.1.0 here.</p>"},{"location":"setup/installing-as-a-windows-service/#running-the-product-in-console-mode","title":"Running the product in console mode","text":"<p>You will now verify that YAJSW is configured correctly for running the WSO2 API Manager as a Windows service.</p> <ol> <li> <p>Open a Windows command prompt and go to the <code>&lt;YAJSW_HOME&gt;/bat/</code> directory. For example:</p> <pre><code>cd C:\\Documents and Settings\\yajsw_home\\bat\n</code></pre> </li> <li> <p>Start the wrapper in console mode using the following command:</p> <pre><code>runConsole.bat\n</code></pre> <p>For example:</p> <p></p> </li> </ol> <p>If the configurations are set properly for YAJSW, you will see console output similar to the following and can now access the WSO2 management console from your web browser via https://localhost:9443/carbon.</p> <p></p>"},{"location":"setup/installing-as-a-windows-service/#working-with-the-wso2carbon-service","title":"Working with the WSO2CARBON service","text":"<p>To install the Carbon-based productWSO2 API Manager as a Windows service, execute the following command in the <code>&lt;YAJSW_HOME&gt;/bat/</code> directory:</p> <pre><code>installService.bat\n</code></pre> <p>The console will display a message confirming that the WSO2CARBON service was installed.</p> <p></p> <p>To start the service, execute the following command in the same console window:</p> <pre><code>startService.bat\n</code></pre> <p>The console will display a message confirming that the WSO2CARBON service was started.</p> <p></p> <p>To stop the service, execute the following command in the same console window:</p> <pre><code>stopService.bat\n</code></pre> <p>The console will display a message confirming that the WSO2CARBON service has stopped.</p> <p></p> <p>To uninstall the service, execute the following command in the same console window:</p> <pre><code>uninstallService.bat\n</code></pre> <p>The console will display a message confirming that the WSO2CARBON service was removed.</p> <p></p>"},{"location":"setup/installing-as-windows-service/","title":"Running the Streaming Integrator as a Windows Service","text":"<p>Follow the instructions given below to run the Streaming Integrator as a Windows service.</p>"},{"location":"setup/installing-as-windows-service/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Go to the WSO2 Enterprise Integrator product page, click Download, and then click Zip Archive to download the product distribution as a ZIP file.</p> </li> <li> <p>Extract the downloaded ZIP file to a location on your computer. The streaming-integrator folder inside the extracted ZIP file is your  directory. <li> <p>Set up a JDK that is compatible with WSO2 Enterprise Integrator and point the <code>java_home</code> variable to your JDK instance. </p> </li> <li> <p>Point the <code>wso2si_home</code> environment variable to the <code>&lt;SI_HOME&gt;</code> directory.</p> </li> <p>Note</p> <p>Be sure to use lower case letters when setting the <code>java_home</code> and <code>wso2si_home</code> in the Windows OS. That is, you must not use <code>JAVA_HOME</code> or <code>WSO2SI_HOME</code>.</p>"},{"location":"setup/installing-as-windows-service/#setting-up-the-yajsw-wrapper","title":"Setting up the YAJSW wrapper","text":"<p>YASJW uses the configurations defined in the <code>&lt;YAJSW_HOME&gt;/conf/wrapper.conf</code> file to wrap Java applications. Replace the contents of this file with the configurations that are relevant to the Streaming Integrator instance that you want to run as a service. Use the wrapper.conf file available in <code>&lt;SI_HOME&gt;/bin/yajsw</code> folder to get the relevant configurations.</p> <p>Info</p> <p>WSO2 recommends Yet Another Java Service Wrapper (YAJSW) version 12.14. If you are running on JDK 11, previous versions of YAJSW will not be compatible.</p>"},{"location":"setup/installing-as-windows-service/#installing-the-service","title":"Installing the service","text":"<p>Navigate to the <code>&lt;YAJSW_HOME&gt;/bat/</code> directory in the Windows command prompt, and execute the following command: </p> <pre><code>installService.bat\n</code></pre>"},{"location":"setup/installing-as-windows-service/#starting-the-service","title":"Starting the service","text":"<p>Navigate to the <code>&lt;YAJSW_HOME&gt;/bat/</code> directory in the Windows command prompt, and execute the following command: </p> <pre><code>startService.bat\n</code></pre>"},{"location":"setup/installing-as-windows-service/#stopping-the-service","title":"Stopping the service","text":"<p>Navigate to the <code>&lt;YAJSW_HOME&gt;/bat/</code> directory in the Windows command prompt and execute the following command: </p> <pre><code>stopService.bat\n</code></pre>"},{"location":"setup/installing-as-windows-service/#uninstalling-the-service","title":"Uninstalling the service","text":"<p>To uninstall the service, navigate to the <code>&lt;YAJSW_HOME&gt;/bat/</code> directory in the Windows command prompt and execute the following command: </p> <pre><code>uninstallServiceService.bat\n</code></pre>"},{"location":"setup/installing-si-as-a-linux-service/","title":"Installing Streaming Integrator as a Linux Service","text":"<p>WSO2 Streaming Integrator can be run as a Linux service.</p>"},{"location":"setup/installing-si-as-a-linux-service/#before-you-begin","title":"Before you begin","text":"<p>Install JDK version 1.8.0_144 and set the <code>JAVA_HOME</code> variable.</p>"},{"location":"setup/installing-si-as-a-linux-service/#download-and-install-the-streaming-integrator","title":"Download and install the Streaming Integrator","text":"<p>Go to the WSO2 Streaming Integrator product page, click Download, and then click Zip Archive to download the product distribution as a ZIP file.</p> <p>Extract the download ZIP file to a location on your computer. The streaming-integrator folder inside the extracted ZIP file will be your SI_HOME directory.</p> <p>For more information, see Installing via the Binary</p>"},{"location":"setup/installing-si-as-a-linux-service/#running-wso2-streaming-integrator-as-a-linux-service","title":"Running WSO2 Streaming Integrator as a Linux Service","text":"<p>To run WSO2 Streaming Integrator as a Linux service, follow the steps below:</p> <ol> <li> <p>To run the product as a service, create a startup script and add it to the boot sequence. The basic structure of the startup script has three parts (i.e., start, stop and restart) as follows:</p> <pre><code>#!/bin/bash\n\ncase \u201c$1\u2033 in\nstart)\n   echo \u201cStarting Service\u201d\n;;\nstop)\n   echo \u201cStopping Service\u201d\n;;\nrestart)\n   echo \u201cRestarting Service\u201d\n;;\n*)\n   echo $\u201dUsage: $0 {start|stop|restart}\u201d\nexit 1\nesac\n</code></pre> </li> </ol> <p>You can write the start up scripts for the Streaming Integrator server and Tooling as follows:</p> <ul> <li> <p>Streaming Integrator Server</p> <p><pre><code>#! /bin/sh\nexport JAVA_HOME=\"/usr/lib/jvm/jdk1.7.0_07\"\n\nstartcmd='/opt/WSO2/wso2si-1.1.0/bin/server.sh start &gt; /dev/null &amp;'\nrestartcmd='/opt/WSO2/wso2si-1.1.0/bin/server.sh restart &gt; /dev/null &amp;'\nstopcmd='/opt/WSO2/wso2si-1.1.0/bin/server.sh stop &gt; /dev/null &amp;'\n\ncase \"$1\" in\nstart)\n   echo \"Starting WSO2 Streaming Integrator ...\"\n   su -c \"${startcmd}\" user1\n;;\nrestart)\n   echo \"Re-starting WSO2 Streaming Integrator ...\"\n   su -c \"${restartcmd}\" user1\n;;\nstop)\n   echo \"Stopping WSO2 Streaming Integrator ...\"\n   su -c \"${stopcmd}\" user1\n;;\n*)\n   echo \"Usage: $0 {start|stop|restart}\"\nexit 1\nesac\n</code></pre> - Streaming Integrator Tooling</p> <pre><code>#! /bin/sh\nexport JAVA_HOME=\"/usr/lib/jvm/jdk1.7.0_07\"\n\nstartcmd='/opt/WSO2/wso2si-tooling-1.1.0/bin/tooling.sh start &gt; /dev/null &amp;'\nrestartcmd='/opt/WSO2/wso2si-tooling-1.1.0/bin/tooling.sh restart &gt; /dev/null &amp;'\nstopcmd='/opt/WSO2/wso2si-tooling-1.1.0/bin/tooling.sh stop &gt; /dev/null &amp;'\n\ncase \"$1\" in\nstart)\n   echo \"WSO2 Streaming Integrator Tooling ...\"\n   su -c \"${startcmd}\" user1\n;;\nrestart)\n   echo \"Re-starting WSO2 Streaming Integrator Tooling ...\"\n   su -c \"${restartcmd}\" user1\n;;\nstop)\n   echo \"Stopping WSO2 Streaming Integrator Tooling ...\"\n   su -c \"${stopcmd}\" user1\n;;\n*)\n   echo \"Usage: $0 {start|stop|restart}\"\nexit 1\nesac\n</code></pre> </li> </ul> <p>In the above script, the server is started via a user named <code>user1</code> rather than the root user. For example, <code>su -c \"${startcmd}\" user1</code>.</p> <ol> <li> <p>Add the script to the <code>/etc/init.d/</code> directory.</p> <p>Info</p> <p>If you want to keep the scripts in a location other than <code>/etc/init.d/</code> directory , you can add a symbolic link to the script in the <code>/etc/init.d/</code> and keep the actual script in a separate location. e.g., If your script name is siserver and it is in the <code>/opt/WSO2/</code> directory, then the commands for adding a link to <code>/etc/init.d/</code> are as follows: - To make the script executable: <code>sudo chmod a+x /opt/WSO2/appserver</code> - To add a link to <code>/etc/init.d/</code>: <code>sudo ln -snf /opt/WSO2/appserver /etc/init.d/appserver</code></p> </li> <li> <p>Install the startup script to respective run levels via the <code>update-rc.d</code> command. e.g., Issue the following command for the sample script given in step 1.</p> <p><code>sudo update-rc.d appserver defaults</code></p> <p>The <code>defaults</code> option in the above command makes the service start in runlevels 2,3,4, and 5, and stop in runlevels 0,1, and 6.</p> <p>Info</p> <p>A runlevel  is a mode of operation in Linux (or any Unix-style operating system). There are several runlevels in a Linux server and each of these runlevels is represented by a single digit integer. Each runlevel designates a different system configuration and allows access to a different combination of processes.</p> </li> </ol> <p>You can now start, stop and restart the server via the <code>service &lt;service name&gt; {start|stop|restart}</code> command. You will be prompted for the password of the username (or root) via which you started the service. </p>"},{"location":"setup/installing-si-binary/","title":"Installing via the Binary","text":"<p>Follow the steps given below to install the WSO2 Streaming Integrator runtime by using the binary distribution of WSO2 Enterprise Integrator.</p>"},{"location":"setup/installing-si-binary/#download-and-install","title":"Download and install","text":"<p>Go to the WSO2 Enterprise Integrator product page, click Download, and then click Zip Archive to download the product distribution as a ZIP file.</p> <p>Extract the download ZIP file to a location on your computer. The streaming-integrator folder inside the extracted ZIP file will be your SI_HOME directory.</p>"},{"location":"setup/installing-si-binary/#prerequisites","title":"Prerequisites","text":"<p>Setting the Java_Home: Set up a JDK that is compatible with WSO2 Enterprise Integrator and point the <code>java_home</code> variable to your JDK instance.</p>"},{"location":"setup/installing-si-binary/#running-the-si-server","title":"Running the SI server","text":""},{"location":"setup/installing-si-binary/#starting-the-si-server","title":"Starting the SI server","text":"<ol> <li> <p>Open a terminal and navigate to the <code>&lt;SI_HOME&gt;/bin/</code> directory, where <code>&lt;SI_HOME&gt;</code> is the home directory of the distribution you downloaded.</p> </li> <li> <p>Execute the relevant command:</p> <p>```bash tab='On MacOS/Linux/CentOS' sh streaming-integrator.sh <pre><code>```bash tab='On Windows'\nstreaming-integrator.bat\n</code></pre></p> </li> </ol> <p>By default, the HTTP listener port is 8290 and the default HTTPS listener port is 8253.</p>"},{"location":"setup/installing-si-binary/#stopping-the-si-server","title":"Stopping the SI server","text":"<p>To stop the Streaming Integrator runtime, press Ctrl+C in the command window.</p> <p>What's Next?</p> <p>Once you have successfully downloaded and installed WSO2 Streaming Integrator, you can proceed to do any of the following: - If you were previously using WSO2 Stream Processor and want to migrate to WSO2 Streaming Integrator, follow the instructions in Migrating from WSO2 Stream Processor. - To deploy WSO2 Streaming Integrator as a single-node deployment or a cluster (based on your requirements), see Deploying Streaming Integrator. - To set up WSO2 Streaming Integrator and make it ready to run in a production environment, see the Production Checklist.</p>"},{"location":"setup/installing-si-in-vm/","title":"Installing via the Installer","text":"<p>Follow the steps given below to install the WSO2 Streaming Integrator runtime.</p>"},{"location":"setup/installing-si-in-vm/#system-requirements","title":"System requirements","text":"Type Requirement CPU You require a minimum of one CPU with 2 cores. It is recommended to have a CPU with 4 cores. Memory ~ 4 GB minimum is recommended   ~ 2 GB heap size Disk ~ 1 GB minimum (excluding space allocated for log files and databases.)"},{"location":"setup/installing-si-in-vm/#download-and-install","title":"Download and install","text":"<p>Go to the WSO2 Enterprise Integrator product page, click Download, and then click the Installer.</p> <p>The product installer that is compatible with your operating system is downloaded.</p> <p>Double-click to open the installation wizard that guides you through the\u00a0installation. Once you finish, all the runtimes of WSO2 Enterprise Integrator are installed and ready for use.</p>"},{"location":"setup/installing-si-in-vm/#running-the-si-server","title":"Running the SI server","text":"<p>If you installed the product using the installer, use the following instructions to start and stop the SI server.</p>"},{"location":"setup/installing-si-in-vm/#starting-the-si-server","title":"Starting the SI server","text":"<p>On MacOS/Linux/CentOS, open a terminal and execute one of the commands given below.</p> <ul> <li> <p>To start the Streaming Integrator as a standalone application:</p> <pre><code>sudo wso2si\n</code></pre> </li> <li> <p>To start the Streaming Integrator as a service:</p> <p>```bash tab='On MacOS'   sudo wso2si-service start   <pre><code>```bash tab='On Linux'\nsudo service wso2si start\n</code></pre></p> <p><code>bash tab='On Centos'   sudo wso2si start</code></p> </li> </ul> <p>On Windows</p> <ul> <li>Go to Start Menu -&gt; WSO2 -&gt; Enterprise Integrator 7.1.0 Streaming Integrator. This opens a terminal and starts the Streaming Integrator.</li> </ul> <p>Tip</p> <p>If you have installed the product using the installer and you want to manually run the product startup script from the <code>&lt;SI_HOME&gt;/bin</code> directory, you need to use the following command: <code>bash sudo sh launcher_streaming-integrator.sh</code> This script automatically assigns the JAVA HOME of your VM to the root user of your Streaming Integrator instance.</p>"},{"location":"setup/installing-si-in-vm/#stopping-the-si-server","title":"Stopping the SI server","text":"<ul> <li> <p>To stop the Streaming Integrator standalone application, go to the terminal and press Ctrl+C.</p> </li> <li> <p>To stop the Streaming Integrator service:</p> <p>```bash tab='On MacOS'   sudo wso2si-service stop   <pre><code>```bash tab='On Linux'\nsudo service wso2si stop\n</code></pre></p> <p>```bash tab='On CentOS'   sudo wso2si stop</p> </li> </ul>"},{"location":"setup/installing-si-in-vm/#accessing-the-home-directory","title":"Accessing the HOME directory","text":"<p>Let's call the installation location of your product the <code>&lt;SI_HOME&gt;</code> directory.</p> <p>If you used the installer to install the product, this is located in a place\u00a0specific to your OS as shown below:</p> OS Home directory Mac OS <code>/Library/WSO2/EnterpriseIntegrator/7.1.0/streaming-integrator</code> Windows <code>C:\\Program Files\\WSO2\\Enterprise Integrator\\7.1.0\\streaming-integrator</code> Ubuntu <code>/usr/lib/wso2/wso2ei/7.1.0/streaming-integrator</code> CentOS <code>/usr/lib64/wso2/wso2ei/7.1.0/streaming-integrator</code>"},{"location":"setup/installing-si-in-vm/#uninstalling-the-product","title":"Uninstalling the product","text":"<p>If you used the installer to install WSO2 Enterprise Integrator, you can uninstall by following the steps given below:</p> OS Instructions Mac OS <p>Open a terminal and run the following command as the root user:</p> <code>sudo bash /Library/WSO2/wso2ei/7.1.0/uninstall.sh</code> Windows Go to Start Menu -&gt; WSO2 -&gt; Uninstall Enterprise Integrator 7.1.0 or search Uninstall Enterprise Integrator 7.1.0 and click the shortcut icon. This uninstalls the product from your computer. Linux <p>Open a terminal and run the following command:</p> <code>sudo dpkg --purge wso2ei-7.1.0</code> CentOS <p>Open a terminal and run the following command:</p> <code>sudo rpm -e wso2ei-7.1.0-1.el7.x86_64</code> <p>What's Next?</p> <p>Once you have successfully downloaded and installed WSO2 Streaming Integrator, you can proceed to do any of the following: - If you were previously using WSO2 Stream Processor and want to migrate to WSO2 Streaming Integrator, follow the instructions in Migrating from WSO2 Stream Processor. - To deploy WSO2 Streaming Integrator as a single-node deployment or a cluster (based on your requirements), see Deploying Streaming Integrator. - To set up WSO2 Streaming Integrator and make it ready to run in a production environment, see the Production Checklist.</p>"},{"location":"setup/installing-si-using-docker/","title":"Installing Streaming Integrator Using Docker","text":"<p>Before you begin:</p> <ul> <li> <p>The system requirements are as follows:</p> <ul> <li>3 GHz Dual-core Xeon/Opteron (or latest)</li> <li>8 GB RAM</li> <li>10 GB free disk space</li> </ul> </li> <li> <p>Install Docker by following the instructions provided in here.</p> </li> </ul> <p>WSO2 provides open source Docker images to run WSO2 Streaming Integrator in Docker Hub. You can view these images In Docker Hub - WSO2.</p>"},{"location":"setup/installing-si-using-docker/#downloading-and-installing-wso2-streaming-integrator","title":"Downloading and installing WSO2 Streaming Integrator","text":"<p>To pull the required WSO2 Streaming Integrator distribution with updates from the Docker image, issue the following command.</p> <pre><code>docker pull wso2/streaming-integrator:1.0.0\n</code></pre>"},{"location":"setup/installing-si-using-docker/#running-wso2-streaming-integrator","title":"Running WSO2 Streaming Integrator","text":"<p>To run WSO2 Streaming Integrator, issue the following command.</p> <pre><code>docker run -it wso2/streaming-integrator/1.0.0\n</code></pre> <p>Tip</p> <p>To expose the required ports via docker when running the docker container, issue the following command.</p> <pre><code>    docker run -it \\\n-p 9443:9443   \\\n-p 9090:9090   \\\n-p 7070:7070   \\\n-p 7443:7443   \\\n-p 9712:9712   \\\n-p 7711:7711   \\\n-p 7611:7611   \\\nwso2/streaming-integrator\n</code></pre> <p>For more details about the ports in Streaming Integrator, see Configuring Default Ports</p>"},{"location":"setup/installing-si-using-kubernetes/","title":"Installing Streaming Integrator Using Kubernetes","text":"<p>WSO2 Streaming Integrator can be deployed natively on Kubernetes via the Siddhi Kubernetes Operator.</p> <p>The Streaming Integrator can be configured in the <code>&lt;SI-TOOLING_HOME&gt;/wso2/server/resources/docker-export/siddhi-process.yaml</code> file and passed to the CRD(Custom Resource Definition)for deployment. Siddhi logic can be directly written in the <code>&lt;SI-TOOLING_HOME&gt;/wso2/server/resources/docker-export/siddhi-process.yaml</code> file or passed as <code>.siddhi</code> files via config maps.</p> <p>To install WSO2 Streaming Integrator via Kubernetes, follow the steps below:</p> <p>Before you begin:</p> <p>Start a Kubernetes cluster. The Kubernetes version must be v1.10.11 or later. To start the cluster, you can use Minikube, GKE(Google Kubernetes Engine) Cluster, Docker for Mac, or any other Kubernetes cluster. Minikube     You can install Minikube from the Kubernetes/Minikube     Siddhi operator automatically creates NGINX ingress. Therefore, for it to work, you can do one of the following:      - Enable ingress on Minikube by issuing the following command. <code>minikube addons enable ingress</code>     - Disable automatic ingress creation by the Siddhi operator. For instructions, see Siddhi Kubernetes Microservice Documentation - Deploy Siddhi Applications without Ingress creation. GKE Cluster     To install Siddhi operator, you have to give cluster admin permission to your account. In order to do this, issue the following command. You need to replace <code>your-address@email.com</code> with your account email address <code>kubectl create clusterrolebinding user-cluster-admin-binding --clusterrole=cluster-admin --user=your-address@email.com</code> Docker for Mac     Siddhi operator automatically creates NGINX ingress. Therefore, for it to work, you can do one of the following:      - Enable NGINX ingress. For instructions, see NGINX Ingress Controller documentation.     - Disable automatic ingress creation by the Siddhi operator. For instructions, see Siddhi Kubernetes Microservice Documentation - Deploy Siddhi Applications without Ingress creation. You are also required to have admin privileges to install the Siddhi operator.</p>"},{"location":"setup/installing-si-using-kubernetes/#install-siddhi-operator-for-streaming-integrator","title":"Install Siddhi Operator for Streaming Integrator","text":"<p>To install the Siddhi Kubernetes operator for WSO2 Streaming Integrator, issue the following commands.</p> <pre><code>kubectl apply -f https://github.com/wso2/streaming-integrator/tree/master/modules/kubenetes/00-prereqs.yaml\nkubectl apply -f https://github.com/wso2/streaming-integrator/tree/master/modules/kubenetes/01-siddhi-operator.yaml\n</code></pre> <p>You can verify the installation by making sure the following deployments are running in your Kubernetes cluster. <pre><code>$ kubectl get deployment\n\nNAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nstreaming-integrator   1         1         1            1           1m\nsiddhi-parser          1         1         1            1           1m\n</code></pre></p>"},{"location":"setup/installing-si-using-kubernetes/#deploy-streaming-integrator","title":"Deploy Streaming Integrator","text":"<p>The Siddhi application that contains the streaming integration logic can be deployed in Kubernetes via the Siddhi operator.</p> <p>To understand how this is done, let's create a very simple Siddhi stream processing application that consumes events via HTTP, filters the input events where the value for <code>deviceType</code> is <code>dryer</code> and the value for <code>power</code> is greater than <code>600</code>, and then logs the output in the console. This can be created by configuring the <code>&lt;SI-TOOLING_HOME&gt;/wso2/server/resources/docker-export/siddhi-process.yaml</code> file as given below.</p> <pre><code>    apiVersion: siddhi.io/v1alpha2\nkind: SiddhiProcess\nmetadata: name: streaming-integrator\nspec: apps: - script: |\n@App:name(\"PowerSurgeDetection\")\n@App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")\n/*\nInput: deviceType string and powerConsuption int(Watt)\nOutput: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.\n*/\n\n@source(\ntype='http',\nreceiver.url='${RECEIVER_URL}',\nbasic.auth.enabled='false',\n@map(type='json')\n)\ndefine stream DevicePowerStream(deviceType string, power int);\n@sink(type='log', prefix='LOGGER')  \ndefine stream PowerSurgeAlertStream(deviceType string, power int); \n@info(name='surge-detector')  \nfrom DevicePowerStream[deviceType == 'dryer' and power &gt;= 600] \nselect deviceType, power  \ninsert into PowerSurgeAlertStream;\ncontainer: env: - name: RECEIVER_URL\nvalue: \"http://0.0.0.0:8080/checkPower\"\n- name: BASIC_AUTH_ENABLED\nvalue: \"false\"\n</code></pre> <p>To change the default configurations in WSO2 Streaming Integrator that are defined in the <code>&lt;SI-TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file, you need to add the required configurations with the required overriding values in the <code>SiddhiProcess.yaml</code> file under a section named <code>runner</code> as shown in the example below.</p> <pre><code>    apiVersion: siddhi.io/v1alpha2\nkind: SiddhiProcess\nmetadata: name: streaming-integrator-app\nspec: apps: - script: |\n@App:name(\"PowerSurgeDetection\")\n@App:description(\"App consumes events from HTTP as a JSON message of { 'deviceType': 'dryer', 'power': 6000 } format and inserts the events into DevicePowerStream, and alerts the user if the power level is greater than or equal to 600W by printing a message in the log.\")\n/*\nInput: deviceType string and powerConsuption int(Watt)\nOutput: Alert user from printing a log, if there is a power surge in the dryer. In other words, notify when power is greater than or equal to 600W.\n*/\n\n@source(\ntype='http',\nreceiver.url='${RECEIVER_URL}',\nbasic.auth.enabled='false',\n@map(type='json')\n)\ndefine stream DevicePowerStream(deviceType string, power int);\n@sink(type='log', prefix='LOGGER')  \ndefine stream PowerSurgeAlertStream(deviceType string, power int); \n@info(name='surge-detector')  \nfrom DevicePowerStream[deviceType == 'dryer' and power &gt;= 600] \nselect deviceType, power  \ninsert into PowerSurgeAlertStream;\ncontainer: env: - name: RECEIVER_URL\nvalue: \"http://0.0.0.0:8080/checkPower\"\n- name: BASIC_AUTH_ENABLED\nvalue: \"false\"\n\nrunner: |\nauth.configs:\ntype: 'local'        # Type of the IdP client used\nuserManager:\nadminRole: admin   # Admin role which is granted all permissions\nuserStore:         # User store\nusers:\n-\nuser:\nusername: root\npassword: YWRtaW4=\nroles: 1\nroles:\n-\nrole:\nid: 1\ndisplayName: root\nrestAPIAuthConfigs:\nexclude:\n- /simulation/*\n- /stores/* \n</code></pre> <p>Here, you have included a configuration for <code>auth.configs</code> to override the default values that are applicable to the Streaming Integrator (i.e., values configured under <code>auth.configs</code> in the <code>&lt;SI-TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <p>To apply the configurations in the <code>siddhi-process.yaml</code> to your Kubernetes cluster, save the file in a preferred location and then issue the following command.</p> <pre><code>kubectl apply -f &lt;PATH_to_siddhi-process.yaml&gt;\n</code></pre>"},{"location":"setup/installing-si-using-kubernetes/#invoke-siddhi-applications","title":"Invoke Siddhi Applications","text":"<p>To invoke Siddhi Applications, follow the steps below:</p> <ol> <li> <p>To obtain the external IP of the Ingress load balancer, issue the following command.</p> <p><code>kubectl get ingress</code></p> <p>This generates a response similar to the following:</p> <pre><code>NAME      HOSTS     ADDRESS     PORTS     AGE\nsiddhi    siddhi    10.0.2.15    80       14d\n</code></pre> </li> <li> <p>Add the host (i.e., <code>siddhi</code>) and the related external IP (i.e., <code>ADDRESS</code>) in the <code>/etc/hosts</code> file in your machine.</p> <p>Info</p> <p>For Minikube, you have to use Minikube IP as the external IP. Therefore, issue the <code>minikube ip</code> command to get the IP of the Minikube cluster.</p> </li> <li> <p>To send events to the <code>PowerSurgeDetection</code> deployed in Kubernetes, issue the following CURL command.</p> <pre><code>curl -X POST \\\n  http://siddhi/streaming-integrator-0/8080/checkPower \\\n    -H 'Accept: */*' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Host: siddhi' \\\n    -d '{\n        \"deviceType\": \"dryer\",\n        \"power\": 600\n        }'\n</code></pre> </li> <li> <p>To monitor the associated logs for the above Siddhi application, list down the available pods by executing the following command.</p> <p><code>kubectl get pods</code></p> <p>The pods are listed as shown in the following sample response.</p> <pre><code>NAME                                        READY    STATUS    RESTARTS    AGE\nstreaming-integrator-app-0-b4dcf85-npgj7     1/1     Running      0        165m\nstreaming-integrator-5f9fcb7679-n4zpj        1/1     Running      0        173m\n</code></pre> </li> <li> <p>Issue the following command in order to monitor the logs of the relevant pod. In this example, let's assume that you need to monitor the logs for the <code>streaming-integrator-app-0-b4dcf85-npgj7</code> pod.</p> <p><code>kubectl logs -f streaming-integrator-app-0-b4dcf85-npgj7</code></p> </li> </ol> <p>Info</p> <p>For more details about the Siddhi Operator, see Siddhi as a Kubernetes Microservice.</p>"},{"location":"setup/migrating-from-stream-processor/","title":"Migrating from WSO2 Stream Processor","text":"<p>The Streaming Integrator performs all functions that are also performed by WSO2 Stream Processor. It also has additional features to trigger integration flows in order to take action in response to results derived after analyzing data.</p> <p>If you are currently using WSO2 Stream Processor to carry out any streaming integration/stream processing activities and want to carry them out in the Streaming Integrator, you can migrate your setup as follows:</p>"},{"location":"setup/migrating-from-stream-processor/#preparing-to-upgrade","title":"Preparing to upgrade","text":"<p>The following prerequisites should be completed before upgrading.</p> <ul> <li>Make a backup of the SP 4.4.0 database and copy the  directory in order to backup the product configurations. <li>Download the Streaming Integrator from the Enterprise Integrator Home</li>"},{"location":"setup/migrating-from-stream-processor/#migrating-databases","title":"Migrating Databases","text":"<p>To connect the Streaming Integrator to the same databases as WSO2 SP 4.4.0 so that the persisted data can be accessed, configure the data sources as follows:</p> <ul> <li>Configure the data sources in the <code>&lt;SI_HOME&gt;/conf/server/deployment.yaml</code> file the same way you have configured them in <code>&lt;SP_HOME&gt;/conf/wso2/worker/deployment.yaml</code> file.</li> <li>Configure the data sources in the <code>&lt;SI__TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file the same way you have configured them in <code>&lt;SP_HOME&gt;/conf/wso2/editor/deployment.yaml</code> file.</li> <li> <p>Check the data source configured for Business Rules  in the <code>&lt;SP_HOME&gt;/conf/wso2/dashboard/deployment.yaml</code> file, and configure that data source with the same parameter values in the <code>&lt;SI__TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> <p>Info</p> <p>The Business Rules feature which was a part of the <code>Dashboard</code> profile of the Stream Processor is now shipped with Streaming Integrator Tooling. Therefore, configurations related to this feature are added in the <code>&lt;SI__TOOLING_HOME&gt;/conf/server/deployment.yaml</code> file.</p> </li> </ul> <p>For the complete list of data sources configured for the Streaming Integrator, see Configuring Data sources.</p>"},{"location":"setup/migrating-from-stream-processor/#migrating-siddhi-applications","title":"Migrating Siddhi applications","text":"<p>To migrate the Siddhi applications that you have deployed in WSO2 SP 4.4.0, follow the procedure below:</p> <ol> <li> <p>Copy all the Siddhi applications in the <code>&lt;SP_HOME/wso2/worker/deployment/siddhi-files</code> directory.</p> </li> <li> <p>Place the Siddhi applications you copied in the <code>&lt;SI_HOME/wso2/server/deployment/siddhi-files</code> directory.</p> </li> </ol>"},{"location":"setup/migrating-from-stream-processor/#testing-the-migration","title":"Testing the migration","text":"<p>Simulate a few events to the Siddhi applications deployed in the Streaming Integrator to test whether they are generating the expected results.</p>"},{"location":"setup/production-checklist/","title":"Production Checklist","text":"<p>Once you download and install WSO2 Streaming Integrator, you may need to update its default configurations based on your requirements.</p> <p>The changes that you need to make include the following:</p>"},{"location":"setup/production-checklist/#providing-access","title":"Providing access","text":"<p>Multiple users with various roles in your organization can require access to your WSO2 Streaming Integrator installation to carry out different activities. In order to manage the level of access provided to each user based on their roles, you are required to configure users roles and permissions. For instructions, see User Management.</p>"},{"location":"setup/production-checklist/#securing-the-streaming-integrator","title":"Securing the Streaming Integrator","text":"<p>WSO2 SI is an open-source product. Therefore, anyone who downloads it has access to the default users and passwords, default keystore settings, etc. Therefore, you are required to update the configurations related to security in order to ensure that your data is secure when you run WSO2 SI in a production environment. For more information, see the following topics:</p> <ul> <li>WSO2 uses key stores to store cryptographic keys and certificates that are used for various purposes. For more information on how to configure and manage them, see Working with Keystores.</li> <li>To protect sensitive data, see Protecting Sensitive Data via the Secure Vault.</li> <li>To understand how WSO2 Streaming Integrator complies with GDPR(General Data Protection Regulations) and how you can comply with the same when you are using WSO2 Streaming Integrator, see General Data Protection Regulations.</li> </ul>"},{"location":"setup/production-checklist/#opening-the-required-ports","title":"Opening the required ports","text":"<p>This involves configuring the network firewall for opening the ports used by WSO2 Streaming Integrator. For more information about the required ports, see Configuring Default Ports.</p>"},{"location":"setup/production-checklist/#setting-up-databases","title":"Setting up databases","text":"<p>If you are integrating data stores in your Streaming Integration flows, you need to set up databases. For information about supported database types and how to configure data sources for them, see Configuring Datasources.</p>"},{"location":"setup/production-checklist/#configuring-transports","title":"Configuring Transports","text":"<p>In order to use certain transports to receive and send data, you are required to configure them with WSO2 Streaming Integrator. For more information, see Supporting Different Transports.</p>"},{"location":"setup/production-checklist/#minimizing-the-impact-of-system-failure","title":"Minimizing the impact of system failure","text":"<p>In order to minimize the loss of data that can result from a system failure, you can configure WSO2 Streaming Integrator to periodically save its state in a database. For more information, see Configuring Database and File System State Persistence.</p>"},{"location":"setup/production-checklist/#creating-business-rules-templates","title":"Creating Business Rules templates","text":"<p>If you want to allow business users with limited coding knowledge to write business rules, you can template Siddhi applications and queries. For more information, see Working with Business Rules.</p>"},{"location":"setup/production-checklist/#monitoring-the-streaming-integrator","title":"Monitoring the Streaming Integrator","text":"<p>To monitor the performance of your Streaming Integrator setup, configure WSO2 SI to publish its statistics in Dashboards as described in Configuring Grafana Dashboards.</p>"},{"location":"setup/upgrading-from-previous-version/","title":"Upgrading from Streaming Integrator 1.0.0","text":"<p>To upgrade from Streaming Integrator 1.0.0 to Streaming Integrator 1.1.0, follow the steps below:</p> <p>Before you begin:</p> <p>Download Streaming Integrator 1.1.0 version from the Streaming Integrator Page</p>"},{"location":"setup/upgrading-from-previous-version/#step-1-deploy-the-siddhi-applications","title":"Step 1: Deploy the Siddhi applications","text":"<p>To deploy the Siddhi applications you have been running in Streaming Integrator 1.0.0 to Streaming Integrator 1.1.0, follow the procedure below:</p> <ol> <li> <p>Open the <code>&lt;SI 1.0.0_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory. Then copy all the siddhi files in it.</p> </li> <li> <p>Paste all the Siddhi files that you copied in the <code>&lt;SI 1.1.0_HOME&gt;/wso2/server/deployment/siddhi-files</code> directory.</p> </li> </ol> <p>Now your Siddhi applications are deployed in Streaming Integrator 1.1.0.</p>"},{"location":"setup/upgrading-from-previous-version/#step-2-update-configuration-files","title":"Step 2: Update configuration files","text":"<p>To configure Streaming Integrator 1.1.0 the same way as Streaming Integrator 1.0.0, open the <code>&lt;SI 1.0.0_HOME&gt;/conf/server/deployment.yaml</code> file. Then read each line, and update the <code>&lt;SI 1.1.0_HOME&gt;/conf/server/deployment.yaml</code> file with the same values</p> <p>Note</p> <p>The deployment.yaml files must not be copied directly between servers due to certain differences in the parameters included in the two Streaming Integrator versions.</p>"},{"location":"setup/upgrading-from-previous-version/#step-3-start-the-si-110-server-and-install-required-extensions","title":"Step 3: Start the SI 1.1.0 server and install required extensions","text":"<p>The purpose of this step is to Start the Streaming Integrator and identify any further reqirements to run the Siddhi applications that are deployed in it.</p> <ol> <li> <p>Navigate to the <code>&lt;SI 1.1.0_HOME&gt;/bin</code> directory and issue the appropriate command based on your operating system:</p> <ul> <li>For Windows     : <code>server.bat</code></li> <li>For Linux/MacOS :<code>./server.sh</code></li> </ul> <p>If any of the deployed Siddhi applications uses a Siddhi extension that is not currently installed, it is indicated via an error in the start up logs as shown in the example below:</p> <p></p> </li> <li> <p>To install all the missing extensions that are required to run the Siddhi applications currently deployed, navigate to the <code>&lt;SI 1.1.0_HOME&gt;/bin</code> directory and issue the appropriate command based on your operating system:</p> <ul> <li>For Windows     : <code>extension-installer.bat install</code></li> <li>For Linux/MacOS : <code>./extension-installer.sh install</code> </li> </ul> <p>As a result, the following message is logged.</p> <p></p> <p>If you enter <code>y</code> to specify that you want to proceed with the installation, the following message appears to inform you of the status of the installation and to prompt you to restart the WSO2 Streaming Integrator server once the installation is complete.</p> <p></p> <p>Info</p> <p>The Extension Installer tool is currently unable to install a few of the supported Siddhi applications. Therefore, if the start-up logs indicate a missing extension, but you are unable to install that extension via the Extension Installer tool, you can install it manually.We will be upgrading the Extension Installer to handle all the supported Siddhi extensions in a future release.</p> </li> </ol>"},{"location":"setup/upgrading-from-previous-version/#step-4-test-the-migration","title":"Step 4: Test the migration","text":"<p>To test the migration, simulate events for the Siddhi applications you have deployed and verify whether they generate the expected results. For instructions to simulate events, see Testing Siddhi Applications. </p>"},{"location":"setup/upgrading-wso2-ei/","title":"Upgrade WSO2 Enterprise Integrator","text":"<p>This document walks you through the process of upgrading the Streaming Integrator component of WSO2 Enterprise Integrator.</p>"},{"location":"setup/upgrading-wso2-ei/#why-upgrade","title":"Why upgrade?","text":"<p>There are multiple reasons why you would want to upgrade the WSO2 product to the latest version. These reasons include but are not limited to the following.</p> <ul> <li>The current product version you are using is reaching its end of life. To see if this is the case, view the support matrix documentation.</li> <li>You want to leverage the new features of the latest version of the product.</li> <li>The version of the product you have does not have certain security and bug fixes that you require.</li> </ul>"},{"location":"setup/upgrading-wso2-ei/#what-has-changed","title":"What has changed","text":"<p>Over the course of its lifetime, WSO2 Enterprise Integrator has changed significantly and some of the features you were using in an older version of the Streaming Integrator may not work the same way.</p>"},{"location":"setup/upgrading-wso2-ei/#get-started","title":"Get started","text":"<p>To make sure that the upgrade process is smooth and you have the best experience, WSO2 recommends that you reach out to WSO2 Support in order to upgrade the Streaming Integrator component of WSO2 Enterprise Integrator with minimal difficulty.</p> <p>If you are ready to start the migration process, follow the instructions given below.</p> <p>If you already have a WSO2 subscription, create a support ticket with your migration requirements and one of our support engineers will get in touch with you.</p> <pre><code>- [Create a ticket](https://support.wso2.com/support)</code></pre> <p>If you are not a WSO2 customer and still need migration assistance and resources, please contact us through the following link. One of our Account Managers will get in touch with you to help.</p> <pre><code>- [Contact us](https://wso2.com/contact/?ref=migrationsupport)</code></pre>"},{"location":"setup/viewing-dashboards/","title":"Monitoring ETL Flows with Grafana","text":"<p>Before you begin:</p> <p>To enable WSO2 Streaming Integrator to publish statistics in Grafana, follow the instructions in Setting up Grafana to Display WSO2 SI Statistics.</p>"},{"location":"setup/viewing-dashboards/#setting-up-grafana-to-monitor-wso2-streaming-integrator","title":"Setting Up Grafana to Monitor WSO2 Streaming Integrator","text":"<p>For the purpose of monitoring ETL (Extract, Transform, Load) statistics WSO2 provides nine pre-configured dashboards. To view them in Grafana, follow the steps below:</p> <ol> <li> <p>Download the following dashboards (i.e., the JSON file with the dashboard configuration) from here.</p> Directory Dashboard <code>overview-statistics</code> - WSO2 Streaming Integrator - Overall Statistics.json  - WSO2 Streaming Integrator - App Statistics.json <code>file-statistics</code> - WSO2 Streaming Integrator - File Statistics.json  - WSO2 Streaming Integrator - File Source Statistics.json  - WSO2 Streaming Integrator - File Sink Statistics.json <code>rdbms-statistics</code> - WSO2 Streaming Integration - RDBMS Statistics.json  - WSO2 Streaming Integration - RDBMS Table Statistics.json <code>cdc-statistics</code> - WSO2 Streaming Integrator - CDC Statistics.json  - WSO2 Streaming Integrator - CDC Streaming Statistics.json  - WSO2 Streaming Integrator - CDC Scheduled Statistics.json </li> <li> <p>Start and access Grafana. Then import the nine dashboards you downloaded in the previous step. For more information, see Managing Grafana Dashboards - Importing Dashboards.</p> </li> <li> <p>In the Dashboards/Manage tab, create four new folders named <code>overview-statistics</code>, <code>file-statistics</code>, <code>rdbms-statistics</code>, and <code>cdc-statistics</code> . Then organize the dashboards you imported in the new folders you created as shown in the image below. For instructions, see Managing Grafana Dashboards - Organizing Dashboards in Folders.</p> <p>Info</p> <p>The following is a suggested method to organize your dashboards, and the following instructions are provided based on this structure.</p> <p></p> </li> </ol>"},{"location":"setup/viewing-dashboards/#accessing-grafana-dashboards-for-monitoring","title":"Accessing Grafana Dashboards for Monitoring","text":"<p>To navigate through the Grafana dashboards you set up for monitoring WSO2 Streaming Integrator and to analyze statistics, follow the procedure below:</p> <ol> <li> <p>To start and access the dashboards in Grafana, follow the steps below:</p> <ol> <li> <p>Start the Prometheus server by issung the following command from the <code>&lt;PROMETHEUS_HOME&gt;</code> directory.</p> <p><code>./prometheus</code></p> </li> <li> <p>Start Grafana.</p> <p>Info</p> <p>The procedure to start Grafana depends on your operating system and the installation process. e.g., If your operating system is Mac OS and you have installed Grafana via Homebrew, you start Grafana by issuing the <code>brew services start grafana</code> command.</p> </li> <li> <p>Start the WSO2 Streaming Integrator server by navigating to the <code>&lt;SI_HOME&gt;/bin</code>directory, and issuing the appropriate command out of the following based on your operating system.</p> <ul> <li>For Linux: <code>./server.sh</code></li> <li>For Windows: <code>server.bat</code></li> </ul> </li> </ol> <p>Access Grafana via <code>http://localhost:3000/</code>.</p> </li> <li> <p>In the left pane, click the Dashboards icon, and then click Manage to open the Dashboards page.</p> </li> <li> <p>In the Dashboards page, click WSO2 Streaming Integrator Overall Statistics.</p> <p></p> <p>The overall statistics are displayed as shown in the following example.</p> <p></p> <p>This provides an overview on the overall performance of the WSO2 Streaming Integrator instance by displaying the total number of input events consumed and the total number of output events. It also provides the breakdown of the total inputs and outputs by Siddhi application. The (Consume/Publish)/Sec graph indicates the rate at which the consumed input events are published per second.</p> </li> <li> <p>To view the statistics specific to a Siddhi application, click on the relevant Siddhi application.</p> <p></p> <p>The WSO2 Streaming Integrator App Statistics dashboard opens. This allows you to access any statistics relating to change data capture, file processing and data store integration activities carried out via the selected Siddhi application.</p> <p></p> <ul> <li> <p>Viewing CDC statistics</p> <p>CDC related statistics are displayed in the Sources section of the WSO2 Streaming Integrator App Statistics dashboard. To view the detailed statistics for a specific table, click on that table.</p> <p>As a result, the CDC Statistics dashboard opens with change data statistice specific to that table.</p> <p></p> <p>If you click on a table for which change data is captured in a streaming manner (i.e., a table in the CDC Streaming Statistics section), the CDC Streaming Statistics dashboard opens with change data related statistics specific to the selected table as shown in the example below.</p> <p></p> <p>If you click on a table for which change data is captured in a scheduled manner by polling the table (i.e., a table in the CDC Scheduled Statistics section), the CDC Scheduled Statistics dashboard opens with change data related statistics specific to the selected table as shown in the example below.</p> <p></p> </li> <li> <p>Viewing File statistics</p> </li> <li> <p>When the content of a file is used as input data, the file is displayed in the Sources section as shown in the example below.</p> <p></p> <p>To view statistics for a specific file, click on it. As a result, the WSO2 Streaming Integrator - File Statistics dashboard opens as shown in the example below.</p> <p></p> <p>When you click on the same file again, the WSO2 Streaming Integrator - File Source Statistics dashboard opens with more detailed information about this source file as shown in the example below. </p> <p> </p> </li> <li> <p>When the content of a file is generated as an output of the Siddhi application, the file is displayed in the Destinations section as shown in the example below.</p> <p> </p> <p>To view statistics for a specific file, click on it. As a result, the WSO2 Streaming Integrator - File Statistics dashboard opens as shown in the example below.</p> <p></p> <p>When you click on the same file again, the WSO2 Streaming Integrator - File Sink Statistics dashboard opens with more detailed information about this sink file as shown in the example below. </p> <p> </p> </li> <li> <p>Viewing RDBMS statistics</p> <p>When the records of an RDBMS database is used as input data by the selected Siddhi application, the database is displayed in the Sources section. When any output generated by the selected Siddhi application is saved in a database, that database is displayed under Destinations.</p> <p>In each section, all the databases of the relevant category is listed in a table as shown in the example below.</p> <p></p> <p>To view detailed statistics of a specific database, click on that database.</p> <p></p> <p>The RDBMS Statistics dashboard opens with RDBMS statistics specific to the selected database.</p> <p></p> <p>To view statistics relating to a specific table, click on the relevant table name in the RDBMS Table table.</p> <p></p> <p>The RDBMS Table Statistics dashboard opens with statistics specific to the selected table.</p> <p></p> </li> </ul> </li> </ol>"}]}